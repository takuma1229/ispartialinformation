
/content/drive/MyDrive/research_discourse_relation/discourse_relation_explainability
/usr/local/lib/python3.10/dist-packages/huggingface_hub/utils/_token.py:89: UserWarning:
The secret `HF_TOKEN` does not exist in your Colab secrets.
To authenticate with the Hugging Face Hub, create a token in your settings tab (https://huggingface.co/settings/tokens), set it as secret in your Google Colab and restart your session.
You will be able to reuse this secret in all of your notebooks.
Please note that authentication is recommended but still optional to access public models or datasets.
  warnings.warn(
num_added_tokens: 16, added_special_tokens: <NOUN>,<PRONOUN>,<ADJECTIVAL-NOUN>,<PRENOUN-ADJECTIVAL>,<ADVERB>,<CONJUNCTION>,<INTERJECTION>,<VERB>,<ADJECTIVE>,<AUXILIARY-VERB>,<PARTICLE>,<PREFIX>,<SUFFIX>,<SYMBOL>,<AUXILIARY-SYMBOL>,<BLANK>
Some weights of BertForSequenceClassification were not initialized from the model checkpoint at cl-tohoku/bert-base-japanese-v3 and are newly initialized: ['classifier.bias', 'classifier.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
  0%|          | 0/11 [00:00<?, ?it/s]W0629 11:52:20.070000 139012379337344 torch/_dynamo/convert_frame.py:824] WON'T CONVERT forward /usr/local/lib/python3.10/dist-packages/transformers/models/bert/modeling_bert.py line 1662
W0629 11:52:20.070000 139012379337344 torch/_dynamo/convert_frame.py:824] due to:
W0629 11:52:20.070000 139012379337344 torch/_dynamo/convert_frame.py:824] Traceback (most recent call last):
W0629 11:52:20.070000 139012379337344 torch/_dynamo/convert_frame.py:824]   File "/usr/local/lib/python3.10/dist-packages/torch/_dynamo/convert_frame.py", line 786, in _convert_frame
W0629 11:52:20.070000 139012379337344 torch/_dynamo/convert_frame.py:824]     result = inner_convert(
W0629 11:52:20.070000 139012379337344 torch/_dynamo/convert_frame.py:824]   File "/usr/local/lib/python3.10/dist-packages/torch/_dynamo/convert_frame.py", line 400, in _convert_frame_assert
W0629 11:52:20.070000 139012379337344 torch/_dynamo/convert_frame.py:824]     return _compile(
W0629 11:52:20.070000 139012379337344 torch/_dynamo/convert_frame.py:824]   File "/usr/lib/python3.10/contextlib.py", line 79, in inner
W0629 11:52:20.070000 139012379337344 torch/_dynamo/convert_frame.py:824]     return func(*args, **kwds)
W0629 11:52:20.070000 139012379337344 torch/_dynamo/convert_frame.py:824]   File "/usr/local/lib/python3.10/dist-packages/torch/_dynamo/convert_frame.py", line 676, in _compile
W0629 11:52:20.070000 139012379337344 torch/_dynamo/convert_frame.py:824]     guarded_code = compile_inner(code, one_graph, hooks, transform)
W0629 11:52:20.070000 139012379337344 torch/_dynamo/convert_frame.py:824]   File "/usr/local/lib/python3.10/dist-packages/torch/_dynamo/utils.py", line 262, in time_wrapper
W0629 11:52:20.070000 139012379337344 torch/_dynamo/convert_frame.py:824]     r = func(*args, **kwargs)
W0629 11:52:20.070000 139012379337344 torch/_dynamo/convert_frame.py:824]   File "/usr/local/lib/python3.10/dist-packages/torch/_dynamo/convert_frame.py", line 535, in compile_inner
W0629 11:52:20.070000 139012379337344 torch/_dynamo/convert_frame.py:824]     out_code = transform_code_object(code, transform)
W0629 11:52:20.070000 139012379337344 torch/_dynamo/convert_frame.py:824]   File "/usr/local/lib/python3.10/dist-packages/torch/_dynamo/bytecode_transformation.py", line 1036, in transform_code_object
W0629 11:52:20.070000 139012379337344 torch/_dynamo/convert_frame.py:824]     transformations(instructions, code_options)
W0629 11:52:20.070000 139012379337344 torch/_dynamo/convert_frame.py:824]   File "/usr/local/lib/python3.10/dist-packages/torch/_dynamo/convert_frame.py", line 165, in _fn
W0629 11:52:20.070000 139012379337344 torch/_dynamo/convert_frame.py:824]     return fn(*args, **kwargs)
W0629 11:52:20.070000 139012379337344 torch/_dynamo/convert_frame.py:824]   File "/usr/local/lib/python3.10/dist-packages/torch/_dynamo/convert_frame.py", line 500, in transform
W0629 11:52:20.070000 139012379337344 torch/_dynamo/convert_frame.py:824]     tracer.run()
W0629 11:52:20.070000 139012379337344 torch/_dynamo/convert_frame.py:824]   File "/usr/local/lib/python3.10/dist-packages/torch/_dynamo/symbolic_convert.py", line 2149, in run
W0629 11:52:20.070000 139012379337344 torch/_dynamo/convert_frame.py:824]     super().run()
W0629 11:52:20.070000 139012379337344 torch/_dynamo/convert_frame.py:824]   File "/usr/local/lib/python3.10/dist-packages/torch/_dynamo/symbolic_convert.py", line 810, in run
W0629 11:52:20.070000 139012379337344 torch/_dynamo/convert_frame.py:824]     and self.step()
W0629 11:52:20.070000 139012379337344 torch/_dynamo/convert_frame.py:824]   File "/usr/local/lib/python3.10/dist-packages/torch/_dynamo/symbolic_convert.py", line 773, in step
W0629 11:52:20.070000 139012379337344 torch/_dynamo/convert_frame.py:824]     getattr(self, inst.opname)(inst)
W0629 11:52:20.070000 139012379337344 torch/_dynamo/convert_frame.py:824]   File "/usr/local/lib/python3.10/dist-packages/torch/_dynamo/symbolic_convert.py", line 1319, in STORE_ATTR
W0629 11:52:20.070000 139012379337344 torch/_dynamo/convert_frame.py:824]     return self.store_attr_graph_break(inst)
W0629 11:52:20.070000 139012379337344 torch/_dynamo/convert_frame.py:824]   File "/usr/local/lib/python3.10/dist-packages/torch/_dynamo/symbolic_convert.py", line 1343, in store_attr_graph_break
W0629 11:52:20.070000 139012379337344 torch/_dynamo/convert_frame.py:824]     self.output.compile_subgraph(
W0629 11:52:20.070000 139012379337344 torch/_dynamo/convert_frame.py:824]   File "/usr/local/lib/python3.10/dist-packages/torch/_dynamo/output_graph.py", line 991, in compile_subgraph
W0629 11:52:20.070000 139012379337344 torch/_dynamo/convert_frame.py:824]     self.compile_and_call_fx_graph(tx, pass2.graph_output_vars(), root)
W0629 11:52:20.070000 139012379337344 torch/_dynamo/convert_frame.py:824]   File "/usr/lib/python3.10/contextlib.py", line 79, in inner
W0629 11:52:20.070000 139012379337344 torch/_dynamo/convert_frame.py:824]     return func(*args, **kwds)
W0629 11:52:20.070000 139012379337344 torch/_dynamo/convert_frame.py:824]   File "/usr/local/lib/python3.10/dist-packages/torch/_dynamo/output_graph.py", line 1168, in compile_and_call_fx_graph
W0629 11:52:20.070000 139012379337344 torch/_dynamo/convert_frame.py:824]     compiled_fn = self.call_user_compiler(gm)
W0629 11:52:20.070000 139012379337344 torch/_dynamo/convert_frame.py:824]   File "/usr/local/lib/python3.10/dist-packages/torch/_dynamo/utils.py", line 262, in time_wrapper
W0629 11:52:20.070000 139012379337344 torch/_dynamo/convert_frame.py:824]     r = func(*args, **kwargs)
W0629 11:52:20.070000 139012379337344 torch/_dynamo/convert_frame.py:824]   File "/usr/local/lib/python3.10/dist-packages/torch/_dynamo/output_graph.py", line 1241, in call_user_compiler
W0629 11:52:20.070000 139012379337344 torch/_dynamo/convert_frame.py:824]     raise BackendCompilerFailed(self.compiler_fn, e).with_traceback(
W0629 11:52:20.070000 139012379337344 torch/_dynamo/convert_frame.py:824]   File "/usr/local/lib/python3.10/dist-packages/torch/_dynamo/output_graph.py", line 1222, in call_user_compiler
W0629 11:52:20.070000 139012379337344 torch/_dynamo/convert_frame.py:824]     compiled_fn = compiler_fn(gm, self.example_inputs())
W0629 11:52:20.070000 139012379337344 torch/_dynamo/convert_frame.py:824]   File "/usr/local/lib/python3.10/dist-packages/torch/_dynamo/repro/after_dynamo.py", line 117, in debug_wrapper
W0629 11:52:20.070000 139012379337344 torch/_dynamo/convert_frame.py:824]     compiled_gm = compiler_fn(gm, example_inputs)
W0629 11:52:20.070000 139012379337344 torch/_dynamo/convert_frame.py:824]   File "/usr/local/lib/python3.10/dist-packages/torch/__init__.py", line 1729, in __call__
W0629 11:52:20.070000 139012379337344 torch/_dynamo/convert_frame.py:824]     return compile_fx(model_, inputs_, config_patches=self.config)
W0629 11:52:20.070000 139012379337344 torch/_dynamo/convert_frame.py:824]   File "/usr/lib/python3.10/contextlib.py", line 79, in inner
W0629 11:52:20.070000 139012379337344 torch/_dynamo/convert_frame.py:824]     return func(*args, **kwds)
W0629 11:52:20.070000 139012379337344 torch/_dynamo/convert_frame.py:824]   File "/usr/local/lib/python3.10/dist-packages/torch/_inductor/compile_fx.py", line 1330, in compile_fx
W0629 11:52:20.070000 139012379337344 torch/_dynamo/convert_frame.py:824]     return aot_autograd(
W0629 11:52:20.070000 139012379337344 torch/_dynamo/convert_frame.py:824]   File "/usr/local/lib/python3.10/dist-packages/torch/_dynamo/backends/common.py", line 58, in compiler_fn
W0629 11:52:20.070000 139012379337344 torch/_dynamo/convert_frame.py:824]     cg = aot_module_simplified(gm, example_inputs, **kwargs)
W0629 11:52:20.070000 139012379337344 torch/_dynamo/convert_frame.py:824]   File "/usr/local/lib/python3.10/dist-packages/torch/_functorch/aot_autograd.py", line 903, in aot_module_simplified
W0629 11:52:20.070000 139012379337344 torch/_dynamo/convert_frame.py:824]     compiled_fn = create_aot_dispatcher_function(
W0629 11:52:20.070000 139012379337344 torch/_dynamo/convert_frame.py:824]   File "/usr/local/lib/python3.10/dist-packages/torch/_dynamo/utils.py", line 262, in time_wrapper
W0629 11:52:20.070000 139012379337344 torch/_dynamo/convert_frame.py:824]     r = func(*args, **kwargs)
W0629 11:52:20.070000 139012379337344 torch/_dynamo/convert_frame.py:824]   File "/usr/local/lib/python3.10/dist-packages/torch/_functorch/aot_autograd.py", line 628, in create_aot_dispatcher_function
W0629 11:52:20.070000 139012379337344 torch/_dynamo/convert_frame.py:824]     compiled_fn = compiler_fn(flat_fn, fake_flat_args, aot_config, fw_metadata=fw_metadata)
W0629 11:52:20.070000 139012379337344 torch/_dynamo/convert_frame.py:824]   File "/usr/local/lib/python3.10/dist-packages/torch/_functorch/_aot_autograd/runtime_wrappers.py", line 443, in aot_wrapper_dedupe
W0629 11:52:20.070000 139012379337344 torch/_dynamo/convert_frame.py:824]     return compiler_fn(flat_fn, leaf_flat_args, aot_config, fw_metadata=fw_metadata)
W0629 11:52:20.070000 139012379337344 torch/_dynamo/convert_frame.py:824]   File "/usr/local/lib/python3.10/dist-packages/torch/_functorch/_aot_autograd/runtime_wrappers.py", line 648, in aot_wrapper_synthetic_base
W0629 11:52:20.070000 139012379337344 torch/_dynamo/convert_frame.py:824]     return compiler_fn(flat_fn, flat_args, aot_config, fw_metadata=fw_metadata)
W0629 11:52:20.070000 139012379337344 torch/_dynamo/convert_frame.py:824]   File "/usr/local/lib/python3.10/dist-packages/torch/_functorch/_aot_autograd/jit_compile_runtime_wrappers.py", line 119, in aot_dispatch_base
W0629 11:52:20.070000 139012379337344 torch/_dynamo/convert_frame.py:824]     compiled_fw = compiler(fw_module, updated_flat_args)
W0629 11:52:20.070000 139012379337344 torch/_dynamo/convert_frame.py:824]   File "/usr/local/lib/python3.10/dist-packages/torch/_dynamo/utils.py", line 262, in time_wrapper
W0629 11:52:20.070000 139012379337344 torch/_dynamo/convert_frame.py:824]     r = func(*args, **kwargs)
W0629 11:52:20.070000 139012379337344 torch/_dynamo/convert_frame.py:824]   File "/usr/local/lib/python3.10/dist-packages/torch/_inductor/compile_fx.py", line 1257, in fw_compiler_base
W0629 11:52:20.070000 139012379337344 torch/_dynamo/convert_frame.py:824]     return inner_compile(
W0629 11:52:20.070000 139012379337344 torch/_dynamo/convert_frame.py:824]   File "/usr/local/lib/python3.10/dist-packages/torch/_dynamo/repro/after_aot.py", line 83, in debug_wrapper
W0629 11:52:20.070000 139012379337344 torch/_dynamo/convert_frame.py:824]     inner_compiled_fn = compiler_fn(gm, example_inputs)
W0629 11:52:20.070000 139012379337344 torch/_dynamo/convert_frame.py:824]   File "/usr/local/lib/python3.10/dist-packages/torch/_inductor/debug.py", line 304, in inner
W0629 11:52:20.070000 139012379337344 torch/_dynamo/convert_frame.py:824]     return fn(*args, **kwargs)
W0629 11:52:20.070000 139012379337344 torch/_dynamo/convert_frame.py:824]   File "/usr/lib/python3.10/contextlib.py", line 79, in inner
W0629 11:52:20.070000 139012379337344 torch/_dynamo/convert_frame.py:824]     return func(*args, **kwds)
W0629 11:52:20.070000 139012379337344 torch/_dynamo/convert_frame.py:824]   File "/usr/lib/python3.10/contextlib.py", line 79, in inner
W0629 11:52:20.070000 139012379337344 torch/_dynamo/convert_frame.py:824]     return func(*args, **kwds)
W0629 11:52:20.070000 139012379337344 torch/_dynamo/convert_frame.py:824]   File "/usr/local/lib/python3.10/dist-packages/torch/_dynamo/utils.py", line 262, in time_wrapper
W0629 11:52:20.070000 139012379337344 torch/_dynamo/convert_frame.py:824]     r = func(*args, **kwargs)
W0629 11:52:20.070000 139012379337344 torch/_dynamo/convert_frame.py:824]   File "/usr/local/lib/python3.10/dist-packages/torch/_inductor/compile_fx.py", line 438, in compile_fx_inner
W0629 11:52:20.070000 139012379337344 torch/_dynamo/convert_frame.py:824]     compiled_graph = fx_codegen_and_compile(
W0629 11:52:20.070000 139012379337344 torch/_dynamo/convert_frame.py:824]   File "/usr/local/lib/python3.10/dist-packages/torch/_inductor/compile_fx.py", line 714, in fx_codegen_and_compile
W0629 11:52:20.070000 139012379337344 torch/_dynamo/convert_frame.py:824]     compiled_fn = graph.compile_to_fn()
W0629 11:52:20.070000 139012379337344 torch/_dynamo/convert_frame.py:824]   File "/usr/local/lib/python3.10/dist-packages/torch/_inductor/graph.py", line 1307, in compile_to_fn
W0629 11:52:20.070000 139012379337344 torch/_dynamo/convert_frame.py:824]     return self.compile_to_module().call
W0629 11:52:20.070000 139012379337344 torch/_dynamo/convert_frame.py:824]   File "/usr/local/lib/python3.10/dist-packages/torch/_dynamo/utils.py", line 262, in time_wrapper
W0629 11:52:20.070000 139012379337344 torch/_dynamo/convert_frame.py:824]     r = func(*args, **kwargs)
W0629 11:52:20.070000 139012379337344 torch/_dynamo/convert_frame.py:824]   File "/usr/local/lib/python3.10/dist-packages/torch/_inductor/graph.py", line 1254, in compile_to_module
W0629 11:52:20.070000 139012379337344 torch/_dynamo/convert_frame.py:824]     mod = PyCodeCache.load_by_key_path(
W0629 11:52:20.070000 139012379337344 torch/_dynamo/convert_frame.py:824]   File "/usr/local/lib/python3.10/dist-packages/torch/_inductor/codecache.py", line 2160, in load_by_key_path
W0629 11:52:20.070000 139012379337344 torch/_dynamo/convert_frame.py:824]     exec(code, mod.__dict__, mod.__dict__)
W0629 11:52:20.070000 139012379337344 torch/_dynamo/convert_frame.py:824]   File "/tmp/torchinductor_root/z2/cz2bh22epnbh7mqrt23ywsmbvfsueq2kauftzoodvlbbtqvgbxb5.py", line 875, in <module>
W0629 11:52:20.070000 139012379337344 torch/_dynamo/convert_frame.py:824]     async_compile.wait(globals())
W0629 11:52:20.070000 139012379337344 torch/_dynamo/convert_frame.py:824]   File "/usr/local/lib/python3.10/dist-packages/torch/_inductor/codecache.py", line 2715, in wait
W0629 11:52:20.070000 139012379337344 torch/_dynamo/convert_frame.py:824]     scope[key] = result.result()
W0629 11:52:20.070000 139012379337344 torch/_dynamo/convert_frame.py:824]   File "/usr/local/lib/python3.10/dist-packages/torch/_inductor/codecache.py", line 2522, in result
W0629 11:52:20.070000 139012379337344 torch/_dynamo/convert_frame.py:824]     self.future.result()
W0629 11:52:20.070000 139012379337344 torch/_dynamo/convert_frame.py:824]   File "/usr/lib/python3.10/concurrent/futures/_base.py", line 458, in result
W0629 11:52:20.070000 139012379337344 torch/_dynamo/convert_frame.py:824]     return self.__get_result()
W0629 11:52:20.070000 139012379337344 torch/_dynamo/convert_frame.py:824]   File "/usr/lib/python3.10/concurrent/futures/_base.py", line 403, in __get_result
W0629 11:52:20.070000 139012379337344 torch/_dynamo/convert_frame.py:824]     raise self._exception
W0629 11:52:20.070000 139012379337344 torch/_dynamo/convert_frame.py:824] torch._dynamo.exc.BackendCompilerFailed: backend='inductor' raised:
W0629 11:52:20.070000 139012379337344 torch/_dynamo/convert_frame.py:824] RuntimeError: Internal Triton PTX codegen error:
W0629 11:52:20.070000 139012379337344 torch/_dynamo/convert_frame.py:824] ptxas /tmp/compile-ptx-src-1c79c8, line 619; error   : Feature '.bf16' requires .target sm_80 or higher
W0629 11:52:20.070000 139012379337344 torch/_dynamo/convert_frame.py:824] ptxas /tmp/compile-ptx-src-1c79c8, line 619; error   : Feature 'cvt.bf16.f32' requires .target sm_80 or higher
W0629 11:52:20.070000 139012379337344 torch/_dynamo/convert_frame.py:824] ptxas /tmp/compile-ptx-src-1c79c8, line 620; error   : Feature '.bf16' requires .target sm_80 or higher
W0629 11:52:20.070000 139012379337344 torch/_dynamo/convert_frame.py:824] ptxas /tmp/compile-ptx-src-1c79c8, line 620; error   : Feature 'cvt.bf16.f32' requires .target sm_80 or higher
W0629 11:52:20.070000 139012379337344 torch/_dynamo/convert_frame.py:824] ptxas /tmp/compile-ptx-src-1c79c8, line 621; error   : Feature '.bf16' requires .target sm_80 or higher
W0629 11:52:20.070000 139012379337344 torch/_dynamo/convert_frame.py:824] ptxas /tmp/compile-ptx-src-1c79c8, line 621; error   : Feature 'cvt.bf16.f32' requires .target sm_80 or higher
W0629 11:52:20.070000 139012379337344 torch/_dynamo/convert_frame.py:824] ptxas /tmp/compile-ptx-src-1c79c8, line 622; error   : Feature '.bf16' requires .target sm_80 or higher
W0629 11:52:20.070000 139012379337344 torch/_dynamo/convert_frame.py:824] ptxas /tmp/compile-ptx-src-1c79c8, line 622; error   : Feature 'cvt.bf16.f32' requires .target sm_80 or higher
W0629 11:52:20.070000 139012379337344 torch/_dynamo/convert_frame.py:824] ptxas fatal   : Ptx assembly aborted due to errors
W0629 11:52:20.070000 139012379337344 torch/_dynamo/convert_frame.py:824]
W0629 11:52:20.070000 139012379337344 torch/_dynamo/convert_frame.py:824]
W0629 11:52:20.070000 139012379337344 torch/_dynamo/convert_frame.py:824] Set TORCH_LOGS="+dynamo" and TORCHDYNAMO_VERBOSE=1 for more information
W0629 11:52:20.070000 139012379337344 torch/_dynamo/convert_frame.py:824]
W0629 11:52:20.070000 139012379337344 torch/_dynamo/convert_frame.py:824] Traceback (most recent call last):
W0629 11:52:20.070000 139012379337344 torch/_dynamo/convert_frame.py:824]   File "/usr/local/lib/python3.10/dist-packages/torch/_dynamo/convert_frame.py", line 786, in _convert_frame
W0629 11:52:20.070000 139012379337344 torch/_dynamo/convert_frame.py:824]     result = inner_convert(
W0629 11:52:20.070000 139012379337344 torch/_dynamo/convert_frame.py:824]   File "/usr/local/lib/python3.10/dist-packages/torch/_dynamo/convert_frame.py", line 400, in _convert_frame_assert
W0629 11:52:20.070000 139012379337344 torch/_dynamo/convert_frame.py:824]     return _compile(
W0629 11:52:20.070000 139012379337344 torch/_dynamo/convert_frame.py:824]   File "/usr/lib/python3.10/contextlib.py", line 79, in inner
W0629 11:52:20.070000 139012379337344 torch/_dynamo/convert_frame.py:824]     return func(*args, **kwds)
W0629 11:52:20.070000 139012379337344 torch/_dynamo/convert_frame.py:824]   File "/usr/local/lib/python3.10/dist-packages/torch/_dynamo/convert_frame.py", line 676, in _compile
W0629 11:52:20.070000 139012379337344 torch/_dynamo/convert_frame.py:824]     guarded_code = compile_inner(code, one_graph, hooks, transform)
W0629 11:52:20.070000 139012379337344 torch/_dynamo/convert_frame.py:824]   File "/usr/local/lib/python3.10/dist-packages/torch/_dynamo/utils.py", line 262, in time_wrapper
W0629 11:52:20.070000 139012379337344 torch/_dynamo/convert_frame.py:824]     r = func(*args, **kwargs)
W0629 11:52:20.070000 139012379337344 torch/_dynamo/convert_frame.py:824]   File "/usr/local/lib/python3.10/dist-packages/torch/_dynamo/convert_frame.py", line 535, in compile_inner
W0629 11:52:20.070000 139012379337344 torch/_dynamo/convert_frame.py:824]     out_code = transform_code_object(code, transform)
W0629 11:52:20.070000 139012379337344 torch/_dynamo/convert_frame.py:824]   File "/usr/local/lib/python3.10/dist-packages/torch/_dynamo/bytecode_transformation.py", line 1036, in transform_code_object
W0629 11:52:20.070000 139012379337344 torch/_dynamo/convert_frame.py:824]     transformations(instructions, code_options)
W0629 11:52:20.070000 139012379337344 torch/_dynamo/convert_frame.py:824]   File "/usr/local/lib/python3.10/dist-packages/torch/_dynamo/convert_frame.py", line 165, in _fn
W0629 11:52:20.070000 139012379337344 torch/_dynamo/convert_frame.py:824]     return fn(*args, **kwargs)
W0629 11:52:20.070000 139012379337344 torch/_dynamo/convert_frame.py:824]   File "/usr/local/lib/python3.10/dist-packages/torch/_dynamo/convert_frame.py", line 500, in transform
W0629 11:52:20.070000 139012379337344 torch/_dynamo/convert_frame.py:824]     tracer.run()
W0629 11:52:20.070000 139012379337344 torch/_dynamo/convert_frame.py:824]   File "/usr/local/lib/python3.10/dist-packages/torch/_dynamo/symbolic_convert.py", line 2149, in run
W0629 11:52:20.070000 139012379337344 torch/_dynamo/convert_frame.py:824]     super().run()
W0629 11:52:20.070000 139012379337344 torch/_dynamo/convert_frame.py:824]   File "/usr/local/lib/python3.10/dist-packages/torch/_dynamo/symbolic_convert.py", line 810, in run
W0629 11:52:20.070000 139012379337344 torch/_dynamo/convert_frame.py:824]     and self.step()
W0629 11:52:20.070000 139012379337344 torch/_dynamo/convert_frame.py:824]   File "/usr/local/lib/python3.10/dist-packages/torch/_dynamo/symbolic_convert.py", line 773, in step
W0629 11:52:20.070000 139012379337344 torch/_dynamo/convert_frame.py:824]     getattr(self, inst.opname)(inst)
W0629 11:52:20.070000 139012379337344 torch/_dynamo/convert_frame.py:824]   File "/usr/local/lib/python3.10/dist-packages/torch/_dynamo/symbolic_convert.py", line 1319, in STORE_ATTR
W0629 11:52:20.070000 139012379337344 torch/_dynamo/convert_frame.py:824]     return self.store_attr_graph_break(inst)
W0629 11:52:20.070000 139012379337344 torch/_dynamo/convert_frame.py:824]   File "/usr/local/lib/python3.10/dist-packages/torch/_dynamo/symbolic_convert.py", line 1343, in store_attr_graph_break
W0629 11:52:20.070000 139012379337344 torch/_dynamo/convert_frame.py:824]     self.output.compile_subgraph(
W0629 11:52:20.070000 139012379337344 torch/_dynamo/convert_frame.py:824]   File "/usr/local/lib/python3.10/dist-packages/torch/_dynamo/output_graph.py", line 991, in compile_subgraph
W0629 11:52:20.070000 139012379337344 torch/_dynamo/convert_frame.py:824]     self.compile_and_call_fx_graph(tx, pass2.graph_output_vars(), root)
W0629 11:52:20.070000 139012379337344 torch/_dynamo/convert_frame.py:824]   File "/usr/lib/python3.10/contextlib.py", line 79, in inner
W0629 11:52:20.070000 139012379337344 torch/_dynamo/convert_frame.py:824]     return func(*args, **kwds)
W0629 11:52:20.070000 139012379337344 torch/_dynamo/convert_frame.py:824]   File "/usr/local/lib/python3.10/dist-packages/torch/_dynamo/output_graph.py", line 1168, in compile_and_call_fx_graph
W0629 11:52:20.070000 139012379337344 torch/_dynamo/convert_frame.py:824]     compiled_fn = self.call_user_compiler(gm)
W0629 11:52:20.070000 139012379337344 torch/_dynamo/convert_frame.py:824]   File "/usr/local/lib/python3.10/dist-packages/torch/_dynamo/utils.py", line 262, in time_wrapper
W0629 11:52:20.070000 139012379337344 torch/_dynamo/convert_frame.py:824]     r = func(*args, **kwargs)
W0629 11:52:20.070000 139012379337344 torch/_dynamo/convert_frame.py:824]   File "/usr/local/lib/python3.10/dist-packages/torch/_dynamo/output_graph.py", line 1241, in call_user_compiler
W0629 11:52:20.070000 139012379337344 torch/_dynamo/convert_frame.py:824]     raise BackendCompilerFailed(self.compiler_fn, e).with_traceback(
W0629 11:52:20.070000 139012379337344 torch/_dynamo/convert_frame.py:824]   File "/usr/local/lib/python3.10/dist-packages/torch/_dynamo/output_graph.py", line 1222, in call_user_compiler
W0629 11:52:20.070000 139012379337344 torch/_dynamo/convert_frame.py:824]     compiled_fn = compiler_fn(gm, self.example_inputs())
W0629 11:52:20.070000 139012379337344 torch/_dynamo/convert_frame.py:824]   File "/usr/local/lib/python3.10/dist-packages/torch/_dynamo/repro/after_dynamo.py", line 117, in debug_wrapper
W0629 11:52:20.070000 139012379337344 torch/_dynamo/convert_frame.py:824]     compiled_gm = compiler_fn(gm, example_inputs)
W0629 11:52:20.070000 139012379337344 torch/_dynamo/convert_frame.py:824]   File "/usr/local/lib/python3.10/dist-packages/torch/__init__.py", line 1729, in __call__
W0629 11:52:20.070000 139012379337344 torch/_dynamo/convert_frame.py:824]     return compile_fx(model_, inputs_, config_patches=self.config)
W0629 11:52:20.070000 139012379337344 torch/_dynamo/convert_frame.py:824]   File "/usr/lib/python3.10/contextlib.py", line 79, in inner
W0629 11:52:20.070000 139012379337344 torch/_dynamo/convert_frame.py:824]     return func(*args, **kwds)
W0629 11:52:20.070000 139012379337344 torch/_dynamo/convert_frame.py:824]   File "/usr/local/lib/python3.10/dist-packages/torch/_inductor/compile_fx.py", line 1330, in compile_fx
W0629 11:52:20.070000 139012379337344 torch/_dynamo/convert_frame.py:824]     return aot_autograd(
W0629 11:52:20.070000 139012379337344 torch/_dynamo/convert_frame.py:824]   File "/usr/local/lib/python3.10/dist-packages/torch/_dynamo/backends/common.py", line 58, in compiler_fn
W0629 11:52:20.070000 139012379337344 torch/_dynamo/convert_frame.py:824]     cg = aot_module_simplified(gm, example_inputs, **kwargs)
W0629 11:52:20.070000 139012379337344 torch/_dynamo/convert_frame.py:824]   File "/usr/local/lib/python3.10/dist-packages/torch/_functorch/aot_autograd.py", line 903, in aot_module_simplified
W0629 11:52:20.070000 139012379337344 torch/_dynamo/convert_frame.py:824]     compiled_fn = create_aot_dispatcher_function(
W0629 11:52:20.070000 139012379337344 torch/_dynamo/convert_frame.py:824]   File "/usr/local/lib/python3.10/dist-packages/torch/_dynamo/utils.py", line 262, in time_wrapper
W0629 11:52:20.070000 139012379337344 torch/_dynamo/convert_frame.py:824]     r = func(*args, **kwargs)
W0629 11:52:20.070000 139012379337344 torch/_dynamo/convert_frame.py:824]   File "/usr/local/lib/python3.10/dist-packages/torch/_functorch/aot_autograd.py", line 628, in create_aot_dispatcher_function
W0629 11:52:20.070000 139012379337344 torch/_dynamo/convert_frame.py:824]     compiled_fn = compiler_fn(flat_fn, fake_flat_args, aot_config, fw_metadata=fw_metadata)
W0629 11:52:20.070000 139012379337344 torch/_dynamo/convert_frame.py:824]   File "/usr/local/lib/python3.10/dist-packages/torch/_functorch/_aot_autograd/runtime_wrappers.py", line 443, in aot_wrapper_dedupe
W0629 11:52:20.070000 139012379337344 torch/_dynamo/convert_frame.py:824]     return compiler_fn(flat_fn, leaf_flat_args, aot_config, fw_metadata=fw_metadata)
W0629 11:52:20.070000 139012379337344 torch/_dynamo/convert_frame.py:824]   File "/usr/local/lib/python3.10/dist-packages/torch/_functorch/_aot_autograd/runtime_wrappers.py", line 648, in aot_wrapper_synthetic_base
W0629 11:52:20.070000 139012379337344 torch/_dynamo/convert_frame.py:824]     return compiler_fn(flat_fn, flat_args, aot_config, fw_metadata=fw_metadata)
W0629 11:52:20.070000 139012379337344 torch/_dynamo/convert_frame.py:824]   File "/usr/local/lib/python3.10/dist-packages/torch/_functorch/_aot_autograd/jit_compile_runtime_wrappers.py", line 119, in aot_dispatch_base
W0629 11:52:20.070000 139012379337344 torch/_dynamo/convert_frame.py:824]     compiled_fw = compiler(fw_module, updated_flat_args)
W0629 11:52:20.070000 139012379337344 torch/_dynamo/convert_frame.py:824]   File "/usr/local/lib/python3.10/dist-packages/torch/_dynamo/utils.py", line 262, in time_wrapper
W0629 11:52:20.070000 139012379337344 torch/_dynamo/convert_frame.py:824]     r = func(*args, **kwargs)
W0629 11:52:20.070000 139012379337344 torch/_dynamo/convert_frame.py:824]   File "/usr/local/lib/python3.10/dist-packages/torch/_inductor/compile_fx.py", line 1257, in fw_compiler_base
W0629 11:52:20.070000 139012379337344 torch/_dynamo/convert_frame.py:824]     return inner_compile(
W0629 11:52:20.070000 139012379337344 torch/_dynamo/convert_frame.py:824]   File "/usr/local/lib/python3.10/dist-packages/torch/_dynamo/repro/after_aot.py", line 83, in debug_wrapper
W0629 11:52:20.070000 139012379337344 torch/_dynamo/convert_frame.py:824]     inner_compiled_fn = compiler_fn(gm, example_inputs)
W0629 11:52:20.070000 139012379337344 torch/_dynamo/convert_frame.py:824]   File "/usr/local/lib/python3.10/dist-packages/torch/_inductor/debug.py", line 304, in inner
W0629 11:52:20.070000 139012379337344 torch/_dynamo/convert_frame.py:824]     return fn(*args, **kwargs)
W0629 11:52:20.070000 139012379337344 torch/_dynamo/convert_frame.py:824]   File "/usr/lib/python3.10/contextlib.py", line 79, in inner
W0629 11:52:20.070000 139012379337344 torch/_dynamo/convert_frame.py:824]     return func(*args, **kwds)
W0629 11:52:20.070000 139012379337344 torch/_dynamo/convert_frame.py:824]   File "/usr/lib/python3.10/contextlib.py", line 79, in inner
W0629 11:52:20.070000 139012379337344 torch/_dynamo/convert_frame.py:824]     return func(*args, **kwds)
W0629 11:52:20.070000 139012379337344 torch/_dynamo/convert_frame.py:824]   File "/usr/local/lib/python3.10/dist-packages/torch/_dynamo/utils.py", line 262, in time_wrapper
W0629 11:52:20.070000 139012379337344 torch/_dynamo/convert_frame.py:824]     r = func(*args, **kwargs)
W0629 11:52:20.070000 139012379337344 torch/_dynamo/convert_frame.py:824]   File "/usr/local/lib/python3.10/dist-packages/torch/_inductor/compile_fx.py", line 438, in compile_fx_inner
W0629 11:52:20.070000 139012379337344 torch/_dynamo/convert_frame.py:824]     compiled_graph = fx_codegen_and_compile(
W0629 11:52:20.070000 139012379337344 torch/_dynamo/convert_frame.py:824]   File "/usr/local/lib/python3.10/dist-packages/torch/_inductor/compile_fx.py", line 714, in fx_codegen_and_compile
W0629 11:52:20.070000 139012379337344 torch/_dynamo/convert_frame.py:824]     compiled_fn = graph.compile_to_fn()
W0629 11:52:20.070000 139012379337344 torch/_dynamo/convert_frame.py:824]   File "/usr/local/lib/python3.10/dist-packages/torch/_inductor/graph.py", line 1307, in compile_to_fn
W0629 11:52:20.070000 139012379337344 torch/_dynamo/convert_frame.py:824]     return self.compile_to_module().call
W0629 11:52:20.070000 139012379337344 torch/_dynamo/convert_frame.py:824]   File "/usr/local/lib/python3.10/dist-packages/torch/_dynamo/utils.py", line 262, in time_wrapper
W0629 11:52:20.070000 139012379337344 torch/_dynamo/convert_frame.py:824]     r = func(*args, **kwargs)
W0629 11:52:20.070000 139012379337344 torch/_dynamo/convert_frame.py:824]   File "/usr/local/lib/python3.10/dist-packages/torch/_inductor/graph.py", line 1254, in compile_to_module
W0629 11:52:20.070000 139012379337344 torch/_dynamo/convert_frame.py:824]     mod = PyCodeCache.load_by_key_path(
W0629 11:52:20.070000 139012379337344 torch/_dynamo/convert_frame.py:824]   File "/usr/local/lib/python3.10/dist-packages/torch/_inductor/codecache.py", line 2160, in load_by_key_path
W0629 11:52:20.070000 139012379337344 torch/_dynamo/convert_frame.py:824]     exec(code, mod.__dict__, mod.__dict__)
W0629 11:52:20.070000 139012379337344 torch/_dynamo/convert_frame.py:824]   File "/tmp/torchinductor_root/z2/cz2bh22epnbh7mqrt23ywsmbvfsueq2kauftzoodvlbbtqvgbxb5.py", line 875, in <module>
W0629 11:52:20.070000 139012379337344 torch/_dynamo/convert_frame.py:824]     async_compile.wait(globals())
W0629 11:52:20.070000 139012379337344 torch/_dynamo/convert_frame.py:824]   File "/usr/local/lib/python3.10/dist-packages/torch/_inductor/codecache.py", line 2715, in wait
W0629 11:52:20.070000 139012379337344 torch/_dynamo/convert_frame.py:824]     scope[key] = result.result()
W0629 11:52:20.070000 139012379337344 torch/_dynamo/convert_frame.py:824]   File "/usr/local/lib/python3.10/dist-packages/torch/_inductor/codecache.py", line 2522, in result
W0629 11:52:20.070000 139012379337344 torch/_dynamo/convert_frame.py:824]     self.future.result()
W0629 11:52:20.070000 139012379337344 torch/_dynamo/convert_frame.py:824]   File "/usr/lib/python3.10/concurrent/futures/_base.py", line 458, in result
W0629 11:52:20.070000 139012379337344 torch/_dynamo/convert_frame.py:824]     return self.__get_result()
W0629 11:52:20.070000 139012379337344 torch/_dynamo/convert_frame.py:824]   File "/usr/lib/python3.10/concurrent/futures/_base.py", line 403, in __get_result
W0629 11:52:20.070000 139012379337344 torch/_dynamo/convert_frame.py:824]     raise self._exception
W0629 11:52:20.070000 139012379337344 torch/_dynamo/convert_frame.py:824] torch._dynamo.exc.BackendCompilerFailed: backend='inductor' raised:
W0629 11:52:20.070000 139012379337344 torch/_dynamo/convert_frame.py:824] RuntimeError: Internal Triton PTX codegen error:
W0629 11:52:20.070000 139012379337344 torch/_dynamo/convert_frame.py:824] ptxas /tmp/compile-ptx-src-1c79c8, line 619; error   : Feature '.bf16' requires .target sm_80 or higher
W0629 11:52:20.070000 139012379337344 torch/_dynamo/convert_frame.py:824] ptxas /tmp/compile-ptx-src-1c79c8, line 619; error   : Feature 'cvt.bf16.f32' requires .target sm_80 or higher
W0629 11:52:20.070000 139012379337344 torch/_dynamo/convert_frame.py:824] ptxas /tmp/compile-ptx-src-1c79c8, line 620; error   : Feature '.bf16' requires .target sm_80 or higher
W0629 11:52:20.070000 139012379337344 torch/_dynamo/convert_frame.py:824] ptxas /tmp/compile-ptx-src-1c79c8, line 620; error   : Feature 'cvt.bf16.f32' requires .target sm_80 or higher
W0629 11:52:20.070000 139012379337344 torch/_dynamo/convert_frame.py:824] ptxas /tmp/compile-ptx-src-1c79c8, line 621; error   : Feature '.bf16' requires .target sm_80 or higher
W0629 11:52:20.070000 139012379337344 torch/_dynamo/convert_frame.py:824] ptxas /tmp/compile-ptx-src-1c79c8, line 621; error   : Feature 'cvt.bf16.f32' requires .target sm_80 or higher
W0629 11:52:20.070000 139012379337344 torch/_dynamo/convert_frame.py:824] ptxas /tmp/compile-ptx-src-1c79c8, line 622; error   : Feature '.bf16' requires .target sm_80 or higher
W0629 11:52:20.070000 139012379337344 torch/_dynamo/convert_frame.py:824] ptxas /tmp/compile-ptx-src-1c79c8, line 622; error   : Feature 'cvt.bf16.f32' requires .target sm_80 or higher
W0629 11:52:20.070000 139012379337344 torch/_dynamo/convert_frame.py:824] ptxas fatal   : Ptx assembly aborted due to errors
W0629 11:52:20.070000 139012379337344 torch/_dynamo/convert_frame.py:824]
W0629 11:52:20.070000 139012379337344 torch/_dynamo/convert_frame.py:824]
W0629 11:52:20.070000 139012379337344 torch/_dynamo/convert_frame.py:824] Set TORCH_LOGS="+dynamo" and TORCHDYNAMO_VERBOSE=1 for more information
W0629 11:52:20.070000 139012379337344 torch/_dynamo/convert_frame.py:824]
W0629 11:52:32.689000 139012379337344 torch/_dynamo/convert_frame.py:824] WON'T CONVERT forward /usr/local/lib/python3.10/dist-packages/transformers/models/bert/modeling_bert.py line 996
W0629 11:52:32.689000 139012379337344 torch/_dynamo/convert_frame.py:824] due to:
W0629 11:52:32.689000 139012379337344 torch/_dynamo/convert_frame.py:824] Traceback (most recent call last):
W0629 11:52:32.689000 139012379337344 torch/_dynamo/convert_frame.py:824]   File "/usr/local/lib/python3.10/dist-packages/torch/_dynamo/convert_frame.py", line 786, in _convert_frame
W0629 11:52:32.689000 139012379337344 torch/_dynamo/convert_frame.py:824]     result = inner_convert(
W0629 11:52:32.689000 139012379337344 torch/_dynamo/convert_frame.py:824]   File "/usr/local/lib/python3.10/dist-packages/torch/_dynamo/convert_frame.py", line 400, in _convert_frame_assert
W0629 11:52:32.689000 139012379337344 torch/_dynamo/convert_frame.py:824]     return _compile(
W0629 11:52:32.689000 139012379337344 torch/_dynamo/convert_frame.py:824]   File "/usr/lib/python3.10/contextlib.py", line 79, in inner
W0629 11:52:32.689000 139012379337344 torch/_dynamo/convert_frame.py:824]     return func(*args, **kwds)
W0629 11:52:32.689000 139012379337344 torch/_dynamo/convert_frame.py:824]   File "/usr/local/lib/python3.10/dist-packages/torch/_dynamo/convert_frame.py", line 676, in _compile
W0629 11:52:32.689000 139012379337344 torch/_dynamo/convert_frame.py:824]     guarded_code = compile_inner(code, one_graph, hooks, transform)
W0629 11:52:32.689000 139012379337344 torch/_dynamo/convert_frame.py:824]   File "/usr/local/lib/python3.10/dist-packages/torch/_dynamo/utils.py", line 262, in time_wrapper
W0629 11:52:32.689000 139012379337344 torch/_dynamo/convert_frame.py:824]     r = func(*args, **kwargs)
W0629 11:52:32.689000 139012379337344 torch/_dynamo/convert_frame.py:824]   File "/usr/local/lib/python3.10/dist-packages/torch/_dynamo/convert_frame.py", line 535, in compile_inner
W0629 11:52:32.689000 139012379337344 torch/_dynamo/convert_frame.py:824]     out_code = transform_code_object(code, transform)
W0629 11:52:32.689000 139012379337344 torch/_dynamo/convert_frame.py:824]   File "/usr/local/lib/python3.10/dist-packages/torch/_dynamo/bytecode_transformation.py", line 1036, in transform_code_object
W0629 11:52:32.689000 139012379337344 torch/_dynamo/convert_frame.py:824]     transformations(instructions, code_options)
W0629 11:52:32.689000 139012379337344 torch/_dynamo/convert_frame.py:824]   File "/usr/local/lib/python3.10/dist-packages/torch/_dynamo/convert_frame.py", line 165, in _fn
W0629 11:52:32.689000 139012379337344 torch/_dynamo/convert_frame.py:824]     return fn(*args, **kwargs)
W0629 11:52:32.689000 139012379337344 torch/_dynamo/convert_frame.py:824]   File "/usr/local/lib/python3.10/dist-packages/torch/_dynamo/convert_frame.py", line 500, in transform
W0629 11:52:32.689000 139012379337344 torch/_dynamo/convert_frame.py:824]     tracer.run()
W0629 11:52:32.689000 139012379337344 torch/_dynamo/convert_frame.py:824]   File "/usr/local/lib/python3.10/dist-packages/torch/_dynamo/symbolic_convert.py", line 2149, in run
W0629 11:52:32.689000 139012379337344 torch/_dynamo/convert_frame.py:824]     super().run()
W0629 11:52:32.689000 139012379337344 torch/_dynamo/convert_frame.py:824]   File "/usr/local/lib/python3.10/dist-packages/torch/_dynamo/symbolic_convert.py", line 810, in run
W0629 11:52:32.689000 139012379337344 torch/_dynamo/convert_frame.py:824]     and self.step()
W0629 11:52:32.689000 139012379337344 torch/_dynamo/convert_frame.py:824]   File "/usr/local/lib/python3.10/dist-packages/torch/_dynamo/symbolic_convert.py", line 773, in step
W0629 11:52:32.689000 139012379337344 torch/_dynamo/convert_frame.py:824]     getattr(self, inst.opname)(inst)
W0629 11:52:32.689000 139012379337344 torch/_dynamo/convert_frame.py:824]   File "/usr/local/lib/python3.10/dist-packages/torch/_dynamo/symbolic_convert.py", line 2268, in RETURN_VALUE
W0629 11:52:32.689000 139012379337344 torch/_dynamo/convert_frame.py:824]     self.output.compile_subgraph(
W0629 11:52:32.689000 139012379337344 torch/_dynamo/convert_frame.py:824]   File "/usr/local/lib/python3.10/dist-packages/torch/_dynamo/output_graph.py", line 991, in compile_subgraph
W0629 11:52:32.689000 139012379337344 torch/_dynamo/convert_frame.py:824]     self.compile_and_call_fx_graph(tx, pass2.graph_output_vars(), root)
W0629 11:52:32.689000 139012379337344 torch/_dynamo/convert_frame.py:824]   File "/usr/lib/python3.10/contextlib.py", line 79, in inner
W0629 11:52:32.689000 139012379337344 torch/_dynamo/convert_frame.py:824]     return func(*args, **kwds)
W0629 11:52:32.689000 139012379337344 torch/_dynamo/convert_frame.py:824]   File "/usr/local/lib/python3.10/dist-packages/torch/_dynamo/output_graph.py", line 1168, in compile_and_call_fx_graph
W0629 11:52:32.689000 139012379337344 torch/_dynamo/convert_frame.py:824]     compiled_fn = self.call_user_compiler(gm)
W0629 11:52:32.689000 139012379337344 torch/_dynamo/convert_frame.py:824]   File "/usr/local/lib/python3.10/dist-packages/torch/_dynamo/utils.py", line 262, in time_wrapper
W0629 11:52:32.689000 139012379337344 torch/_dynamo/convert_frame.py:824]     r = func(*args, **kwargs)
W0629 11:52:32.689000 139012379337344 torch/_dynamo/convert_frame.py:824]   File "/usr/local/lib/python3.10/dist-packages/torch/_dynamo/output_graph.py", line 1241, in call_user_compiler
W0629 11:52:32.689000 139012379337344 torch/_dynamo/convert_frame.py:824]     raise BackendCompilerFailed(self.compiler_fn, e).with_traceback(
W0629 11:52:32.689000 139012379337344 torch/_dynamo/convert_frame.py:824]   File "/usr/local/lib/python3.10/dist-packages/torch/_dynamo/output_graph.py", line 1222, in call_user_compiler
W0629 11:52:32.689000 139012379337344 torch/_dynamo/convert_frame.py:824]     compiled_fn = compiler_fn(gm, self.example_inputs())
W0629 11:52:32.689000 139012379337344 torch/_dynamo/convert_frame.py:824]   File "/usr/local/lib/python3.10/dist-packages/torch/_dynamo/repro/after_dynamo.py", line 117, in debug_wrapper
W0629 11:52:32.689000 139012379337344 torch/_dynamo/convert_frame.py:824]     compiled_gm = compiler_fn(gm, example_inputs)
W0629 11:52:32.689000 139012379337344 torch/_dynamo/convert_frame.py:824]   File "/usr/local/lib/python3.10/dist-packages/torch/__init__.py", line 1729, in __call__
W0629 11:52:32.689000 139012379337344 torch/_dynamo/convert_frame.py:824]     return compile_fx(model_, inputs_, config_patches=self.config)
W0629 11:52:32.689000 139012379337344 torch/_dynamo/convert_frame.py:824]   File "/usr/lib/python3.10/contextlib.py", line 79, in inner
W0629 11:52:32.689000 139012379337344 torch/_dynamo/convert_frame.py:824]     return func(*args, **kwds)
W0629 11:52:32.689000 139012379337344 torch/_dynamo/convert_frame.py:824]   File "/usr/local/lib/python3.10/dist-packages/torch/_inductor/compile_fx.py", line 1330, in compile_fx
W0629 11:52:32.689000 139012379337344 torch/_dynamo/convert_frame.py:824]     return aot_autograd(
W0629 11:52:32.689000 139012379337344 torch/_dynamo/convert_frame.py:824]   File "/usr/local/lib/python3.10/dist-packages/torch/_dynamo/backends/common.py", line 58, in compiler_fn
W0629 11:52:32.689000 139012379337344 torch/_dynamo/convert_frame.py:824]     cg = aot_module_simplified(gm, example_inputs, **kwargs)
W0629 11:52:32.689000 139012379337344 torch/_dynamo/convert_frame.py:824]   File "/usr/local/lib/python3.10/dist-packages/torch/_functorch/aot_autograd.py", line 903, in aot_module_simplified
W0629 11:52:32.689000 139012379337344 torch/_dynamo/convert_frame.py:824]     compiled_fn = create_aot_dispatcher_function(
W0629 11:52:32.689000 139012379337344 torch/_dynamo/convert_frame.py:824]   File "/usr/local/lib/python3.10/dist-packages/torch/_dynamo/utils.py", line 262, in time_wrapper
W0629 11:52:32.689000 139012379337344 torch/_dynamo/convert_frame.py:824]     r = func(*args, **kwargs)
W0629 11:52:32.689000 139012379337344 torch/_dynamo/convert_frame.py:824]   File "/usr/local/lib/python3.10/dist-packages/torch/_functorch/aot_autograd.py", line 628, in create_aot_dispatcher_function
W0629 11:52:32.689000 139012379337344 torch/_dynamo/convert_frame.py:824]     compiled_fn = compiler_fn(flat_fn, fake_flat_args, aot_config, fw_metadata=fw_metadata)
W0629 11:52:32.689000 139012379337344 torch/_dynamo/convert_frame.py:824]   File "/usr/local/lib/python3.10/dist-packages/torch/_functorch/_aot_autograd/runtime_wrappers.py", line 443, in aot_wrapper_dedupe
W0629 11:52:32.689000 139012379337344 torch/_dynamo/convert_frame.py:824]     return compiler_fn(flat_fn, leaf_flat_args, aot_config, fw_metadata=fw_metadata)
W0629 11:52:32.689000 139012379337344 torch/_dynamo/convert_frame.py:824]   File "/usr/local/lib/python3.10/dist-packages/torch/_functorch/_aot_autograd/runtime_wrappers.py", line 648, in aot_wrapper_synthetic_base
W0629 11:52:32.689000 139012379337344 torch/_dynamo/convert_frame.py:824]     return compiler_fn(flat_fn, flat_args, aot_config, fw_metadata=fw_metadata)
W0629 11:52:32.689000 139012379337344 torch/_dynamo/convert_frame.py:824]   File "/usr/local/lib/python3.10/dist-packages/torch/_functorch/_aot_autograd/jit_compile_runtime_wrappers.py", line 119, in aot_dispatch_base
W0629 11:52:32.689000 139012379337344 torch/_dynamo/convert_frame.py:824]     compiled_fw = compiler(fw_module, updated_flat_args)
W0629 11:52:32.689000 139012379337344 torch/_dynamo/convert_frame.py:824]   File "/usr/local/lib/python3.10/dist-packages/torch/_dynamo/utils.py", line 262, in time_wrapper
W0629 11:52:32.689000 139012379337344 torch/_dynamo/convert_frame.py:824]     r = func(*args, **kwargs)
W0629 11:52:32.689000 139012379337344 torch/_dynamo/convert_frame.py:824]   File "/usr/local/lib/python3.10/dist-packages/torch/_inductor/compile_fx.py", line 1257, in fw_compiler_base
W0629 11:52:32.689000 139012379337344 torch/_dynamo/convert_frame.py:824]     return inner_compile(
W0629 11:52:32.689000 139012379337344 torch/_dynamo/convert_frame.py:824]   File "/usr/local/lib/python3.10/dist-packages/torch/_dynamo/repro/after_aot.py", line 83, in debug_wrapper
W0629 11:52:32.689000 139012379337344 torch/_dynamo/convert_frame.py:824]     inner_compiled_fn = compiler_fn(gm, example_inputs)
W0629 11:52:32.689000 139012379337344 torch/_dynamo/convert_frame.py:824]   File "/usr/local/lib/python3.10/dist-packages/torch/_inductor/debug.py", line 304, in inner
W0629 11:52:32.689000 139012379337344 torch/_dynamo/convert_frame.py:824]     return fn(*args, **kwargs)
W0629 11:52:32.689000 139012379337344 torch/_dynamo/convert_frame.py:824]   File "/usr/lib/python3.10/contextlib.py", line 79, in inner
W0629 11:52:32.689000 139012379337344 torch/_dynamo/convert_frame.py:824]     return func(*args, **kwds)
W0629 11:52:32.689000 139012379337344 torch/_dynamo/convert_frame.py:824]   File "/usr/lib/python3.10/contextlib.py", line 79, in inner
W0629 11:52:32.689000 139012379337344 torch/_dynamo/convert_frame.py:824]     return func(*args, **kwds)
W0629 11:52:32.689000 139012379337344 torch/_dynamo/convert_frame.py:824]   File "/usr/local/lib/python3.10/dist-packages/torch/_dynamo/utils.py", line 262, in time_wrapper
W0629 11:52:32.689000 139012379337344 torch/_dynamo/convert_frame.py:824]     r = func(*args, **kwargs)
W0629 11:52:32.689000 139012379337344 torch/_dynamo/convert_frame.py:824]   File "/usr/local/lib/python3.10/dist-packages/torch/_inductor/compile_fx.py", line 438, in compile_fx_inner
W0629 11:52:32.689000 139012379337344 torch/_dynamo/convert_frame.py:824]     compiled_graph = fx_codegen_and_compile(
W0629 11:52:32.689000 139012379337344 torch/_dynamo/convert_frame.py:824]   File "/usr/local/lib/python3.10/dist-packages/torch/_inductor/compile_fx.py", line 714, in fx_codegen_and_compile
W0629 11:52:32.689000 139012379337344 torch/_dynamo/convert_frame.py:824]     compiled_fn = graph.compile_to_fn()
W0629 11:52:32.689000 139012379337344 torch/_dynamo/convert_frame.py:824]   File "/usr/local/lib/python3.10/dist-packages/torch/_inductor/graph.py", line 1307, in compile_to_fn
W0629 11:52:32.689000 139012379337344 torch/_dynamo/convert_frame.py:824]     return self.compile_to_module().call
W0629 11:52:32.689000 139012379337344 torch/_dynamo/convert_frame.py:824]   File "/usr/local/lib/python3.10/dist-packages/torch/_dynamo/utils.py", line 262, in time_wrapper
W0629 11:52:32.689000 139012379337344 torch/_dynamo/convert_frame.py:824]     r = func(*args, **kwargs)
W0629 11:52:32.689000 139012379337344 torch/_dynamo/convert_frame.py:824]   File "/usr/local/lib/python3.10/dist-packages/torch/_inductor/graph.py", line 1254, in compile_to_module
W0629 11:52:32.689000 139012379337344 torch/_dynamo/convert_frame.py:824]     mod = PyCodeCache.load_by_key_path(
W0629 11:52:32.689000 139012379337344 torch/_dynamo/convert_frame.py:824]   File "/usr/local/lib/python3.10/dist-packages/torch/_inductor/codecache.py", line 2160, in load_by_key_path
W0629 11:52:32.689000 139012379337344 torch/_dynamo/convert_frame.py:824]     exec(code, mod.__dict__, mod.__dict__)
W0629 11:52:32.689000 139012379337344 torch/_dynamo/convert_frame.py:824]   File "/tmp/torchinductor_root/ol/colqlwqsubchice5tdjobnxm5uuaqld7lpff3new2oyiqgkqt2zi.py", line 807, in <module>
W0629 11:52:32.689000 139012379337344 torch/_dynamo/convert_frame.py:824]     async_compile.wait(globals())
W0629 11:52:32.689000 139012379337344 torch/_dynamo/convert_frame.py:824]   File "/usr/local/lib/python3.10/dist-packages/torch/_inductor/codecache.py", line 2715, in wait
W0629 11:52:32.689000 139012379337344 torch/_dynamo/convert_frame.py:824]     scope[key] = result.result()
W0629 11:52:32.689000 139012379337344 torch/_dynamo/convert_frame.py:824]   File "/usr/local/lib/python3.10/dist-packages/torch/_inductor/codecache.py", line 2522, in result
W0629 11:52:32.689000 139012379337344 torch/_dynamo/convert_frame.py:824]     self.future.result()
W0629 11:52:32.689000 139012379337344 torch/_dynamo/convert_frame.py:824]   File "/usr/lib/python3.10/concurrent/futures/_base.py", line 458, in result
W0629 11:52:32.689000 139012379337344 torch/_dynamo/convert_frame.py:824]     return self.__get_result()
W0629 11:52:32.689000 139012379337344 torch/_dynamo/convert_frame.py:824]   File "/usr/lib/python3.10/concurrent/futures/_base.py", line 403, in __get_result
W0629 11:52:32.689000 139012379337344 torch/_dynamo/convert_frame.py:824]     raise self._exception
W0629 11:52:32.689000 139012379337344 torch/_dynamo/convert_frame.py:824] torch._dynamo.exc.BackendCompilerFailed: backend='inductor' raised:
W0629 11:52:32.689000 139012379337344 torch/_dynamo/convert_frame.py:824] RuntimeError: Internal Triton PTX codegen error:
W0629 11:52:32.689000 139012379337344 torch/_dynamo/convert_frame.py:824] ptxas /tmp/compile-ptx-src-777918, line 619; error   : Feature '.bf16' requires .target sm_80 or higher
W0629 11:52:32.689000 139012379337344 torch/_dynamo/convert_frame.py:824] ptxas /tmp/compile-ptx-src-777918, line 619; error   : Feature 'cvt.bf16.f32' requires .target sm_80 or higher
W0629 11:52:32.689000 139012379337344 torch/_dynamo/convert_frame.py:824] ptxas /tmp/compile-ptx-src-777918, line 620; error   : Feature '.bf16' requires .target sm_80 or higher
W0629 11:52:32.689000 139012379337344 torch/_dynamo/convert_frame.py:824] ptxas /tmp/compile-ptx-src-777918, line 620; error   : Feature 'cvt.bf16.f32' requires .target sm_80 or higher
W0629 11:52:32.689000 139012379337344 torch/_dynamo/convert_frame.py:824] ptxas /tmp/compile-ptx-src-777918, line 621; error   : Feature '.bf16' requires .target sm_80 or higher
W0629 11:52:32.689000 139012379337344 torch/_dynamo/convert_frame.py:824] ptxas /tmp/compile-ptx-src-777918, line 621; error   : Feature 'cvt.bf16.f32' requires .target sm_80 or higher
W0629 11:52:32.689000 139012379337344 torch/_dynamo/convert_frame.py:824] ptxas /tmp/compile-ptx-src-777918, line 622; error   : Feature '.bf16' requires .target sm_80 or higher
W0629 11:52:32.689000 139012379337344 torch/_dynamo/convert_frame.py:824] ptxas /tmp/compile-ptx-src-777918, line 622; error   : Feature 'cvt.bf16.f32' requires .target sm_80 or higher
W0629 11:52:32.689000 139012379337344 torch/_dynamo/convert_frame.py:824] ptxas fatal   : Ptx assembly aborted due to errors
W0629 11:52:32.689000 139012379337344 torch/_dynamo/convert_frame.py:824]
W0629 11:52:32.689000 139012379337344 torch/_dynamo/convert_frame.py:824]
W0629 11:52:32.689000 139012379337344 torch/_dynamo/convert_frame.py:824] Set TORCH_LOGS="+dynamo" and TORCHDYNAMO_VERBOSE=1 for more information
W0629 11:52:32.689000 139012379337344 torch/_dynamo/convert_frame.py:824]
W0629 11:52:32.689000 139012379337344 torch/_dynamo/convert_frame.py:824] Traceback (most recent call last):
W0629 11:52:32.689000 139012379337344 torch/_dynamo/convert_frame.py:824]   File "/usr/local/lib/python3.10/dist-packages/torch/_dynamo/convert_frame.py", line 786, in _convert_frame
W0629 11:52:32.689000 139012379337344 torch/_dynamo/convert_frame.py:824]     result = inner_convert(
W0629 11:52:32.689000 139012379337344 torch/_dynamo/convert_frame.py:824]   File "/usr/local/lib/python3.10/dist-packages/torch/_dynamo/convert_frame.py", line 400, in _convert_frame_assert
W0629 11:52:32.689000 139012379337344 torch/_dynamo/convert_frame.py:824]     return _compile(
W0629 11:52:32.689000 139012379337344 torch/_dynamo/convert_frame.py:824]   File "/usr/lib/python3.10/contextlib.py", line 79, in inner
W0629 11:52:32.689000 139012379337344 torch/_dynamo/convert_frame.py:824]     return func(*args, **kwds)
W0629 11:52:32.689000 139012379337344 torch/_dynamo/convert_frame.py:824]   File "/usr/local/lib/python3.10/dist-packages/torch/_dynamo/convert_frame.py", line 676, in _compile
W0629 11:52:32.689000 139012379337344 torch/_dynamo/convert_frame.py:824]     guarded_code = compile_inner(code, one_graph, hooks, transform)
W0629 11:52:32.689000 139012379337344 torch/_dynamo/convert_frame.py:824]   File "/usr/local/lib/python3.10/dist-packages/torch/_dynamo/utils.py", line 262, in time_wrapper
W0629 11:52:32.689000 139012379337344 torch/_dynamo/convert_frame.py:824]     r = func(*args, **kwargs)
W0629 11:52:32.689000 139012379337344 torch/_dynamo/convert_frame.py:824]   File "/usr/local/lib/python3.10/dist-packages/torch/_dynamo/convert_frame.py", line 535, in compile_inner
W0629 11:52:32.689000 139012379337344 torch/_dynamo/convert_frame.py:824]     out_code = transform_code_object(code, transform)
W0629 11:52:32.689000 139012379337344 torch/_dynamo/convert_frame.py:824]   File "/usr/local/lib/python3.10/dist-packages/torch/_dynamo/bytecode_transformation.py", line 1036, in transform_code_object
W0629 11:52:32.689000 139012379337344 torch/_dynamo/convert_frame.py:824]     transformations(instructions, code_options)
W0629 11:52:32.689000 139012379337344 torch/_dynamo/convert_frame.py:824]   File "/usr/local/lib/python3.10/dist-packages/torch/_dynamo/convert_frame.py", line 165, in _fn
W0629 11:52:32.689000 139012379337344 torch/_dynamo/convert_frame.py:824]     return fn(*args, **kwargs)
W0629 11:52:32.689000 139012379337344 torch/_dynamo/convert_frame.py:824]   File "/usr/local/lib/python3.10/dist-packages/torch/_dynamo/convert_frame.py", line 500, in transform
W0629 11:52:32.689000 139012379337344 torch/_dynamo/convert_frame.py:824]     tracer.run()
W0629 11:52:32.689000 139012379337344 torch/_dynamo/convert_frame.py:824]   File "/usr/local/lib/python3.10/dist-packages/torch/_dynamo/symbolic_convert.py", line 2149, in run
W0629 11:52:32.689000 139012379337344 torch/_dynamo/convert_frame.py:824]     super().run()
W0629 11:52:32.689000 139012379337344 torch/_dynamo/convert_frame.py:824]   File "/usr/local/lib/python3.10/dist-packages/torch/_dynamo/symbolic_convert.py", line 810, in run
W0629 11:52:32.689000 139012379337344 torch/_dynamo/convert_frame.py:824]     and self.step()
W0629 11:52:32.689000 139012379337344 torch/_dynamo/convert_frame.py:824]   File "/usr/local/lib/python3.10/dist-packages/torch/_dynamo/symbolic_convert.py", line 773, in step
W0629 11:52:32.689000 139012379337344 torch/_dynamo/convert_frame.py:824]     getattr(self, inst.opname)(inst)
W0629 11:52:32.689000 139012379337344 torch/_dynamo/convert_frame.py:824]   File "/usr/local/lib/python3.10/dist-packages/torch/_dynamo/symbolic_convert.py", line 2268, in RETURN_VALUE
W0629 11:52:32.689000 139012379337344 torch/_dynamo/convert_frame.py:824]     self.output.compile_subgraph(
W0629 11:52:32.689000 139012379337344 torch/_dynamo/convert_frame.py:824]   File "/usr/local/lib/python3.10/dist-packages/torch/_dynamo/output_graph.py", line 991, in compile_subgraph
W0629 11:52:32.689000 139012379337344 torch/_dynamo/convert_frame.py:824]     self.compile_and_call_fx_graph(tx, pass2.graph_output_vars(), root)
W0629 11:52:32.689000 139012379337344 torch/_dynamo/convert_frame.py:824]   File "/usr/lib/python3.10/contextlib.py", line 79, in inner
W0629 11:52:32.689000 139012379337344 torch/_dynamo/convert_frame.py:824]     return func(*args, **kwds)
W0629 11:52:32.689000 139012379337344 torch/_dynamo/convert_frame.py:824]   File "/usr/local/lib/python3.10/dist-packages/torch/_dynamo/output_graph.py", line 1168, in compile_and_call_fx_graph
W0629 11:52:32.689000 139012379337344 torch/_dynamo/convert_frame.py:824]     compiled_fn = self.call_user_compiler(gm)
W0629 11:52:32.689000 139012379337344 torch/_dynamo/convert_frame.py:824]   File "/usr/local/lib/python3.10/dist-packages/torch/_dynamo/utils.py", line 262, in time_wrapper
W0629 11:52:32.689000 139012379337344 torch/_dynamo/convert_frame.py:824]     r = func(*args, **kwargs)
W0629 11:52:32.689000 139012379337344 torch/_dynamo/convert_frame.py:824]   File "/usr/local/lib/python3.10/dist-packages/torch/_dynamo/output_graph.py", line 1241, in call_user_compiler
W0629 11:52:32.689000 139012379337344 torch/_dynamo/convert_frame.py:824]     raise BackendCompilerFailed(self.compiler_fn, e).with_traceback(
W0629 11:52:32.689000 139012379337344 torch/_dynamo/convert_frame.py:824]   File "/usr/local/lib/python3.10/dist-packages/torch/_dynamo/output_graph.py", line 1222, in call_user_compiler
W0629 11:52:32.689000 139012379337344 torch/_dynamo/convert_frame.py:824]     compiled_fn = compiler_fn(gm, self.example_inputs())
W0629 11:52:32.689000 139012379337344 torch/_dynamo/convert_frame.py:824]   File "/usr/local/lib/python3.10/dist-packages/torch/_dynamo/repro/after_dynamo.py", line 117, in debug_wrapper
W0629 11:52:32.689000 139012379337344 torch/_dynamo/convert_frame.py:824]     compiled_gm = compiler_fn(gm, example_inputs)
W0629 11:52:32.689000 139012379337344 torch/_dynamo/convert_frame.py:824]   File "/usr/local/lib/python3.10/dist-packages/torch/__init__.py", line 1729, in __call__
W0629 11:52:32.689000 139012379337344 torch/_dynamo/convert_frame.py:824]     return compile_fx(model_, inputs_, config_patches=self.config)
W0629 11:52:32.689000 139012379337344 torch/_dynamo/convert_frame.py:824]   File "/usr/lib/python3.10/contextlib.py", line 79, in inner
W0629 11:52:32.689000 139012379337344 torch/_dynamo/convert_frame.py:824]     return func(*args, **kwds)
W0629 11:52:32.689000 139012379337344 torch/_dynamo/convert_frame.py:824]   File "/usr/local/lib/python3.10/dist-packages/torch/_inductor/compile_fx.py", line 1330, in compile_fx
W0629 11:52:32.689000 139012379337344 torch/_dynamo/convert_frame.py:824]     return aot_autograd(
W0629 11:52:32.689000 139012379337344 torch/_dynamo/convert_frame.py:824]   File "/usr/local/lib/python3.10/dist-packages/torch/_dynamo/backends/common.py", line 58, in compiler_fn
W0629 11:52:32.689000 139012379337344 torch/_dynamo/convert_frame.py:824]     cg = aot_module_simplified(gm, example_inputs, **kwargs)
W0629 11:52:32.689000 139012379337344 torch/_dynamo/convert_frame.py:824]   File "/usr/local/lib/python3.10/dist-packages/torch/_functorch/aot_autograd.py", line 903, in aot_module_simplified
W0629 11:52:32.689000 139012379337344 torch/_dynamo/convert_frame.py:824]     compiled_fn = create_aot_dispatcher_function(
W0629 11:52:32.689000 139012379337344 torch/_dynamo/convert_frame.py:824]   File "/usr/local/lib/python3.10/dist-packages/torch/_dynamo/utils.py", line 262, in time_wrapper
W0629 11:52:32.689000 139012379337344 torch/_dynamo/convert_frame.py:824]     r = func(*args, **kwargs)
W0629 11:52:32.689000 139012379337344 torch/_dynamo/convert_frame.py:824]   File "/usr/local/lib/python3.10/dist-packages/torch/_functorch/aot_autograd.py", line 628, in create_aot_dispatcher_function
W0629 11:52:32.689000 139012379337344 torch/_dynamo/convert_frame.py:824]     compiled_fn = compiler_fn(flat_fn, fake_flat_args, aot_config, fw_metadata=fw_metadata)
W0629 11:52:32.689000 139012379337344 torch/_dynamo/convert_frame.py:824]   File "/usr/local/lib/python3.10/dist-packages/torch/_functorch/_aot_autograd/runtime_wrappers.py", line 443, in aot_wrapper_dedupe
W0629 11:52:32.689000 139012379337344 torch/_dynamo/convert_frame.py:824]     return compiler_fn(flat_fn, leaf_flat_args, aot_config, fw_metadata=fw_metadata)
W0629 11:52:32.689000 139012379337344 torch/_dynamo/convert_frame.py:824]   File "/usr/local/lib/python3.10/dist-packages/torch/_functorch/_aot_autograd/runtime_wrappers.py", line 648, in aot_wrapper_synthetic_base
W0629 11:52:32.689000 139012379337344 torch/_dynamo/convert_frame.py:824]     return compiler_fn(flat_fn, flat_args, aot_config, fw_metadata=fw_metadata)
W0629 11:52:32.689000 139012379337344 torch/_dynamo/convert_frame.py:824]   File "/usr/local/lib/python3.10/dist-packages/torch/_functorch/_aot_autograd/jit_compile_runtime_wrappers.py", line 119, in aot_dispatch_base
W0629 11:52:32.689000 139012379337344 torch/_dynamo/convert_frame.py:824]     compiled_fw = compiler(fw_module, updated_flat_args)
W0629 11:52:32.689000 139012379337344 torch/_dynamo/convert_frame.py:824]   File "/usr/local/lib/python3.10/dist-packages/torch/_dynamo/utils.py", line 262, in time_wrapper
W0629 11:52:32.689000 139012379337344 torch/_dynamo/convert_frame.py:824]     r = func(*args, **kwargs)
W0629 11:52:32.689000 139012379337344 torch/_dynamo/convert_frame.py:824]   File "/usr/local/lib/python3.10/dist-packages/torch/_inductor/compile_fx.py", line 1257, in fw_compiler_base
W0629 11:52:32.689000 139012379337344 torch/_dynamo/convert_frame.py:824]     return inner_compile(
W0629 11:52:32.689000 139012379337344 torch/_dynamo/convert_frame.py:824]   File "/usr/local/lib/python3.10/dist-packages/torch/_dynamo/repro/after_aot.py", line 83, in debug_wrapper
W0629 11:52:32.689000 139012379337344 torch/_dynamo/convert_frame.py:824]     inner_compiled_fn = compiler_fn(gm, example_inputs)
W0629 11:52:32.689000 139012379337344 torch/_dynamo/convert_frame.py:824]   File "/usr/local/lib/python3.10/dist-packages/torch/_inductor/debug.py", line 304, in inner
W0629 11:52:32.689000 139012379337344 torch/_dynamo/convert_frame.py:824]     return fn(*args, **kwargs)
W0629 11:52:32.689000 139012379337344 torch/_dynamo/convert_frame.py:824]   File "/usr/lib/python3.10/contextlib.py", line 79, in inner
W0629 11:52:32.689000 139012379337344 torch/_dynamo/convert_frame.py:824]     return func(*args, **kwds)
W0629 11:52:32.689000 139012379337344 torch/_dynamo/convert_frame.py:824]   File "/usr/lib/python3.10/contextlib.py", line 79, in inner
W0629 11:52:32.689000 139012379337344 torch/_dynamo/convert_frame.py:824]     return func(*args, **kwds)
W0629 11:52:32.689000 139012379337344 torch/_dynamo/convert_frame.py:824]   File "/usr/local/lib/python3.10/dist-packages/torch/_dynamo/utils.py", line 262, in time_wrapper
W0629 11:52:32.689000 139012379337344 torch/_dynamo/convert_frame.py:824]     r = func(*args, **kwargs)
W0629 11:52:32.689000 139012379337344 torch/_dynamo/convert_frame.py:824]   File "/usr/local/lib/python3.10/dist-packages/torch/_inductor/compile_fx.py", line 438, in compile_fx_inner
W0629 11:52:32.689000 139012379337344 torch/_dynamo/convert_frame.py:824]     compiled_graph = fx_codegen_and_compile(
W0629 11:52:32.689000 139012379337344 torch/_dynamo/convert_frame.py:824]   File "/usr/local/lib/python3.10/dist-packages/torch/_inductor/compile_fx.py", line 714, in fx_codegen_and_compile
W0629 11:52:32.689000 139012379337344 torch/_dynamo/convert_frame.py:824]     compiled_fn = graph.compile_to_fn()
W0629 11:52:32.689000 139012379337344 torch/_dynamo/convert_frame.py:824]   File "/usr/local/lib/python3.10/dist-packages/torch/_inductor/graph.py", line 1307, in compile_to_fn
W0629 11:52:32.689000 139012379337344 torch/_dynamo/convert_frame.py:824]     return self.compile_to_module().call
W0629 11:52:32.689000 139012379337344 torch/_dynamo/convert_frame.py:824]   File "/usr/local/lib/python3.10/dist-packages/torch/_dynamo/utils.py", line 262, in time_wrapper
W0629 11:52:32.689000 139012379337344 torch/_dynamo/convert_frame.py:824]     r = func(*args, **kwargs)
W0629 11:52:32.689000 139012379337344 torch/_dynamo/convert_frame.py:824]   File "/usr/local/lib/python3.10/dist-packages/torch/_inductor/graph.py", line 1254, in compile_to_module
W0629 11:52:32.689000 139012379337344 torch/_dynamo/convert_frame.py:824]     mod = PyCodeCache.load_by_key_path(
W0629 11:52:32.689000 139012379337344 torch/_dynamo/convert_frame.py:824]   File "/usr/local/lib/python3.10/dist-packages/torch/_inductor/codecache.py", line 2160, in load_by_key_path
W0629 11:52:32.689000 139012379337344 torch/_dynamo/convert_frame.py:824]     exec(code, mod.__dict__, mod.__dict__)
W0629 11:52:32.689000 139012379337344 torch/_dynamo/convert_frame.py:824]   File "/tmp/torchinductor_root/ol/colqlwqsubchice5tdjobnxm5uuaqld7lpff3new2oyiqgkqt2zi.py", line 807, in <module>
W0629 11:52:32.689000 139012379337344 torch/_dynamo/convert_frame.py:824]     async_compile.wait(globals())
W0629 11:52:32.689000 139012379337344 torch/_dynamo/convert_frame.py:824]   File "/usr/local/lib/python3.10/dist-packages/torch/_inductor/codecache.py", line 2715, in wait
W0629 11:52:32.689000 139012379337344 torch/_dynamo/convert_frame.py:824]     scope[key] = result.result()
W0629 11:52:32.689000 139012379337344 torch/_dynamo/convert_frame.py:824]   File "/usr/local/lib/python3.10/dist-packages/torch/_inductor/codecache.py", line 2522, in result
W0629 11:52:32.689000 139012379337344 torch/_dynamo/convert_frame.py:824]     self.future.result()
W0629 11:52:32.689000 139012379337344 torch/_dynamo/convert_frame.py:824]   File "/usr/lib/python3.10/concurrent/futures/_base.py", line 458, in result
W0629 11:52:32.689000 139012379337344 torch/_dynamo/convert_frame.py:824]     return self.__get_result()
W0629 11:52:32.689000 139012379337344 torch/_dynamo/convert_frame.py:824]   File "/usr/lib/python3.10/concurrent/futures/_base.py", line 403, in __get_result
W0629 11:52:32.689000 139012379337344 torch/_dynamo/convert_frame.py:824]     raise self._exception
W0629 11:52:32.689000 139012379337344 torch/_dynamo/convert_frame.py:824] torch._dynamo.exc.BackendCompilerFailed: backend='inductor' raised:
W0629 11:52:32.689000 139012379337344 torch/_dynamo/convert_frame.py:824] RuntimeError: Internal Triton PTX codegen error:
W0629 11:52:32.689000 139012379337344 torch/_dynamo/convert_frame.py:824] ptxas /tmp/compile-ptx-src-777918, line 619; error   : Feature '.bf16' requires .target sm_80 or higher
W0629 11:52:32.689000 139012379337344 torch/_dynamo/convert_frame.py:824] ptxas /tmp/compile-ptx-src-777918, line 619; error   : Feature 'cvt.bf16.f32' requires .target sm_80 or higher
W0629 11:52:32.689000 139012379337344 torch/_dynamo/convert_frame.py:824] ptxas /tmp/compile-ptx-src-777918, line 620; error   : Feature '.bf16' requires .target sm_80 or higher
W0629 11:52:32.689000 139012379337344 torch/_dynamo/convert_frame.py:824] ptxas /tmp/compile-ptx-src-777918, line 620; error   : Feature 'cvt.bf16.f32' requires .target sm_80 or higher
W0629 11:52:32.689000 139012379337344 torch/_dynamo/convert_frame.py:824] ptxas /tmp/compile-ptx-src-777918, line 621; error   : Feature '.bf16' requires .target sm_80 or higher
W0629 11:52:32.689000 139012379337344 torch/_dynamo/convert_frame.py:824] ptxas /tmp/compile-ptx-src-777918, line 621; error   : Feature 'cvt.bf16.f32' requires .target sm_80 or higher
W0629 11:52:32.689000 139012379337344 torch/_dynamo/convert_frame.py:824] ptxas /tmp/compile-ptx-src-777918, line 622; error   : Feature '.bf16' requires .target sm_80 or higher
W0629 11:52:32.689000 139012379337344 torch/_dynamo/convert_frame.py:824] ptxas /tmp/compile-ptx-src-777918, line 622; error   : Feature 'cvt.bf16.f32' requires .target sm_80 or higher
W0629 11:52:32.689000 139012379337344 torch/_dynamo/convert_frame.py:824] ptxas fatal   : Ptx assembly aborted due to errors
W0629 11:52:32.689000 139012379337344 torch/_dynamo/convert_frame.py:824]
W0629 11:52:32.689000 139012379337344 torch/_dynamo/convert_frame.py:824]
W0629 11:52:32.689000 139012379337344 torch/_dynamo/convert_frame.py:824] Set TORCH_LOGS="+dynamo" and TORCHDYNAMO_VERBOSE=1 for more information
W0629 11:52:32.689000 139012379337344 torch/_dynamo/convert_frame.py:824]
W0629 11:52:46.862000 139012379337344 torch/_dynamo/convert_frame.py:824] WON'T CONVERT forward /usr/local/lib/python3.10/dist-packages/transformers/models/bert/modeling_bert.py line 646
W0629 11:52:46.862000 139012379337344 torch/_dynamo/convert_frame.py:824] due to:
W0629 11:52:46.862000 139012379337344 torch/_dynamo/convert_frame.py:824] Traceback (most recent call last):
W0629 11:52:46.862000 139012379337344 torch/_dynamo/convert_frame.py:824]   File "/usr/local/lib/python3.10/dist-packages/torch/_dynamo/convert_frame.py", line 786, in _convert_frame
W0629 11:52:46.862000 139012379337344 torch/_dynamo/convert_frame.py:824]     result = inner_convert(
W0629 11:52:46.862000 139012379337344 torch/_dynamo/convert_frame.py:824]   File "/usr/local/lib/python3.10/dist-packages/torch/_dynamo/convert_frame.py", line 400, in _convert_frame_assert
W0629 11:52:46.862000 139012379337344 torch/_dynamo/convert_frame.py:824]     return _compile(
W0629 11:52:46.862000 139012379337344 torch/_dynamo/convert_frame.py:824]   File "/usr/lib/python3.10/contextlib.py", line 79, in inner
W0629 11:52:46.862000 139012379337344 torch/_dynamo/convert_frame.py:824]     return func(*args, **kwds)
W0629 11:52:46.862000 139012379337344 torch/_dynamo/convert_frame.py:824]   File "/usr/local/lib/python3.10/dist-packages/torch/_dynamo/convert_frame.py", line 676, in _compile
W0629 11:52:46.862000 139012379337344 torch/_dynamo/convert_frame.py:824]     guarded_code = compile_inner(code, one_graph, hooks, transform)
W0629 11:52:46.862000 139012379337344 torch/_dynamo/convert_frame.py:824]   File "/usr/local/lib/python3.10/dist-packages/torch/_dynamo/utils.py", line 262, in time_wrapper
W0629 11:52:46.862000 139012379337344 torch/_dynamo/convert_frame.py:824]     r = func(*args, **kwargs)
W0629 11:52:46.862000 139012379337344 torch/_dynamo/convert_frame.py:824]   File "/usr/local/lib/python3.10/dist-packages/torch/_dynamo/convert_frame.py", line 535, in compile_inner
W0629 11:52:46.862000 139012379337344 torch/_dynamo/convert_frame.py:824]     out_code = transform_code_object(code, transform)
W0629 11:52:46.862000 139012379337344 torch/_dynamo/convert_frame.py:824]   File "/usr/local/lib/python3.10/dist-packages/torch/_dynamo/bytecode_transformation.py", line 1036, in transform_code_object
W0629 11:52:46.862000 139012379337344 torch/_dynamo/convert_frame.py:824]     transformations(instructions, code_options)
W0629 11:52:46.862000 139012379337344 torch/_dynamo/convert_frame.py:824]   File "/usr/local/lib/python3.10/dist-packages/torch/_dynamo/convert_frame.py", line 165, in _fn
W0629 11:52:46.862000 139012379337344 torch/_dynamo/convert_frame.py:824]     return fn(*args, **kwargs)
W0629 11:52:46.862000 139012379337344 torch/_dynamo/convert_frame.py:824]   File "/usr/local/lib/python3.10/dist-packages/torch/_dynamo/convert_frame.py", line 500, in transform
W0629 11:52:46.862000 139012379337344 torch/_dynamo/convert_frame.py:824]     tracer.run()
W0629 11:52:46.862000 139012379337344 torch/_dynamo/convert_frame.py:824]   File "/usr/local/lib/python3.10/dist-packages/torch/_dynamo/symbolic_convert.py", line 2149, in run
W0629 11:52:46.862000 139012379337344 torch/_dynamo/convert_frame.py:824]     super().run()
W0629 11:52:46.862000 139012379337344 torch/_dynamo/convert_frame.py:824]   File "/usr/local/lib/python3.10/dist-packages/torch/_dynamo/symbolic_convert.py", line 810, in run
W0629 11:52:46.862000 139012379337344 torch/_dynamo/convert_frame.py:824]     and self.step()
W0629 11:52:46.862000 139012379337344 torch/_dynamo/convert_frame.py:824]   File "/usr/local/lib/python3.10/dist-packages/torch/_dynamo/symbolic_convert.py", line 773, in step
W0629 11:52:46.862000 139012379337344 torch/_dynamo/convert_frame.py:824]     getattr(self, inst.opname)(inst)
W0629 11:52:46.862000 139012379337344 torch/_dynamo/convert_frame.py:824]   File "/usr/local/lib/python3.10/dist-packages/torch/_dynamo/symbolic_convert.py", line 2268, in RETURN_VALUE
W0629 11:52:46.862000 139012379337344 torch/_dynamo/convert_frame.py:824]     self.output.compile_subgraph(
W0629 11:52:46.862000 139012379337344 torch/_dynamo/convert_frame.py:824]   File "/usr/local/lib/python3.10/dist-packages/torch/_dynamo/output_graph.py", line 991, in compile_subgraph
W0629 11:52:46.862000 139012379337344 torch/_dynamo/convert_frame.py:824]     self.compile_and_call_fx_graph(tx, pass2.graph_output_vars(), root)
W0629 11:52:46.862000 139012379337344 torch/_dynamo/convert_frame.py:824]   File "/usr/lib/python3.10/contextlib.py", line 79, in inner
W0629 11:52:46.862000 139012379337344 torch/_dynamo/convert_frame.py:824]     return func(*args, **kwds)
W0629 11:52:46.862000 139012379337344 torch/_dynamo/convert_frame.py:824]   File "/usr/local/lib/python3.10/dist-packages/torch/_dynamo/output_graph.py", line 1168, in compile_and_call_fx_graph
W0629 11:52:46.862000 139012379337344 torch/_dynamo/convert_frame.py:824]     compiled_fn = self.call_user_compiler(gm)
W0629 11:52:46.862000 139012379337344 torch/_dynamo/convert_frame.py:824]   File "/usr/local/lib/python3.10/dist-packages/torch/_dynamo/utils.py", line 262, in time_wrapper
W0629 11:52:46.862000 139012379337344 torch/_dynamo/convert_frame.py:824]     r = func(*args, **kwargs)
W0629 11:52:46.862000 139012379337344 torch/_dynamo/convert_frame.py:824]   File "/usr/local/lib/python3.10/dist-packages/torch/_dynamo/output_graph.py", line 1241, in call_user_compiler
W0629 11:52:46.862000 139012379337344 torch/_dynamo/convert_frame.py:824]     raise BackendCompilerFailed(self.compiler_fn, e).with_traceback(
W0629 11:52:46.862000 139012379337344 torch/_dynamo/convert_frame.py:824]   File "/usr/local/lib/python3.10/dist-packages/torch/_dynamo/output_graph.py", line 1222, in call_user_compiler
W0629 11:52:46.862000 139012379337344 torch/_dynamo/convert_frame.py:824]     compiled_fn = compiler_fn(gm, self.example_inputs())
W0629 11:52:46.862000 139012379337344 torch/_dynamo/convert_frame.py:824]   File "/usr/local/lib/python3.10/dist-packages/torch/_dynamo/repro/after_dynamo.py", line 117, in debug_wrapper
W0629 11:52:46.862000 139012379337344 torch/_dynamo/convert_frame.py:824]     compiled_gm = compiler_fn(gm, example_inputs)
W0629 11:52:46.862000 139012379337344 torch/_dynamo/convert_frame.py:824]   File "/usr/local/lib/python3.10/dist-packages/torch/__init__.py", line 1729, in __call__
W0629 11:52:46.862000 139012379337344 torch/_dynamo/convert_frame.py:824]     return compile_fx(model_, inputs_, config_patches=self.config)
W0629 11:52:46.862000 139012379337344 torch/_dynamo/convert_frame.py:824]   File "/usr/lib/python3.10/contextlib.py", line 79, in inner
W0629 11:52:46.862000 139012379337344 torch/_dynamo/convert_frame.py:824]     return func(*args, **kwds)
W0629 11:52:46.862000 139012379337344 torch/_dynamo/convert_frame.py:824]   File "/usr/local/lib/python3.10/dist-packages/torch/_inductor/compile_fx.py", line 1330, in compile_fx
W0629 11:52:46.862000 139012379337344 torch/_dynamo/convert_frame.py:824]     return aot_autograd(
W0629 11:52:46.862000 139012379337344 torch/_dynamo/convert_frame.py:824]   File "/usr/local/lib/python3.10/dist-packages/torch/_dynamo/backends/common.py", line 58, in compiler_fn
W0629 11:52:46.862000 139012379337344 torch/_dynamo/convert_frame.py:824]     cg = aot_module_simplified(gm, example_inputs, **kwargs)
W0629 11:52:46.862000 139012379337344 torch/_dynamo/convert_frame.py:824]   File "/usr/local/lib/python3.10/dist-packages/torch/_functorch/aot_autograd.py", line 903, in aot_module_simplified
W0629 11:52:46.862000 139012379337344 torch/_dynamo/convert_frame.py:824]     compiled_fn = create_aot_dispatcher_function(
W0629 11:52:46.862000 139012379337344 torch/_dynamo/convert_frame.py:824]   File "/usr/local/lib/python3.10/dist-packages/torch/_dynamo/utils.py", line 262, in time_wrapper
W0629 11:52:46.862000 139012379337344 torch/_dynamo/convert_frame.py:824]     r = func(*args, **kwargs)
W0629 11:52:46.862000 139012379337344 torch/_dynamo/convert_frame.py:824]   File "/usr/local/lib/python3.10/dist-packages/torch/_functorch/aot_autograd.py", line 628, in create_aot_dispatcher_function
W0629 11:52:46.862000 139012379337344 torch/_dynamo/convert_frame.py:824]     compiled_fn = compiler_fn(flat_fn, fake_flat_args, aot_config, fw_metadata=fw_metadata)
W0629 11:52:46.862000 139012379337344 torch/_dynamo/convert_frame.py:824]   File "/usr/local/lib/python3.10/dist-packages/torch/_functorch/_aot_autograd/runtime_wrappers.py", line 443, in aot_wrapper_dedupe
W0629 11:52:46.862000 139012379337344 torch/_dynamo/convert_frame.py:824]     return compiler_fn(flat_fn, leaf_flat_args, aot_config, fw_metadata=fw_metadata)
W0629 11:52:46.862000 139012379337344 torch/_dynamo/convert_frame.py:824]   File "/usr/local/lib/python3.10/dist-packages/torch/_functorch/_aot_autograd/runtime_wrappers.py", line 648, in aot_wrapper_synthetic_base
W0629 11:52:46.862000 139012379337344 torch/_dynamo/convert_frame.py:824]     return compiler_fn(flat_fn, flat_args, aot_config, fw_metadata=fw_metadata)
W0629 11:52:46.862000 139012379337344 torch/_dynamo/convert_frame.py:824]   File "/usr/local/lib/python3.10/dist-packages/torch/_functorch/_aot_autograd/jit_compile_runtime_wrappers.py", line 119, in aot_dispatch_base
W0629 11:52:46.862000 139012379337344 torch/_dynamo/convert_frame.py:824]     compiled_fw = compiler(fw_module, updated_flat_args)
W0629 11:52:46.862000 139012379337344 torch/_dynamo/convert_frame.py:824]   File "/usr/local/lib/python3.10/dist-packages/torch/_dynamo/utils.py", line 262, in time_wrapper
W0629 11:52:46.862000 139012379337344 torch/_dynamo/convert_frame.py:824]     r = func(*args, **kwargs)
W0629 11:52:46.862000 139012379337344 torch/_dynamo/convert_frame.py:824]   File "/usr/local/lib/python3.10/dist-packages/torch/_inductor/compile_fx.py", line 1257, in fw_compiler_base
W0629 11:52:46.862000 139012379337344 torch/_dynamo/convert_frame.py:824]     return inner_compile(
W0629 11:52:46.862000 139012379337344 torch/_dynamo/convert_frame.py:824]   File "/usr/local/lib/python3.10/dist-packages/torch/_dynamo/repro/after_aot.py", line 83, in debug_wrapper
W0629 11:52:46.862000 139012379337344 torch/_dynamo/convert_frame.py:824]     inner_compiled_fn = compiler_fn(gm, example_inputs)
W0629 11:52:46.862000 139012379337344 torch/_dynamo/convert_frame.py:824]   File "/usr/local/lib/python3.10/dist-packages/torch/_inductor/debug.py", line 304, in inner
W0629 11:52:46.862000 139012379337344 torch/_dynamo/convert_frame.py:824]     return fn(*args, **kwargs)
W0629 11:52:46.862000 139012379337344 torch/_dynamo/convert_frame.py:824]   File "/usr/lib/python3.10/contextlib.py", line 79, in inner
W0629 11:52:46.862000 139012379337344 torch/_dynamo/convert_frame.py:824]     return func(*args, **kwds)
W0629 11:52:46.862000 139012379337344 torch/_dynamo/convert_frame.py:824]   File "/usr/lib/python3.10/contextlib.py", line 79, in inner
W0629 11:52:46.862000 139012379337344 torch/_dynamo/convert_frame.py:824]     return func(*args, **kwds)
W0629 11:52:46.862000 139012379337344 torch/_dynamo/convert_frame.py:824]   File "/usr/local/lib/python3.10/dist-packages/torch/_dynamo/utils.py", line 262, in time_wrapper
W0629 11:52:46.862000 139012379337344 torch/_dynamo/convert_frame.py:824]     r = func(*args, **kwargs)
W0629 11:52:46.862000 139012379337344 torch/_dynamo/convert_frame.py:824]   File "/usr/local/lib/python3.10/dist-packages/torch/_inductor/compile_fx.py", line 438, in compile_fx_inner
W0629 11:52:46.862000 139012379337344 torch/_dynamo/convert_frame.py:824]     compiled_graph = fx_codegen_and_compile(
W0629 11:52:46.862000 139012379337344 torch/_dynamo/convert_frame.py:824]   File "/usr/local/lib/python3.10/dist-packages/torch/_inductor/compile_fx.py", line 714, in fx_codegen_and_compile
W0629 11:52:46.862000 139012379337344 torch/_dynamo/convert_frame.py:824]     compiled_fn = graph.compile_to_fn()
W0629 11:52:46.862000 139012379337344 torch/_dynamo/convert_frame.py:824]   File "/usr/local/lib/python3.10/dist-packages/torch/_inductor/graph.py", line 1307, in compile_to_fn
W0629 11:52:46.862000 139012379337344 torch/_dynamo/convert_frame.py:824]     return self.compile_to_module().call
W0629 11:52:46.862000 139012379337344 torch/_dynamo/convert_frame.py:824]   File "/usr/local/lib/python3.10/dist-packages/torch/_dynamo/utils.py", line 262, in time_wrapper
W0629 11:52:46.862000 139012379337344 torch/_dynamo/convert_frame.py:824]     r = func(*args, **kwargs)
W0629 11:52:46.862000 139012379337344 torch/_dynamo/convert_frame.py:824]   File "/usr/local/lib/python3.10/dist-packages/torch/_inductor/graph.py", line 1254, in compile_to_module
W0629 11:52:46.862000 139012379337344 torch/_dynamo/convert_frame.py:824]     mod = PyCodeCache.load_by_key_path(
W0629 11:52:46.862000 139012379337344 torch/_dynamo/convert_frame.py:824]   File "/usr/local/lib/python3.10/dist-packages/torch/_inductor/codecache.py", line 2160, in load_by_key_path
W0629 11:52:46.862000 139012379337344 torch/_dynamo/convert_frame.py:824]     exec(code, mod.__dict__, mod.__dict__)
W0629 11:52:46.862000 139012379337344 torch/_dynamo/convert_frame.py:824]   File "/tmp/torchinductor_root/6z/c6z2dcicth7eilpxfn7syq26exha2mdgvmxozqrflnt23pjw6r3s.py", line 652, in <module>
W0629 11:52:46.862000 139012379337344 torch/_dynamo/convert_frame.py:824]     async_compile.wait(globals())
W0629 11:52:46.862000 139012379337344 torch/_dynamo/convert_frame.py:824]   File "/usr/local/lib/python3.10/dist-packages/torch/_inductor/codecache.py", line 2715, in wait
W0629 11:52:46.862000 139012379337344 torch/_dynamo/convert_frame.py:824]     scope[key] = result.result()
W0629 11:52:46.862000 139012379337344 torch/_dynamo/convert_frame.py:824]   File "/usr/local/lib/python3.10/dist-packages/torch/_inductor/codecache.py", line 2522, in result
W0629 11:52:46.862000 139012379337344 torch/_dynamo/convert_frame.py:824]     self.future.result()
W0629 11:52:46.862000 139012379337344 torch/_dynamo/convert_frame.py:824]   File "/usr/lib/python3.10/concurrent/futures/_base.py", line 458, in result
W0629 11:52:46.862000 139012379337344 torch/_dynamo/convert_frame.py:824]     return self.__get_result()
W0629 11:52:46.862000 139012379337344 torch/_dynamo/convert_frame.py:824]   File "/usr/lib/python3.10/concurrent/futures/_base.py", line 403, in __get_result
W0629 11:52:46.862000 139012379337344 torch/_dynamo/convert_frame.py:824]     raise self._exception
W0629 11:52:46.862000 139012379337344 torch/_dynamo/convert_frame.py:824] torch._dynamo.exc.BackendCompilerFailed: backend='inductor' raised:
W0629 11:52:46.862000 139012379337344 torch/_dynamo/convert_frame.py:824] RuntimeError: Internal Triton PTX codegen error:
W0629 11:52:46.862000 139012379337344 torch/_dynamo/convert_frame.py:824] ptxas /tmp/compile-ptx-src-52b2ff, line 63; error   : Feature '.bf16' requires .target sm_80 or higher
W0629 11:52:46.862000 139012379337344 torch/_dynamo/convert_frame.py:824] ptxas /tmp/compile-ptx-src-52b2ff, line 63; error   : Feature 'cvt.bf16.f32' requires .target sm_80 or higher
W0629 11:52:46.862000 139012379337344 torch/_dynamo/convert_frame.py:824] ptxas /tmp/compile-ptx-src-52b2ff, line 64; error   : Feature '.bf16' requires .target sm_80 or higher
W0629 11:52:46.862000 139012379337344 torch/_dynamo/convert_frame.py:824] ptxas /tmp/compile-ptx-src-52b2ff, line 64; error   : Feature 'cvt.bf16.f32' requires .target sm_80 or higher
W0629 11:52:46.862000 139012379337344 torch/_dynamo/convert_frame.py:824] ptxas /tmp/compile-ptx-src-52b2ff, line 65; error   : Feature '.bf16' requires .target sm_80 or higher
W0629 11:52:46.862000 139012379337344 torch/_dynamo/convert_frame.py:824] ptxas /tmp/compile-ptx-src-52b2ff, line 65; error   : Feature 'cvt.bf16.f32' requires .target sm_80 or higher
W0629 11:52:46.862000 139012379337344 torch/_dynamo/convert_frame.py:824] ptxas /tmp/compile-ptx-src-52b2ff, line 66; error   : Feature '.bf16' requires .target sm_80 or higher
W0629 11:52:46.862000 139012379337344 torch/_dynamo/convert_frame.py:824] ptxas /tmp/compile-ptx-src-52b2ff, line 66; error   : Feature 'cvt.bf16.f32' requires .target sm_80 or higher
W0629 11:52:46.862000 139012379337344 torch/_dynamo/convert_frame.py:824] ptxas /tmp/compile-ptx-src-52b2ff, line 67; error   : Feature '.bf16' requires .target sm_80 or higher
W0629 11:52:46.862000 139012379337344 torch/_dynamo/convert_frame.py:824] ptxas /tmp/compile-ptx-src-52b2ff, line 67; error   : Feature 'cvt.bf16.f32' requires .target sm_80 or higher
W0629 11:52:46.862000 139012379337344 torch/_dynamo/convert_frame.py:824] ptxas /tmp/compile-ptx-src-52b2ff, line 68; error   : Feature '.bf16' requires .target sm_80 or higher
W0629 11:52:46.862000 139012379337344 torch/_dynamo/convert_frame.py:824] ptxas /tmp/compile-ptx-src-52b2ff, line 68; error   : Feature 'cvt.bf16.f32' requires .target sm_80 or higher
W0629 11:52:46.862000 139012379337344 torch/_dynamo/convert_frame.py:824] ptxas /tmp/compile-ptx-src-52b2ff, line 69; error   : Feature '.bf16' requires .target sm_80 or higher
W0629 11:52:46.862000 139012379337344 torch/_dynamo/convert_frame.py:824] ptxas /tmp/compile-ptx-src-52b2ff, line 69; error   : Feature 'cvt.bf16.f32' requires .target sm_80 or higher
W0629 11:52:46.862000 139012379337344 torch/_dynamo/convert_frame.py:824] ptxas /tmp/compile-ptx-src-52b2ff, line 70; error   : Feature '.bf16' requires .target sm_80 or higher
W0629 11:52:46.862000 139012379337344 torch/_dynamo/convert_frame.py:824] ptxas /tmp/compile-ptx-src-52b2ff, line 70; error   : Feature 'cvt.bf16.f32' requires .target sm_80 or higher
W0629 11:52:46.862000 139012379337344 torch/_dynamo/convert_frame.py:824] ptxas fatal   : Ptx assembly aborted due to errors
W0629 11:52:46.862000 139012379337344 torch/_dynamo/convert_frame.py:824]
W0629 11:52:46.862000 139012379337344 torch/_dynamo/convert_frame.py:824]
W0629 11:52:46.862000 139012379337344 torch/_dynamo/convert_frame.py:824] Set TORCH_LOGS="+dynamo" and TORCHDYNAMO_VERBOSE=1 for more information
W0629 11:52:46.862000 139012379337344 torch/_dynamo/convert_frame.py:824]
W0629 11:52:46.862000 139012379337344 torch/_dynamo/convert_frame.py:824] Traceback (most recent call last):
W0629 11:52:46.862000 139012379337344 torch/_dynamo/convert_frame.py:824]   File "/usr/local/lib/python3.10/dist-packages/torch/_dynamo/convert_frame.py", line 786, in _convert_frame
W0629 11:52:46.862000 139012379337344 torch/_dynamo/convert_frame.py:824]     result = inner_convert(
W0629 11:52:46.862000 139012379337344 torch/_dynamo/convert_frame.py:824]   File "/usr/local/lib/python3.10/dist-packages/torch/_dynamo/convert_frame.py", line 400, in _convert_frame_assert
W0629 11:52:46.862000 139012379337344 torch/_dynamo/convert_frame.py:824]     return _compile(
W0629 11:52:46.862000 139012379337344 torch/_dynamo/convert_frame.py:824]   File "/usr/lib/python3.10/contextlib.py", line 79, in inner
W0629 11:52:46.862000 139012379337344 torch/_dynamo/convert_frame.py:824]     return func(*args, **kwds)
W0629 11:52:46.862000 139012379337344 torch/_dynamo/convert_frame.py:824]   File "/usr/local/lib/python3.10/dist-packages/torch/_dynamo/convert_frame.py", line 676, in _compile
W0629 11:52:46.862000 139012379337344 torch/_dynamo/convert_frame.py:824]     guarded_code = compile_inner(code, one_graph, hooks, transform)
W0629 11:52:46.862000 139012379337344 torch/_dynamo/convert_frame.py:824]   File "/usr/local/lib/python3.10/dist-packages/torch/_dynamo/utils.py", line 262, in time_wrapper
W0629 11:52:46.862000 139012379337344 torch/_dynamo/convert_frame.py:824]     r = func(*args, **kwargs)
W0629 11:52:46.862000 139012379337344 torch/_dynamo/convert_frame.py:824]   File "/usr/local/lib/python3.10/dist-packages/torch/_dynamo/convert_frame.py", line 535, in compile_inner
W0629 11:52:46.862000 139012379337344 torch/_dynamo/convert_frame.py:824]     out_code = transform_code_object(code, transform)
W0629 11:52:46.862000 139012379337344 torch/_dynamo/convert_frame.py:824]   File "/usr/local/lib/python3.10/dist-packages/torch/_dynamo/bytecode_transformation.py", line 1036, in transform_code_object
W0629 11:52:46.862000 139012379337344 torch/_dynamo/convert_frame.py:824]     transformations(instructions, code_options)
W0629 11:52:46.862000 139012379337344 torch/_dynamo/convert_frame.py:824]   File "/usr/local/lib/python3.10/dist-packages/torch/_dynamo/convert_frame.py", line 165, in _fn
W0629 11:52:46.862000 139012379337344 torch/_dynamo/convert_frame.py:824]     return fn(*args, **kwargs)
W0629 11:52:46.862000 139012379337344 torch/_dynamo/convert_frame.py:824]   File "/usr/local/lib/python3.10/dist-packages/torch/_dynamo/convert_frame.py", line 500, in transform
W0629 11:52:46.862000 139012379337344 torch/_dynamo/convert_frame.py:824]     tracer.run()
W0629 11:52:46.862000 139012379337344 torch/_dynamo/convert_frame.py:824]   File "/usr/local/lib/python3.10/dist-packages/torch/_dynamo/symbolic_convert.py", line 2149, in run
W0629 11:52:46.862000 139012379337344 torch/_dynamo/convert_frame.py:824]     super().run()
W0629 11:52:46.862000 139012379337344 torch/_dynamo/convert_frame.py:824]   File "/usr/local/lib/python3.10/dist-packages/torch/_dynamo/symbolic_convert.py", line 810, in run
W0629 11:52:46.862000 139012379337344 torch/_dynamo/convert_frame.py:824]     and self.step()
W0629 11:52:46.862000 139012379337344 torch/_dynamo/convert_frame.py:824]   File "/usr/local/lib/python3.10/dist-packages/torch/_dynamo/symbolic_convert.py", line 773, in step
W0629 11:52:46.862000 139012379337344 torch/_dynamo/convert_frame.py:824]     getattr(self, inst.opname)(inst)
W0629 11:52:46.862000 139012379337344 torch/_dynamo/convert_frame.py:824]   File "/usr/local/lib/python3.10/dist-packages/torch/_dynamo/symbolic_convert.py", line 2268, in RETURN_VALUE
W0629 11:52:46.862000 139012379337344 torch/_dynamo/convert_frame.py:824]     self.output.compile_subgraph(
W0629 11:52:46.862000 139012379337344 torch/_dynamo/convert_frame.py:824]   File "/usr/local/lib/python3.10/dist-packages/torch/_dynamo/output_graph.py", line 991, in compile_subgraph
W0629 11:52:46.862000 139012379337344 torch/_dynamo/convert_frame.py:824]     self.compile_and_call_fx_graph(tx, pass2.graph_output_vars(), root)
W0629 11:52:46.862000 139012379337344 torch/_dynamo/convert_frame.py:824]   File "/usr/lib/python3.10/contextlib.py", line 79, in inner
W0629 11:52:46.862000 139012379337344 torch/_dynamo/convert_frame.py:824]     return func(*args, **kwds)
W0629 11:52:46.862000 139012379337344 torch/_dynamo/convert_frame.py:824]   File "/usr/local/lib/python3.10/dist-packages/torch/_dynamo/output_graph.py", line 1168, in compile_and_call_fx_graph
W0629 11:52:46.862000 139012379337344 torch/_dynamo/convert_frame.py:824]     compiled_fn = self.call_user_compiler(gm)
W0629 11:52:46.862000 139012379337344 torch/_dynamo/convert_frame.py:824]   File "/usr/local/lib/python3.10/dist-packages/torch/_dynamo/utils.py", line 262, in time_wrapper
W0629 11:52:46.862000 139012379337344 torch/_dynamo/convert_frame.py:824]     r = func(*args, **kwargs)
W0629 11:52:46.862000 139012379337344 torch/_dynamo/convert_frame.py:824]   File "/usr/local/lib/python3.10/dist-packages/torch/_dynamo/output_graph.py", line 1241, in call_user_compiler
W0629 11:52:46.862000 139012379337344 torch/_dynamo/convert_frame.py:824]     raise BackendCompilerFailed(self.compiler_fn, e).with_traceback(
W0629 11:52:46.862000 139012379337344 torch/_dynamo/convert_frame.py:824]   File "/usr/local/lib/python3.10/dist-packages/torch/_dynamo/output_graph.py", line 1222, in call_user_compiler
W0629 11:52:46.862000 139012379337344 torch/_dynamo/convert_frame.py:824]     compiled_fn = compiler_fn(gm, self.example_inputs())
W0629 11:52:46.862000 139012379337344 torch/_dynamo/convert_frame.py:824]   File "/usr/local/lib/python3.10/dist-packages/torch/_dynamo/repro/after_dynamo.py", line 117, in debug_wrapper
W0629 11:52:46.862000 139012379337344 torch/_dynamo/convert_frame.py:824]     compiled_gm = compiler_fn(gm, example_inputs)
W0629 11:52:46.862000 139012379337344 torch/_dynamo/convert_frame.py:824]   File "/usr/local/lib/python3.10/dist-packages/torch/__init__.py", line 1729, in __call__
W0629 11:52:46.862000 139012379337344 torch/_dynamo/convert_frame.py:824]     return compile_fx(model_, inputs_, config_patches=self.config)
W0629 11:52:46.862000 139012379337344 torch/_dynamo/convert_frame.py:824]   File "/usr/lib/python3.10/contextlib.py", line 79, in inner
W0629 11:52:46.862000 139012379337344 torch/_dynamo/convert_frame.py:824]     return func(*args, **kwds)
W0629 11:52:46.862000 139012379337344 torch/_dynamo/convert_frame.py:824]   File "/usr/local/lib/python3.10/dist-packages/torch/_inductor/compile_fx.py", line 1330, in compile_fx
W0629 11:52:46.862000 139012379337344 torch/_dynamo/convert_frame.py:824]     return aot_autograd(
W0629 11:52:46.862000 139012379337344 torch/_dynamo/convert_frame.py:824]   File "/usr/local/lib/python3.10/dist-packages/torch/_dynamo/backends/common.py", line 58, in compiler_fn
W0629 11:52:46.862000 139012379337344 torch/_dynamo/convert_frame.py:824]     cg = aot_module_simplified(gm, example_inputs, **kwargs)
W0629 11:52:46.862000 139012379337344 torch/_dynamo/convert_frame.py:824]   File "/usr/local/lib/python3.10/dist-packages/torch/_functorch/aot_autograd.py", line 903, in aot_module_simplified
W0629 11:52:46.862000 139012379337344 torch/_dynamo/convert_frame.py:824]     compiled_fn = create_aot_dispatcher_function(
W0629 11:52:46.862000 139012379337344 torch/_dynamo/convert_frame.py:824]   File "/usr/local/lib/python3.10/dist-packages/torch/_dynamo/utils.py", line 262, in time_wrapper
W0629 11:52:46.862000 139012379337344 torch/_dynamo/convert_frame.py:824]     r = func(*args, **kwargs)
W0629 11:52:46.862000 139012379337344 torch/_dynamo/convert_frame.py:824]   File "/usr/local/lib/python3.10/dist-packages/torch/_functorch/aot_autograd.py", line 628, in create_aot_dispatcher_function
W0629 11:52:46.862000 139012379337344 torch/_dynamo/convert_frame.py:824]     compiled_fn = compiler_fn(flat_fn, fake_flat_args, aot_config, fw_metadata=fw_metadata)
W0629 11:52:46.862000 139012379337344 torch/_dynamo/convert_frame.py:824]   File "/usr/local/lib/python3.10/dist-packages/torch/_functorch/_aot_autograd/runtime_wrappers.py", line 443, in aot_wrapper_dedupe
W0629 11:52:46.862000 139012379337344 torch/_dynamo/convert_frame.py:824]     return compiler_fn(flat_fn, leaf_flat_args, aot_config, fw_metadata=fw_metadata)
W0629 11:52:46.862000 139012379337344 torch/_dynamo/convert_frame.py:824]   File "/usr/local/lib/python3.10/dist-packages/torch/_functorch/_aot_autograd/runtime_wrappers.py", line 648, in aot_wrapper_synthetic_base
W0629 11:52:46.862000 139012379337344 torch/_dynamo/convert_frame.py:824]     return compiler_fn(flat_fn, flat_args, aot_config, fw_metadata=fw_metadata)
W0629 11:52:46.862000 139012379337344 torch/_dynamo/convert_frame.py:824]   File "/usr/local/lib/python3.10/dist-packages/torch/_functorch/_aot_autograd/jit_compile_runtime_wrappers.py", line 119, in aot_dispatch_base
W0629 11:52:46.862000 139012379337344 torch/_dynamo/convert_frame.py:824]     compiled_fw = compiler(fw_module, updated_flat_args)
W0629 11:52:46.862000 139012379337344 torch/_dynamo/convert_frame.py:824]   File "/usr/local/lib/python3.10/dist-packages/torch/_dynamo/utils.py", line 262, in time_wrapper
W0629 11:52:46.862000 139012379337344 torch/_dynamo/convert_frame.py:824]     r = func(*args, **kwargs)
W0629 11:52:46.862000 139012379337344 torch/_dynamo/convert_frame.py:824]   File "/usr/local/lib/python3.10/dist-packages/torch/_inductor/compile_fx.py", line 1257, in fw_compiler_base
W0629 11:52:46.862000 139012379337344 torch/_dynamo/convert_frame.py:824]     return inner_compile(
W0629 11:52:46.862000 139012379337344 torch/_dynamo/convert_frame.py:824]   File "/usr/local/lib/python3.10/dist-packages/torch/_dynamo/repro/after_aot.py", line 83, in debug_wrapper
W0629 11:52:46.862000 139012379337344 torch/_dynamo/convert_frame.py:824]     inner_compiled_fn = compiler_fn(gm, example_inputs)
W0629 11:52:46.862000 139012379337344 torch/_dynamo/convert_frame.py:824]   File "/usr/local/lib/python3.10/dist-packages/torch/_inductor/debug.py", line 304, in inner
W0629 11:52:46.862000 139012379337344 torch/_dynamo/convert_frame.py:824]     return fn(*args, **kwargs)
W0629 11:52:46.862000 139012379337344 torch/_dynamo/convert_frame.py:824]   File "/usr/lib/python3.10/contextlib.py", line 79, in inner
W0629 11:52:46.862000 139012379337344 torch/_dynamo/convert_frame.py:824]     return func(*args, **kwds)
W0629 11:52:46.862000 139012379337344 torch/_dynamo/convert_frame.py:824]   File "/usr/lib/python3.10/contextlib.py", line 79, in inner
W0629 11:52:46.862000 139012379337344 torch/_dynamo/convert_frame.py:824]     return func(*args, **kwds)
W0629 11:52:46.862000 139012379337344 torch/_dynamo/convert_frame.py:824]   File "/usr/local/lib/python3.10/dist-packages/torch/_dynamo/utils.py", line 262, in time_wrapper
W0629 11:52:46.862000 139012379337344 torch/_dynamo/convert_frame.py:824]     r = func(*args, **kwargs)
W0629 11:52:46.862000 139012379337344 torch/_dynamo/convert_frame.py:824]   File "/usr/local/lib/python3.10/dist-packages/torch/_inductor/compile_fx.py", line 438, in compile_fx_inner
W0629 11:52:46.862000 139012379337344 torch/_dynamo/convert_frame.py:824]     compiled_graph = fx_codegen_and_compile(
W0629 11:52:46.862000 139012379337344 torch/_dynamo/convert_frame.py:824]   File "/usr/local/lib/python3.10/dist-packages/torch/_inductor/compile_fx.py", line 714, in fx_codegen_and_compile
W0629 11:52:46.862000 139012379337344 torch/_dynamo/convert_frame.py:824]     compiled_fn = graph.compile_to_fn()
W0629 11:52:46.862000 139012379337344 torch/_dynamo/convert_frame.py:824]   File "/usr/local/lib/python3.10/dist-packages/torch/_inductor/graph.py", line 1307, in compile_to_fn
W0629 11:52:46.862000 139012379337344 torch/_dynamo/convert_frame.py:824]     return self.compile_to_module().call
W0629 11:52:46.862000 139012379337344 torch/_dynamo/convert_frame.py:824]   File "/usr/local/lib/python3.10/dist-packages/torch/_dynamo/utils.py", line 262, in time_wrapper
W0629 11:52:46.862000 139012379337344 torch/_dynamo/convert_frame.py:824]     r = func(*args, **kwargs)
W0629 11:52:46.862000 139012379337344 torch/_dynamo/convert_frame.py:824]   File "/usr/local/lib/python3.10/dist-packages/torch/_inductor/graph.py", line 1254, in compile_to_module
W0629 11:52:46.862000 139012379337344 torch/_dynamo/convert_frame.py:824]     mod = PyCodeCache.load_by_key_path(
W0629 11:52:46.862000 139012379337344 torch/_dynamo/convert_frame.py:824]   File "/usr/local/lib/python3.10/dist-packages/torch/_inductor/codecache.py", line 2160, in load_by_key_path
W0629 11:52:46.862000 139012379337344 torch/_dynamo/convert_frame.py:824]     exec(code, mod.__dict__, mod.__dict__)
W0629 11:52:46.862000 139012379337344 torch/_dynamo/convert_frame.py:824]   File "/tmp/torchinductor_root/6z/c6z2dcicth7eilpxfn7syq26exha2mdgvmxozqrflnt23pjw6r3s.py", line 652, in <module>
W0629 11:52:46.862000 139012379337344 torch/_dynamo/convert_frame.py:824]     async_compile.wait(globals())
W0629 11:52:46.862000 139012379337344 torch/_dynamo/convert_frame.py:824]   File "/usr/local/lib/python3.10/dist-packages/torch/_inductor/codecache.py", line 2715, in wait
W0629 11:52:46.862000 139012379337344 torch/_dynamo/convert_frame.py:824]     scope[key] = result.result()
W0629 11:52:46.862000 139012379337344 torch/_dynamo/convert_frame.py:824]   File "/usr/local/lib/python3.10/dist-packages/torch/_inductor/codecache.py", line 2522, in result
W0629 11:52:46.862000 139012379337344 torch/_dynamo/convert_frame.py:824]     self.future.result()
W0629 11:52:46.862000 139012379337344 torch/_dynamo/convert_frame.py:824]   File "/usr/lib/python3.10/concurrent/futures/_base.py", line 458, in result
W0629 11:52:46.862000 139012379337344 torch/_dynamo/convert_frame.py:824]     return self.__get_result()
W0629 11:52:46.862000 139012379337344 torch/_dynamo/convert_frame.py:824]   File "/usr/lib/python3.10/concurrent/futures/_base.py", line 403, in __get_result
W0629 11:52:46.862000 139012379337344 torch/_dynamo/convert_frame.py:824]     raise self._exception
W0629 11:52:46.862000 139012379337344 torch/_dynamo/convert_frame.py:824] torch._dynamo.exc.BackendCompilerFailed: backend='inductor' raised:
W0629 11:52:46.862000 139012379337344 torch/_dynamo/convert_frame.py:824] RuntimeError: Internal Triton PTX codegen error:
W0629 11:52:46.862000 139012379337344 torch/_dynamo/convert_frame.py:824] ptxas /tmp/compile-ptx-src-52b2ff, line 63; error   : Feature '.bf16' requires .target sm_80 or higher
W0629 11:52:46.862000 139012379337344 torch/_dynamo/convert_frame.py:824] ptxas /tmp/compile-ptx-src-52b2ff, line 63; error   : Feature 'cvt.bf16.f32' requires .target sm_80 or higher
W0629 11:52:46.862000 139012379337344 torch/_dynamo/convert_frame.py:824] ptxas /tmp/compile-ptx-src-52b2ff, line 64; error   : Feature '.bf16' requires .target sm_80 or higher
W0629 11:52:46.862000 139012379337344 torch/_dynamo/convert_frame.py:824] ptxas /tmp/compile-ptx-src-52b2ff, line 64; error   : Feature 'cvt.bf16.f32' requires .target sm_80 or higher
W0629 11:52:46.862000 139012379337344 torch/_dynamo/convert_frame.py:824] ptxas /tmp/compile-ptx-src-52b2ff, line 65; error   : Feature '.bf16' requires .target sm_80 or higher
W0629 11:52:46.862000 139012379337344 torch/_dynamo/convert_frame.py:824] ptxas /tmp/compile-ptx-src-52b2ff, line 65; error   : Feature 'cvt.bf16.f32' requires .target sm_80 or higher
W0629 11:52:46.862000 139012379337344 torch/_dynamo/convert_frame.py:824] ptxas /tmp/compile-ptx-src-52b2ff, line 66; error   : Feature '.bf16' requires .target sm_80 or higher
W0629 11:52:46.862000 139012379337344 torch/_dynamo/convert_frame.py:824] ptxas /tmp/compile-ptx-src-52b2ff, line 66; error   : Feature 'cvt.bf16.f32' requires .target sm_80 or higher
W0629 11:52:46.862000 139012379337344 torch/_dynamo/convert_frame.py:824] ptxas /tmp/compile-ptx-src-52b2ff, line 67; error   : Feature '.bf16' requires .target sm_80 or higher
W0629 11:52:46.862000 139012379337344 torch/_dynamo/convert_frame.py:824] ptxas /tmp/compile-ptx-src-52b2ff, line 67; error   : Feature 'cvt.bf16.f32' requires .target sm_80 or higher
W0629 11:52:46.862000 139012379337344 torch/_dynamo/convert_frame.py:824] ptxas /tmp/compile-ptx-src-52b2ff, line 68; error   : Feature '.bf16' requires .target sm_80 or higher
W0629 11:52:46.862000 139012379337344 torch/_dynamo/convert_frame.py:824] ptxas /tmp/compile-ptx-src-52b2ff, line 68; error   : Feature 'cvt.bf16.f32' requires .target sm_80 or higher
W0629 11:52:46.862000 139012379337344 torch/_dynamo/convert_frame.py:824] ptxas /tmp/compile-ptx-src-52b2ff, line 69; error   : Feature '.bf16' requires .target sm_80 or higher
W0629 11:52:46.862000 139012379337344 torch/_dynamo/convert_frame.py:824] ptxas /tmp/compile-ptx-src-52b2ff, line 69; error   : Feature 'cvt.bf16.f32' requires .target sm_80 or higher
W0629 11:52:46.862000 139012379337344 torch/_dynamo/convert_frame.py:824] ptxas /tmp/compile-ptx-src-52b2ff, line 70; error   : Feature '.bf16' requires .target sm_80 or higher
W0629 11:52:46.862000 139012379337344 torch/_dynamo/convert_frame.py:824] ptxas /tmp/compile-ptx-src-52b2ff, line 70; error   : Feature 'cvt.bf16.f32' requires .target sm_80 or higher
W0629 11:52:46.862000 139012379337344 torch/_dynamo/convert_frame.py:824] ptxas fatal   : Ptx assembly aborted due to errors
W0629 11:52:46.862000 139012379337344 torch/_dynamo/convert_frame.py:824]
W0629 11:52:46.862000 139012379337344 torch/_dynamo/convert_frame.py:824]
W0629 11:52:46.862000 139012379337344 torch/_dynamo/convert_frame.py:824] Set TORCH_LOGS="+dynamo" and TORCHDYNAMO_VERBOSE=1 for more information
W0629 11:52:46.862000 139012379337344 torch/_dynamo/convert_frame.py:824]
W0629 11:52:48.016000 139012379337344 torch/_dynamo/convert_frame.py:824] WON'T CONVERT forward /usr/local/lib/python3.10/dist-packages/transformers/models/bert/modeling_bert.py line 568
W0629 11:52:48.016000 139012379337344 torch/_dynamo/convert_frame.py:824] due to:
W0629 11:52:48.016000 139012379337344 torch/_dynamo/convert_frame.py:824] Traceback (most recent call last):
W0629 11:52:48.016000 139012379337344 torch/_dynamo/convert_frame.py:824]   File "/usr/local/lib/python3.10/dist-packages/torch/_dynamo/convert_frame.py", line 786, in _convert_frame
W0629 11:52:48.016000 139012379337344 torch/_dynamo/convert_frame.py:824]     result = inner_convert(
W0629 11:52:48.016000 139012379337344 torch/_dynamo/convert_frame.py:824]   File "/usr/local/lib/python3.10/dist-packages/torch/_dynamo/convert_frame.py", line 400, in _convert_frame_assert
W0629 11:52:48.016000 139012379337344 torch/_dynamo/convert_frame.py:824]     return _compile(
W0629 11:52:48.016000 139012379337344 torch/_dynamo/convert_frame.py:824]   File "/usr/lib/python3.10/contextlib.py", line 79, in inner
W0629 11:52:48.016000 139012379337344 torch/_dynamo/convert_frame.py:824]     return func(*args, **kwds)
W0629 11:52:48.016000 139012379337344 torch/_dynamo/convert_frame.py:824]   File "/usr/local/lib/python3.10/dist-packages/torch/_dynamo/convert_frame.py", line 676, in _compile
W0629 11:52:48.016000 139012379337344 torch/_dynamo/convert_frame.py:824]     guarded_code = compile_inner(code, one_graph, hooks, transform)
W0629 11:52:48.016000 139012379337344 torch/_dynamo/convert_frame.py:824]   File "/usr/local/lib/python3.10/dist-packages/torch/_dynamo/utils.py", line 262, in time_wrapper
W0629 11:52:48.016000 139012379337344 torch/_dynamo/convert_frame.py:824]     r = func(*args, **kwargs)
W0629 11:52:48.016000 139012379337344 torch/_dynamo/convert_frame.py:824]   File "/usr/local/lib/python3.10/dist-packages/torch/_dynamo/convert_frame.py", line 535, in compile_inner
W0629 11:52:48.016000 139012379337344 torch/_dynamo/convert_frame.py:824]     out_code = transform_code_object(code, transform)
W0629 11:52:48.016000 139012379337344 torch/_dynamo/convert_frame.py:824]   File "/usr/local/lib/python3.10/dist-packages/torch/_dynamo/bytecode_transformation.py", line 1036, in transform_code_object
W0629 11:52:48.016000 139012379337344 torch/_dynamo/convert_frame.py:824]     transformations(instructions, code_options)
W0629 11:52:48.016000 139012379337344 torch/_dynamo/convert_frame.py:824]   File "/usr/local/lib/python3.10/dist-packages/torch/_dynamo/convert_frame.py", line 165, in _fn
W0629 11:52:48.016000 139012379337344 torch/_dynamo/convert_frame.py:824]     return fn(*args, **kwargs)
W0629 11:52:48.016000 139012379337344 torch/_dynamo/convert_frame.py:824]   File "/usr/local/lib/python3.10/dist-packages/torch/_dynamo/convert_frame.py", line 500, in transform
W0629 11:52:48.016000 139012379337344 torch/_dynamo/convert_frame.py:824]     tracer.run()
W0629 11:52:48.016000 139012379337344 torch/_dynamo/convert_frame.py:824]   File "/usr/local/lib/python3.10/dist-packages/torch/_dynamo/symbolic_convert.py", line 2149, in run
W0629 11:52:48.016000 139012379337344 torch/_dynamo/convert_frame.py:824]     super().run()
W0629 11:52:48.016000 139012379337344 torch/_dynamo/convert_frame.py:824]   File "/usr/local/lib/python3.10/dist-packages/torch/_dynamo/symbolic_convert.py", line 810, in run
W0629 11:52:48.016000 139012379337344 torch/_dynamo/convert_frame.py:824]     and self.step()
W0629 11:52:48.016000 139012379337344 torch/_dynamo/convert_frame.py:824]   File "/usr/local/lib/python3.10/dist-packages/torch/_dynamo/symbolic_convert.py", line 773, in step
W0629 11:52:48.016000 139012379337344 torch/_dynamo/convert_frame.py:824]     getattr(self, inst.opname)(inst)
W0629 11:52:48.016000 139012379337344 torch/_dynamo/convert_frame.py:824]   File "/usr/local/lib/python3.10/dist-packages/torch/_dynamo/symbolic_convert.py", line 2268, in RETURN_VALUE
W0629 11:52:48.016000 139012379337344 torch/_dynamo/convert_frame.py:824]     self.output.compile_subgraph(
W0629 11:52:48.016000 139012379337344 torch/_dynamo/convert_frame.py:824]   File "/usr/local/lib/python3.10/dist-packages/torch/_dynamo/output_graph.py", line 991, in compile_subgraph
W0629 11:52:48.016000 139012379337344 torch/_dynamo/convert_frame.py:824]     self.compile_and_call_fx_graph(tx, pass2.graph_output_vars(), root)
W0629 11:52:48.016000 139012379337344 torch/_dynamo/convert_frame.py:824]   File "/usr/lib/python3.10/contextlib.py", line 79, in inner
W0629 11:52:48.016000 139012379337344 torch/_dynamo/convert_frame.py:824]     return func(*args, **kwds)
W0629 11:52:48.016000 139012379337344 torch/_dynamo/convert_frame.py:824]   File "/usr/local/lib/python3.10/dist-packages/torch/_dynamo/output_graph.py", line 1168, in compile_and_call_fx_graph
W0629 11:52:48.016000 139012379337344 torch/_dynamo/convert_frame.py:824]     compiled_fn = self.call_user_compiler(gm)
W0629 11:52:48.016000 139012379337344 torch/_dynamo/convert_frame.py:824]   File "/usr/local/lib/python3.10/dist-packages/torch/_dynamo/utils.py", line 262, in time_wrapper
W0629 11:52:48.016000 139012379337344 torch/_dynamo/convert_frame.py:824]     r = func(*args, **kwargs)
W0629 11:52:48.016000 139012379337344 torch/_dynamo/convert_frame.py:824]   File "/usr/local/lib/python3.10/dist-packages/torch/_dynamo/output_graph.py", line 1241, in call_user_compiler
W0629 11:52:48.016000 139012379337344 torch/_dynamo/convert_frame.py:824]     raise BackendCompilerFailed(self.compiler_fn, e).with_traceback(
W0629 11:52:48.016000 139012379337344 torch/_dynamo/convert_frame.py:824]   File "/usr/local/lib/python3.10/dist-packages/torch/_dynamo/output_graph.py", line 1222, in call_user_compiler
W0629 11:52:48.016000 139012379337344 torch/_dynamo/convert_frame.py:824]     compiled_fn = compiler_fn(gm, self.example_inputs())
W0629 11:52:48.016000 139012379337344 torch/_dynamo/convert_frame.py:824]   File "/usr/local/lib/python3.10/dist-packages/torch/_dynamo/repro/after_dynamo.py", line 117, in debug_wrapper
W0629 11:52:48.016000 139012379337344 torch/_dynamo/convert_frame.py:824]     compiled_gm = compiler_fn(gm, example_inputs)
W0629 11:52:48.016000 139012379337344 torch/_dynamo/convert_frame.py:824]   File "/usr/local/lib/python3.10/dist-packages/torch/__init__.py", line 1729, in __call__
W0629 11:52:48.016000 139012379337344 torch/_dynamo/convert_frame.py:824]     return compile_fx(model_, inputs_, config_patches=self.config)
W0629 11:52:48.016000 139012379337344 torch/_dynamo/convert_frame.py:824]   File "/usr/lib/python3.10/contextlib.py", line 79, in inner
W0629 11:52:48.016000 139012379337344 torch/_dynamo/convert_frame.py:824]     return func(*args, **kwds)
W0629 11:52:48.016000 139012379337344 torch/_dynamo/convert_frame.py:824]   File "/usr/local/lib/python3.10/dist-packages/torch/_inductor/compile_fx.py", line 1330, in compile_fx
W0629 11:52:48.016000 139012379337344 torch/_dynamo/convert_frame.py:824]     return aot_autograd(
W0629 11:52:48.016000 139012379337344 torch/_dynamo/convert_frame.py:824]   File "/usr/local/lib/python3.10/dist-packages/torch/_dynamo/backends/common.py", line 58, in compiler_fn
W0629 11:52:48.016000 139012379337344 torch/_dynamo/convert_frame.py:824]     cg = aot_module_simplified(gm, example_inputs, **kwargs)
W0629 11:52:48.016000 139012379337344 torch/_dynamo/convert_frame.py:824]   File "/usr/local/lib/python3.10/dist-packages/torch/_functorch/aot_autograd.py", line 903, in aot_module_simplified
W0629 11:52:48.016000 139012379337344 torch/_dynamo/convert_frame.py:824]     compiled_fn = create_aot_dispatcher_function(
W0629 11:52:48.016000 139012379337344 torch/_dynamo/convert_frame.py:824]   File "/usr/local/lib/python3.10/dist-packages/torch/_dynamo/utils.py", line 262, in time_wrapper
W0629 11:52:48.016000 139012379337344 torch/_dynamo/convert_frame.py:824]     r = func(*args, **kwargs)
W0629 11:52:48.016000 139012379337344 torch/_dynamo/convert_frame.py:824]   File "/usr/local/lib/python3.10/dist-packages/torch/_functorch/aot_autograd.py", line 628, in create_aot_dispatcher_function
W0629 11:52:48.016000 139012379337344 torch/_dynamo/convert_frame.py:824]     compiled_fn = compiler_fn(flat_fn, fake_flat_args, aot_config, fw_metadata=fw_metadata)
W0629 11:52:48.016000 139012379337344 torch/_dynamo/convert_frame.py:824]   File "/usr/local/lib/python3.10/dist-packages/torch/_functorch/_aot_autograd/runtime_wrappers.py", line 443, in aot_wrapper_dedupe
W0629 11:52:48.016000 139012379337344 torch/_dynamo/convert_frame.py:824]     return compiler_fn(flat_fn, leaf_flat_args, aot_config, fw_metadata=fw_metadata)
W0629 11:52:48.016000 139012379337344 torch/_dynamo/convert_frame.py:824]   File "/usr/local/lib/python3.10/dist-packages/torch/_functorch/_aot_autograd/runtime_wrappers.py", line 648, in aot_wrapper_synthetic_base
W0629 11:52:48.016000 139012379337344 torch/_dynamo/convert_frame.py:824]     return compiler_fn(flat_fn, flat_args, aot_config, fw_metadata=fw_metadata)
W0629 11:52:48.016000 139012379337344 torch/_dynamo/convert_frame.py:824]   File "/usr/local/lib/python3.10/dist-packages/torch/_functorch/_aot_autograd/jit_compile_runtime_wrappers.py", line 119, in aot_dispatch_base
W0629 11:52:48.016000 139012379337344 torch/_dynamo/convert_frame.py:824]     compiled_fw = compiler(fw_module, updated_flat_args)
W0629 11:52:48.016000 139012379337344 torch/_dynamo/convert_frame.py:824]   File "/usr/local/lib/python3.10/dist-packages/torch/_dynamo/utils.py", line 262, in time_wrapper
W0629 11:52:48.016000 139012379337344 torch/_dynamo/convert_frame.py:824]     r = func(*args, **kwargs)
W0629 11:52:48.016000 139012379337344 torch/_dynamo/convert_frame.py:824]   File "/usr/local/lib/python3.10/dist-packages/torch/_inductor/compile_fx.py", line 1257, in fw_compiler_base
W0629 11:52:48.016000 139012379337344 torch/_dynamo/convert_frame.py:824]     return inner_compile(
W0629 11:52:48.016000 139012379337344 torch/_dynamo/convert_frame.py:824]   File "/usr/local/lib/python3.10/dist-packages/torch/_dynamo/repro/after_aot.py", line 83, in debug_wrapper
W0629 11:52:48.016000 139012379337344 torch/_dynamo/convert_frame.py:824]     inner_compiled_fn = compiler_fn(gm, example_inputs)
W0629 11:52:48.016000 139012379337344 torch/_dynamo/convert_frame.py:824]   File "/usr/local/lib/python3.10/dist-packages/torch/_inductor/debug.py", line 304, in inner
W0629 11:52:48.016000 139012379337344 torch/_dynamo/convert_frame.py:824]     return fn(*args, **kwargs)
W0629 11:52:48.016000 139012379337344 torch/_dynamo/convert_frame.py:824]   File "/usr/lib/python3.10/contextlib.py", line 79, in inner
W0629 11:52:48.016000 139012379337344 torch/_dynamo/convert_frame.py:824]     return func(*args, **kwds)
W0629 11:52:48.016000 139012379337344 torch/_dynamo/convert_frame.py:824]   File "/usr/lib/python3.10/contextlib.py", line 79, in inner
W0629 11:52:48.016000 139012379337344 torch/_dynamo/convert_frame.py:824]     return func(*args, **kwds)
W0629 11:52:48.016000 139012379337344 torch/_dynamo/convert_frame.py:824]   File "/usr/local/lib/python3.10/dist-packages/torch/_dynamo/utils.py", line 262, in time_wrapper
W0629 11:52:48.016000 139012379337344 torch/_dynamo/convert_frame.py:824]     r = func(*args, **kwargs)
W0629 11:52:48.016000 139012379337344 torch/_dynamo/convert_frame.py:824]   File "/usr/local/lib/python3.10/dist-packages/torch/_inductor/compile_fx.py", line 438, in compile_fx_inner
W0629 11:52:48.016000 139012379337344 torch/_dynamo/convert_frame.py:824]     compiled_graph = fx_codegen_and_compile(
W0629 11:52:48.016000 139012379337344 torch/_dynamo/convert_frame.py:824]   File "/usr/local/lib/python3.10/dist-packages/torch/_inductor/compile_fx.py", line 714, in fx_codegen_and_compile
W0629 11:52:48.016000 139012379337344 torch/_dynamo/convert_frame.py:824]     compiled_fn = graph.compile_to_fn()
W0629 11:52:48.016000 139012379337344 torch/_dynamo/convert_frame.py:824]   File "/usr/local/lib/python3.10/dist-packages/torch/_inductor/graph.py", line 1307, in compile_to_fn
W0629 11:52:48.016000 139012379337344 torch/_dynamo/convert_frame.py:824]     return self.compile_to_module().call
W0629 11:52:48.016000 139012379337344 torch/_dynamo/convert_frame.py:824]   File "/usr/local/lib/python3.10/dist-packages/torch/_dynamo/utils.py", line 262, in time_wrapper
W0629 11:52:48.016000 139012379337344 torch/_dynamo/convert_frame.py:824]     r = func(*args, **kwargs)
W0629 11:52:48.016000 139012379337344 torch/_dynamo/convert_frame.py:824]   File "/usr/local/lib/python3.10/dist-packages/torch/_inductor/graph.py", line 1254, in compile_to_module
W0629 11:52:48.016000 139012379337344 torch/_dynamo/convert_frame.py:824]     mod = PyCodeCache.load_by_key_path(
W0629 11:52:48.016000 139012379337344 torch/_dynamo/convert_frame.py:824]   File "/usr/local/lib/python3.10/dist-packages/torch/_inductor/codecache.py", line 2160, in load_by_key_path
W0629 11:52:48.016000 139012379337344 torch/_dynamo/convert_frame.py:824]     exec(code, mod.__dict__, mod.__dict__)
W0629 11:52:48.016000 139012379337344 torch/_dynamo/convert_frame.py:824]   File "/tmp/torchinductor_root/cv/ccvlx4s6y54jyy76geigg7o2wsmcyk36de3hp2yfss5a4pcp3f7t.py", line 574, in <module>
W0629 11:52:48.016000 139012379337344 torch/_dynamo/convert_frame.py:824]     async_compile.wait(globals())
W0629 11:52:48.016000 139012379337344 torch/_dynamo/convert_frame.py:824]   File "/usr/local/lib/python3.10/dist-packages/torch/_inductor/codecache.py", line 2715, in wait
W0629 11:52:48.016000 139012379337344 torch/_dynamo/convert_frame.py:824]     scope[key] = result.result()
W0629 11:52:48.016000 139012379337344 torch/_dynamo/convert_frame.py:824]   File "/usr/local/lib/python3.10/dist-packages/torch/_inductor/codecache.py", line 2522, in result
W0629 11:52:48.016000 139012379337344 torch/_dynamo/convert_frame.py:824]     self.future.result()
W0629 11:52:48.016000 139012379337344 torch/_dynamo/convert_frame.py:824]   File "/usr/lib/python3.10/concurrent/futures/_base.py", line 458, in result
W0629 11:52:48.016000 139012379337344 torch/_dynamo/convert_frame.py:824]     return self.__get_result()
W0629 11:52:48.016000 139012379337344 torch/_dynamo/convert_frame.py:824]   File "/usr/lib/python3.10/concurrent/futures/_base.py", line 403, in __get_result
W0629 11:52:48.016000 139012379337344 torch/_dynamo/convert_frame.py:824]     raise self._exception
W0629 11:52:48.016000 139012379337344 torch/_dynamo/convert_frame.py:824] torch._dynamo.exc.BackendCompilerFailed: backend='inductor' raised:
W0629 11:52:48.016000 139012379337344 torch/_dynamo/convert_frame.py:824] RuntimeError: Internal Triton PTX codegen error:
W0629 11:52:48.016000 139012379337344 torch/_dynamo/convert_frame.py:824] ptxas /tmp/compile-ptx-src-72227c, line 63; error   : Feature '.bf16' requires .target sm_80 or higher
W0629 11:52:48.016000 139012379337344 torch/_dynamo/convert_frame.py:824] ptxas /tmp/compile-ptx-src-72227c, line 63; error   : Feature 'cvt.bf16.f32' requires .target sm_80 or higher
W0629 11:52:48.016000 139012379337344 torch/_dynamo/convert_frame.py:824] ptxas /tmp/compile-ptx-src-72227c, line 64; error   : Feature '.bf16' requires .target sm_80 or higher
W0629 11:52:48.016000 139012379337344 torch/_dynamo/convert_frame.py:824] ptxas /tmp/compile-ptx-src-72227c, line 64; error   : Feature 'cvt.bf16.f32' requires .target sm_80 or higher
W0629 11:52:48.016000 139012379337344 torch/_dynamo/convert_frame.py:824] ptxas /tmp/compile-ptx-src-72227c, line 65; error   : Feature '.bf16' requires .target sm_80 or higher
W0629 11:52:48.016000 139012379337344 torch/_dynamo/convert_frame.py:824] ptxas /tmp/compile-ptx-src-72227c, line 65; error   : Feature 'cvt.bf16.f32' requires .target sm_80 or higher
W0629 11:52:48.016000 139012379337344 torch/_dynamo/convert_frame.py:824] ptxas /tmp/compile-ptx-src-72227c, line 66; error   : Feature '.bf16' requires .target sm_80 or higher
W0629 11:52:48.016000 139012379337344 torch/_dynamo/convert_frame.py:824] ptxas /tmp/compile-ptx-src-72227c, line 66; error   : Feature 'cvt.bf16.f32' requires .target sm_80 or higher
W0629 11:52:48.016000 139012379337344 torch/_dynamo/convert_frame.py:824] ptxas /tmp/compile-ptx-src-72227c, line 67; error   : Feature '.bf16' requires .target sm_80 or higher
W0629 11:52:48.016000 139012379337344 torch/_dynamo/convert_frame.py:824] ptxas /tmp/compile-ptx-src-72227c, line 67; error   : Feature 'cvt.bf16.f32' requires .target sm_80 or higher
W0629 11:52:48.016000 139012379337344 torch/_dynamo/convert_frame.py:824] ptxas /tmp/compile-ptx-src-72227c, line 68; error   : Feature '.bf16' requires .target sm_80 or higher
W0629 11:52:48.016000 139012379337344 torch/_dynamo/convert_frame.py:824] ptxas /tmp/compile-ptx-src-72227c, line 68; error   : Feature 'cvt.bf16.f32' requires .target sm_80 or higher
W0629 11:52:48.016000 139012379337344 torch/_dynamo/convert_frame.py:824] ptxas /tmp/compile-ptx-src-72227c, line 69; error   : Feature '.bf16' requires .target sm_80 or higher
W0629 11:52:48.016000 139012379337344 torch/_dynamo/convert_frame.py:824] ptxas /tmp/compile-ptx-src-72227c, line 69; error   : Feature 'cvt.bf16.f32' requires .target sm_80 or higher
W0629 11:52:48.016000 139012379337344 torch/_dynamo/convert_frame.py:824] ptxas /tmp/compile-ptx-src-72227c, line 70; error   : Feature '.bf16' requires .target sm_80 or higher
W0629 11:52:48.016000 139012379337344 torch/_dynamo/convert_frame.py:824] ptxas /tmp/compile-ptx-src-72227c, line 70; error   : Feature 'cvt.bf16.f32' requires .target sm_80 or higher
W0629 11:52:48.016000 139012379337344 torch/_dynamo/convert_frame.py:824] ptxas fatal   : Ptx assembly aborted due to errors
W0629 11:52:48.016000 139012379337344 torch/_dynamo/convert_frame.py:824]
W0629 11:52:48.016000 139012379337344 torch/_dynamo/convert_frame.py:824]
W0629 11:52:48.016000 139012379337344 torch/_dynamo/convert_frame.py:824] Set TORCH_LOGS="+dynamo" and TORCHDYNAMO_VERBOSE=1 for more information
W0629 11:52:48.016000 139012379337344 torch/_dynamo/convert_frame.py:824]
W0629 11:52:48.016000 139012379337344 torch/_dynamo/convert_frame.py:824] Traceback (most recent call last):
W0629 11:52:48.016000 139012379337344 torch/_dynamo/convert_frame.py:824]   File "/usr/local/lib/python3.10/dist-packages/torch/_dynamo/convert_frame.py", line 786, in _convert_frame
W0629 11:52:48.016000 139012379337344 torch/_dynamo/convert_frame.py:824]     result = inner_convert(
W0629 11:52:48.016000 139012379337344 torch/_dynamo/convert_frame.py:824]   File "/usr/local/lib/python3.10/dist-packages/torch/_dynamo/convert_frame.py", line 400, in _convert_frame_assert
W0629 11:52:48.016000 139012379337344 torch/_dynamo/convert_frame.py:824]     return _compile(
W0629 11:52:48.016000 139012379337344 torch/_dynamo/convert_frame.py:824]   File "/usr/lib/python3.10/contextlib.py", line 79, in inner
W0629 11:52:48.016000 139012379337344 torch/_dynamo/convert_frame.py:824]     return func(*args, **kwds)
W0629 11:52:48.016000 139012379337344 torch/_dynamo/convert_frame.py:824]   File "/usr/local/lib/python3.10/dist-packages/torch/_dynamo/convert_frame.py", line 676, in _compile
W0629 11:52:48.016000 139012379337344 torch/_dynamo/convert_frame.py:824]     guarded_code = compile_inner(code, one_graph, hooks, transform)
W0629 11:52:48.016000 139012379337344 torch/_dynamo/convert_frame.py:824]   File "/usr/local/lib/python3.10/dist-packages/torch/_dynamo/utils.py", line 262, in time_wrapper
W0629 11:52:48.016000 139012379337344 torch/_dynamo/convert_frame.py:824]     r = func(*args, **kwargs)
W0629 11:52:48.016000 139012379337344 torch/_dynamo/convert_frame.py:824]   File "/usr/local/lib/python3.10/dist-packages/torch/_dynamo/convert_frame.py", line 535, in compile_inner
W0629 11:52:48.016000 139012379337344 torch/_dynamo/convert_frame.py:824]     out_code = transform_code_object(code, transform)
W0629 11:52:48.016000 139012379337344 torch/_dynamo/convert_frame.py:824]   File "/usr/local/lib/python3.10/dist-packages/torch/_dynamo/bytecode_transformation.py", line 1036, in transform_code_object
W0629 11:52:48.016000 139012379337344 torch/_dynamo/convert_frame.py:824]     transformations(instructions, code_options)
W0629 11:52:48.016000 139012379337344 torch/_dynamo/convert_frame.py:824]   File "/usr/local/lib/python3.10/dist-packages/torch/_dynamo/convert_frame.py", line 165, in _fn
W0629 11:52:48.016000 139012379337344 torch/_dynamo/convert_frame.py:824]     return fn(*args, **kwargs)
W0629 11:52:48.016000 139012379337344 torch/_dynamo/convert_frame.py:824]   File "/usr/local/lib/python3.10/dist-packages/torch/_dynamo/convert_frame.py", line 500, in transform
W0629 11:52:48.016000 139012379337344 torch/_dynamo/convert_frame.py:824]     tracer.run()
W0629 11:52:48.016000 139012379337344 torch/_dynamo/convert_frame.py:824]   File "/usr/local/lib/python3.10/dist-packages/torch/_dynamo/symbolic_convert.py", line 2149, in run
W0629 11:52:48.016000 139012379337344 torch/_dynamo/convert_frame.py:824]     super().run()
W0629 11:52:48.016000 139012379337344 torch/_dynamo/convert_frame.py:824]   File "/usr/local/lib/python3.10/dist-packages/torch/_dynamo/symbolic_convert.py", line 810, in run
W0629 11:52:48.016000 139012379337344 torch/_dynamo/convert_frame.py:824]     and self.step()
W0629 11:52:48.016000 139012379337344 torch/_dynamo/convert_frame.py:824]   File "/usr/local/lib/python3.10/dist-packages/torch/_dynamo/symbolic_convert.py", line 773, in step
W0629 11:52:48.016000 139012379337344 torch/_dynamo/convert_frame.py:824]     getattr(self, inst.opname)(inst)
W0629 11:52:48.016000 139012379337344 torch/_dynamo/convert_frame.py:824]   File "/usr/local/lib/python3.10/dist-packages/torch/_dynamo/symbolic_convert.py", line 2268, in RETURN_VALUE
W0629 11:52:48.016000 139012379337344 torch/_dynamo/convert_frame.py:824]     self.output.compile_subgraph(
W0629 11:52:48.016000 139012379337344 torch/_dynamo/convert_frame.py:824]   File "/usr/local/lib/python3.10/dist-packages/torch/_dynamo/output_graph.py", line 991, in compile_subgraph
W0629 11:52:48.016000 139012379337344 torch/_dynamo/convert_frame.py:824]     self.compile_and_call_fx_graph(tx, pass2.graph_output_vars(), root)
W0629 11:52:48.016000 139012379337344 torch/_dynamo/convert_frame.py:824]   File "/usr/lib/python3.10/contextlib.py", line 79, in inner
W0629 11:52:48.016000 139012379337344 torch/_dynamo/convert_frame.py:824]     return func(*args, **kwds)
W0629 11:52:48.016000 139012379337344 torch/_dynamo/convert_frame.py:824]   File "/usr/local/lib/python3.10/dist-packages/torch/_dynamo/output_graph.py", line 1168, in compile_and_call_fx_graph
W0629 11:52:48.016000 139012379337344 torch/_dynamo/convert_frame.py:824]     compiled_fn = self.call_user_compiler(gm)
W0629 11:52:48.016000 139012379337344 torch/_dynamo/convert_frame.py:824]   File "/usr/local/lib/python3.10/dist-packages/torch/_dynamo/utils.py", line 262, in time_wrapper
W0629 11:52:48.016000 139012379337344 torch/_dynamo/convert_frame.py:824]     r = func(*args, **kwargs)
W0629 11:52:48.016000 139012379337344 torch/_dynamo/convert_frame.py:824]   File "/usr/local/lib/python3.10/dist-packages/torch/_dynamo/output_graph.py", line 1241, in call_user_compiler
W0629 11:52:48.016000 139012379337344 torch/_dynamo/convert_frame.py:824]     raise BackendCompilerFailed(self.compiler_fn, e).with_traceback(
W0629 11:52:48.016000 139012379337344 torch/_dynamo/convert_frame.py:824]   File "/usr/local/lib/python3.10/dist-packages/torch/_dynamo/output_graph.py", line 1222, in call_user_compiler
W0629 11:52:48.016000 139012379337344 torch/_dynamo/convert_frame.py:824]     compiled_fn = compiler_fn(gm, self.example_inputs())
W0629 11:52:48.016000 139012379337344 torch/_dynamo/convert_frame.py:824]   File "/usr/local/lib/python3.10/dist-packages/torch/_dynamo/repro/after_dynamo.py", line 117, in debug_wrapper
W0629 11:52:48.016000 139012379337344 torch/_dynamo/convert_frame.py:824]     compiled_gm = compiler_fn(gm, example_inputs)
W0629 11:52:48.016000 139012379337344 torch/_dynamo/convert_frame.py:824]   File "/usr/local/lib/python3.10/dist-packages/torch/__init__.py", line 1729, in __call__
W0629 11:52:48.016000 139012379337344 torch/_dynamo/convert_frame.py:824]     return compile_fx(model_, inputs_, config_patches=self.config)
W0629 11:52:48.016000 139012379337344 torch/_dynamo/convert_frame.py:824]   File "/usr/lib/python3.10/contextlib.py", line 79, in inner
W0629 11:52:48.016000 139012379337344 torch/_dynamo/convert_frame.py:824]     return func(*args, **kwds)
W0629 11:52:48.016000 139012379337344 torch/_dynamo/convert_frame.py:824]   File "/usr/local/lib/python3.10/dist-packages/torch/_inductor/compile_fx.py", line 1330, in compile_fx
W0629 11:52:48.016000 139012379337344 torch/_dynamo/convert_frame.py:824]     return aot_autograd(
W0629 11:52:48.016000 139012379337344 torch/_dynamo/convert_frame.py:824]   File "/usr/local/lib/python3.10/dist-packages/torch/_dynamo/backends/common.py", line 58, in compiler_fn
W0629 11:52:48.016000 139012379337344 torch/_dynamo/convert_frame.py:824]     cg = aot_module_simplified(gm, example_inputs, **kwargs)
W0629 11:52:48.016000 139012379337344 torch/_dynamo/convert_frame.py:824]   File "/usr/local/lib/python3.10/dist-packages/torch/_functorch/aot_autograd.py", line 903, in aot_module_simplified
W0629 11:52:48.016000 139012379337344 torch/_dynamo/convert_frame.py:824]     compiled_fn = create_aot_dispatcher_function(
W0629 11:52:48.016000 139012379337344 torch/_dynamo/convert_frame.py:824]   File "/usr/local/lib/python3.10/dist-packages/torch/_dynamo/utils.py", line 262, in time_wrapper
W0629 11:52:48.016000 139012379337344 torch/_dynamo/convert_frame.py:824]     r = func(*args, **kwargs)
W0629 11:52:48.016000 139012379337344 torch/_dynamo/convert_frame.py:824]   File "/usr/local/lib/python3.10/dist-packages/torch/_functorch/aot_autograd.py", line 628, in create_aot_dispatcher_function
W0629 11:52:48.016000 139012379337344 torch/_dynamo/convert_frame.py:824]     compiled_fn = compiler_fn(flat_fn, fake_flat_args, aot_config, fw_metadata=fw_metadata)
W0629 11:52:48.016000 139012379337344 torch/_dynamo/convert_frame.py:824]   File "/usr/local/lib/python3.10/dist-packages/torch/_functorch/_aot_autograd/runtime_wrappers.py", line 443, in aot_wrapper_dedupe
W0629 11:52:48.016000 139012379337344 torch/_dynamo/convert_frame.py:824]     return compiler_fn(flat_fn, leaf_flat_args, aot_config, fw_metadata=fw_metadata)
W0629 11:52:48.016000 139012379337344 torch/_dynamo/convert_frame.py:824]   File "/usr/local/lib/python3.10/dist-packages/torch/_functorch/_aot_autograd/runtime_wrappers.py", line 648, in aot_wrapper_synthetic_base
W0629 11:52:48.016000 139012379337344 torch/_dynamo/convert_frame.py:824]     return compiler_fn(flat_fn, flat_args, aot_config, fw_metadata=fw_metadata)
W0629 11:52:48.016000 139012379337344 torch/_dynamo/convert_frame.py:824]   File "/usr/local/lib/python3.10/dist-packages/torch/_functorch/_aot_autograd/jit_compile_runtime_wrappers.py", line 119, in aot_dispatch_base
W0629 11:52:48.016000 139012379337344 torch/_dynamo/convert_frame.py:824]     compiled_fw = compiler(fw_module, updated_flat_args)
W0629 11:52:48.016000 139012379337344 torch/_dynamo/convert_frame.py:824]   File "/usr/local/lib/python3.10/dist-packages/torch/_dynamo/utils.py", line 262, in time_wrapper
W0629 11:52:48.016000 139012379337344 torch/_dynamo/convert_frame.py:824]     r = func(*args, **kwargs)
W0629 11:52:48.016000 139012379337344 torch/_dynamo/convert_frame.py:824]   File "/usr/local/lib/python3.10/dist-packages/torch/_inductor/compile_fx.py", line 1257, in fw_compiler_base
W0629 11:52:48.016000 139012379337344 torch/_dynamo/convert_frame.py:824]     return inner_compile(
W0629 11:52:48.016000 139012379337344 torch/_dynamo/convert_frame.py:824]   File "/usr/local/lib/python3.10/dist-packages/torch/_dynamo/repro/after_aot.py", line 83, in debug_wrapper
W0629 11:52:48.016000 139012379337344 torch/_dynamo/convert_frame.py:824]     inner_compiled_fn = compiler_fn(gm, example_inputs)
W0629 11:52:48.016000 139012379337344 torch/_dynamo/convert_frame.py:824]   File "/usr/local/lib/python3.10/dist-packages/torch/_inductor/debug.py", line 304, in inner
W0629 11:52:48.016000 139012379337344 torch/_dynamo/convert_frame.py:824]     return fn(*args, **kwargs)
W0629 11:52:48.016000 139012379337344 torch/_dynamo/convert_frame.py:824]   File "/usr/lib/python3.10/contextlib.py", line 79, in inner
W0629 11:52:48.016000 139012379337344 torch/_dynamo/convert_frame.py:824]     return func(*args, **kwds)
W0629 11:52:48.016000 139012379337344 torch/_dynamo/convert_frame.py:824]   File "/usr/lib/python3.10/contextlib.py", line 79, in inner
W0629 11:52:48.016000 139012379337344 torch/_dynamo/convert_frame.py:824]     return func(*args, **kwds)
W0629 11:52:48.016000 139012379337344 torch/_dynamo/convert_frame.py:824]   File "/usr/local/lib/python3.10/dist-packages/torch/_dynamo/utils.py", line 262, in time_wrapper
W0629 11:52:48.016000 139012379337344 torch/_dynamo/convert_frame.py:824]     r = func(*args, **kwargs)
W0629 11:52:48.016000 139012379337344 torch/_dynamo/convert_frame.py:824]   File "/usr/local/lib/python3.10/dist-packages/torch/_inductor/compile_fx.py", line 438, in compile_fx_inner
W0629 11:52:48.016000 139012379337344 torch/_dynamo/convert_frame.py:824]     compiled_graph = fx_codegen_and_compile(
W0629 11:52:48.016000 139012379337344 torch/_dynamo/convert_frame.py:824]   File "/usr/local/lib/python3.10/dist-packages/torch/_inductor/compile_fx.py", line 714, in fx_codegen_and_compile
W0629 11:52:48.016000 139012379337344 torch/_dynamo/convert_frame.py:824]     compiled_fn = graph.compile_to_fn()
W0629 11:52:48.016000 139012379337344 torch/_dynamo/convert_frame.py:824]   File "/usr/local/lib/python3.10/dist-packages/torch/_inductor/graph.py", line 1307, in compile_to_fn
W0629 11:52:48.016000 139012379337344 torch/_dynamo/convert_frame.py:824]     return self.compile_to_module().call
W0629 11:52:48.016000 139012379337344 torch/_dynamo/convert_frame.py:824]   File "/usr/local/lib/python3.10/dist-packages/torch/_dynamo/utils.py", line 262, in time_wrapper
W0629 11:52:48.016000 139012379337344 torch/_dynamo/convert_frame.py:824]     r = func(*args, **kwargs)
W0629 11:52:48.016000 139012379337344 torch/_dynamo/convert_frame.py:824]   File "/usr/local/lib/python3.10/dist-packages/torch/_inductor/graph.py", line 1254, in compile_to_module
W0629 11:52:48.016000 139012379337344 torch/_dynamo/convert_frame.py:824]     mod = PyCodeCache.load_by_key_path(
W0629 11:52:48.016000 139012379337344 torch/_dynamo/convert_frame.py:824]   File "/usr/local/lib/python3.10/dist-packages/torch/_inductor/codecache.py", line 2160, in load_by_key_path
W0629 11:52:48.016000 139012379337344 torch/_dynamo/convert_frame.py:824]     exec(code, mod.__dict__, mod.__dict__)
W0629 11:52:48.016000 139012379337344 torch/_dynamo/convert_frame.py:824]   File "/tmp/torchinductor_root/cv/ccvlx4s6y54jyy76geigg7o2wsmcyk36de3hp2yfss5a4pcp3f7t.py", line 574, in <module>
W0629 11:52:48.016000 139012379337344 torch/_dynamo/convert_frame.py:824]     async_compile.wait(globals())
W0629 11:52:48.016000 139012379337344 torch/_dynamo/convert_frame.py:824]   File "/usr/local/lib/python3.10/dist-packages/torch/_inductor/codecache.py", line 2715, in wait
W0629 11:52:48.016000 139012379337344 torch/_dynamo/convert_frame.py:824]     scope[key] = result.result()
W0629 11:52:48.016000 139012379337344 torch/_dynamo/convert_frame.py:824]   File "/usr/local/lib/python3.10/dist-packages/torch/_inductor/codecache.py", line 2522, in result
W0629 11:52:48.016000 139012379337344 torch/_dynamo/convert_frame.py:824]     self.future.result()
W0629 11:52:48.016000 139012379337344 torch/_dynamo/convert_frame.py:824]   File "/usr/lib/python3.10/concurrent/futures/_base.py", line 458, in result
W0629 11:52:48.016000 139012379337344 torch/_dynamo/convert_frame.py:824]     return self.__get_result()
W0629 11:52:48.016000 139012379337344 torch/_dynamo/convert_frame.py:824]   File "/usr/lib/python3.10/concurrent/futures/_base.py", line 403, in __get_result
W0629 11:52:48.016000 139012379337344 torch/_dynamo/convert_frame.py:824]     raise self._exception
W0629 11:52:48.016000 139012379337344 torch/_dynamo/convert_frame.py:824] torch._dynamo.exc.BackendCompilerFailed: backend='inductor' raised:
W0629 11:52:48.016000 139012379337344 torch/_dynamo/convert_frame.py:824] RuntimeError: Internal Triton PTX codegen error:
W0629 11:52:48.016000 139012379337344 torch/_dynamo/convert_frame.py:824] ptxas /tmp/compile-ptx-src-72227c, line 63; error   : Feature '.bf16' requires .target sm_80 or higher
W0629 11:52:48.016000 139012379337344 torch/_dynamo/convert_frame.py:824] ptxas /tmp/compile-ptx-src-72227c, line 63; error   : Feature 'cvt.bf16.f32' requires .target sm_80 or higher
W0629 11:52:48.016000 139012379337344 torch/_dynamo/convert_frame.py:824] ptxas /tmp/compile-ptx-src-72227c, line 64; error   : Feature '.bf16' requires .target sm_80 or higher
W0629 11:52:48.016000 139012379337344 torch/_dynamo/convert_frame.py:824] ptxas /tmp/compile-ptx-src-72227c, line 64; error   : Feature 'cvt.bf16.f32' requires .target sm_80 or higher
W0629 11:52:48.016000 139012379337344 torch/_dynamo/convert_frame.py:824] ptxas /tmp/compile-ptx-src-72227c, line 65; error   : Feature '.bf16' requires .target sm_80 or higher
W0629 11:52:48.016000 139012379337344 torch/_dynamo/convert_frame.py:824] ptxas /tmp/compile-ptx-src-72227c, line 65; error   : Feature 'cvt.bf16.f32' requires .target sm_80 or higher
W0629 11:52:48.016000 139012379337344 torch/_dynamo/convert_frame.py:824] ptxas /tmp/compile-ptx-src-72227c, line 66; error   : Feature '.bf16' requires .target sm_80 or higher
W0629 11:52:48.016000 139012379337344 torch/_dynamo/convert_frame.py:824] ptxas /tmp/compile-ptx-src-72227c, line 66; error   : Feature 'cvt.bf16.f32' requires .target sm_80 or higher
W0629 11:52:48.016000 139012379337344 torch/_dynamo/convert_frame.py:824] ptxas /tmp/compile-ptx-src-72227c, line 67; error   : Feature '.bf16' requires .target sm_80 or higher
W0629 11:52:48.016000 139012379337344 torch/_dynamo/convert_frame.py:824] ptxas /tmp/compile-ptx-src-72227c, line 67; error   : Feature 'cvt.bf16.f32' requires .target sm_80 or higher
W0629 11:52:48.016000 139012379337344 torch/_dynamo/convert_frame.py:824] ptxas /tmp/compile-ptx-src-72227c, line 68; error   : Feature '.bf16' requires .target sm_80 or higher
W0629 11:52:48.016000 139012379337344 torch/_dynamo/convert_frame.py:824] ptxas /tmp/compile-ptx-src-72227c, line 68; error   : Feature 'cvt.bf16.f32' requires .target sm_80 or higher
W0629 11:52:48.016000 139012379337344 torch/_dynamo/convert_frame.py:824] ptxas /tmp/compile-ptx-src-72227c, line 69; error   : Feature '.bf16' requires .target sm_80 or higher
W0629 11:52:48.016000 139012379337344 torch/_dynamo/convert_frame.py:824] ptxas /tmp/compile-ptx-src-72227c, line 69; error   : Feature 'cvt.bf16.f32' requires .target sm_80 or higher
W0629 11:52:48.016000 139012379337344 torch/_dynamo/convert_frame.py:824] ptxas /tmp/compile-ptx-src-72227c, line 70; error   : Feature '.bf16' requires .target sm_80 or higher
W0629 11:52:48.016000 139012379337344 torch/_dynamo/convert_frame.py:824] ptxas /tmp/compile-ptx-src-72227c, line 70; error   : Feature 'cvt.bf16.f32' requires .target sm_80 or higher
W0629 11:52:48.016000 139012379337344 torch/_dynamo/convert_frame.py:824] ptxas fatal   : Ptx assembly aborted due to errors
W0629 11:52:48.016000 139012379337344 torch/_dynamo/convert_frame.py:824]
W0629 11:52:48.016000 139012379337344 torch/_dynamo/convert_frame.py:824]
W0629 11:52:48.016000 139012379337344 torch/_dynamo/convert_frame.py:824] Set TORCH_LOGS="+dynamo" and TORCHDYNAMO_VERBOSE=1 for more information
W0629 11:52:48.016000 139012379337344 torch/_dynamo/convert_frame.py:824]
W0629 11:52:48.851000 139012379337344 torch/_dynamo/convert_frame.py:824] WON'T CONVERT forward /usr/local/lib/python3.10/dist-packages/transformers/models/bert/modeling_bert.py line 500
W0629 11:52:48.851000 139012379337344 torch/_dynamo/convert_frame.py:824] due to:
W0629 11:52:48.851000 139012379337344 torch/_dynamo/convert_frame.py:824] Traceback (most recent call last):
W0629 11:52:48.851000 139012379337344 torch/_dynamo/convert_frame.py:824]   File "/usr/local/lib/python3.10/dist-packages/torch/_dynamo/convert_frame.py", line 786, in _convert_frame
W0629 11:52:48.851000 139012379337344 torch/_dynamo/convert_frame.py:824]     result = inner_convert(
W0629 11:52:48.851000 139012379337344 torch/_dynamo/convert_frame.py:824]   File "/usr/local/lib/python3.10/dist-packages/torch/_dynamo/convert_frame.py", line 400, in _convert_frame_assert
W0629 11:52:48.851000 139012379337344 torch/_dynamo/convert_frame.py:824]     return _compile(
W0629 11:52:48.851000 139012379337344 torch/_dynamo/convert_frame.py:824]   File "/usr/lib/python3.10/contextlib.py", line 79, in inner
W0629 11:52:48.851000 139012379337344 torch/_dynamo/convert_frame.py:824]     return func(*args, **kwds)
W0629 11:52:48.851000 139012379337344 torch/_dynamo/convert_frame.py:824]   File "/usr/local/lib/python3.10/dist-packages/torch/_dynamo/convert_frame.py", line 676, in _compile
W0629 11:52:48.851000 139012379337344 torch/_dynamo/convert_frame.py:824]     guarded_code = compile_inner(code, one_graph, hooks, transform)
W0629 11:52:48.851000 139012379337344 torch/_dynamo/convert_frame.py:824]   File "/usr/local/lib/python3.10/dist-packages/torch/_dynamo/utils.py", line 262, in time_wrapper
W0629 11:52:48.851000 139012379337344 torch/_dynamo/convert_frame.py:824]     r = func(*args, **kwargs)
W0629 11:52:48.851000 139012379337344 torch/_dynamo/convert_frame.py:824]   File "/usr/local/lib/python3.10/dist-packages/torch/_dynamo/convert_frame.py", line 535, in compile_inner
W0629 11:52:48.851000 139012379337344 torch/_dynamo/convert_frame.py:824]     out_code = transform_code_object(code, transform)
W0629 11:52:48.851000 139012379337344 torch/_dynamo/convert_frame.py:824]   File "/usr/local/lib/python3.10/dist-packages/torch/_dynamo/bytecode_transformation.py", line 1036, in transform_code_object
W0629 11:52:48.851000 139012379337344 torch/_dynamo/convert_frame.py:824]     transformations(instructions, code_options)
W0629 11:52:48.851000 139012379337344 torch/_dynamo/convert_frame.py:824]   File "/usr/local/lib/python3.10/dist-packages/torch/_dynamo/convert_frame.py", line 165, in _fn
W0629 11:52:48.851000 139012379337344 torch/_dynamo/convert_frame.py:824]     return fn(*args, **kwargs)
W0629 11:52:48.851000 139012379337344 torch/_dynamo/convert_frame.py:824]   File "/usr/local/lib/python3.10/dist-packages/torch/_dynamo/convert_frame.py", line 500, in transform
W0629 11:52:48.851000 139012379337344 torch/_dynamo/convert_frame.py:824]     tracer.run()
W0629 11:52:48.851000 139012379337344 torch/_dynamo/convert_frame.py:824]   File "/usr/local/lib/python3.10/dist-packages/torch/_dynamo/symbolic_convert.py", line 2149, in run
W0629 11:52:48.851000 139012379337344 torch/_dynamo/convert_frame.py:824]     super().run()
W0629 11:52:48.851000 139012379337344 torch/_dynamo/convert_frame.py:824]   File "/usr/local/lib/python3.10/dist-packages/torch/_dynamo/symbolic_convert.py", line 810, in run
W0629 11:52:48.851000 139012379337344 torch/_dynamo/convert_frame.py:824]     and self.step()
W0629 11:52:48.851000 139012379337344 torch/_dynamo/convert_frame.py:824]   File "/usr/local/lib/python3.10/dist-packages/torch/_dynamo/symbolic_convert.py", line 773, in step
W0629 11:52:48.851000 139012379337344 torch/_dynamo/convert_frame.py:824]     getattr(self, inst.opname)(inst)
W0629 11:52:48.851000 139012379337344 torch/_dynamo/convert_frame.py:824]   File "/usr/local/lib/python3.10/dist-packages/torch/_dynamo/symbolic_convert.py", line 2268, in RETURN_VALUE
W0629 11:52:48.851000 139012379337344 torch/_dynamo/convert_frame.py:824]     self.output.compile_subgraph(
W0629 11:52:48.851000 139012379337344 torch/_dynamo/convert_frame.py:824]   File "/usr/local/lib/python3.10/dist-packages/torch/_dynamo/output_graph.py", line 991, in compile_subgraph
W0629 11:52:48.851000 139012379337344 torch/_dynamo/convert_frame.py:824]     self.compile_and_call_fx_graph(tx, pass2.graph_output_vars(), root)
W0629 11:52:48.851000 139012379337344 torch/_dynamo/convert_frame.py:824]   File "/usr/lib/python3.10/contextlib.py", line 79, in inner
W0629 11:52:48.851000 139012379337344 torch/_dynamo/convert_frame.py:824]     return func(*args, **kwds)
W0629 11:52:48.851000 139012379337344 torch/_dynamo/convert_frame.py:824]   File "/usr/local/lib/python3.10/dist-packages/torch/_dynamo/output_graph.py", line 1168, in compile_and_call_fx_graph
W0629 11:52:48.851000 139012379337344 torch/_dynamo/convert_frame.py:824]     compiled_fn = self.call_user_compiler(gm)
W0629 11:52:48.851000 139012379337344 torch/_dynamo/convert_frame.py:824]   File "/usr/local/lib/python3.10/dist-packages/torch/_dynamo/utils.py", line 262, in time_wrapper
W0629 11:52:48.851000 139012379337344 torch/_dynamo/convert_frame.py:824]     r = func(*args, **kwargs)
W0629 11:52:48.851000 139012379337344 torch/_dynamo/convert_frame.py:824]   File "/usr/local/lib/python3.10/dist-packages/torch/_dynamo/output_graph.py", line 1241, in call_user_compiler
W0629 11:52:48.851000 139012379337344 torch/_dynamo/convert_frame.py:824]     raise BackendCompilerFailed(self.compiler_fn, e).with_traceback(
W0629 11:52:48.851000 139012379337344 torch/_dynamo/convert_frame.py:824]   File "/usr/local/lib/python3.10/dist-packages/torch/_dynamo/output_graph.py", line 1222, in call_user_compiler
W0629 11:52:48.851000 139012379337344 torch/_dynamo/convert_frame.py:824]     compiled_fn = compiler_fn(gm, self.example_inputs())
W0629 11:52:48.851000 139012379337344 torch/_dynamo/convert_frame.py:824]   File "/usr/local/lib/python3.10/dist-packages/torch/_dynamo/repro/after_dynamo.py", line 117, in debug_wrapper
W0629 11:52:48.851000 139012379337344 torch/_dynamo/convert_frame.py:824]     compiled_gm = compiler_fn(gm, example_inputs)
W0629 11:52:48.851000 139012379337344 torch/_dynamo/convert_frame.py:824]   File "/usr/local/lib/python3.10/dist-packages/torch/__init__.py", line 1729, in __call__
W0629 11:52:48.851000 139012379337344 torch/_dynamo/convert_frame.py:824]     return compile_fx(model_, inputs_, config_patches=self.config)
W0629 11:52:48.851000 139012379337344 torch/_dynamo/convert_frame.py:824]   File "/usr/lib/python3.10/contextlib.py", line 79, in inner
W0629 11:52:48.851000 139012379337344 torch/_dynamo/convert_frame.py:824]     return func(*args, **kwds)
W0629 11:52:48.851000 139012379337344 torch/_dynamo/convert_frame.py:824]   File "/usr/local/lib/python3.10/dist-packages/torch/_inductor/compile_fx.py", line 1330, in compile_fx
W0629 11:52:48.851000 139012379337344 torch/_dynamo/convert_frame.py:824]     return aot_autograd(
W0629 11:52:48.851000 139012379337344 torch/_dynamo/convert_frame.py:824]   File "/usr/local/lib/python3.10/dist-packages/torch/_dynamo/backends/common.py", line 58, in compiler_fn
W0629 11:52:48.851000 139012379337344 torch/_dynamo/convert_frame.py:824]     cg = aot_module_simplified(gm, example_inputs, **kwargs)
W0629 11:52:48.851000 139012379337344 torch/_dynamo/convert_frame.py:824]   File "/usr/local/lib/python3.10/dist-packages/torch/_functorch/aot_autograd.py", line 903, in aot_module_simplified
W0629 11:52:48.851000 139012379337344 torch/_dynamo/convert_frame.py:824]     compiled_fn = create_aot_dispatcher_function(
W0629 11:52:48.851000 139012379337344 torch/_dynamo/convert_frame.py:824]   File "/usr/local/lib/python3.10/dist-packages/torch/_dynamo/utils.py", line 262, in time_wrapper
W0629 11:52:48.851000 139012379337344 torch/_dynamo/convert_frame.py:824]     r = func(*args, **kwargs)
W0629 11:52:48.851000 139012379337344 torch/_dynamo/convert_frame.py:824]   File "/usr/local/lib/python3.10/dist-packages/torch/_functorch/aot_autograd.py", line 628, in create_aot_dispatcher_function
W0629 11:52:48.851000 139012379337344 torch/_dynamo/convert_frame.py:824]     compiled_fn = compiler_fn(flat_fn, fake_flat_args, aot_config, fw_metadata=fw_metadata)
W0629 11:52:48.851000 139012379337344 torch/_dynamo/convert_frame.py:824]   File "/usr/local/lib/python3.10/dist-packages/torch/_functorch/_aot_autograd/runtime_wrappers.py", line 443, in aot_wrapper_dedupe
W0629 11:52:48.851000 139012379337344 torch/_dynamo/convert_frame.py:824]     return compiler_fn(flat_fn, leaf_flat_args, aot_config, fw_metadata=fw_metadata)
W0629 11:52:48.851000 139012379337344 torch/_dynamo/convert_frame.py:824]   File "/usr/local/lib/python3.10/dist-packages/torch/_functorch/_aot_autograd/runtime_wrappers.py", line 648, in aot_wrapper_synthetic_base
W0629 11:52:48.851000 139012379337344 torch/_dynamo/convert_frame.py:824]     return compiler_fn(flat_fn, flat_args, aot_config, fw_metadata=fw_metadata)
W0629 11:52:48.851000 139012379337344 torch/_dynamo/convert_frame.py:824]   File "/usr/local/lib/python3.10/dist-packages/torch/_functorch/_aot_autograd/jit_compile_runtime_wrappers.py", line 119, in aot_dispatch_base
W0629 11:52:48.851000 139012379337344 torch/_dynamo/convert_frame.py:824]     compiled_fw = compiler(fw_module, updated_flat_args)
W0629 11:52:48.851000 139012379337344 torch/_dynamo/convert_frame.py:824]   File "/usr/local/lib/python3.10/dist-packages/torch/_dynamo/utils.py", line 262, in time_wrapper
W0629 11:52:48.851000 139012379337344 torch/_dynamo/convert_frame.py:824]     r = func(*args, **kwargs)
W0629 11:52:48.851000 139012379337344 torch/_dynamo/convert_frame.py:824]   File "/usr/local/lib/python3.10/dist-packages/torch/_inductor/compile_fx.py", line 1257, in fw_compiler_base
W0629 11:52:48.851000 139012379337344 torch/_dynamo/convert_frame.py:824]     return inner_compile(
W0629 11:52:48.851000 139012379337344 torch/_dynamo/convert_frame.py:824]   File "/usr/local/lib/python3.10/dist-packages/torch/_dynamo/repro/after_aot.py", line 83, in debug_wrapper
W0629 11:52:48.851000 139012379337344 torch/_dynamo/convert_frame.py:824]     inner_compiled_fn = compiler_fn(gm, example_inputs)
W0629 11:52:48.851000 139012379337344 torch/_dynamo/convert_frame.py:824]   File "/usr/local/lib/python3.10/dist-packages/torch/_inductor/debug.py", line 304, in inner
W0629 11:52:48.851000 139012379337344 torch/_dynamo/convert_frame.py:824]     return fn(*args, **kwargs)
W0629 11:52:48.851000 139012379337344 torch/_dynamo/convert_frame.py:824]   File "/usr/lib/python3.10/contextlib.py", line 79, in inner
W0629 11:52:48.851000 139012379337344 torch/_dynamo/convert_frame.py:824]     return func(*args, **kwds)
W0629 11:52:48.851000 139012379337344 torch/_dynamo/convert_frame.py:824]   File "/usr/lib/python3.10/contextlib.py", line 79, in inner
W0629 11:52:48.851000 139012379337344 torch/_dynamo/convert_frame.py:824]     return func(*args, **kwds)
W0629 11:52:48.851000 139012379337344 torch/_dynamo/convert_frame.py:824]   File "/usr/local/lib/python3.10/dist-packages/torch/_dynamo/utils.py", line 262, in time_wrapper
W0629 11:52:48.851000 139012379337344 torch/_dynamo/convert_frame.py:824]     r = func(*args, **kwargs)
W0629 11:52:48.851000 139012379337344 torch/_dynamo/convert_frame.py:824]   File "/usr/local/lib/python3.10/dist-packages/torch/_inductor/compile_fx.py", line 438, in compile_fx_inner
W0629 11:52:48.851000 139012379337344 torch/_dynamo/convert_frame.py:824]     compiled_graph = fx_codegen_and_compile(
W0629 11:52:48.851000 139012379337344 torch/_dynamo/convert_frame.py:824]   File "/usr/local/lib/python3.10/dist-packages/torch/_inductor/compile_fx.py", line 714, in fx_codegen_and_compile
W0629 11:52:48.851000 139012379337344 torch/_dynamo/convert_frame.py:824]     compiled_fn = graph.compile_to_fn()
W0629 11:52:48.851000 139012379337344 torch/_dynamo/convert_frame.py:824]   File "/usr/local/lib/python3.10/dist-packages/torch/_inductor/graph.py", line 1307, in compile_to_fn
W0629 11:52:48.851000 139012379337344 torch/_dynamo/convert_frame.py:824]     return self.compile_to_module().call
W0629 11:52:48.851000 139012379337344 torch/_dynamo/convert_frame.py:824]   File "/usr/local/lib/python3.10/dist-packages/torch/_dynamo/utils.py", line 262, in time_wrapper
W0629 11:52:48.851000 139012379337344 torch/_dynamo/convert_frame.py:824]     r = func(*args, **kwargs)
W0629 11:52:48.851000 139012379337344 torch/_dynamo/convert_frame.py:824]   File "/usr/local/lib/python3.10/dist-packages/torch/_inductor/graph.py", line 1254, in compile_to_module
W0629 11:52:48.851000 139012379337344 torch/_dynamo/convert_frame.py:824]     mod = PyCodeCache.load_by_key_path(
W0629 11:52:48.851000 139012379337344 torch/_dynamo/convert_frame.py:824]   File "/usr/local/lib/python3.10/dist-packages/torch/_inductor/codecache.py", line 2160, in load_by_key_path
W0629 11:52:48.851000 139012379337344 torch/_dynamo/convert_frame.py:824]     exec(code, mod.__dict__, mod.__dict__)
W0629 11:52:48.851000 139012379337344 torch/_dynamo/convert_frame.py:824]   File "/tmp/torchinductor_root/2q/c2qyt6tdezedsfihmagcwcwrww3uy5yyqk4ymr6yzhn6lg3uhdtn.py", line 419, in <module>
W0629 11:52:48.851000 139012379337344 torch/_dynamo/convert_frame.py:824]     async_compile.wait(globals())
W0629 11:52:48.851000 139012379337344 torch/_dynamo/convert_frame.py:824]   File "/usr/local/lib/python3.10/dist-packages/torch/_inductor/codecache.py", line 2715, in wait
W0629 11:52:48.851000 139012379337344 torch/_dynamo/convert_frame.py:824]     scope[key] = result.result()
W0629 11:52:48.851000 139012379337344 torch/_dynamo/convert_frame.py:824]   File "/usr/local/lib/python3.10/dist-packages/torch/_inductor/codecache.py", line 2522, in result
W0629 11:52:48.851000 139012379337344 torch/_dynamo/convert_frame.py:824]     self.future.result()
W0629 11:52:48.851000 139012379337344 torch/_dynamo/convert_frame.py:824]   File "/usr/lib/python3.10/concurrent/futures/_base.py", line 458, in result
W0629 11:52:48.851000 139012379337344 torch/_dynamo/convert_frame.py:824]     return self.__get_result()
W0629 11:52:48.851000 139012379337344 torch/_dynamo/convert_frame.py:824]   File "/usr/lib/python3.10/concurrent/futures/_base.py", line 403, in __get_result
W0629 11:52:48.851000 139012379337344 torch/_dynamo/convert_frame.py:824]     raise self._exception
W0629 11:52:48.851000 139012379337344 torch/_dynamo/convert_frame.py:824] torch._dynamo.exc.BackendCompilerFailed: backend='inductor' raised:
W0629 11:52:48.851000 139012379337344 torch/_dynamo/convert_frame.py:824] RuntimeError: Internal Triton PTX codegen error:
W0629 11:52:48.851000 139012379337344 torch/_dynamo/convert_frame.py:824] ptxas /tmp/compile-ptx-src-add99a, line 63; error   : Feature '.bf16' requires .target sm_80 or higher
W0629 11:52:48.851000 139012379337344 torch/_dynamo/convert_frame.py:824] ptxas /tmp/compile-ptx-src-add99a, line 63; error   : Feature 'cvt.bf16.f32' requires .target sm_80 or higher
W0629 11:52:48.851000 139012379337344 torch/_dynamo/convert_frame.py:824] ptxas /tmp/compile-ptx-src-add99a, line 64; error   : Feature '.bf16' requires .target sm_80 or higher
W0629 11:52:48.851000 139012379337344 torch/_dynamo/convert_frame.py:824] ptxas /tmp/compile-ptx-src-add99a, line 64; error   : Feature 'cvt.bf16.f32' requires .target sm_80 or higher
W0629 11:52:48.851000 139012379337344 torch/_dynamo/convert_frame.py:824] ptxas /tmp/compile-ptx-src-add99a, line 65; error   : Feature '.bf16' requires .target sm_80 or higher
W0629 11:52:48.851000 139012379337344 torch/_dynamo/convert_frame.py:824] ptxas /tmp/compile-ptx-src-add99a, line 65; error   : Feature 'cvt.bf16.f32' requires .target sm_80 or higher
W0629 11:52:48.851000 139012379337344 torch/_dynamo/convert_frame.py:824] ptxas /tmp/compile-ptx-src-add99a, line 66; error   : Feature '.bf16' requires .target sm_80 or higher
W0629 11:52:48.851000 139012379337344 torch/_dynamo/convert_frame.py:824] ptxas /tmp/compile-ptx-src-add99a, line 66; error   : Feature 'cvt.bf16.f32' requires .target sm_80 or higher
W0629 11:52:48.851000 139012379337344 torch/_dynamo/convert_frame.py:824] ptxas /tmp/compile-ptx-src-add99a, line 67; error   : Feature '.bf16' requires .target sm_80 or higher
W0629 11:52:48.851000 139012379337344 torch/_dynamo/convert_frame.py:824] ptxas /tmp/compile-ptx-src-add99a, line 67; error   : Feature 'cvt.bf16.f32' requires .target sm_80 or higher
W0629 11:52:48.851000 139012379337344 torch/_dynamo/convert_frame.py:824] ptxas /tmp/compile-ptx-src-add99a, line 68; error   : Feature '.bf16' requires .target sm_80 or higher
W0629 11:52:48.851000 139012379337344 torch/_dynamo/convert_frame.py:824] ptxas /tmp/compile-ptx-src-add99a, line 68; error   : Feature 'cvt.bf16.f32' requires .target sm_80 or higher
W0629 11:52:48.851000 139012379337344 torch/_dynamo/convert_frame.py:824] ptxas /tmp/compile-ptx-src-add99a, line 69; error   : Feature '.bf16' requires .target sm_80 or higher
W0629 11:52:48.851000 139012379337344 torch/_dynamo/convert_frame.py:824] ptxas /tmp/compile-ptx-src-add99a, line 69; error   : Feature 'cvt.bf16.f32' requires .target sm_80 or higher
W0629 11:52:48.851000 139012379337344 torch/_dynamo/convert_frame.py:824] ptxas /tmp/compile-ptx-src-add99a, line 70; error   : Feature '.bf16' requires .target sm_80 or higher
W0629 11:52:48.851000 139012379337344 torch/_dynamo/convert_frame.py:824] ptxas /tmp/compile-ptx-src-add99a, line 70; error   : Feature 'cvt.bf16.f32' requires .target sm_80 or higher
W0629 11:52:48.851000 139012379337344 torch/_dynamo/convert_frame.py:824] ptxas fatal   : Ptx assembly aborted due to errors
W0629 11:52:48.851000 139012379337344 torch/_dynamo/convert_frame.py:824]
W0629 11:52:48.851000 139012379337344 torch/_dynamo/convert_frame.py:824]
W0629 11:52:48.851000 139012379337344 torch/_dynamo/convert_frame.py:824] Set TORCH_LOGS="+dynamo" and TORCHDYNAMO_VERBOSE=1 for more information
W0629 11:52:48.851000 139012379337344 torch/_dynamo/convert_frame.py:824]
W0629 11:52:48.851000 139012379337344 torch/_dynamo/convert_frame.py:824] Traceback (most recent call last):
W0629 11:52:48.851000 139012379337344 torch/_dynamo/convert_frame.py:824]   File "/usr/local/lib/python3.10/dist-packages/torch/_dynamo/convert_frame.py", line 786, in _convert_frame
W0629 11:52:48.851000 139012379337344 torch/_dynamo/convert_frame.py:824]     result = inner_convert(
W0629 11:52:48.851000 139012379337344 torch/_dynamo/convert_frame.py:824]   File "/usr/local/lib/python3.10/dist-packages/torch/_dynamo/convert_frame.py", line 400, in _convert_frame_assert
W0629 11:52:48.851000 139012379337344 torch/_dynamo/convert_frame.py:824]     return _compile(
W0629 11:52:48.851000 139012379337344 torch/_dynamo/convert_frame.py:824]   File "/usr/lib/python3.10/contextlib.py", line 79, in inner
W0629 11:52:48.851000 139012379337344 torch/_dynamo/convert_frame.py:824]     return func(*args, **kwds)
W0629 11:52:48.851000 139012379337344 torch/_dynamo/convert_frame.py:824]   File "/usr/local/lib/python3.10/dist-packages/torch/_dynamo/convert_frame.py", line 676, in _compile
W0629 11:52:48.851000 139012379337344 torch/_dynamo/convert_frame.py:824]     guarded_code = compile_inner(code, one_graph, hooks, transform)
W0629 11:52:48.851000 139012379337344 torch/_dynamo/convert_frame.py:824]   File "/usr/local/lib/python3.10/dist-packages/torch/_dynamo/utils.py", line 262, in time_wrapper
W0629 11:52:48.851000 139012379337344 torch/_dynamo/convert_frame.py:824]     r = func(*args, **kwargs)
W0629 11:52:48.851000 139012379337344 torch/_dynamo/convert_frame.py:824]   File "/usr/local/lib/python3.10/dist-packages/torch/_dynamo/convert_frame.py", line 535, in compile_inner
W0629 11:52:48.851000 139012379337344 torch/_dynamo/convert_frame.py:824]     out_code = transform_code_object(code, transform)
W0629 11:52:48.851000 139012379337344 torch/_dynamo/convert_frame.py:824]   File "/usr/local/lib/python3.10/dist-packages/torch/_dynamo/bytecode_transformation.py", line 1036, in transform_code_object
W0629 11:52:48.851000 139012379337344 torch/_dynamo/convert_frame.py:824]     transformations(instructions, code_options)
W0629 11:52:48.851000 139012379337344 torch/_dynamo/convert_frame.py:824]   File "/usr/local/lib/python3.10/dist-packages/torch/_dynamo/convert_frame.py", line 165, in _fn
W0629 11:52:48.851000 139012379337344 torch/_dynamo/convert_frame.py:824]     return fn(*args, **kwargs)
W0629 11:52:48.851000 139012379337344 torch/_dynamo/convert_frame.py:824]   File "/usr/local/lib/python3.10/dist-packages/torch/_dynamo/convert_frame.py", line 500, in transform
W0629 11:52:48.851000 139012379337344 torch/_dynamo/convert_frame.py:824]     tracer.run()
W0629 11:52:48.851000 139012379337344 torch/_dynamo/convert_frame.py:824]   File "/usr/local/lib/python3.10/dist-packages/torch/_dynamo/symbolic_convert.py", line 2149, in run
W0629 11:52:48.851000 139012379337344 torch/_dynamo/convert_frame.py:824]     super().run()
W0629 11:52:48.851000 139012379337344 torch/_dynamo/convert_frame.py:824]   File "/usr/local/lib/python3.10/dist-packages/torch/_dynamo/symbolic_convert.py", line 810, in run
W0629 11:52:48.851000 139012379337344 torch/_dynamo/convert_frame.py:824]     and self.step()
W0629 11:52:48.851000 139012379337344 torch/_dynamo/convert_frame.py:824]   File "/usr/local/lib/python3.10/dist-packages/torch/_dynamo/symbolic_convert.py", line 773, in step
W0629 11:52:48.851000 139012379337344 torch/_dynamo/convert_frame.py:824]     getattr(self, inst.opname)(inst)
W0629 11:52:48.851000 139012379337344 torch/_dynamo/convert_frame.py:824]   File "/usr/local/lib/python3.10/dist-packages/torch/_dynamo/symbolic_convert.py", line 2268, in RETURN_VALUE
W0629 11:52:48.851000 139012379337344 torch/_dynamo/convert_frame.py:824]     self.output.compile_subgraph(
W0629 11:52:48.851000 139012379337344 torch/_dynamo/convert_frame.py:824]   File "/usr/local/lib/python3.10/dist-packages/torch/_dynamo/output_graph.py", line 991, in compile_subgraph
W0629 11:52:48.851000 139012379337344 torch/_dynamo/convert_frame.py:824]     self.compile_and_call_fx_graph(tx, pass2.graph_output_vars(), root)
W0629 11:52:48.851000 139012379337344 torch/_dynamo/convert_frame.py:824]   File "/usr/lib/python3.10/contextlib.py", line 79, in inner
W0629 11:52:48.851000 139012379337344 torch/_dynamo/convert_frame.py:824]     return func(*args, **kwds)
W0629 11:52:48.851000 139012379337344 torch/_dynamo/convert_frame.py:824]   File "/usr/local/lib/python3.10/dist-packages/torch/_dynamo/output_graph.py", line 1168, in compile_and_call_fx_graph
W0629 11:52:48.851000 139012379337344 torch/_dynamo/convert_frame.py:824]     compiled_fn = self.call_user_compiler(gm)
W0629 11:52:48.851000 139012379337344 torch/_dynamo/convert_frame.py:824]   File "/usr/local/lib/python3.10/dist-packages/torch/_dynamo/utils.py", line 262, in time_wrapper
W0629 11:52:48.851000 139012379337344 torch/_dynamo/convert_frame.py:824]     r = func(*args, **kwargs)
W0629 11:52:48.851000 139012379337344 torch/_dynamo/convert_frame.py:824]   File "/usr/local/lib/python3.10/dist-packages/torch/_dynamo/output_graph.py", line 1241, in call_user_compiler
W0629 11:52:48.851000 139012379337344 torch/_dynamo/convert_frame.py:824]     raise BackendCompilerFailed(self.compiler_fn, e).with_traceback(
W0629 11:52:48.851000 139012379337344 torch/_dynamo/convert_frame.py:824]   File "/usr/local/lib/python3.10/dist-packages/torch/_dynamo/output_graph.py", line 1222, in call_user_compiler
W0629 11:52:48.851000 139012379337344 torch/_dynamo/convert_frame.py:824]     compiled_fn = compiler_fn(gm, self.example_inputs())
W0629 11:52:48.851000 139012379337344 torch/_dynamo/convert_frame.py:824]   File "/usr/local/lib/python3.10/dist-packages/torch/_dynamo/repro/after_dynamo.py", line 117, in debug_wrapper
W0629 11:52:48.851000 139012379337344 torch/_dynamo/convert_frame.py:824]     compiled_gm = compiler_fn(gm, example_inputs)
W0629 11:52:48.851000 139012379337344 torch/_dynamo/convert_frame.py:824]   File "/usr/local/lib/python3.10/dist-packages/torch/__init__.py", line 1729, in __call__
W0629 11:52:48.851000 139012379337344 torch/_dynamo/convert_frame.py:824]     return compile_fx(model_, inputs_, config_patches=self.config)
W0629 11:52:48.851000 139012379337344 torch/_dynamo/convert_frame.py:824]   File "/usr/lib/python3.10/contextlib.py", line 79, in inner
W0629 11:52:48.851000 139012379337344 torch/_dynamo/convert_frame.py:824]     return func(*args, **kwds)
W0629 11:52:48.851000 139012379337344 torch/_dynamo/convert_frame.py:824]   File "/usr/local/lib/python3.10/dist-packages/torch/_inductor/compile_fx.py", line 1330, in compile_fx
W0629 11:52:48.851000 139012379337344 torch/_dynamo/convert_frame.py:824]     return aot_autograd(
W0629 11:52:48.851000 139012379337344 torch/_dynamo/convert_frame.py:824]   File "/usr/local/lib/python3.10/dist-packages/torch/_dynamo/backends/common.py", line 58, in compiler_fn
W0629 11:52:48.851000 139012379337344 torch/_dynamo/convert_frame.py:824]     cg = aot_module_simplified(gm, example_inputs, **kwargs)
W0629 11:52:48.851000 139012379337344 torch/_dynamo/convert_frame.py:824]   File "/usr/local/lib/python3.10/dist-packages/torch/_functorch/aot_autograd.py", line 903, in aot_module_simplified
W0629 11:52:48.851000 139012379337344 torch/_dynamo/convert_frame.py:824]     compiled_fn = create_aot_dispatcher_function(
W0629 11:52:48.851000 139012379337344 torch/_dynamo/convert_frame.py:824]   File "/usr/local/lib/python3.10/dist-packages/torch/_dynamo/utils.py", line 262, in time_wrapper
W0629 11:52:48.851000 139012379337344 torch/_dynamo/convert_frame.py:824]     r = func(*args, **kwargs)
W0629 11:52:48.851000 139012379337344 torch/_dynamo/convert_frame.py:824]   File "/usr/local/lib/python3.10/dist-packages/torch/_functorch/aot_autograd.py", line 628, in create_aot_dispatcher_function
W0629 11:52:48.851000 139012379337344 torch/_dynamo/convert_frame.py:824]     compiled_fn = compiler_fn(flat_fn, fake_flat_args, aot_config, fw_metadata=fw_metadata)
W0629 11:52:48.851000 139012379337344 torch/_dynamo/convert_frame.py:824]   File "/usr/local/lib/python3.10/dist-packages/torch/_functorch/_aot_autograd/runtime_wrappers.py", line 443, in aot_wrapper_dedupe
W0629 11:52:48.851000 139012379337344 torch/_dynamo/convert_frame.py:824]     return compiler_fn(flat_fn, leaf_flat_args, aot_config, fw_metadata=fw_metadata)
W0629 11:52:48.851000 139012379337344 torch/_dynamo/convert_frame.py:824]   File "/usr/local/lib/python3.10/dist-packages/torch/_functorch/_aot_autograd/runtime_wrappers.py", line 648, in aot_wrapper_synthetic_base
W0629 11:52:48.851000 139012379337344 torch/_dynamo/convert_frame.py:824]     return compiler_fn(flat_fn, flat_args, aot_config, fw_metadata=fw_metadata)
W0629 11:52:48.851000 139012379337344 torch/_dynamo/convert_frame.py:824]   File "/usr/local/lib/python3.10/dist-packages/torch/_functorch/_aot_autograd/jit_compile_runtime_wrappers.py", line 119, in aot_dispatch_base
W0629 11:52:48.851000 139012379337344 torch/_dynamo/convert_frame.py:824]     compiled_fw = compiler(fw_module, updated_flat_args)
W0629 11:52:48.851000 139012379337344 torch/_dynamo/convert_frame.py:824]   File "/usr/local/lib/python3.10/dist-packages/torch/_dynamo/utils.py", line 262, in time_wrapper
W0629 11:52:48.851000 139012379337344 torch/_dynamo/convert_frame.py:824]     r = func(*args, **kwargs)
W0629 11:52:48.851000 139012379337344 torch/_dynamo/convert_frame.py:824]   File "/usr/local/lib/python3.10/dist-packages/torch/_inductor/compile_fx.py", line 1257, in fw_compiler_base
W0629 11:52:48.851000 139012379337344 torch/_dynamo/convert_frame.py:824]     return inner_compile(
W0629 11:52:48.851000 139012379337344 torch/_dynamo/convert_frame.py:824]   File "/usr/local/lib/python3.10/dist-packages/torch/_dynamo/repro/after_aot.py", line 83, in debug_wrapper
W0629 11:52:48.851000 139012379337344 torch/_dynamo/convert_frame.py:824]     inner_compiled_fn = compiler_fn(gm, example_inputs)
W0629 11:52:48.851000 139012379337344 torch/_dynamo/convert_frame.py:824]   File "/usr/local/lib/python3.10/dist-packages/torch/_inductor/debug.py", line 304, in inner
W0629 11:52:48.851000 139012379337344 torch/_dynamo/convert_frame.py:824]     return fn(*args, **kwargs)
W0629 11:52:48.851000 139012379337344 torch/_dynamo/convert_frame.py:824]   File "/usr/lib/python3.10/contextlib.py", line 79, in inner
W0629 11:52:48.851000 139012379337344 torch/_dynamo/convert_frame.py:824]     return func(*args, **kwds)
W0629 11:52:48.851000 139012379337344 torch/_dynamo/convert_frame.py:824]   File "/usr/lib/python3.10/contextlib.py", line 79, in inner
W0629 11:52:48.851000 139012379337344 torch/_dynamo/convert_frame.py:824]     return func(*args, **kwds)
W0629 11:52:48.851000 139012379337344 torch/_dynamo/convert_frame.py:824]   File "/usr/local/lib/python3.10/dist-packages/torch/_dynamo/utils.py", line 262, in time_wrapper
W0629 11:52:48.851000 139012379337344 torch/_dynamo/convert_frame.py:824]     r = func(*args, **kwargs)
W0629 11:52:48.851000 139012379337344 torch/_dynamo/convert_frame.py:824]   File "/usr/local/lib/python3.10/dist-packages/torch/_inductor/compile_fx.py", line 438, in compile_fx_inner
W0629 11:52:48.851000 139012379337344 torch/_dynamo/convert_frame.py:824]     compiled_graph = fx_codegen_and_compile(
W0629 11:52:48.851000 139012379337344 torch/_dynamo/convert_frame.py:824]   File "/usr/local/lib/python3.10/dist-packages/torch/_inductor/compile_fx.py", line 714, in fx_codegen_and_compile
W0629 11:52:48.851000 139012379337344 torch/_dynamo/convert_frame.py:824]     compiled_fn = graph.compile_to_fn()
W0629 11:52:48.851000 139012379337344 torch/_dynamo/convert_frame.py:824]   File "/usr/local/lib/python3.10/dist-packages/torch/_inductor/graph.py", line 1307, in compile_to_fn
W0629 11:52:48.851000 139012379337344 torch/_dynamo/convert_frame.py:824]     return self.compile_to_module().call
W0629 11:52:48.851000 139012379337344 torch/_dynamo/convert_frame.py:824]   File "/usr/local/lib/python3.10/dist-packages/torch/_dynamo/utils.py", line 262, in time_wrapper
W0629 11:52:48.851000 139012379337344 torch/_dynamo/convert_frame.py:824]     r = func(*args, **kwargs)
W0629 11:52:48.851000 139012379337344 torch/_dynamo/convert_frame.py:824]   File "/usr/local/lib/python3.10/dist-packages/torch/_inductor/graph.py", line 1254, in compile_to_module
W0629 11:52:48.851000 139012379337344 torch/_dynamo/convert_frame.py:824]     mod = PyCodeCache.load_by_key_path(
W0629 11:52:48.851000 139012379337344 torch/_dynamo/convert_frame.py:824]   File "/usr/local/lib/python3.10/dist-packages/torch/_inductor/codecache.py", line 2160, in load_by_key_path
W0629 11:52:48.851000 139012379337344 torch/_dynamo/convert_frame.py:824]     exec(code, mod.__dict__, mod.__dict__)
W0629 11:52:48.851000 139012379337344 torch/_dynamo/convert_frame.py:824]   File "/tmp/torchinductor_root/2q/c2qyt6tdezedsfihmagcwcwrww3uy5yyqk4ymr6yzhn6lg3uhdtn.py", line 419, in <module>
W0629 11:52:48.851000 139012379337344 torch/_dynamo/convert_frame.py:824]     async_compile.wait(globals())
W0629 11:52:48.851000 139012379337344 torch/_dynamo/convert_frame.py:824]   File "/usr/local/lib/python3.10/dist-packages/torch/_inductor/codecache.py", line 2715, in wait
W0629 11:52:48.851000 139012379337344 torch/_dynamo/convert_frame.py:824]     scope[key] = result.result()
W0629 11:52:48.851000 139012379337344 torch/_dynamo/convert_frame.py:824]   File "/usr/local/lib/python3.10/dist-packages/torch/_inductor/codecache.py", line 2522, in result
W0629 11:52:48.851000 139012379337344 torch/_dynamo/convert_frame.py:824]     self.future.result()
W0629 11:52:48.851000 139012379337344 torch/_dynamo/convert_frame.py:824]   File "/usr/lib/python3.10/concurrent/futures/_base.py", line 458, in result
W0629 11:52:48.851000 139012379337344 torch/_dynamo/convert_frame.py:824]     return self.__get_result()
W0629 11:52:48.851000 139012379337344 torch/_dynamo/convert_frame.py:824]   File "/usr/lib/python3.10/concurrent/futures/_base.py", line 403, in __get_result
W0629 11:52:48.851000 139012379337344 torch/_dynamo/convert_frame.py:824]     raise self._exception
W0629 11:52:48.851000 139012379337344 torch/_dynamo/convert_frame.py:824] torch._dynamo.exc.BackendCompilerFailed: backend='inductor' raised:
W0629 11:52:48.851000 139012379337344 torch/_dynamo/convert_frame.py:824] RuntimeError: Internal Triton PTX codegen error:
W0629 11:52:48.851000 139012379337344 torch/_dynamo/convert_frame.py:824] ptxas /tmp/compile-ptx-src-add99a, line 63; error   : Feature '.bf16' requires .target sm_80 or higher
W0629 11:52:48.851000 139012379337344 torch/_dynamo/convert_frame.py:824] ptxas /tmp/compile-ptx-src-add99a, line 63; error   : Feature 'cvt.bf16.f32' requires .target sm_80 or higher
W0629 11:52:48.851000 139012379337344 torch/_dynamo/convert_frame.py:824] ptxas /tmp/compile-ptx-src-add99a, line 64; error   : Feature '.bf16' requires .target sm_80 or higher
W0629 11:52:48.851000 139012379337344 torch/_dynamo/convert_frame.py:824] ptxas /tmp/compile-ptx-src-add99a, line 64; error   : Feature 'cvt.bf16.f32' requires .target sm_80 or higher
W0629 11:52:48.851000 139012379337344 torch/_dynamo/convert_frame.py:824] ptxas /tmp/compile-ptx-src-add99a, line 65; error   : Feature '.bf16' requires .target sm_80 or higher
W0629 11:52:48.851000 139012379337344 torch/_dynamo/convert_frame.py:824] ptxas /tmp/compile-ptx-src-add99a, line 65; error   : Feature 'cvt.bf16.f32' requires .target sm_80 or higher
W0629 11:52:48.851000 139012379337344 torch/_dynamo/convert_frame.py:824] ptxas /tmp/compile-ptx-src-add99a, line 66; error   : Feature '.bf16' requires .target sm_80 or higher
W0629 11:52:48.851000 139012379337344 torch/_dynamo/convert_frame.py:824] ptxas /tmp/compile-ptx-src-add99a, line 66; error   : Feature 'cvt.bf16.f32' requires .target sm_80 or higher
W0629 11:52:48.851000 139012379337344 torch/_dynamo/convert_frame.py:824] ptxas /tmp/compile-ptx-src-add99a, line 67; error   : Feature '.bf16' requires .target sm_80 or higher
W0629 11:52:48.851000 139012379337344 torch/_dynamo/convert_frame.py:824] ptxas /tmp/compile-ptx-src-add99a, line 67; error   : Feature 'cvt.bf16.f32' requires .target sm_80 or higher
W0629 11:52:48.851000 139012379337344 torch/_dynamo/convert_frame.py:824] ptxas /tmp/compile-ptx-src-add99a, line 68; error   : Feature '.bf16' requires .target sm_80 or higher
W0629 11:52:48.851000 139012379337344 torch/_dynamo/convert_frame.py:824] ptxas /tmp/compile-ptx-src-add99a, line 68; error   : Feature 'cvt.bf16.f32' requires .target sm_80 or higher
W0629 11:52:48.851000 139012379337344 torch/_dynamo/convert_frame.py:824] ptxas /tmp/compile-ptx-src-add99a, line 69; error   : Feature '.bf16' requires .target sm_80 or higher
W0629 11:52:48.851000 139012379337344 torch/_dynamo/convert_frame.py:824] ptxas /tmp/compile-ptx-src-add99a, line 69; error   : Feature 'cvt.bf16.f32' requires .target sm_80 or higher
W0629 11:52:48.851000 139012379337344 torch/_dynamo/convert_frame.py:824] ptxas /tmp/compile-ptx-src-add99a, line 70; error   : Feature '.bf16' requires .target sm_80 or higher
W0629 11:52:48.851000 139012379337344 torch/_dynamo/convert_frame.py:824] ptxas /tmp/compile-ptx-src-add99a, line 70; error   : Feature 'cvt.bf16.f32' requires .target sm_80 or higher
W0629 11:52:48.851000 139012379337344 torch/_dynamo/convert_frame.py:824] ptxas fatal   : Ptx assembly aborted due to errors
W0629 11:52:48.851000 139012379337344 torch/_dynamo/convert_frame.py:824]
W0629 11:52:48.851000 139012379337344 torch/_dynamo/convert_frame.py:824]
W0629 11:52:48.851000 139012379337344 torch/_dynamo/convert_frame.py:824] Set TORCH_LOGS="+dynamo" and TORCHDYNAMO_VERBOSE=1 for more information
W0629 11:52:48.851000 139012379337344 torch/_dynamo/convert_frame.py:824]
W0629 11:52:49.445000 139012379337344 torch/_dynamo/convert_frame.py:824] WON'T CONVERT forward /usr/local/lib/python3.10/dist-packages/transformers/models/bert/modeling_bert.py line 363
W0629 11:52:49.445000 139012379337344 torch/_dynamo/convert_frame.py:824] due to:
W0629 11:52:49.445000 139012379337344 torch/_dynamo/convert_frame.py:824] Traceback (most recent call last):
W0629 11:52:49.445000 139012379337344 torch/_dynamo/convert_frame.py:824]   File "/usr/local/lib/python3.10/dist-packages/torch/_dynamo/convert_frame.py", line 786, in _convert_frame
W0629 11:52:49.445000 139012379337344 torch/_dynamo/convert_frame.py:824]     result = inner_convert(
W0629 11:52:49.445000 139012379337344 torch/_dynamo/convert_frame.py:824]   File "/usr/local/lib/python3.10/dist-packages/torch/_dynamo/convert_frame.py", line 400, in _convert_frame_assert
W0629 11:52:49.445000 139012379337344 torch/_dynamo/convert_frame.py:824]     return _compile(
W0629 11:52:49.445000 139012379337344 torch/_dynamo/convert_frame.py:824]   File "/usr/lib/python3.10/contextlib.py", line 79, in inner
W0629 11:52:49.445000 139012379337344 torch/_dynamo/convert_frame.py:824]     return func(*args, **kwds)
W0629 11:52:49.445000 139012379337344 torch/_dynamo/convert_frame.py:824]   File "/usr/local/lib/python3.10/dist-packages/torch/_dynamo/convert_frame.py", line 676, in _compile
W0629 11:52:49.445000 139012379337344 torch/_dynamo/convert_frame.py:824]     guarded_code = compile_inner(code, one_graph, hooks, transform)
W0629 11:52:49.445000 139012379337344 torch/_dynamo/convert_frame.py:824]   File "/usr/local/lib/python3.10/dist-packages/torch/_dynamo/utils.py", line 262, in time_wrapper
W0629 11:52:49.445000 139012379337344 torch/_dynamo/convert_frame.py:824]     r = func(*args, **kwargs)
W0629 11:52:49.445000 139012379337344 torch/_dynamo/convert_frame.py:824]   File "/usr/local/lib/python3.10/dist-packages/torch/_dynamo/convert_frame.py", line 535, in compile_inner
W0629 11:52:49.445000 139012379337344 torch/_dynamo/convert_frame.py:824]     out_code = transform_code_object(code, transform)
W0629 11:52:49.445000 139012379337344 torch/_dynamo/convert_frame.py:824]   File "/usr/local/lib/python3.10/dist-packages/torch/_dynamo/bytecode_transformation.py", line 1036, in transform_code_object
W0629 11:52:49.445000 139012379337344 torch/_dynamo/convert_frame.py:824]     transformations(instructions, code_options)
W0629 11:52:49.445000 139012379337344 torch/_dynamo/convert_frame.py:824]   File "/usr/local/lib/python3.10/dist-packages/torch/_dynamo/convert_frame.py", line 165, in _fn
W0629 11:52:49.445000 139012379337344 torch/_dynamo/convert_frame.py:824]     return fn(*args, **kwargs)
W0629 11:52:49.445000 139012379337344 torch/_dynamo/convert_frame.py:824]   File "/usr/local/lib/python3.10/dist-packages/torch/_dynamo/convert_frame.py", line 500, in transform
W0629 11:52:49.445000 139012379337344 torch/_dynamo/convert_frame.py:824]     tracer.run()
W0629 11:52:49.445000 139012379337344 torch/_dynamo/convert_frame.py:824]   File "/usr/local/lib/python3.10/dist-packages/torch/_dynamo/symbolic_convert.py", line 2149, in run
W0629 11:52:49.445000 139012379337344 torch/_dynamo/convert_frame.py:824]     super().run()
W0629 11:52:49.445000 139012379337344 torch/_dynamo/convert_frame.py:824]   File "/usr/local/lib/python3.10/dist-packages/torch/_dynamo/symbolic_convert.py", line 810, in run
W0629 11:52:49.445000 139012379337344 torch/_dynamo/convert_frame.py:824]     and self.step()
W0629 11:52:49.445000 139012379337344 torch/_dynamo/convert_frame.py:824]   File "/usr/local/lib/python3.10/dist-packages/torch/_dynamo/symbolic_convert.py", line 773, in step
W0629 11:52:49.445000 139012379337344 torch/_dynamo/convert_frame.py:824]     getattr(self, inst.opname)(inst)
W0629 11:52:49.445000 139012379337344 torch/_dynamo/convert_frame.py:824]   File "/usr/local/lib/python3.10/dist-packages/torch/_dynamo/symbolic_convert.py", line 2268, in RETURN_VALUE
W0629 11:52:49.445000 139012379337344 torch/_dynamo/convert_frame.py:824]     self.output.compile_subgraph(
W0629 11:52:49.445000 139012379337344 torch/_dynamo/convert_frame.py:824]   File "/usr/local/lib/python3.10/dist-packages/torch/_dynamo/output_graph.py", line 991, in compile_subgraph
W0629 11:52:49.445000 139012379337344 torch/_dynamo/convert_frame.py:824]     self.compile_and_call_fx_graph(tx, pass2.graph_output_vars(), root)
W0629 11:52:49.445000 139012379337344 torch/_dynamo/convert_frame.py:824]   File "/usr/lib/python3.10/contextlib.py", line 79, in inner
W0629 11:52:49.445000 139012379337344 torch/_dynamo/convert_frame.py:824]     return func(*args, **kwds)
W0629 11:52:49.445000 139012379337344 torch/_dynamo/convert_frame.py:824]   File "/usr/local/lib/python3.10/dist-packages/torch/_dynamo/output_graph.py", line 1168, in compile_and_call_fx_graph
W0629 11:52:49.445000 139012379337344 torch/_dynamo/convert_frame.py:824]     compiled_fn = self.call_user_compiler(gm)
W0629 11:52:49.445000 139012379337344 torch/_dynamo/convert_frame.py:824]   File "/usr/local/lib/python3.10/dist-packages/torch/_dynamo/utils.py", line 262, in time_wrapper
W0629 11:52:49.445000 139012379337344 torch/_dynamo/convert_frame.py:824]     r = func(*args, **kwargs)
W0629 11:52:49.445000 139012379337344 torch/_dynamo/convert_frame.py:824]   File "/usr/local/lib/python3.10/dist-packages/torch/_dynamo/output_graph.py", line 1241, in call_user_compiler
W0629 11:52:49.445000 139012379337344 torch/_dynamo/convert_frame.py:824]     raise BackendCompilerFailed(self.compiler_fn, e).with_traceback(
W0629 11:52:49.445000 139012379337344 torch/_dynamo/convert_frame.py:824]   File "/usr/local/lib/python3.10/dist-packages/torch/_dynamo/output_graph.py", line 1222, in call_user_compiler
W0629 11:52:49.445000 139012379337344 torch/_dynamo/convert_frame.py:824]     compiled_fn = compiler_fn(gm, self.example_inputs())
W0629 11:52:49.445000 139012379337344 torch/_dynamo/convert_frame.py:824]   File "/usr/local/lib/python3.10/dist-packages/torch/_dynamo/repro/after_dynamo.py", line 117, in debug_wrapper
W0629 11:52:49.445000 139012379337344 torch/_dynamo/convert_frame.py:824]     compiled_gm = compiler_fn(gm, example_inputs)
W0629 11:52:49.445000 139012379337344 torch/_dynamo/convert_frame.py:824]   File "/usr/local/lib/python3.10/dist-packages/torch/__init__.py", line 1729, in __call__
W0629 11:52:49.445000 139012379337344 torch/_dynamo/convert_frame.py:824]     return compile_fx(model_, inputs_, config_patches=self.config)
W0629 11:52:49.445000 139012379337344 torch/_dynamo/convert_frame.py:824]   File "/usr/lib/python3.10/contextlib.py", line 79, in inner
W0629 11:52:49.445000 139012379337344 torch/_dynamo/convert_frame.py:824]     return func(*args, **kwds)
W0629 11:52:49.445000 139012379337344 torch/_dynamo/convert_frame.py:824]   File "/usr/local/lib/python3.10/dist-packages/torch/_inductor/compile_fx.py", line 1330, in compile_fx
W0629 11:52:49.445000 139012379337344 torch/_dynamo/convert_frame.py:824]     return aot_autograd(
W0629 11:52:49.445000 139012379337344 torch/_dynamo/convert_frame.py:824]   File "/usr/local/lib/python3.10/dist-packages/torch/_dynamo/backends/common.py", line 58, in compiler_fn
W0629 11:52:49.445000 139012379337344 torch/_dynamo/convert_frame.py:824]     cg = aot_module_simplified(gm, example_inputs, **kwargs)
W0629 11:52:49.445000 139012379337344 torch/_dynamo/convert_frame.py:824]   File "/usr/local/lib/python3.10/dist-packages/torch/_functorch/aot_autograd.py", line 903, in aot_module_simplified
W0629 11:52:49.445000 139012379337344 torch/_dynamo/convert_frame.py:824]     compiled_fn = create_aot_dispatcher_function(
W0629 11:52:49.445000 139012379337344 torch/_dynamo/convert_frame.py:824]   File "/usr/local/lib/python3.10/dist-packages/torch/_dynamo/utils.py", line 262, in time_wrapper
W0629 11:52:49.445000 139012379337344 torch/_dynamo/convert_frame.py:824]     r = func(*args, **kwargs)
W0629 11:52:49.445000 139012379337344 torch/_dynamo/convert_frame.py:824]   File "/usr/local/lib/python3.10/dist-packages/torch/_functorch/aot_autograd.py", line 628, in create_aot_dispatcher_function
W0629 11:52:49.445000 139012379337344 torch/_dynamo/convert_frame.py:824]     compiled_fn = compiler_fn(flat_fn, fake_flat_args, aot_config, fw_metadata=fw_metadata)
W0629 11:52:49.445000 139012379337344 torch/_dynamo/convert_frame.py:824]   File "/usr/local/lib/python3.10/dist-packages/torch/_functorch/_aot_autograd/runtime_wrappers.py", line 443, in aot_wrapper_dedupe
W0629 11:52:49.445000 139012379337344 torch/_dynamo/convert_frame.py:824]     return compiler_fn(flat_fn, leaf_flat_args, aot_config, fw_metadata=fw_metadata)
W0629 11:52:49.445000 139012379337344 torch/_dynamo/convert_frame.py:824]   File "/usr/local/lib/python3.10/dist-packages/torch/_functorch/_aot_autograd/runtime_wrappers.py", line 648, in aot_wrapper_synthetic_base
W0629 11:52:49.445000 139012379337344 torch/_dynamo/convert_frame.py:824]     return compiler_fn(flat_fn, flat_args, aot_config, fw_metadata=fw_metadata)
W0629 11:52:49.445000 139012379337344 torch/_dynamo/convert_frame.py:824]   File "/usr/local/lib/python3.10/dist-packages/torch/_functorch/_aot_autograd/jit_compile_runtime_wrappers.py", line 119, in aot_dispatch_base
W0629 11:52:49.445000 139012379337344 torch/_dynamo/convert_frame.py:824]     compiled_fw = compiler(fw_module, updated_flat_args)
W0629 11:52:49.445000 139012379337344 torch/_dynamo/convert_frame.py:824]   File "/usr/local/lib/python3.10/dist-packages/torch/_dynamo/utils.py", line 262, in time_wrapper
W0629 11:52:49.445000 139012379337344 torch/_dynamo/convert_frame.py:824]     r = func(*args, **kwargs)
W0629 11:52:49.445000 139012379337344 torch/_dynamo/convert_frame.py:824]   File "/usr/local/lib/python3.10/dist-packages/torch/_inductor/compile_fx.py", line 1257, in fw_compiler_base
W0629 11:52:49.445000 139012379337344 torch/_dynamo/convert_frame.py:824]     return inner_compile(
W0629 11:52:49.445000 139012379337344 torch/_dynamo/convert_frame.py:824]   File "/usr/local/lib/python3.10/dist-packages/torch/_dynamo/repro/after_aot.py", line 83, in debug_wrapper
W0629 11:52:49.445000 139012379337344 torch/_dynamo/convert_frame.py:824]     inner_compiled_fn = compiler_fn(gm, example_inputs)
W0629 11:52:49.445000 139012379337344 torch/_dynamo/convert_frame.py:824]   File "/usr/local/lib/python3.10/dist-packages/torch/_inductor/debug.py", line 304, in inner
W0629 11:52:49.445000 139012379337344 torch/_dynamo/convert_frame.py:824]     return fn(*args, **kwargs)
W0629 11:52:49.445000 139012379337344 torch/_dynamo/convert_frame.py:824]   File "/usr/lib/python3.10/contextlib.py", line 79, in inner
W0629 11:52:49.445000 139012379337344 torch/_dynamo/convert_frame.py:824]     return func(*args, **kwds)
W0629 11:52:49.445000 139012379337344 torch/_dynamo/convert_frame.py:824]   File "/usr/lib/python3.10/contextlib.py", line 79, in inner
W0629 11:52:49.445000 139012379337344 torch/_dynamo/convert_frame.py:824]     return func(*args, **kwds)
W0629 11:52:49.445000 139012379337344 torch/_dynamo/convert_frame.py:824]   File "/usr/local/lib/python3.10/dist-packages/torch/_dynamo/utils.py", line 262, in time_wrapper
W0629 11:52:49.445000 139012379337344 torch/_dynamo/convert_frame.py:824]     r = func(*args, **kwargs)
W0629 11:52:49.445000 139012379337344 torch/_dynamo/convert_frame.py:824]   File "/usr/local/lib/python3.10/dist-packages/torch/_inductor/compile_fx.py", line 438, in compile_fx_inner
W0629 11:52:49.445000 139012379337344 torch/_dynamo/convert_frame.py:824]     compiled_graph = fx_codegen_and_compile(
W0629 11:52:49.445000 139012379337344 torch/_dynamo/convert_frame.py:824]   File "/usr/local/lib/python3.10/dist-packages/torch/_inductor/compile_fx.py", line 714, in fx_codegen_and_compile
W0629 11:52:49.445000 139012379337344 torch/_dynamo/convert_frame.py:824]     compiled_fn = graph.compile_to_fn()
W0629 11:52:49.445000 139012379337344 torch/_dynamo/convert_frame.py:824]   File "/usr/local/lib/python3.10/dist-packages/torch/_inductor/graph.py", line 1307, in compile_to_fn
W0629 11:52:49.445000 139012379337344 torch/_dynamo/convert_frame.py:824]     return self.compile_to_module().call
W0629 11:52:49.445000 139012379337344 torch/_dynamo/convert_frame.py:824]   File "/usr/local/lib/python3.10/dist-packages/torch/_dynamo/utils.py", line 262, in time_wrapper
W0629 11:52:49.445000 139012379337344 torch/_dynamo/convert_frame.py:824]     r = func(*args, **kwargs)
W0629 11:52:49.445000 139012379337344 torch/_dynamo/convert_frame.py:824]   File "/usr/local/lib/python3.10/dist-packages/torch/_inductor/graph.py", line 1254, in compile_to_module
W0629 11:52:49.445000 139012379337344 torch/_dynamo/convert_frame.py:824]     mod = PyCodeCache.load_by_key_path(
W0629 11:52:49.445000 139012379337344 torch/_dynamo/convert_frame.py:824]   File "/usr/local/lib/python3.10/dist-packages/torch/_inductor/codecache.py", line 2160, in load_by_key_path
W0629 11:52:49.445000 139012379337344 torch/_dynamo/convert_frame.py:824]     exec(code, mod.__dict__, mod.__dict__)
W0629 11:52:49.445000 139012379337344 torch/_dynamo/convert_frame.py:824]   File "/tmp/torchinductor_root/c4/cc4efsa7nxktr5aeledbm3aypz3khxyc4xavti7ycnwonffhjjni.py", line 348, in <module>
W0629 11:52:49.445000 139012379337344 torch/_dynamo/convert_frame.py:824]     async_compile.wait(globals())
W0629 11:52:49.445000 139012379337344 torch/_dynamo/convert_frame.py:824]   File "/usr/local/lib/python3.10/dist-packages/torch/_inductor/codecache.py", line 2715, in wait
W0629 11:52:49.445000 139012379337344 torch/_dynamo/convert_frame.py:824]     scope[key] = result.result()
W0629 11:52:49.445000 139012379337344 torch/_dynamo/convert_frame.py:824]   File "/usr/local/lib/python3.10/dist-packages/torch/_inductor/codecache.py", line 2522, in result
W0629 11:52:49.445000 139012379337344 torch/_dynamo/convert_frame.py:824]     self.future.result()
W0629 11:52:49.445000 139012379337344 torch/_dynamo/convert_frame.py:824]   File "/usr/lib/python3.10/concurrent/futures/_base.py", line 458, in result
W0629 11:52:49.445000 139012379337344 torch/_dynamo/convert_frame.py:824]     return self.__get_result()
W0629 11:52:49.445000 139012379337344 torch/_dynamo/convert_frame.py:824]   File "/usr/lib/python3.10/concurrent/futures/_base.py", line 403, in __get_result
W0629 11:52:49.445000 139012379337344 torch/_dynamo/convert_frame.py:824]     raise self._exception
W0629 11:52:49.445000 139012379337344 torch/_dynamo/convert_frame.py:824] torch._dynamo.exc.BackendCompilerFailed: backend='inductor' raised:
W0629 11:52:49.445000 139012379337344 torch/_dynamo/convert_frame.py:824] RuntimeError: Internal Triton PTX codegen error:
W0629 11:52:49.445000 139012379337344 torch/_dynamo/convert_frame.py:824] ptxas /tmp/compile-ptx-src-052f27, line 63; error   : Feature '.bf16' requires .target sm_80 or higher
W0629 11:52:49.445000 139012379337344 torch/_dynamo/convert_frame.py:824] ptxas /tmp/compile-ptx-src-052f27, line 63; error   : Feature 'cvt.bf16.f32' requires .target sm_80 or higher
W0629 11:52:49.445000 139012379337344 torch/_dynamo/convert_frame.py:824] ptxas /tmp/compile-ptx-src-052f27, line 64; error   : Feature '.bf16' requires .target sm_80 or higher
W0629 11:52:49.445000 139012379337344 torch/_dynamo/convert_frame.py:824] ptxas /tmp/compile-ptx-src-052f27, line 64; error   : Feature 'cvt.bf16.f32' requires .target sm_80 or higher
W0629 11:52:49.445000 139012379337344 torch/_dynamo/convert_frame.py:824] ptxas /tmp/compile-ptx-src-052f27, line 65; error   : Feature '.bf16' requires .target sm_80 or higher
W0629 11:52:49.445000 139012379337344 torch/_dynamo/convert_frame.py:824] ptxas /tmp/compile-ptx-src-052f27, line 65; error   : Feature 'cvt.bf16.f32' requires .target sm_80 or higher
W0629 11:52:49.445000 139012379337344 torch/_dynamo/convert_frame.py:824] ptxas /tmp/compile-ptx-src-052f27, line 66; error   : Feature '.bf16' requires .target sm_80 or higher
W0629 11:52:49.445000 139012379337344 torch/_dynamo/convert_frame.py:824] ptxas /tmp/compile-ptx-src-052f27, line 66; error   : Feature 'cvt.bf16.f32' requires .target sm_80 or higher
W0629 11:52:49.445000 139012379337344 torch/_dynamo/convert_frame.py:824] ptxas /tmp/compile-ptx-src-052f27, line 67; error   : Feature '.bf16' requires .target sm_80 or higher
W0629 11:52:49.445000 139012379337344 torch/_dynamo/convert_frame.py:824] ptxas /tmp/compile-ptx-src-052f27, line 67; error   : Feature 'cvt.bf16.f32' requires .target sm_80 or higher
W0629 11:52:49.445000 139012379337344 torch/_dynamo/convert_frame.py:824] ptxas /tmp/compile-ptx-src-052f27, line 68; error   : Feature '.bf16' requires .target sm_80 or higher
W0629 11:52:49.445000 139012379337344 torch/_dynamo/convert_frame.py:824] ptxas /tmp/compile-ptx-src-052f27, line 68; error   : Feature 'cvt.bf16.f32' requires .target sm_80 or higher
W0629 11:52:49.445000 139012379337344 torch/_dynamo/convert_frame.py:824] ptxas /tmp/compile-ptx-src-052f27, line 69; error   : Feature '.bf16' requires .target sm_80 or higher
W0629 11:52:49.445000 139012379337344 torch/_dynamo/convert_frame.py:824] ptxas /tmp/compile-ptx-src-052f27, line 69; error   : Feature 'cvt.bf16.f32' requires .target sm_80 or higher
W0629 11:52:49.445000 139012379337344 torch/_dynamo/convert_frame.py:824] ptxas /tmp/compile-ptx-src-052f27, line 70; error   : Feature '.bf16' requires .target sm_80 or higher
W0629 11:52:49.445000 139012379337344 torch/_dynamo/convert_frame.py:824] ptxas /tmp/compile-ptx-src-052f27, line 70; error   : Feature 'cvt.bf16.f32' requires .target sm_80 or higher
W0629 11:52:49.445000 139012379337344 torch/_dynamo/convert_frame.py:824] ptxas fatal   : Ptx assembly aborted due to errors
W0629 11:52:49.445000 139012379337344 torch/_dynamo/convert_frame.py:824]
W0629 11:52:49.445000 139012379337344 torch/_dynamo/convert_frame.py:824]
W0629 11:52:49.445000 139012379337344 torch/_dynamo/convert_frame.py:824] Set TORCH_LOGS="+dynamo" and TORCHDYNAMO_VERBOSE=1 for more information
W0629 11:52:49.445000 139012379337344 torch/_dynamo/convert_frame.py:824]
W0629 11:52:49.445000 139012379337344 torch/_dynamo/convert_frame.py:824] Traceback (most recent call last):
W0629 11:52:49.445000 139012379337344 torch/_dynamo/convert_frame.py:824]   File "/usr/local/lib/python3.10/dist-packages/torch/_dynamo/convert_frame.py", line 786, in _convert_frame
W0629 11:52:49.445000 139012379337344 torch/_dynamo/convert_frame.py:824]     result = inner_convert(
W0629 11:52:49.445000 139012379337344 torch/_dynamo/convert_frame.py:824]   File "/usr/local/lib/python3.10/dist-packages/torch/_dynamo/convert_frame.py", line 400, in _convert_frame_assert
W0629 11:52:49.445000 139012379337344 torch/_dynamo/convert_frame.py:824]     return _compile(
W0629 11:52:49.445000 139012379337344 torch/_dynamo/convert_frame.py:824]   File "/usr/lib/python3.10/contextlib.py", line 79, in inner
W0629 11:52:49.445000 139012379337344 torch/_dynamo/convert_frame.py:824]     return func(*args, **kwds)
W0629 11:52:49.445000 139012379337344 torch/_dynamo/convert_frame.py:824]   File "/usr/local/lib/python3.10/dist-packages/torch/_dynamo/convert_frame.py", line 676, in _compile
W0629 11:52:49.445000 139012379337344 torch/_dynamo/convert_frame.py:824]     guarded_code = compile_inner(code, one_graph, hooks, transform)
W0629 11:52:49.445000 139012379337344 torch/_dynamo/convert_frame.py:824]   File "/usr/local/lib/python3.10/dist-packages/torch/_dynamo/utils.py", line 262, in time_wrapper
W0629 11:52:49.445000 139012379337344 torch/_dynamo/convert_frame.py:824]     r = func(*args, **kwargs)
W0629 11:52:49.445000 139012379337344 torch/_dynamo/convert_frame.py:824]   File "/usr/local/lib/python3.10/dist-packages/torch/_dynamo/convert_frame.py", line 535, in compile_inner
W0629 11:52:49.445000 139012379337344 torch/_dynamo/convert_frame.py:824]     out_code = transform_code_object(code, transform)
W0629 11:52:49.445000 139012379337344 torch/_dynamo/convert_frame.py:824]   File "/usr/local/lib/python3.10/dist-packages/torch/_dynamo/bytecode_transformation.py", line 1036, in transform_code_object
W0629 11:52:49.445000 139012379337344 torch/_dynamo/convert_frame.py:824]     transformations(instructions, code_options)
W0629 11:52:49.445000 139012379337344 torch/_dynamo/convert_frame.py:824]   File "/usr/local/lib/python3.10/dist-packages/torch/_dynamo/convert_frame.py", line 165, in _fn
W0629 11:52:49.445000 139012379337344 torch/_dynamo/convert_frame.py:824]     return fn(*args, **kwargs)
W0629 11:52:49.445000 139012379337344 torch/_dynamo/convert_frame.py:824]   File "/usr/local/lib/python3.10/dist-packages/torch/_dynamo/convert_frame.py", line 500, in transform
W0629 11:52:49.445000 139012379337344 torch/_dynamo/convert_frame.py:824]     tracer.run()
W0629 11:52:49.445000 139012379337344 torch/_dynamo/convert_frame.py:824]   File "/usr/local/lib/python3.10/dist-packages/torch/_dynamo/symbolic_convert.py", line 2149, in run
W0629 11:52:49.445000 139012379337344 torch/_dynamo/convert_frame.py:824]     super().run()
W0629 11:52:49.445000 139012379337344 torch/_dynamo/convert_frame.py:824]   File "/usr/local/lib/python3.10/dist-packages/torch/_dynamo/symbolic_convert.py", line 810, in run
W0629 11:52:49.445000 139012379337344 torch/_dynamo/convert_frame.py:824]     and self.step()
W0629 11:52:49.445000 139012379337344 torch/_dynamo/convert_frame.py:824]   File "/usr/local/lib/python3.10/dist-packages/torch/_dynamo/symbolic_convert.py", line 773, in step
W0629 11:52:49.445000 139012379337344 torch/_dynamo/convert_frame.py:824]     getattr(self, inst.opname)(inst)
W0629 11:52:49.445000 139012379337344 torch/_dynamo/convert_frame.py:824]   File "/usr/local/lib/python3.10/dist-packages/torch/_dynamo/symbolic_convert.py", line 2268, in RETURN_VALUE
W0629 11:52:49.445000 139012379337344 torch/_dynamo/convert_frame.py:824]     self.output.compile_subgraph(
W0629 11:52:49.445000 139012379337344 torch/_dynamo/convert_frame.py:824]   File "/usr/local/lib/python3.10/dist-packages/torch/_dynamo/output_graph.py", line 991, in compile_subgraph
W0629 11:52:49.445000 139012379337344 torch/_dynamo/convert_frame.py:824]     self.compile_and_call_fx_graph(tx, pass2.graph_output_vars(), root)
W0629 11:52:49.445000 139012379337344 torch/_dynamo/convert_frame.py:824]   File "/usr/lib/python3.10/contextlib.py", line 79, in inner
W0629 11:52:49.445000 139012379337344 torch/_dynamo/convert_frame.py:824]     return func(*args, **kwds)
W0629 11:52:49.445000 139012379337344 torch/_dynamo/convert_frame.py:824]   File "/usr/local/lib/python3.10/dist-packages/torch/_dynamo/output_graph.py", line 1168, in compile_and_call_fx_graph
W0629 11:52:49.445000 139012379337344 torch/_dynamo/convert_frame.py:824]     compiled_fn = self.call_user_compiler(gm)
W0629 11:52:49.445000 139012379337344 torch/_dynamo/convert_frame.py:824]   File "/usr/local/lib/python3.10/dist-packages/torch/_dynamo/utils.py", line 262, in time_wrapper
W0629 11:52:49.445000 139012379337344 torch/_dynamo/convert_frame.py:824]     r = func(*args, **kwargs)
W0629 11:52:49.445000 139012379337344 torch/_dynamo/convert_frame.py:824]   File "/usr/local/lib/python3.10/dist-packages/torch/_dynamo/output_graph.py", line 1241, in call_user_compiler
W0629 11:52:49.445000 139012379337344 torch/_dynamo/convert_frame.py:824]     raise BackendCompilerFailed(self.compiler_fn, e).with_traceback(
W0629 11:52:49.445000 139012379337344 torch/_dynamo/convert_frame.py:824]   File "/usr/local/lib/python3.10/dist-packages/torch/_dynamo/output_graph.py", line 1222, in call_user_compiler
W0629 11:52:49.445000 139012379337344 torch/_dynamo/convert_frame.py:824]     compiled_fn = compiler_fn(gm, self.example_inputs())
W0629 11:52:49.445000 139012379337344 torch/_dynamo/convert_frame.py:824]   File "/usr/local/lib/python3.10/dist-packages/torch/_dynamo/repro/after_dynamo.py", line 117, in debug_wrapper
W0629 11:52:49.445000 139012379337344 torch/_dynamo/convert_frame.py:824]     compiled_gm = compiler_fn(gm, example_inputs)
W0629 11:52:49.445000 139012379337344 torch/_dynamo/convert_frame.py:824]   File "/usr/local/lib/python3.10/dist-packages/torch/__init__.py", line 1729, in __call__
W0629 11:52:49.445000 139012379337344 torch/_dynamo/convert_frame.py:824]     return compile_fx(model_, inputs_, config_patches=self.config)
W0629 11:52:49.445000 139012379337344 torch/_dynamo/convert_frame.py:824]   File "/usr/lib/python3.10/contextlib.py", line 79, in inner
W0629 11:52:49.445000 139012379337344 torch/_dynamo/convert_frame.py:824]     return func(*args, **kwds)
W0629 11:52:49.445000 139012379337344 torch/_dynamo/convert_frame.py:824]   File "/usr/local/lib/python3.10/dist-packages/torch/_inductor/compile_fx.py", line 1330, in compile_fx
W0629 11:52:49.445000 139012379337344 torch/_dynamo/convert_frame.py:824]     return aot_autograd(
W0629 11:52:49.445000 139012379337344 torch/_dynamo/convert_frame.py:824]   File "/usr/local/lib/python3.10/dist-packages/torch/_dynamo/backends/common.py", line 58, in compiler_fn
W0629 11:52:49.445000 139012379337344 torch/_dynamo/convert_frame.py:824]     cg = aot_module_simplified(gm, example_inputs, **kwargs)
W0629 11:52:49.445000 139012379337344 torch/_dynamo/convert_frame.py:824]   File "/usr/local/lib/python3.10/dist-packages/torch/_functorch/aot_autograd.py", line 903, in aot_module_simplified
W0629 11:52:49.445000 139012379337344 torch/_dynamo/convert_frame.py:824]     compiled_fn = create_aot_dispatcher_function(
W0629 11:52:49.445000 139012379337344 torch/_dynamo/convert_frame.py:824]   File "/usr/local/lib/python3.10/dist-packages/torch/_dynamo/utils.py", line 262, in time_wrapper
W0629 11:52:49.445000 139012379337344 torch/_dynamo/convert_frame.py:824]     r = func(*args, **kwargs)
W0629 11:52:49.445000 139012379337344 torch/_dynamo/convert_frame.py:824]   File "/usr/local/lib/python3.10/dist-packages/torch/_functorch/aot_autograd.py", line 628, in create_aot_dispatcher_function
W0629 11:52:49.445000 139012379337344 torch/_dynamo/convert_frame.py:824]     compiled_fn = compiler_fn(flat_fn, fake_flat_args, aot_config, fw_metadata=fw_metadata)
W0629 11:52:49.445000 139012379337344 torch/_dynamo/convert_frame.py:824]   File "/usr/local/lib/python3.10/dist-packages/torch/_functorch/_aot_autograd/runtime_wrappers.py", line 443, in aot_wrapper_dedupe
W0629 11:52:49.445000 139012379337344 torch/_dynamo/convert_frame.py:824]     return compiler_fn(flat_fn, leaf_flat_args, aot_config, fw_metadata=fw_metadata)
W0629 11:52:49.445000 139012379337344 torch/_dynamo/convert_frame.py:824]   File "/usr/local/lib/python3.10/dist-packages/torch/_functorch/_aot_autograd/runtime_wrappers.py", line 648, in aot_wrapper_synthetic_base
W0629 11:52:49.445000 139012379337344 torch/_dynamo/convert_frame.py:824]     return compiler_fn(flat_fn, flat_args, aot_config, fw_metadata=fw_metadata)
W0629 11:52:49.445000 139012379337344 torch/_dynamo/convert_frame.py:824]   File "/usr/local/lib/python3.10/dist-packages/torch/_functorch/_aot_autograd/jit_compile_runtime_wrappers.py", line 119, in aot_dispatch_base
W0629 11:52:49.445000 139012379337344 torch/_dynamo/convert_frame.py:824]     compiled_fw = compiler(fw_module, updated_flat_args)
W0629 11:52:49.445000 139012379337344 torch/_dynamo/convert_frame.py:824]   File "/usr/local/lib/python3.10/dist-packages/torch/_dynamo/utils.py", line 262, in time_wrapper
W0629 11:52:49.445000 139012379337344 torch/_dynamo/convert_frame.py:824]     r = func(*args, **kwargs)
W0629 11:52:49.445000 139012379337344 torch/_dynamo/convert_frame.py:824]   File "/usr/local/lib/python3.10/dist-packages/torch/_inductor/compile_fx.py", line 1257, in fw_compiler_base
W0629 11:52:49.445000 139012379337344 torch/_dynamo/convert_frame.py:824]     return inner_compile(
W0629 11:52:49.445000 139012379337344 torch/_dynamo/convert_frame.py:824]   File "/usr/local/lib/python3.10/dist-packages/torch/_dynamo/repro/after_aot.py", line 83, in debug_wrapper
W0629 11:52:49.445000 139012379337344 torch/_dynamo/convert_frame.py:824]     inner_compiled_fn = compiler_fn(gm, example_inputs)
W0629 11:52:49.445000 139012379337344 torch/_dynamo/convert_frame.py:824]   File "/usr/local/lib/python3.10/dist-packages/torch/_inductor/debug.py", line 304, in inner
W0629 11:52:49.445000 139012379337344 torch/_dynamo/convert_frame.py:824]     return fn(*args, **kwargs)
W0629 11:52:49.445000 139012379337344 torch/_dynamo/convert_frame.py:824]   File "/usr/lib/python3.10/contextlib.py", line 79, in inner
W0629 11:52:49.445000 139012379337344 torch/_dynamo/convert_frame.py:824]     return func(*args, **kwds)
W0629 11:52:49.445000 139012379337344 torch/_dynamo/convert_frame.py:824]   File "/usr/lib/python3.10/contextlib.py", line 79, in inner
W0629 11:52:49.445000 139012379337344 torch/_dynamo/convert_frame.py:824]     return func(*args, **kwds)
W0629 11:52:49.445000 139012379337344 torch/_dynamo/convert_frame.py:824]   File "/usr/local/lib/python3.10/dist-packages/torch/_dynamo/utils.py", line 262, in time_wrapper
W0629 11:52:49.445000 139012379337344 torch/_dynamo/convert_frame.py:824]     r = func(*args, **kwargs)
W0629 11:52:49.445000 139012379337344 torch/_dynamo/convert_frame.py:824]   File "/usr/local/lib/python3.10/dist-packages/torch/_inductor/compile_fx.py", line 438, in compile_fx_inner
W0629 11:52:49.445000 139012379337344 torch/_dynamo/convert_frame.py:824]     compiled_graph = fx_codegen_and_compile(
W0629 11:52:49.445000 139012379337344 torch/_dynamo/convert_frame.py:824]   File "/usr/local/lib/python3.10/dist-packages/torch/_inductor/compile_fx.py", line 714, in fx_codegen_and_compile
W0629 11:52:49.445000 139012379337344 torch/_dynamo/convert_frame.py:824]     compiled_fn = graph.compile_to_fn()
W0629 11:52:49.445000 139012379337344 torch/_dynamo/convert_frame.py:824]   File "/usr/local/lib/python3.10/dist-packages/torch/_inductor/graph.py", line 1307, in compile_to_fn
W0629 11:52:49.445000 139012379337344 torch/_dynamo/convert_frame.py:824]     return self.compile_to_module().call
W0629 11:52:49.445000 139012379337344 torch/_dynamo/convert_frame.py:824]   File "/usr/local/lib/python3.10/dist-packages/torch/_dynamo/utils.py", line 262, in time_wrapper
W0629 11:52:49.445000 139012379337344 torch/_dynamo/convert_frame.py:824]     r = func(*args, **kwargs)
W0629 11:52:49.445000 139012379337344 torch/_dynamo/convert_frame.py:824]   File "/usr/local/lib/python3.10/dist-packages/torch/_inductor/graph.py", line 1254, in compile_to_module
W0629 11:52:49.445000 139012379337344 torch/_dynamo/convert_frame.py:824]     mod = PyCodeCache.load_by_key_path(
W0629 11:52:49.445000 139012379337344 torch/_dynamo/convert_frame.py:824]   File "/usr/local/lib/python3.10/dist-packages/torch/_inductor/codecache.py", line 2160, in load_by_key_path
W0629 11:52:49.445000 139012379337344 torch/_dynamo/convert_frame.py:824]     exec(code, mod.__dict__, mod.__dict__)
W0629 11:52:49.445000 139012379337344 torch/_dynamo/convert_frame.py:824]   File "/tmp/torchinductor_root/c4/cc4efsa7nxktr5aeledbm3aypz3khxyc4xavti7ycnwonffhjjni.py", line 348, in <module>
W0629 11:52:49.445000 139012379337344 torch/_dynamo/convert_frame.py:824]     async_compile.wait(globals())
W0629 11:52:49.445000 139012379337344 torch/_dynamo/convert_frame.py:824]   File "/usr/local/lib/python3.10/dist-packages/torch/_inductor/codecache.py", line 2715, in wait
W0629 11:52:49.445000 139012379337344 torch/_dynamo/convert_frame.py:824]     scope[key] = result.result()
W0629 11:52:49.445000 139012379337344 torch/_dynamo/convert_frame.py:824]   File "/usr/local/lib/python3.10/dist-packages/torch/_inductor/codecache.py", line 2522, in result
W0629 11:52:49.445000 139012379337344 torch/_dynamo/convert_frame.py:824]     self.future.result()
W0629 11:52:49.445000 139012379337344 torch/_dynamo/convert_frame.py:824]   File "/usr/lib/python3.10/concurrent/futures/_base.py", line 458, in result
W0629 11:52:49.445000 139012379337344 torch/_dynamo/convert_frame.py:824]     return self.__get_result()
W0629 11:52:49.445000 139012379337344 torch/_dynamo/convert_frame.py:824]   File "/usr/lib/python3.10/concurrent/futures/_base.py", line 403, in __get_result
W0629 11:52:49.445000 139012379337344 torch/_dynamo/convert_frame.py:824]     raise self._exception
W0629 11:52:49.445000 139012379337344 torch/_dynamo/convert_frame.py:824] torch._dynamo.exc.BackendCompilerFailed: backend='inductor' raised:
W0629 11:52:49.445000 139012379337344 torch/_dynamo/convert_frame.py:824] RuntimeError: Internal Triton PTX codegen error:
W0629 11:52:49.445000 139012379337344 torch/_dynamo/convert_frame.py:824] ptxas /tmp/compile-ptx-src-052f27, line 63; error   : Feature '.bf16' requires .target sm_80 or higher
W0629 11:52:49.445000 139012379337344 torch/_dynamo/convert_frame.py:824] ptxas /tmp/compile-ptx-src-052f27, line 63; error   : Feature 'cvt.bf16.f32' requires .target sm_80 or higher
W0629 11:52:49.445000 139012379337344 torch/_dynamo/convert_frame.py:824] ptxas /tmp/compile-ptx-src-052f27, line 64; error   : Feature '.bf16' requires .target sm_80 or higher
W0629 11:52:49.445000 139012379337344 torch/_dynamo/convert_frame.py:824] ptxas /tmp/compile-ptx-src-052f27, line 64; error   : Feature 'cvt.bf16.f32' requires .target sm_80 or higher
W0629 11:52:49.445000 139012379337344 torch/_dynamo/convert_frame.py:824] ptxas /tmp/compile-ptx-src-052f27, line 65; error   : Feature '.bf16' requires .target sm_80 or higher
W0629 11:52:49.445000 139012379337344 torch/_dynamo/convert_frame.py:824] ptxas /tmp/compile-ptx-src-052f27, line 65; error   : Feature 'cvt.bf16.f32' requires .target sm_80 or higher
W0629 11:52:49.445000 139012379337344 torch/_dynamo/convert_frame.py:824] ptxas /tmp/compile-ptx-src-052f27, line 66; error   : Feature '.bf16' requires .target sm_80 or higher
W0629 11:52:49.445000 139012379337344 torch/_dynamo/convert_frame.py:824] ptxas /tmp/compile-ptx-src-052f27, line 66; error   : Feature 'cvt.bf16.f32' requires .target sm_80 or higher
W0629 11:52:49.445000 139012379337344 torch/_dynamo/convert_frame.py:824] ptxas /tmp/compile-ptx-src-052f27, line 67; error   : Feature '.bf16' requires .target sm_80 or higher
W0629 11:52:49.445000 139012379337344 torch/_dynamo/convert_frame.py:824] ptxas /tmp/compile-ptx-src-052f27, line 67; error   : Feature 'cvt.bf16.f32' requires .target sm_80 or higher
W0629 11:52:49.445000 139012379337344 torch/_dynamo/convert_frame.py:824] ptxas /tmp/compile-ptx-src-052f27, line 68; error   : Feature '.bf16' requires .target sm_80 or higher
W0629 11:52:49.445000 139012379337344 torch/_dynamo/convert_frame.py:824] ptxas /tmp/compile-ptx-src-052f27, line 68; error   : Feature 'cvt.bf16.f32' requires .target sm_80 or higher
W0629 11:52:49.445000 139012379337344 torch/_dynamo/convert_frame.py:824] ptxas /tmp/compile-ptx-src-052f27, line 69; error   : Feature '.bf16' requires .target sm_80 or higher
W0629 11:52:49.445000 139012379337344 torch/_dynamo/convert_frame.py:824] ptxas /tmp/compile-ptx-src-052f27, line 69; error   : Feature 'cvt.bf16.f32' requires .target sm_80 or higher
W0629 11:52:49.445000 139012379337344 torch/_dynamo/convert_frame.py:824] ptxas /tmp/compile-ptx-src-052f27, line 70; error   : Feature '.bf16' requires .target sm_80 or higher
W0629 11:52:49.445000 139012379337344 torch/_dynamo/convert_frame.py:824] ptxas /tmp/compile-ptx-src-052f27, line 70; error   : Feature 'cvt.bf16.f32' requires .target sm_80 or higher
W0629 11:52:49.445000 139012379337344 torch/_dynamo/convert_frame.py:824] ptxas fatal   : Ptx assembly aborted due to errors
W0629 11:52:49.445000 139012379337344 torch/_dynamo/convert_frame.py:824]
W0629 11:52:49.445000 139012379337344 torch/_dynamo/convert_frame.py:824]
W0629 11:52:49.445000 139012379337344 torch/_dynamo/convert_frame.py:824] Set TORCH_LOGS="+dynamo" and TORCHDYNAMO_VERBOSE=1 for more information
W0629 11:52:49.445000 139012379337344 torch/_dynamo/convert_frame.py:824]
W0629 11:52:49.853000 139012379337344 torch/_dynamo/convert_frame.py:824] WON'T CONVERT forward /usr/local/lib/python3.10/dist-packages/transformers/models/bert/modeling_bert.py line 460
W0629 11:52:49.853000 139012379337344 torch/_dynamo/convert_frame.py:824] due to:
W0629 11:52:49.853000 139012379337344 torch/_dynamo/convert_frame.py:824] Traceback (most recent call last):
W0629 11:52:49.853000 139012379337344 torch/_dynamo/convert_frame.py:824]   File "/usr/local/lib/python3.10/dist-packages/torch/_dynamo/convert_frame.py", line 786, in _convert_frame
W0629 11:52:49.853000 139012379337344 torch/_dynamo/convert_frame.py:824]     result = inner_convert(
W0629 11:52:49.853000 139012379337344 torch/_dynamo/convert_frame.py:824]   File "/usr/local/lib/python3.10/dist-packages/torch/_dynamo/convert_frame.py", line 400, in _convert_frame_assert
W0629 11:52:49.853000 139012379337344 torch/_dynamo/convert_frame.py:824]     return _compile(
W0629 11:52:49.853000 139012379337344 torch/_dynamo/convert_frame.py:824]   File "/usr/lib/python3.10/contextlib.py", line 79, in inner
W0629 11:52:49.853000 139012379337344 torch/_dynamo/convert_frame.py:824]     return func(*args, **kwds)
W0629 11:52:49.853000 139012379337344 torch/_dynamo/convert_frame.py:824]   File "/usr/local/lib/python3.10/dist-packages/torch/_dynamo/convert_frame.py", line 676, in _compile
W0629 11:52:49.853000 139012379337344 torch/_dynamo/convert_frame.py:824]     guarded_code = compile_inner(code, one_graph, hooks, transform)
W0629 11:52:49.853000 139012379337344 torch/_dynamo/convert_frame.py:824]   File "/usr/local/lib/python3.10/dist-packages/torch/_dynamo/utils.py", line 262, in time_wrapper
W0629 11:52:49.853000 139012379337344 torch/_dynamo/convert_frame.py:824]     r = func(*args, **kwargs)
W0629 11:52:49.853000 139012379337344 torch/_dynamo/convert_frame.py:824]   File "/usr/local/lib/python3.10/dist-packages/torch/_dynamo/convert_frame.py", line 535, in compile_inner
W0629 11:52:49.853000 139012379337344 torch/_dynamo/convert_frame.py:824]     out_code = transform_code_object(code, transform)
W0629 11:52:49.853000 139012379337344 torch/_dynamo/convert_frame.py:824]   File "/usr/local/lib/python3.10/dist-packages/torch/_dynamo/bytecode_transformation.py", line 1036, in transform_code_object
W0629 11:52:49.853000 139012379337344 torch/_dynamo/convert_frame.py:824]     transformations(instructions, code_options)
W0629 11:52:49.853000 139012379337344 torch/_dynamo/convert_frame.py:824]   File "/usr/local/lib/python3.10/dist-packages/torch/_dynamo/convert_frame.py", line 165, in _fn
W0629 11:52:49.853000 139012379337344 torch/_dynamo/convert_frame.py:824]     return fn(*args, **kwargs)
W0629 11:52:49.853000 139012379337344 torch/_dynamo/convert_frame.py:824]   File "/usr/local/lib/python3.10/dist-packages/torch/_dynamo/convert_frame.py", line 500, in transform
W0629 11:52:49.853000 139012379337344 torch/_dynamo/convert_frame.py:824]     tracer.run()
W0629 11:52:49.853000 139012379337344 torch/_dynamo/convert_frame.py:824]   File "/usr/local/lib/python3.10/dist-packages/torch/_dynamo/symbolic_convert.py", line 2149, in run
W0629 11:52:49.853000 139012379337344 torch/_dynamo/convert_frame.py:824]     super().run()
W0629 11:52:49.853000 139012379337344 torch/_dynamo/convert_frame.py:824]   File "/usr/local/lib/python3.10/dist-packages/torch/_dynamo/symbolic_convert.py", line 810, in run
W0629 11:52:49.853000 139012379337344 torch/_dynamo/convert_frame.py:824]     and self.step()
W0629 11:52:49.853000 139012379337344 torch/_dynamo/convert_frame.py:824]   File "/usr/local/lib/python3.10/dist-packages/torch/_dynamo/symbolic_convert.py", line 773, in step
W0629 11:52:49.853000 139012379337344 torch/_dynamo/convert_frame.py:824]     getattr(self, inst.opname)(inst)
W0629 11:52:49.853000 139012379337344 torch/_dynamo/convert_frame.py:824]   File "/usr/local/lib/python3.10/dist-packages/torch/_dynamo/symbolic_convert.py", line 2268, in RETURN_VALUE
W0629 11:52:49.853000 139012379337344 torch/_dynamo/convert_frame.py:824]     self.output.compile_subgraph(
W0629 11:52:49.853000 139012379337344 torch/_dynamo/convert_frame.py:824]   File "/usr/local/lib/python3.10/dist-packages/torch/_dynamo/output_graph.py", line 971, in compile_subgraph
W0629 11:52:49.853000 139012379337344 torch/_dynamo/convert_frame.py:824]     self.compile_and_call_fx_graph(tx, list(reversed(stack_values)), root)
W0629 11:52:49.853000 139012379337344 torch/_dynamo/convert_frame.py:824]   File "/usr/lib/python3.10/contextlib.py", line 79, in inner
W0629 11:52:49.853000 139012379337344 torch/_dynamo/convert_frame.py:824]     return func(*args, **kwds)
W0629 11:52:49.853000 139012379337344 torch/_dynamo/convert_frame.py:824]   File "/usr/local/lib/python3.10/dist-packages/torch/_dynamo/output_graph.py", line 1168, in compile_and_call_fx_graph
W0629 11:52:49.853000 139012379337344 torch/_dynamo/convert_frame.py:824]     compiled_fn = self.call_user_compiler(gm)
W0629 11:52:49.853000 139012379337344 torch/_dynamo/convert_frame.py:824]   File "/usr/local/lib/python3.10/dist-packages/torch/_dynamo/utils.py", line 262, in time_wrapper
W0629 11:52:49.853000 139012379337344 torch/_dynamo/convert_frame.py:824]     r = func(*args, **kwargs)
W0629 11:52:49.853000 139012379337344 torch/_dynamo/convert_frame.py:824]   File "/usr/local/lib/python3.10/dist-packages/torch/_dynamo/output_graph.py", line 1241, in call_user_compiler
W0629 11:52:49.853000 139012379337344 torch/_dynamo/convert_frame.py:824]     raise BackendCompilerFailed(self.compiler_fn, e).with_traceback(
W0629 11:52:49.853000 139012379337344 torch/_dynamo/convert_frame.py:824]   File "/usr/local/lib/python3.10/dist-packages/torch/_dynamo/output_graph.py", line 1222, in call_user_compiler
W0629 11:52:49.853000 139012379337344 torch/_dynamo/convert_frame.py:824]     compiled_fn = compiler_fn(gm, self.example_inputs())
W0629 11:52:49.853000 139012379337344 torch/_dynamo/convert_frame.py:824]   File "/usr/local/lib/python3.10/dist-packages/torch/_dynamo/repro/after_dynamo.py", line 117, in debug_wrapper
W0629 11:52:49.853000 139012379337344 torch/_dynamo/convert_frame.py:824]     compiled_gm = compiler_fn(gm, example_inputs)
W0629 11:52:49.853000 139012379337344 torch/_dynamo/convert_frame.py:824]   File "/usr/local/lib/python3.10/dist-packages/torch/__init__.py", line 1729, in __call__
W0629 11:52:49.853000 139012379337344 torch/_dynamo/convert_frame.py:824]     return compile_fx(model_, inputs_, config_patches=self.config)
W0629 11:52:49.853000 139012379337344 torch/_dynamo/convert_frame.py:824]   File "/usr/lib/python3.10/contextlib.py", line 79, in inner
W0629 11:52:49.853000 139012379337344 torch/_dynamo/convert_frame.py:824]     return func(*args, **kwds)
W0629 11:52:49.853000 139012379337344 torch/_dynamo/convert_frame.py:824]   File "/usr/local/lib/python3.10/dist-packages/torch/_inductor/compile_fx.py", line 1330, in compile_fx
W0629 11:52:49.853000 139012379337344 torch/_dynamo/convert_frame.py:824]     return aot_autograd(
W0629 11:52:49.853000 139012379337344 torch/_dynamo/convert_frame.py:824]   File "/usr/local/lib/python3.10/dist-packages/torch/_dynamo/backends/common.py", line 58, in compiler_fn
W0629 11:52:49.853000 139012379337344 torch/_dynamo/convert_frame.py:824]     cg = aot_module_simplified(gm, example_inputs, **kwargs)
W0629 11:52:49.853000 139012379337344 torch/_dynamo/convert_frame.py:824]   File "/usr/local/lib/python3.10/dist-packages/torch/_functorch/aot_autograd.py", line 903, in aot_module_simplified
W0629 11:52:49.853000 139012379337344 torch/_dynamo/convert_frame.py:824]     compiled_fn = create_aot_dispatcher_function(
W0629 11:52:49.853000 139012379337344 torch/_dynamo/convert_frame.py:824]   File "/usr/local/lib/python3.10/dist-packages/torch/_dynamo/utils.py", line 262, in time_wrapper
W0629 11:52:49.853000 139012379337344 torch/_dynamo/convert_frame.py:824]     r = func(*args, **kwargs)
W0629 11:52:49.853000 139012379337344 torch/_dynamo/convert_frame.py:824]   File "/usr/local/lib/python3.10/dist-packages/torch/_functorch/aot_autograd.py", line 628, in create_aot_dispatcher_function
W0629 11:52:49.853000 139012379337344 torch/_dynamo/convert_frame.py:824]     compiled_fn = compiler_fn(flat_fn, fake_flat_args, aot_config, fw_metadata=fw_metadata)
W0629 11:52:49.853000 139012379337344 torch/_dynamo/convert_frame.py:824]   File "/usr/local/lib/python3.10/dist-packages/torch/_functorch/_aot_autograd/runtime_wrappers.py", line 443, in aot_wrapper_dedupe
W0629 11:52:49.853000 139012379337344 torch/_dynamo/convert_frame.py:824]     return compiler_fn(flat_fn, leaf_flat_args, aot_config, fw_metadata=fw_metadata)
W0629 11:52:49.853000 139012379337344 torch/_dynamo/convert_frame.py:824]   File "/usr/local/lib/python3.10/dist-packages/torch/_functorch/_aot_autograd/runtime_wrappers.py", line 648, in aot_wrapper_synthetic_base
W0629 11:52:49.853000 139012379337344 torch/_dynamo/convert_frame.py:824]     return compiler_fn(flat_fn, flat_args, aot_config, fw_metadata=fw_metadata)
W0629 11:52:49.853000 139012379337344 torch/_dynamo/convert_frame.py:824]   File "/usr/local/lib/python3.10/dist-packages/torch/_functorch/_aot_autograd/jit_compile_runtime_wrappers.py", line 119, in aot_dispatch_base
W0629 11:52:49.853000 139012379337344 torch/_dynamo/convert_frame.py:824]     compiled_fw = compiler(fw_module, updated_flat_args)
W0629 11:52:49.853000 139012379337344 torch/_dynamo/convert_frame.py:824]   File "/usr/local/lib/python3.10/dist-packages/torch/_dynamo/utils.py", line 262, in time_wrapper
W0629 11:52:49.853000 139012379337344 torch/_dynamo/convert_frame.py:824]     r = func(*args, **kwargs)
W0629 11:52:49.853000 139012379337344 torch/_dynamo/convert_frame.py:824]   File "/usr/local/lib/python3.10/dist-packages/torch/_inductor/compile_fx.py", line 1257, in fw_compiler_base
W0629 11:52:49.853000 139012379337344 torch/_dynamo/convert_frame.py:824]     return inner_compile(
W0629 11:52:49.853000 139012379337344 torch/_dynamo/convert_frame.py:824]   File "/usr/local/lib/python3.10/dist-packages/torch/_dynamo/repro/after_aot.py", line 83, in debug_wrapper
W0629 11:52:49.853000 139012379337344 torch/_dynamo/convert_frame.py:824]     inner_compiled_fn = compiler_fn(gm, example_inputs)
W0629 11:52:49.853000 139012379337344 torch/_dynamo/convert_frame.py:824]   File "/usr/local/lib/python3.10/dist-packages/torch/_inductor/debug.py", line 304, in inner
W0629 11:52:49.853000 139012379337344 torch/_dynamo/convert_frame.py:824]     return fn(*args, **kwargs)
W0629 11:52:49.853000 139012379337344 torch/_dynamo/convert_frame.py:824]   File "/usr/lib/python3.10/contextlib.py", line 79, in inner
W0629 11:52:49.853000 139012379337344 torch/_dynamo/convert_frame.py:824]     return func(*args, **kwds)
W0629 11:52:49.853000 139012379337344 torch/_dynamo/convert_frame.py:824]   File "/usr/lib/python3.10/contextlib.py", line 79, in inner
W0629 11:52:49.853000 139012379337344 torch/_dynamo/convert_frame.py:824]     return func(*args, **kwds)
W0629 11:52:49.853000 139012379337344 torch/_dynamo/convert_frame.py:824]   File "/usr/local/lib/python3.10/dist-packages/torch/_dynamo/utils.py", line 262, in time_wrapper
W0629 11:52:49.853000 139012379337344 torch/_dynamo/convert_frame.py:824]     r = func(*args, **kwargs)
W0629 11:52:49.853000 139012379337344 torch/_dynamo/convert_frame.py:824]   File "/usr/local/lib/python3.10/dist-packages/torch/_inductor/compile_fx.py", line 438, in compile_fx_inner
W0629 11:52:49.853000 139012379337344 torch/_dynamo/convert_frame.py:824]     compiled_graph = fx_codegen_and_compile(
W0629 11:52:49.853000 139012379337344 torch/_dynamo/convert_frame.py:824]   File "/usr/local/lib/python3.10/dist-packages/torch/_inductor/compile_fx.py", line 714, in fx_codegen_and_compile
W0629 11:52:49.853000 139012379337344 torch/_dynamo/convert_frame.py:824]     compiled_fn = graph.compile_to_fn()
W0629 11:52:49.853000 139012379337344 torch/_dynamo/convert_frame.py:824]   File "/usr/local/lib/python3.10/dist-packages/torch/_inductor/graph.py", line 1307, in compile_to_fn
W0629 11:52:49.853000 139012379337344 torch/_dynamo/convert_frame.py:824]     return self.compile_to_module().call
W0629 11:52:49.853000 139012379337344 torch/_dynamo/convert_frame.py:824]   File "/usr/local/lib/python3.10/dist-packages/torch/_dynamo/utils.py", line 262, in time_wrapper
W0629 11:52:49.853000 139012379337344 torch/_dynamo/convert_frame.py:824]     r = func(*args, **kwargs)
W0629 11:52:49.853000 139012379337344 torch/_dynamo/convert_frame.py:824]   File "/usr/local/lib/python3.10/dist-packages/torch/_inductor/graph.py", line 1254, in compile_to_module
W0629 11:52:49.853000 139012379337344 torch/_dynamo/convert_frame.py:824]     mod = PyCodeCache.load_by_key_path(
W0629 11:52:49.853000 139012379337344 torch/_dynamo/convert_frame.py:824]   File "/usr/local/lib/python3.10/dist-packages/torch/_inductor/codecache.py", line 2160, in load_by_key_path
W0629 11:52:49.853000 139012379337344 torch/_dynamo/convert_frame.py:824]     exec(code, mod.__dict__, mod.__dict__)
W0629 11:52:49.853000 139012379337344 torch/_dynamo/convert_frame.py:824]   File "/tmp/torchinductor_root/z6/cz6p74noey2srsneav5iie6h5pfzppuxq7x66jnq7ikrjkgznefs.py", line 138, in <module>
W0629 11:52:49.853000 139012379337344 torch/_dynamo/convert_frame.py:824]     async_compile.wait(globals())
W0629 11:52:49.853000 139012379337344 torch/_dynamo/convert_frame.py:824]   File "/usr/local/lib/python3.10/dist-packages/torch/_inductor/codecache.py", line 2715, in wait
W0629 11:52:49.853000 139012379337344 torch/_dynamo/convert_frame.py:824]     scope[key] = result.result()
W0629 11:52:49.853000 139012379337344 torch/_dynamo/convert_frame.py:824]   File "/usr/local/lib/python3.10/dist-packages/torch/_inductor/codecache.py", line 2522, in result
W0629 11:52:49.853000 139012379337344 torch/_dynamo/convert_frame.py:824]     self.future.result()
W0629 11:52:49.853000 139012379337344 torch/_dynamo/convert_frame.py:824]   File "/usr/lib/python3.10/concurrent/futures/_base.py", line 458, in result
W0629 11:52:49.853000 139012379337344 torch/_dynamo/convert_frame.py:824]     return self.__get_result()
W0629 11:52:49.853000 139012379337344 torch/_dynamo/convert_frame.py:824]   File "/usr/lib/python3.10/concurrent/futures/_base.py", line 403, in __get_result
W0629 11:52:49.853000 139012379337344 torch/_dynamo/convert_frame.py:824]     raise self._exception
W0629 11:52:49.853000 139012379337344 torch/_dynamo/convert_frame.py:824] torch._dynamo.exc.BackendCompilerFailed: backend='inductor' raised:
W0629 11:52:49.853000 139012379337344 torch/_dynamo/convert_frame.py:824] RuntimeError: Internal Triton PTX codegen error:
W0629 11:52:49.853000 139012379337344 torch/_dynamo/convert_frame.py:824] ptxas /tmp/compile-ptx-src-43281d, line 59; error   : Feature '.bf16' requires .target sm_80 or higher
W0629 11:52:49.853000 139012379337344 torch/_dynamo/convert_frame.py:824] ptxas /tmp/compile-ptx-src-43281d, line 59; error   : Feature 'cvt.bf16.f32' requires .target sm_80 or higher
W0629 11:52:49.853000 139012379337344 torch/_dynamo/convert_frame.py:824] ptxas /tmp/compile-ptx-src-43281d, line 60; error   : Feature '.bf16' requires .target sm_80 or higher
W0629 11:52:49.853000 139012379337344 torch/_dynamo/convert_frame.py:824] ptxas /tmp/compile-ptx-src-43281d, line 60; error   : Feature 'cvt.bf16.f32' requires .target sm_80 or higher
W0629 11:52:49.853000 139012379337344 torch/_dynamo/convert_frame.py:824] ptxas /tmp/compile-ptx-src-43281d, line 61; error   : Feature '.bf16' requires .target sm_80 or higher
W0629 11:52:49.853000 139012379337344 torch/_dynamo/convert_frame.py:824] ptxas /tmp/compile-ptx-src-43281d, line 61; error   : Feature 'cvt.bf16.f32' requires .target sm_80 or higher
W0629 11:52:49.853000 139012379337344 torch/_dynamo/convert_frame.py:824] ptxas /tmp/compile-ptx-src-43281d, line 62; error   : Feature '.bf16' requires .target sm_80 or higher
W0629 11:52:49.853000 139012379337344 torch/_dynamo/convert_frame.py:824] ptxas /tmp/compile-ptx-src-43281d, line 62; error   : Feature 'cvt.bf16.f32' requires .target sm_80 or higher
W0629 11:52:49.853000 139012379337344 torch/_dynamo/convert_frame.py:824] ptxas /tmp/compile-ptx-src-43281d, line 63; error   : Feature '.bf16' requires .target sm_80 or higher
W0629 11:52:49.853000 139012379337344 torch/_dynamo/convert_frame.py:824] ptxas /tmp/compile-ptx-src-43281d, line 63; error   : Feature 'cvt.bf16.f32' requires .target sm_80 or higher
W0629 11:52:49.853000 139012379337344 torch/_dynamo/convert_frame.py:824] ptxas /tmp/compile-ptx-src-43281d, line 64; error   : Feature '.bf16' requires .target sm_80 or higher
W0629 11:52:49.853000 139012379337344 torch/_dynamo/convert_frame.py:824] ptxas /tmp/compile-ptx-src-43281d, line 64; error   : Feature 'cvt.bf16.f32' requires .target sm_80 or higher
W0629 11:52:49.853000 139012379337344 torch/_dynamo/convert_frame.py:824] ptxas /tmp/compile-ptx-src-43281d, line 65; error   : Feature '.bf16' requires .target sm_80 or higher
W0629 11:52:49.853000 139012379337344 torch/_dynamo/convert_frame.py:824] ptxas /tmp/compile-ptx-src-43281d, line 65; error   : Feature 'cvt.bf16.f32' requires .target sm_80 or higher
W0629 11:52:49.853000 139012379337344 torch/_dynamo/convert_frame.py:824] ptxas /tmp/compile-ptx-src-43281d, line 66; error   : Feature '.bf16' requires .target sm_80 or higher
W0629 11:52:49.853000 139012379337344 torch/_dynamo/convert_frame.py:824] ptxas /tmp/compile-ptx-src-43281d, line 66; error   : Feature 'cvt.bf16.f32' requires .target sm_80 or higher
W0629 11:52:49.853000 139012379337344 torch/_dynamo/convert_frame.py:824] ptxas fatal   : Ptx assembly aborted due to errors
W0629 11:52:49.853000 139012379337344 torch/_dynamo/convert_frame.py:824]
W0629 11:52:49.853000 139012379337344 torch/_dynamo/convert_frame.py:824]
W0629 11:52:49.853000 139012379337344 torch/_dynamo/convert_frame.py:824] Set TORCH_LOGS="+dynamo" and TORCHDYNAMO_VERBOSE=1 for more information
W0629 11:52:49.853000 139012379337344 torch/_dynamo/convert_frame.py:824]
W0629 11:52:49.853000 139012379337344 torch/_dynamo/convert_frame.py:824] Traceback (most recent call last):
W0629 11:52:49.853000 139012379337344 torch/_dynamo/convert_frame.py:824]   File "/usr/local/lib/python3.10/dist-packages/torch/_dynamo/convert_frame.py", line 786, in _convert_frame
W0629 11:52:49.853000 139012379337344 torch/_dynamo/convert_frame.py:824]     result = inner_convert(
W0629 11:52:49.853000 139012379337344 torch/_dynamo/convert_frame.py:824]   File "/usr/local/lib/python3.10/dist-packages/torch/_dynamo/convert_frame.py", line 400, in _convert_frame_assert
W0629 11:52:49.853000 139012379337344 torch/_dynamo/convert_frame.py:824]     return _compile(
W0629 11:52:49.853000 139012379337344 torch/_dynamo/convert_frame.py:824]   File "/usr/lib/python3.10/contextlib.py", line 79, in inner
W0629 11:52:49.853000 139012379337344 torch/_dynamo/convert_frame.py:824]     return func(*args, **kwds)
W0629 11:52:49.853000 139012379337344 torch/_dynamo/convert_frame.py:824]   File "/usr/local/lib/python3.10/dist-packages/torch/_dynamo/convert_frame.py", line 676, in _compile
W0629 11:52:49.853000 139012379337344 torch/_dynamo/convert_frame.py:824]     guarded_code = compile_inner(code, one_graph, hooks, transform)
W0629 11:52:49.853000 139012379337344 torch/_dynamo/convert_frame.py:824]   File "/usr/local/lib/python3.10/dist-packages/torch/_dynamo/utils.py", line 262, in time_wrapper
W0629 11:52:49.853000 139012379337344 torch/_dynamo/convert_frame.py:824]     r = func(*args, **kwargs)
W0629 11:52:49.853000 139012379337344 torch/_dynamo/convert_frame.py:824]   File "/usr/local/lib/python3.10/dist-packages/torch/_dynamo/convert_frame.py", line 535, in compile_inner
W0629 11:52:49.853000 139012379337344 torch/_dynamo/convert_frame.py:824]     out_code = transform_code_object(code, transform)
W0629 11:52:49.853000 139012379337344 torch/_dynamo/convert_frame.py:824]   File "/usr/local/lib/python3.10/dist-packages/torch/_dynamo/bytecode_transformation.py", line 1036, in transform_code_object
W0629 11:52:49.853000 139012379337344 torch/_dynamo/convert_frame.py:824]     transformations(instructions, code_options)
W0629 11:52:49.853000 139012379337344 torch/_dynamo/convert_frame.py:824]   File "/usr/local/lib/python3.10/dist-packages/torch/_dynamo/convert_frame.py", line 165, in _fn
W0629 11:52:49.853000 139012379337344 torch/_dynamo/convert_frame.py:824]     return fn(*args, **kwargs)
W0629 11:52:49.853000 139012379337344 torch/_dynamo/convert_frame.py:824]   File "/usr/local/lib/python3.10/dist-packages/torch/_dynamo/convert_frame.py", line 500, in transform
W0629 11:52:49.853000 139012379337344 torch/_dynamo/convert_frame.py:824]     tracer.run()
W0629 11:52:49.853000 139012379337344 torch/_dynamo/convert_frame.py:824]   File "/usr/local/lib/python3.10/dist-packages/torch/_dynamo/symbolic_convert.py", line 2149, in run
W0629 11:52:49.853000 139012379337344 torch/_dynamo/convert_frame.py:824]     super().run()
W0629 11:52:49.853000 139012379337344 torch/_dynamo/convert_frame.py:824]   File "/usr/local/lib/python3.10/dist-packages/torch/_dynamo/symbolic_convert.py", line 810, in run
W0629 11:52:49.853000 139012379337344 torch/_dynamo/convert_frame.py:824]     and self.step()
W0629 11:52:49.853000 139012379337344 torch/_dynamo/convert_frame.py:824]   File "/usr/local/lib/python3.10/dist-packages/torch/_dynamo/symbolic_convert.py", line 773, in step
W0629 11:52:49.853000 139012379337344 torch/_dynamo/convert_frame.py:824]     getattr(self, inst.opname)(inst)
W0629 11:52:49.853000 139012379337344 torch/_dynamo/convert_frame.py:824]   File "/usr/local/lib/python3.10/dist-packages/torch/_dynamo/symbolic_convert.py", line 2268, in RETURN_VALUE
W0629 11:52:49.853000 139012379337344 torch/_dynamo/convert_frame.py:824]     self.output.compile_subgraph(
W0629 11:52:49.853000 139012379337344 torch/_dynamo/convert_frame.py:824]   File "/usr/local/lib/python3.10/dist-packages/torch/_dynamo/output_graph.py", line 971, in compile_subgraph
W0629 11:52:49.853000 139012379337344 torch/_dynamo/convert_frame.py:824]     self.compile_and_call_fx_graph(tx, list(reversed(stack_values)), root)
W0629 11:52:49.853000 139012379337344 torch/_dynamo/convert_frame.py:824]   File "/usr/lib/python3.10/contextlib.py", line 79, in inner
W0629 11:52:49.853000 139012379337344 torch/_dynamo/convert_frame.py:824]     return func(*args, **kwds)
W0629 11:52:49.853000 139012379337344 torch/_dynamo/convert_frame.py:824]   File "/usr/local/lib/python3.10/dist-packages/torch/_dynamo/output_graph.py", line 1168, in compile_and_call_fx_graph
W0629 11:52:49.853000 139012379337344 torch/_dynamo/convert_frame.py:824]     compiled_fn = self.call_user_compiler(gm)
W0629 11:52:49.853000 139012379337344 torch/_dynamo/convert_frame.py:824]   File "/usr/local/lib/python3.10/dist-packages/torch/_dynamo/utils.py", line 262, in time_wrapper
W0629 11:52:49.853000 139012379337344 torch/_dynamo/convert_frame.py:824]     r = func(*args, **kwargs)
W0629 11:52:49.853000 139012379337344 torch/_dynamo/convert_frame.py:824]   File "/usr/local/lib/python3.10/dist-packages/torch/_dynamo/output_graph.py", line 1241, in call_user_compiler
W0629 11:52:49.853000 139012379337344 torch/_dynamo/convert_frame.py:824]     raise BackendCompilerFailed(self.compiler_fn, e).with_traceback(
W0629 11:52:49.853000 139012379337344 torch/_dynamo/convert_frame.py:824]   File "/usr/local/lib/python3.10/dist-packages/torch/_dynamo/output_graph.py", line 1222, in call_user_compiler
W0629 11:52:49.853000 139012379337344 torch/_dynamo/convert_frame.py:824]     compiled_fn = compiler_fn(gm, self.example_inputs())
W0629 11:52:49.853000 139012379337344 torch/_dynamo/convert_frame.py:824]   File "/usr/local/lib/python3.10/dist-packages/torch/_dynamo/repro/after_dynamo.py", line 117, in debug_wrapper
W0629 11:52:49.853000 139012379337344 torch/_dynamo/convert_frame.py:824]     compiled_gm = compiler_fn(gm, example_inputs)
W0629 11:52:49.853000 139012379337344 torch/_dynamo/convert_frame.py:824]   File "/usr/local/lib/python3.10/dist-packages/torch/__init__.py", line 1729, in __call__
W0629 11:52:49.853000 139012379337344 torch/_dynamo/convert_frame.py:824]     return compile_fx(model_, inputs_, config_patches=self.config)
W0629 11:52:49.853000 139012379337344 torch/_dynamo/convert_frame.py:824]   File "/usr/lib/python3.10/contextlib.py", line 79, in inner
W0629 11:52:49.853000 139012379337344 torch/_dynamo/convert_frame.py:824]     return func(*args, **kwds)
W0629 11:52:49.853000 139012379337344 torch/_dynamo/convert_frame.py:824]   File "/usr/local/lib/python3.10/dist-packages/torch/_inductor/compile_fx.py", line 1330, in compile_fx
W0629 11:52:49.853000 139012379337344 torch/_dynamo/convert_frame.py:824]     return aot_autograd(
W0629 11:52:49.853000 139012379337344 torch/_dynamo/convert_frame.py:824]   File "/usr/local/lib/python3.10/dist-packages/torch/_dynamo/backends/common.py", line 58, in compiler_fn
W0629 11:52:49.853000 139012379337344 torch/_dynamo/convert_frame.py:824]     cg = aot_module_simplified(gm, example_inputs, **kwargs)
W0629 11:52:49.853000 139012379337344 torch/_dynamo/convert_frame.py:824]   File "/usr/local/lib/python3.10/dist-packages/torch/_functorch/aot_autograd.py", line 903, in aot_module_simplified
W0629 11:52:49.853000 139012379337344 torch/_dynamo/convert_frame.py:824]     compiled_fn = create_aot_dispatcher_function(
W0629 11:52:49.853000 139012379337344 torch/_dynamo/convert_frame.py:824]   File "/usr/local/lib/python3.10/dist-packages/torch/_dynamo/utils.py", line 262, in time_wrapper
W0629 11:52:49.853000 139012379337344 torch/_dynamo/convert_frame.py:824]     r = func(*args, **kwargs)
W0629 11:52:49.853000 139012379337344 torch/_dynamo/convert_frame.py:824]   File "/usr/local/lib/python3.10/dist-packages/torch/_functorch/aot_autograd.py", line 628, in create_aot_dispatcher_function
W0629 11:52:49.853000 139012379337344 torch/_dynamo/convert_frame.py:824]     compiled_fn = compiler_fn(flat_fn, fake_flat_args, aot_config, fw_metadata=fw_metadata)
W0629 11:52:49.853000 139012379337344 torch/_dynamo/convert_frame.py:824]   File "/usr/local/lib/python3.10/dist-packages/torch/_functorch/_aot_autograd/runtime_wrappers.py", line 443, in aot_wrapper_dedupe
W0629 11:52:49.853000 139012379337344 torch/_dynamo/convert_frame.py:824]     return compiler_fn(flat_fn, leaf_flat_args, aot_config, fw_metadata=fw_metadata)
W0629 11:52:49.853000 139012379337344 torch/_dynamo/convert_frame.py:824]   File "/usr/local/lib/python3.10/dist-packages/torch/_functorch/_aot_autograd/runtime_wrappers.py", line 648, in aot_wrapper_synthetic_base
W0629 11:52:49.853000 139012379337344 torch/_dynamo/convert_frame.py:824]     return compiler_fn(flat_fn, flat_args, aot_config, fw_metadata=fw_metadata)
W0629 11:52:49.853000 139012379337344 torch/_dynamo/convert_frame.py:824]   File "/usr/local/lib/python3.10/dist-packages/torch/_functorch/_aot_autograd/jit_compile_runtime_wrappers.py", line 119, in aot_dispatch_base
W0629 11:52:49.853000 139012379337344 torch/_dynamo/convert_frame.py:824]     compiled_fw = compiler(fw_module, updated_flat_args)
W0629 11:52:49.853000 139012379337344 torch/_dynamo/convert_frame.py:824]   File "/usr/local/lib/python3.10/dist-packages/torch/_dynamo/utils.py", line 262, in time_wrapper
W0629 11:52:49.853000 139012379337344 torch/_dynamo/convert_frame.py:824]     r = func(*args, **kwargs)
W0629 11:52:49.853000 139012379337344 torch/_dynamo/convert_frame.py:824]   File "/usr/local/lib/python3.10/dist-packages/torch/_inductor/compile_fx.py", line 1257, in fw_compiler_base
W0629 11:52:49.853000 139012379337344 torch/_dynamo/convert_frame.py:824]     return inner_compile(
W0629 11:52:49.853000 139012379337344 torch/_dynamo/convert_frame.py:824]   File "/usr/local/lib/python3.10/dist-packages/torch/_dynamo/repro/after_aot.py", line 83, in debug_wrapper
W0629 11:52:49.853000 139012379337344 torch/_dynamo/convert_frame.py:824]     inner_compiled_fn = compiler_fn(gm, example_inputs)
W0629 11:52:49.853000 139012379337344 torch/_dynamo/convert_frame.py:824]   File "/usr/local/lib/python3.10/dist-packages/torch/_inductor/debug.py", line 304, in inner
W0629 11:52:49.853000 139012379337344 torch/_dynamo/convert_frame.py:824]     return fn(*args, **kwargs)
W0629 11:52:49.853000 139012379337344 torch/_dynamo/convert_frame.py:824]   File "/usr/lib/python3.10/contextlib.py", line 79, in inner
W0629 11:52:49.853000 139012379337344 torch/_dynamo/convert_frame.py:824]     return func(*args, **kwds)
W0629 11:52:49.853000 139012379337344 torch/_dynamo/convert_frame.py:824]   File "/usr/lib/python3.10/contextlib.py", line 79, in inner
W0629 11:52:49.853000 139012379337344 torch/_dynamo/convert_frame.py:824]     return func(*args, **kwds)
W0629 11:52:49.853000 139012379337344 torch/_dynamo/convert_frame.py:824]   File "/usr/local/lib/python3.10/dist-packages/torch/_dynamo/utils.py", line 262, in time_wrapper
W0629 11:52:49.853000 139012379337344 torch/_dynamo/convert_frame.py:824]     r = func(*args, **kwargs)
W0629 11:52:49.853000 139012379337344 torch/_dynamo/convert_frame.py:824]   File "/usr/local/lib/python3.10/dist-packages/torch/_inductor/compile_fx.py", line 438, in compile_fx_inner
W0629 11:52:49.853000 139012379337344 torch/_dynamo/convert_frame.py:824]     compiled_graph = fx_codegen_and_compile(
W0629 11:52:49.853000 139012379337344 torch/_dynamo/convert_frame.py:824]   File "/usr/local/lib/python3.10/dist-packages/torch/_inductor/compile_fx.py", line 714, in fx_codegen_and_compile
W0629 11:52:49.853000 139012379337344 torch/_dynamo/convert_frame.py:824]     compiled_fn = graph.compile_to_fn()
W0629 11:52:49.853000 139012379337344 torch/_dynamo/convert_frame.py:824]   File "/usr/local/lib/python3.10/dist-packages/torch/_inductor/graph.py", line 1307, in compile_to_fn
W0629 11:52:49.853000 139012379337344 torch/_dynamo/convert_frame.py:824]     return self.compile_to_module().call
W0629 11:52:49.853000 139012379337344 torch/_dynamo/convert_frame.py:824]   File "/usr/local/lib/python3.10/dist-packages/torch/_dynamo/utils.py", line 262, in time_wrapper
W0629 11:52:49.853000 139012379337344 torch/_dynamo/convert_frame.py:824]     r = func(*args, **kwargs)
W0629 11:52:49.853000 139012379337344 torch/_dynamo/convert_frame.py:824]   File "/usr/local/lib/python3.10/dist-packages/torch/_inductor/graph.py", line 1254, in compile_to_module
W0629 11:52:49.853000 139012379337344 torch/_dynamo/convert_frame.py:824]     mod = PyCodeCache.load_by_key_path(
W0629 11:52:49.853000 139012379337344 torch/_dynamo/convert_frame.py:824]   File "/usr/local/lib/python3.10/dist-packages/torch/_inductor/codecache.py", line 2160, in load_by_key_path
W0629 11:52:49.853000 139012379337344 torch/_dynamo/convert_frame.py:824]     exec(code, mod.__dict__, mod.__dict__)
W0629 11:52:49.853000 139012379337344 torch/_dynamo/convert_frame.py:824]   File "/tmp/torchinductor_root/z6/cz6p74noey2srsneav5iie6h5pfzppuxq7x66jnq7ikrjkgznefs.py", line 138, in <module>
W0629 11:52:49.853000 139012379337344 torch/_dynamo/convert_frame.py:824]     async_compile.wait(globals())
W0629 11:52:49.853000 139012379337344 torch/_dynamo/convert_frame.py:824]   File "/usr/local/lib/python3.10/dist-packages/torch/_inductor/codecache.py", line 2715, in wait
W0629 11:52:49.853000 139012379337344 torch/_dynamo/convert_frame.py:824]     scope[key] = result.result()
W0629 11:52:49.853000 139012379337344 torch/_dynamo/convert_frame.py:824]   File "/usr/local/lib/python3.10/dist-packages/torch/_inductor/codecache.py", line 2522, in result
W0629 11:52:49.853000 139012379337344 torch/_dynamo/convert_frame.py:824]     self.future.result()
W0629 11:52:49.853000 139012379337344 torch/_dynamo/convert_frame.py:824]   File "/usr/lib/python3.10/concurrent/futures/_base.py", line 458, in result
W0629 11:52:49.853000 139012379337344 torch/_dynamo/convert_frame.py:824]     return self.__get_result()
W0629 11:52:49.853000 139012379337344 torch/_dynamo/convert_frame.py:824]   File "/usr/lib/python3.10/concurrent/futures/_base.py", line 403, in __get_result
W0629 11:52:49.853000 139012379337344 torch/_dynamo/convert_frame.py:824]     raise self._exception
W0629 11:52:49.853000 139012379337344 torch/_dynamo/convert_frame.py:824] torch._dynamo.exc.BackendCompilerFailed: backend='inductor' raised:
W0629 11:52:49.853000 139012379337344 torch/_dynamo/convert_frame.py:824] RuntimeError: Internal Triton PTX codegen error:
W0629 11:52:49.853000 139012379337344 torch/_dynamo/convert_frame.py:824] ptxas /tmp/compile-ptx-src-43281d, line 59; error   : Feature '.bf16' requires .target sm_80 or higher
W0629 11:52:49.853000 139012379337344 torch/_dynamo/convert_frame.py:824] ptxas /tmp/compile-ptx-src-43281d, line 59; error   : Feature 'cvt.bf16.f32' requires .target sm_80 or higher
W0629 11:52:49.853000 139012379337344 torch/_dynamo/convert_frame.py:824] ptxas /tmp/compile-ptx-src-43281d, line 60; error   : Feature '.bf16' requires .target sm_80 or higher
W0629 11:52:49.853000 139012379337344 torch/_dynamo/convert_frame.py:824] ptxas /tmp/compile-ptx-src-43281d, line 60; error   : Feature 'cvt.bf16.f32' requires .target sm_80 or higher
W0629 11:52:49.853000 139012379337344 torch/_dynamo/convert_frame.py:824] ptxas /tmp/compile-ptx-src-43281d, line 61; error   : Feature '.bf16' requires .target sm_80 or higher
W0629 11:52:49.853000 139012379337344 torch/_dynamo/convert_frame.py:824] ptxas /tmp/compile-ptx-src-43281d, line 61; error   : Feature 'cvt.bf16.f32' requires .target sm_80 or higher
W0629 11:52:49.853000 139012379337344 torch/_dynamo/convert_frame.py:824] ptxas /tmp/compile-ptx-src-43281d, line 62; error   : Feature '.bf16' requires .target sm_80 or higher
W0629 11:52:49.853000 139012379337344 torch/_dynamo/convert_frame.py:824] ptxas /tmp/compile-ptx-src-43281d, line 62; error   : Feature 'cvt.bf16.f32' requires .target sm_80 or higher
W0629 11:52:49.853000 139012379337344 torch/_dynamo/convert_frame.py:824] ptxas /tmp/compile-ptx-src-43281d, line 63; error   : Feature '.bf16' requires .target sm_80 or higher
W0629 11:52:49.853000 139012379337344 torch/_dynamo/convert_frame.py:824] ptxas /tmp/compile-ptx-src-43281d, line 63; error   : Feature 'cvt.bf16.f32' requires .target sm_80 or higher
W0629 11:52:49.853000 139012379337344 torch/_dynamo/convert_frame.py:824] ptxas /tmp/compile-ptx-src-43281d, line 64; error   : Feature '.bf16' requires .target sm_80 or higher
W0629 11:52:49.853000 139012379337344 torch/_dynamo/convert_frame.py:824] ptxas /tmp/compile-ptx-src-43281d, line 64; error   : Feature 'cvt.bf16.f32' requires .target sm_80 or higher
W0629 11:52:49.853000 139012379337344 torch/_dynamo/convert_frame.py:824] ptxas /tmp/compile-ptx-src-43281d, line 65; error   : Feature '.bf16' requires .target sm_80 or higher
W0629 11:52:49.853000 139012379337344 torch/_dynamo/convert_frame.py:824] ptxas /tmp/compile-ptx-src-43281d, line 65; error   : Feature 'cvt.bf16.f32' requires .target sm_80 or higher
W0629 11:52:49.853000 139012379337344 torch/_dynamo/convert_frame.py:824] ptxas /tmp/compile-ptx-src-43281d, line 66; error   : Feature '.bf16' requires .target sm_80 or higher
W0629 11:52:49.853000 139012379337344 torch/_dynamo/convert_frame.py:824] ptxas /tmp/compile-ptx-src-43281d, line 66; error   : Feature 'cvt.bf16.f32' requires .target sm_80 or higher
W0629 11:52:49.853000 139012379337344 torch/_dynamo/convert_frame.py:824] ptxas fatal   : Ptx assembly aborted due to errors
W0629 11:52:49.853000 139012379337344 torch/_dynamo/convert_frame.py:824]
W0629 11:52:49.853000 139012379337344 torch/_dynamo/convert_frame.py:824]
W0629 11:52:49.853000 139012379337344 torch/_dynamo/convert_frame.py:824] Set TORCH_LOGS="+dynamo" and TORCHDYNAMO_VERBOSE=1 for more information
W0629 11:52:49.853000 139012379337344 torch/_dynamo/convert_frame.py:824]
W0629 11:52:50.259000 139012379337344 torch/_dynamo/convert_frame.py:824] WON'T CONVERT apply_chunking_to_forward /usr/local/lib/python3.10/dist-packages/transformers/pytorch_utils.py line 166
W0629 11:52:50.259000 139012379337344 torch/_dynamo/convert_frame.py:824] due to:
W0629 11:52:50.259000 139012379337344 torch/_dynamo/convert_frame.py:824] Traceback (most recent call last):
W0629 11:52:50.259000 139012379337344 torch/_dynamo/convert_frame.py:824]   File "/usr/local/lib/python3.10/dist-packages/torch/_dynamo/convert_frame.py", line 786, in _convert_frame
W0629 11:52:50.259000 139012379337344 torch/_dynamo/convert_frame.py:824]     result = inner_convert(
W0629 11:52:50.259000 139012379337344 torch/_dynamo/convert_frame.py:824]   File "/usr/local/lib/python3.10/dist-packages/torch/_dynamo/convert_frame.py", line 400, in _convert_frame_assert
W0629 11:52:50.259000 139012379337344 torch/_dynamo/convert_frame.py:824]     return _compile(
W0629 11:52:50.259000 139012379337344 torch/_dynamo/convert_frame.py:824]   File "/usr/lib/python3.10/contextlib.py", line 79, in inner
W0629 11:52:50.259000 139012379337344 torch/_dynamo/convert_frame.py:824]     return func(*args, **kwds)
W0629 11:52:50.259000 139012379337344 torch/_dynamo/convert_frame.py:824]   File "/usr/local/lib/python3.10/dist-packages/torch/_dynamo/convert_frame.py", line 676, in _compile
W0629 11:52:50.259000 139012379337344 torch/_dynamo/convert_frame.py:824]     guarded_code = compile_inner(code, one_graph, hooks, transform)
W0629 11:52:50.259000 139012379337344 torch/_dynamo/convert_frame.py:824]   File "/usr/local/lib/python3.10/dist-packages/torch/_dynamo/utils.py", line 262, in time_wrapper
W0629 11:52:50.259000 139012379337344 torch/_dynamo/convert_frame.py:824]     r = func(*args, **kwargs)
W0629 11:52:50.259000 139012379337344 torch/_dynamo/convert_frame.py:824]   File "/usr/local/lib/python3.10/dist-packages/torch/_dynamo/convert_frame.py", line 535, in compile_inner
W0629 11:52:50.259000 139012379337344 torch/_dynamo/convert_frame.py:824]     out_code = transform_code_object(code, transform)
W0629 11:52:50.259000 139012379337344 torch/_dynamo/convert_frame.py:824]   File "/usr/local/lib/python3.10/dist-packages/torch/_dynamo/bytecode_transformation.py", line 1036, in transform_code_object
W0629 11:52:50.259000 139012379337344 torch/_dynamo/convert_frame.py:824]     transformations(instructions, code_options)
W0629 11:52:50.259000 139012379337344 torch/_dynamo/convert_frame.py:824]   File "/usr/local/lib/python3.10/dist-packages/torch/_dynamo/convert_frame.py", line 165, in _fn
W0629 11:52:50.259000 139012379337344 torch/_dynamo/convert_frame.py:824]     return fn(*args, **kwargs)
W0629 11:52:50.259000 139012379337344 torch/_dynamo/convert_frame.py:824]   File "/usr/local/lib/python3.10/dist-packages/torch/_dynamo/convert_frame.py", line 500, in transform
W0629 11:52:50.259000 139012379337344 torch/_dynamo/convert_frame.py:824]     tracer.run()
W0629 11:52:50.259000 139012379337344 torch/_dynamo/convert_frame.py:824]   File "/usr/local/lib/python3.10/dist-packages/torch/_dynamo/symbolic_convert.py", line 2149, in run
W0629 11:52:50.259000 139012379337344 torch/_dynamo/convert_frame.py:824]     super().run()
W0629 11:52:50.259000 139012379337344 torch/_dynamo/convert_frame.py:824]   File "/usr/local/lib/python3.10/dist-packages/torch/_dynamo/symbolic_convert.py", line 810, in run
W0629 11:52:50.259000 139012379337344 torch/_dynamo/convert_frame.py:824]     and self.step()
W0629 11:52:50.259000 139012379337344 torch/_dynamo/convert_frame.py:824]   File "/usr/local/lib/python3.10/dist-packages/torch/_dynamo/symbolic_convert.py", line 773, in step
W0629 11:52:50.259000 139012379337344 torch/_dynamo/convert_frame.py:824]     getattr(self, inst.opname)(inst)
W0629 11:52:50.259000 139012379337344 torch/_dynamo/convert_frame.py:824]   File "/usr/local/lib/python3.10/dist-packages/torch/_dynamo/symbolic_convert.py", line 2268, in RETURN_VALUE
W0629 11:52:50.259000 139012379337344 torch/_dynamo/convert_frame.py:824]     self.output.compile_subgraph(
W0629 11:52:50.259000 139012379337344 torch/_dynamo/convert_frame.py:824]   File "/usr/local/lib/python3.10/dist-packages/torch/_dynamo/output_graph.py", line 971, in compile_subgraph
W0629 11:52:50.259000 139012379337344 torch/_dynamo/convert_frame.py:824]     self.compile_and_call_fx_graph(tx, list(reversed(stack_values)), root)
W0629 11:52:50.259000 139012379337344 torch/_dynamo/convert_frame.py:824]   File "/usr/lib/python3.10/contextlib.py", line 79, in inner
W0629 11:52:50.259000 139012379337344 torch/_dynamo/convert_frame.py:824]     return func(*args, **kwds)
W0629 11:52:50.259000 139012379337344 torch/_dynamo/convert_frame.py:824]   File "/usr/local/lib/python3.10/dist-packages/torch/_dynamo/output_graph.py", line 1168, in compile_and_call_fx_graph
W0629 11:52:50.259000 139012379337344 torch/_dynamo/convert_frame.py:824]     compiled_fn = self.call_user_compiler(gm)
W0629 11:52:50.259000 139012379337344 torch/_dynamo/convert_frame.py:824]   File "/usr/local/lib/python3.10/dist-packages/torch/_dynamo/utils.py", line 262, in time_wrapper
W0629 11:52:50.259000 139012379337344 torch/_dynamo/convert_frame.py:824]     r = func(*args, **kwargs)
W0629 11:52:50.259000 139012379337344 torch/_dynamo/convert_frame.py:824]   File "/usr/local/lib/python3.10/dist-packages/torch/_dynamo/output_graph.py", line 1241, in call_user_compiler
W0629 11:52:50.259000 139012379337344 torch/_dynamo/convert_frame.py:824]     raise BackendCompilerFailed(self.compiler_fn, e).with_traceback(
W0629 11:52:50.259000 139012379337344 torch/_dynamo/convert_frame.py:824]   File "/usr/local/lib/python3.10/dist-packages/torch/_dynamo/output_graph.py", line 1222, in call_user_compiler
W0629 11:52:50.259000 139012379337344 torch/_dynamo/convert_frame.py:824]     compiled_fn = compiler_fn(gm, self.example_inputs())
W0629 11:52:50.259000 139012379337344 torch/_dynamo/convert_frame.py:824]   File "/usr/local/lib/python3.10/dist-packages/torch/_dynamo/repro/after_dynamo.py", line 117, in debug_wrapper
W0629 11:52:50.259000 139012379337344 torch/_dynamo/convert_frame.py:824]     compiled_gm = compiler_fn(gm, example_inputs)
W0629 11:52:50.259000 139012379337344 torch/_dynamo/convert_frame.py:824]   File "/usr/local/lib/python3.10/dist-packages/torch/__init__.py", line 1729, in __call__
W0629 11:52:50.259000 139012379337344 torch/_dynamo/convert_frame.py:824]     return compile_fx(model_, inputs_, config_patches=self.config)
W0629 11:52:50.259000 139012379337344 torch/_dynamo/convert_frame.py:824]   File "/usr/lib/python3.10/contextlib.py", line 79, in inner
W0629 11:52:50.259000 139012379337344 torch/_dynamo/convert_frame.py:824]     return func(*args, **kwds)
W0629 11:52:50.259000 139012379337344 torch/_dynamo/convert_frame.py:824]   File "/usr/local/lib/python3.10/dist-packages/torch/_inductor/compile_fx.py", line 1330, in compile_fx
W0629 11:52:50.259000 139012379337344 torch/_dynamo/convert_frame.py:824]     return aot_autograd(
W0629 11:52:50.259000 139012379337344 torch/_dynamo/convert_frame.py:824]   File "/usr/local/lib/python3.10/dist-packages/torch/_dynamo/backends/common.py", line 58, in compiler_fn
W0629 11:52:50.259000 139012379337344 torch/_dynamo/convert_frame.py:824]     cg = aot_module_simplified(gm, example_inputs, **kwargs)
W0629 11:52:50.259000 139012379337344 torch/_dynamo/convert_frame.py:824]   File "/usr/local/lib/python3.10/dist-packages/torch/_functorch/aot_autograd.py", line 903, in aot_module_simplified
W0629 11:52:50.259000 139012379337344 torch/_dynamo/convert_frame.py:824]     compiled_fn = create_aot_dispatcher_function(
W0629 11:52:50.259000 139012379337344 torch/_dynamo/convert_frame.py:824]   File "/usr/local/lib/python3.10/dist-packages/torch/_dynamo/utils.py", line 262, in time_wrapper
W0629 11:52:50.259000 139012379337344 torch/_dynamo/convert_frame.py:824]     r = func(*args, **kwargs)
W0629 11:52:50.259000 139012379337344 torch/_dynamo/convert_frame.py:824]   File "/usr/local/lib/python3.10/dist-packages/torch/_functorch/aot_autograd.py", line 628, in create_aot_dispatcher_function
W0629 11:52:50.259000 139012379337344 torch/_dynamo/convert_frame.py:824]     compiled_fn = compiler_fn(flat_fn, fake_flat_args, aot_config, fw_metadata=fw_metadata)
W0629 11:52:50.259000 139012379337344 torch/_dynamo/convert_frame.py:824]   File "/usr/local/lib/python3.10/dist-packages/torch/_functorch/_aot_autograd/runtime_wrappers.py", line 443, in aot_wrapper_dedupe
W0629 11:52:50.259000 139012379337344 torch/_dynamo/convert_frame.py:824]     return compiler_fn(flat_fn, leaf_flat_args, aot_config, fw_metadata=fw_metadata)
W0629 11:52:50.259000 139012379337344 torch/_dynamo/convert_frame.py:824]   File "/usr/local/lib/python3.10/dist-packages/torch/_functorch/_aot_autograd/runtime_wrappers.py", line 648, in aot_wrapper_synthetic_base
W0629 11:52:50.259000 139012379337344 torch/_dynamo/convert_frame.py:824]     return compiler_fn(flat_fn, flat_args, aot_config, fw_metadata=fw_metadata)
W0629 11:52:50.259000 139012379337344 torch/_dynamo/convert_frame.py:824]   File "/usr/local/lib/python3.10/dist-packages/torch/_functorch/_aot_autograd/jit_compile_runtime_wrappers.py", line 119, in aot_dispatch_base
W0629 11:52:50.259000 139012379337344 torch/_dynamo/convert_frame.py:824]     compiled_fw = compiler(fw_module, updated_flat_args)
W0629 11:52:50.259000 139012379337344 torch/_dynamo/convert_frame.py:824]   File "/usr/local/lib/python3.10/dist-packages/torch/_dynamo/utils.py", line 262, in time_wrapper
W0629 11:52:50.259000 139012379337344 torch/_dynamo/convert_frame.py:824]     r = func(*args, **kwargs)
W0629 11:52:50.259000 139012379337344 torch/_dynamo/convert_frame.py:824]   File "/usr/local/lib/python3.10/dist-packages/torch/_inductor/compile_fx.py", line 1257, in fw_compiler_base
W0629 11:52:50.259000 139012379337344 torch/_dynamo/convert_frame.py:824]     return inner_compile(
W0629 11:52:50.259000 139012379337344 torch/_dynamo/convert_frame.py:824]   File "/usr/local/lib/python3.10/dist-packages/torch/_dynamo/repro/after_aot.py", line 83, in debug_wrapper
W0629 11:52:50.259000 139012379337344 torch/_dynamo/convert_frame.py:824]     inner_compiled_fn = compiler_fn(gm, example_inputs)
W0629 11:52:50.259000 139012379337344 torch/_dynamo/convert_frame.py:824]   File "/usr/local/lib/python3.10/dist-packages/torch/_inductor/debug.py", line 304, in inner
W0629 11:52:50.259000 139012379337344 torch/_dynamo/convert_frame.py:824]     return fn(*args, **kwargs)
W0629 11:52:50.259000 139012379337344 torch/_dynamo/convert_frame.py:824]   File "/usr/lib/python3.10/contextlib.py", line 79, in inner
W0629 11:52:50.259000 139012379337344 torch/_dynamo/convert_frame.py:824]     return func(*args, **kwds)
W0629 11:52:50.259000 139012379337344 torch/_dynamo/convert_frame.py:824]   File "/usr/lib/python3.10/contextlib.py", line 79, in inner
W0629 11:52:50.259000 139012379337344 torch/_dynamo/convert_frame.py:824]     return func(*args, **kwds)
W0629 11:52:50.259000 139012379337344 torch/_dynamo/convert_frame.py:824]   File "/usr/local/lib/python3.10/dist-packages/torch/_dynamo/utils.py", line 262, in time_wrapper
W0629 11:52:50.259000 139012379337344 torch/_dynamo/convert_frame.py:824]     r = func(*args, **kwargs)
W0629 11:52:50.259000 139012379337344 torch/_dynamo/convert_frame.py:824]   File "/usr/local/lib/python3.10/dist-packages/torch/_inductor/compile_fx.py", line 438, in compile_fx_inner
W0629 11:52:50.259000 139012379337344 torch/_dynamo/convert_frame.py:824]     compiled_graph = fx_codegen_and_compile(
W0629 11:52:50.259000 139012379337344 torch/_dynamo/convert_frame.py:824]   File "/usr/local/lib/python3.10/dist-packages/torch/_inductor/compile_fx.py", line 714, in fx_codegen_and_compile
W0629 11:52:50.259000 139012379337344 torch/_dynamo/convert_frame.py:824]     compiled_fn = graph.compile_to_fn()
W0629 11:52:50.259000 139012379337344 torch/_dynamo/convert_frame.py:824]   File "/usr/local/lib/python3.10/dist-packages/torch/_inductor/graph.py", line 1307, in compile_to_fn
W0629 11:52:50.259000 139012379337344 torch/_dynamo/convert_frame.py:824]     return self.compile_to_module().call
W0629 11:52:50.259000 139012379337344 torch/_dynamo/convert_frame.py:824]   File "/usr/local/lib/python3.10/dist-packages/torch/_dynamo/utils.py", line 262, in time_wrapper
W0629 11:52:50.259000 139012379337344 torch/_dynamo/convert_frame.py:824]     r = func(*args, **kwargs)
W0629 11:52:50.259000 139012379337344 torch/_dynamo/convert_frame.py:824]   File "/usr/local/lib/python3.10/dist-packages/torch/_inductor/graph.py", line 1254, in compile_to_module
W0629 11:52:50.259000 139012379337344 torch/_dynamo/convert_frame.py:824]     mod = PyCodeCache.load_by_key_path(
W0629 11:52:50.259000 139012379337344 torch/_dynamo/convert_frame.py:824]   File "/usr/local/lib/python3.10/dist-packages/torch/_inductor/codecache.py", line 2160, in load_by_key_path
W0629 11:52:50.259000 139012379337344 torch/_dynamo/convert_frame.py:824]     exec(code, mod.__dict__, mod.__dict__)
W0629 11:52:50.259000 139012379337344 torch/_dynamo/convert_frame.py:824]   File "/tmp/torchinductor_root/3c/c3c6zsdgckmnmg37zo6kwa6twnu6nrfqfprpb24pozwh3nm6caxf.py", line 219, in <module>
W0629 11:52:50.259000 139012379337344 torch/_dynamo/convert_frame.py:824]     async_compile.wait(globals())
W0629 11:52:50.259000 139012379337344 torch/_dynamo/convert_frame.py:824]   File "/usr/local/lib/python3.10/dist-packages/torch/_inductor/codecache.py", line 2715, in wait
W0629 11:52:50.259000 139012379337344 torch/_dynamo/convert_frame.py:824]     scope[key] = result.result()
W0629 11:52:50.259000 139012379337344 torch/_dynamo/convert_frame.py:824]   File "/usr/local/lib/python3.10/dist-packages/torch/_inductor/codecache.py", line 2522, in result
W0629 11:52:50.259000 139012379337344 torch/_dynamo/convert_frame.py:824]     self.future.result()
W0629 11:52:50.259000 139012379337344 torch/_dynamo/convert_frame.py:824]   File "/usr/lib/python3.10/concurrent/futures/_base.py", line 458, in result
W0629 11:52:50.259000 139012379337344 torch/_dynamo/convert_frame.py:824]     return self.__get_result()
W0629 11:52:50.259000 139012379337344 torch/_dynamo/convert_frame.py:824]   File "/usr/lib/python3.10/concurrent/futures/_base.py", line 403, in __get_result
W0629 11:52:50.259000 139012379337344 torch/_dynamo/convert_frame.py:824]     raise self._exception
W0629 11:52:50.259000 139012379337344 torch/_dynamo/convert_frame.py:824] torch._dynamo.exc.BackendCompilerFailed: backend='inductor' raised:
W0629 11:52:50.259000 139012379337344 torch/_dynamo/convert_frame.py:824] RuntimeError: Internal Triton PTX codegen error:
W0629 11:52:50.259000 139012379337344 torch/_dynamo/convert_frame.py:824] ptxas /tmp/compile-ptx-src-db8149, line 59; error   : Feature '.bf16' requires .target sm_80 or higher
W0629 11:52:50.259000 139012379337344 torch/_dynamo/convert_frame.py:824] ptxas /tmp/compile-ptx-src-db8149, line 59; error   : Feature 'cvt.bf16.f32' requires .target sm_80 or higher
W0629 11:52:50.259000 139012379337344 torch/_dynamo/convert_frame.py:824] ptxas /tmp/compile-ptx-src-db8149, line 60; error   : Feature '.bf16' requires .target sm_80 or higher
W0629 11:52:50.259000 139012379337344 torch/_dynamo/convert_frame.py:824] ptxas /tmp/compile-ptx-src-db8149, line 60; error   : Feature 'cvt.bf16.f32' requires .target sm_80 or higher
W0629 11:52:50.259000 139012379337344 torch/_dynamo/convert_frame.py:824] ptxas /tmp/compile-ptx-src-db8149, line 61; error   : Feature '.bf16' requires .target sm_80 or higher
W0629 11:52:50.259000 139012379337344 torch/_dynamo/convert_frame.py:824] ptxas /tmp/compile-ptx-src-db8149, line 61; error   : Feature 'cvt.bf16.f32' requires .target sm_80 or higher
W0629 11:52:50.259000 139012379337344 torch/_dynamo/convert_frame.py:824] ptxas /tmp/compile-ptx-src-db8149, line 62; error   : Feature '.bf16' requires .target sm_80 or higher
W0629 11:52:50.259000 139012379337344 torch/_dynamo/convert_frame.py:824] ptxas /tmp/compile-ptx-src-db8149, line 62; error   : Feature 'cvt.bf16.f32' requires .target sm_80 or higher
W0629 11:52:50.259000 139012379337344 torch/_dynamo/convert_frame.py:824] ptxas /tmp/compile-ptx-src-db8149, line 63; error   : Feature '.bf16' requires .target sm_80 or higher
W0629 11:52:50.259000 139012379337344 torch/_dynamo/convert_frame.py:824] ptxas /tmp/compile-ptx-src-db8149, line 63; error   : Feature 'cvt.bf16.f32' requires .target sm_80 or higher
W0629 11:52:50.259000 139012379337344 torch/_dynamo/convert_frame.py:824] ptxas /tmp/compile-ptx-src-db8149, line 64; error   : Feature '.bf16' requires .target sm_80 or higher
W0629 11:52:50.259000 139012379337344 torch/_dynamo/convert_frame.py:824] ptxas /tmp/compile-ptx-src-db8149, line 64; error   : Feature 'cvt.bf16.f32' requires .target sm_80 or higher
W0629 11:52:50.259000 139012379337344 torch/_dynamo/convert_frame.py:824] ptxas /tmp/compile-ptx-src-db8149, line 65; error   : Feature '.bf16' requires .target sm_80 or higher
W0629 11:52:50.259000 139012379337344 torch/_dynamo/convert_frame.py:824] ptxas /tmp/compile-ptx-src-db8149, line 65; error   : Feature 'cvt.bf16.f32' requires .target sm_80 or higher
W0629 11:52:50.259000 139012379337344 torch/_dynamo/convert_frame.py:824] ptxas /tmp/compile-ptx-src-db8149, line 66; error   : Feature '.bf16' requires .target sm_80 or higher
W0629 11:52:50.259000 139012379337344 torch/_dynamo/convert_frame.py:824] ptxas /tmp/compile-ptx-src-db8149, line 66; error   : Feature 'cvt.bf16.f32' requires .target sm_80 or higher
W0629 11:52:50.259000 139012379337344 torch/_dynamo/convert_frame.py:824] ptxas fatal   : Ptx assembly aborted due to errors
W0629 11:52:50.259000 139012379337344 torch/_dynamo/convert_frame.py:824]
W0629 11:52:50.259000 139012379337344 torch/_dynamo/convert_frame.py:824]
W0629 11:52:50.259000 139012379337344 torch/_dynamo/convert_frame.py:824] Set TORCH_LOGS="+dynamo" and TORCHDYNAMO_VERBOSE=1 for more information
W0629 11:52:50.259000 139012379337344 torch/_dynamo/convert_frame.py:824]
W0629 11:52:50.259000 139012379337344 torch/_dynamo/convert_frame.py:824] Traceback (most recent call last):
W0629 11:52:50.259000 139012379337344 torch/_dynamo/convert_frame.py:824]   File "/usr/local/lib/python3.10/dist-packages/torch/_dynamo/convert_frame.py", line 786, in _convert_frame
W0629 11:52:50.259000 139012379337344 torch/_dynamo/convert_frame.py:824]     result = inner_convert(
W0629 11:52:50.259000 139012379337344 torch/_dynamo/convert_frame.py:824]   File "/usr/local/lib/python3.10/dist-packages/torch/_dynamo/convert_frame.py", line 400, in _convert_frame_assert
W0629 11:52:50.259000 139012379337344 torch/_dynamo/convert_frame.py:824]     return _compile(
W0629 11:52:50.259000 139012379337344 torch/_dynamo/convert_frame.py:824]   File "/usr/lib/python3.10/contextlib.py", line 79, in inner
W0629 11:52:50.259000 139012379337344 torch/_dynamo/convert_frame.py:824]     return func(*args, **kwds)
W0629 11:52:50.259000 139012379337344 torch/_dynamo/convert_frame.py:824]   File "/usr/local/lib/python3.10/dist-packages/torch/_dynamo/convert_frame.py", line 676, in _compile
W0629 11:52:50.259000 139012379337344 torch/_dynamo/convert_frame.py:824]     guarded_code = compile_inner(code, one_graph, hooks, transform)
W0629 11:52:50.259000 139012379337344 torch/_dynamo/convert_frame.py:824]   File "/usr/local/lib/python3.10/dist-packages/torch/_dynamo/utils.py", line 262, in time_wrapper
W0629 11:52:50.259000 139012379337344 torch/_dynamo/convert_frame.py:824]     r = func(*args, **kwargs)
W0629 11:52:50.259000 139012379337344 torch/_dynamo/convert_frame.py:824]   File "/usr/local/lib/python3.10/dist-packages/torch/_dynamo/convert_frame.py", line 535, in compile_inner
W0629 11:52:50.259000 139012379337344 torch/_dynamo/convert_frame.py:824]     out_code = transform_code_object(code, transform)
W0629 11:52:50.259000 139012379337344 torch/_dynamo/convert_frame.py:824]   File "/usr/local/lib/python3.10/dist-packages/torch/_dynamo/bytecode_transformation.py", line 1036, in transform_code_object
W0629 11:52:50.259000 139012379337344 torch/_dynamo/convert_frame.py:824]     transformations(instructions, code_options)
W0629 11:52:50.259000 139012379337344 torch/_dynamo/convert_frame.py:824]   File "/usr/local/lib/python3.10/dist-packages/torch/_dynamo/convert_frame.py", line 165, in _fn
W0629 11:52:50.259000 139012379337344 torch/_dynamo/convert_frame.py:824]     return fn(*args, **kwargs)
W0629 11:52:50.259000 139012379337344 torch/_dynamo/convert_frame.py:824]   File "/usr/local/lib/python3.10/dist-packages/torch/_dynamo/convert_frame.py", line 500, in transform
W0629 11:52:50.259000 139012379337344 torch/_dynamo/convert_frame.py:824]     tracer.run()
W0629 11:52:50.259000 139012379337344 torch/_dynamo/convert_frame.py:824]   File "/usr/local/lib/python3.10/dist-packages/torch/_dynamo/symbolic_convert.py", line 2149, in run
W0629 11:52:50.259000 139012379337344 torch/_dynamo/convert_frame.py:824]     super().run()
W0629 11:52:50.259000 139012379337344 torch/_dynamo/convert_frame.py:824]   File "/usr/local/lib/python3.10/dist-packages/torch/_dynamo/symbolic_convert.py", line 810, in run
W0629 11:52:50.259000 139012379337344 torch/_dynamo/convert_frame.py:824]     and self.step()
W0629 11:52:50.259000 139012379337344 torch/_dynamo/convert_frame.py:824]   File "/usr/local/lib/python3.10/dist-packages/torch/_dynamo/symbolic_convert.py", line 773, in step
W0629 11:52:50.259000 139012379337344 torch/_dynamo/convert_frame.py:824]     getattr(self, inst.opname)(inst)
W0629 11:52:50.259000 139012379337344 torch/_dynamo/convert_frame.py:824]   File "/usr/local/lib/python3.10/dist-packages/torch/_dynamo/symbolic_convert.py", line 2268, in RETURN_VALUE
W0629 11:52:50.259000 139012379337344 torch/_dynamo/convert_frame.py:824]     self.output.compile_subgraph(
W0629 11:52:50.259000 139012379337344 torch/_dynamo/convert_frame.py:824]   File "/usr/local/lib/python3.10/dist-packages/torch/_dynamo/output_graph.py", line 971, in compile_subgraph
W0629 11:52:50.259000 139012379337344 torch/_dynamo/convert_frame.py:824]     self.compile_and_call_fx_graph(tx, list(reversed(stack_values)), root)
W0629 11:52:50.259000 139012379337344 torch/_dynamo/convert_frame.py:824]   File "/usr/lib/python3.10/contextlib.py", line 79, in inner
W0629 11:52:50.259000 139012379337344 torch/_dynamo/convert_frame.py:824]     return func(*args, **kwds)
W0629 11:52:50.259000 139012379337344 torch/_dynamo/convert_frame.py:824]   File "/usr/local/lib/python3.10/dist-packages/torch/_dynamo/output_graph.py", line 1168, in compile_and_call_fx_graph
W0629 11:52:50.259000 139012379337344 torch/_dynamo/convert_frame.py:824]     compiled_fn = self.call_user_compiler(gm)
W0629 11:52:50.259000 139012379337344 torch/_dynamo/convert_frame.py:824]   File "/usr/local/lib/python3.10/dist-packages/torch/_dynamo/utils.py", line 262, in time_wrapper
W0629 11:52:50.259000 139012379337344 torch/_dynamo/convert_frame.py:824]     r = func(*args, **kwargs)
W0629 11:52:50.259000 139012379337344 torch/_dynamo/convert_frame.py:824]   File "/usr/local/lib/python3.10/dist-packages/torch/_dynamo/output_graph.py", line 1241, in call_user_compiler
W0629 11:52:50.259000 139012379337344 torch/_dynamo/convert_frame.py:824]     raise BackendCompilerFailed(self.compiler_fn, e).with_traceback(
W0629 11:52:50.259000 139012379337344 torch/_dynamo/convert_frame.py:824]   File "/usr/local/lib/python3.10/dist-packages/torch/_dynamo/output_graph.py", line 1222, in call_user_compiler
W0629 11:52:50.259000 139012379337344 torch/_dynamo/convert_frame.py:824]     compiled_fn = compiler_fn(gm, self.example_inputs())
W0629 11:52:50.259000 139012379337344 torch/_dynamo/convert_frame.py:824]   File "/usr/local/lib/python3.10/dist-packages/torch/_dynamo/repro/after_dynamo.py", line 117, in debug_wrapper
W0629 11:52:50.259000 139012379337344 torch/_dynamo/convert_frame.py:824]     compiled_gm = compiler_fn(gm, example_inputs)
W0629 11:52:50.259000 139012379337344 torch/_dynamo/convert_frame.py:824]   File "/usr/local/lib/python3.10/dist-packages/torch/__init__.py", line 1729, in __call__
W0629 11:52:50.259000 139012379337344 torch/_dynamo/convert_frame.py:824]     return compile_fx(model_, inputs_, config_patches=self.config)
W0629 11:52:50.259000 139012379337344 torch/_dynamo/convert_frame.py:824]   File "/usr/lib/python3.10/contextlib.py", line 79, in inner
W0629 11:52:50.259000 139012379337344 torch/_dynamo/convert_frame.py:824]     return func(*args, **kwds)
W0629 11:52:50.259000 139012379337344 torch/_dynamo/convert_frame.py:824]   File "/usr/local/lib/python3.10/dist-packages/torch/_inductor/compile_fx.py", line 1330, in compile_fx
W0629 11:52:50.259000 139012379337344 torch/_dynamo/convert_frame.py:824]     return aot_autograd(
W0629 11:52:50.259000 139012379337344 torch/_dynamo/convert_frame.py:824]   File "/usr/local/lib/python3.10/dist-packages/torch/_dynamo/backends/common.py", line 58, in compiler_fn
W0629 11:52:50.259000 139012379337344 torch/_dynamo/convert_frame.py:824]     cg = aot_module_simplified(gm, example_inputs, **kwargs)
W0629 11:52:50.259000 139012379337344 torch/_dynamo/convert_frame.py:824]   File "/usr/local/lib/python3.10/dist-packages/torch/_functorch/aot_autograd.py", line 903, in aot_module_simplified
W0629 11:52:50.259000 139012379337344 torch/_dynamo/convert_frame.py:824]     compiled_fn = create_aot_dispatcher_function(
W0629 11:52:50.259000 139012379337344 torch/_dynamo/convert_frame.py:824]   File "/usr/local/lib/python3.10/dist-packages/torch/_dynamo/utils.py", line 262, in time_wrapper
W0629 11:52:50.259000 139012379337344 torch/_dynamo/convert_frame.py:824]     r = func(*args, **kwargs)
W0629 11:52:50.259000 139012379337344 torch/_dynamo/convert_frame.py:824]   File "/usr/local/lib/python3.10/dist-packages/torch/_functorch/aot_autograd.py", line 628, in create_aot_dispatcher_function
W0629 11:52:50.259000 139012379337344 torch/_dynamo/convert_frame.py:824]     compiled_fn = compiler_fn(flat_fn, fake_flat_args, aot_config, fw_metadata=fw_metadata)
W0629 11:52:50.259000 139012379337344 torch/_dynamo/convert_frame.py:824]   File "/usr/local/lib/python3.10/dist-packages/torch/_functorch/_aot_autograd/runtime_wrappers.py", line 443, in aot_wrapper_dedupe
W0629 11:52:50.259000 139012379337344 torch/_dynamo/convert_frame.py:824]     return compiler_fn(flat_fn, leaf_flat_args, aot_config, fw_metadata=fw_metadata)
W0629 11:52:50.259000 139012379337344 torch/_dynamo/convert_frame.py:824]   File "/usr/local/lib/python3.10/dist-packages/torch/_functorch/_aot_autograd/runtime_wrappers.py", line 648, in aot_wrapper_synthetic_base
W0629 11:52:50.259000 139012379337344 torch/_dynamo/convert_frame.py:824]     return compiler_fn(flat_fn, flat_args, aot_config, fw_metadata=fw_metadata)
W0629 11:52:50.259000 139012379337344 torch/_dynamo/convert_frame.py:824]   File "/usr/local/lib/python3.10/dist-packages/torch/_functorch/_aot_autograd/jit_compile_runtime_wrappers.py", line 119, in aot_dispatch_base
W0629 11:52:50.259000 139012379337344 torch/_dynamo/convert_frame.py:824]     compiled_fw = compiler(fw_module, updated_flat_args)
W0629 11:52:50.259000 139012379337344 torch/_dynamo/convert_frame.py:824]   File "/usr/local/lib/python3.10/dist-packages/torch/_dynamo/utils.py", line 262, in time_wrapper
W0629 11:52:50.259000 139012379337344 torch/_dynamo/convert_frame.py:824]     r = func(*args, **kwargs)
W0629 11:52:50.259000 139012379337344 torch/_dynamo/convert_frame.py:824]   File "/usr/local/lib/python3.10/dist-packages/torch/_inductor/compile_fx.py", line 1257, in fw_compiler_base
W0629 11:52:50.259000 139012379337344 torch/_dynamo/convert_frame.py:824]     return inner_compile(
W0629 11:52:50.259000 139012379337344 torch/_dynamo/convert_frame.py:824]   File "/usr/local/lib/python3.10/dist-packages/torch/_dynamo/repro/after_aot.py", line 83, in debug_wrapper
W0629 11:52:50.259000 139012379337344 torch/_dynamo/convert_frame.py:824]     inner_compiled_fn = compiler_fn(gm, example_inputs)
W0629 11:52:50.259000 139012379337344 torch/_dynamo/convert_frame.py:824]   File "/usr/local/lib/python3.10/dist-packages/torch/_inductor/debug.py", line 304, in inner
W0629 11:52:50.259000 139012379337344 torch/_dynamo/convert_frame.py:824]     return fn(*args, **kwargs)
W0629 11:52:50.259000 139012379337344 torch/_dynamo/convert_frame.py:824]   File "/usr/lib/python3.10/contextlib.py", line 79, in inner
W0629 11:52:50.259000 139012379337344 torch/_dynamo/convert_frame.py:824]     return func(*args, **kwds)
W0629 11:52:50.259000 139012379337344 torch/_dynamo/convert_frame.py:824]   File "/usr/lib/python3.10/contextlib.py", line 79, in inner
W0629 11:52:50.259000 139012379337344 torch/_dynamo/convert_frame.py:824]     return func(*args, **kwds)
W0629 11:52:50.259000 139012379337344 torch/_dynamo/convert_frame.py:824]   File "/usr/local/lib/python3.10/dist-packages/torch/_dynamo/utils.py", line 262, in time_wrapper
W0629 11:52:50.259000 139012379337344 torch/_dynamo/convert_frame.py:824]     r = func(*args, **kwargs)
W0629 11:52:50.259000 139012379337344 torch/_dynamo/convert_frame.py:824]   File "/usr/local/lib/python3.10/dist-packages/torch/_inductor/compile_fx.py", line 438, in compile_fx_inner
W0629 11:52:50.259000 139012379337344 torch/_dynamo/convert_frame.py:824]     compiled_graph = fx_codegen_and_compile(
W0629 11:52:50.259000 139012379337344 torch/_dynamo/convert_frame.py:824]   File "/usr/local/lib/python3.10/dist-packages/torch/_inductor/compile_fx.py", line 714, in fx_codegen_and_compile
W0629 11:52:50.259000 139012379337344 torch/_dynamo/convert_frame.py:824]     compiled_fn = graph.compile_to_fn()
W0629 11:52:50.259000 139012379337344 torch/_dynamo/convert_frame.py:824]   File "/usr/local/lib/python3.10/dist-packages/torch/_inductor/graph.py", line 1307, in compile_to_fn
W0629 11:52:50.259000 139012379337344 torch/_dynamo/convert_frame.py:824]     return self.compile_to_module().call
W0629 11:52:50.259000 139012379337344 torch/_dynamo/convert_frame.py:824]   File "/usr/local/lib/python3.10/dist-packages/torch/_dynamo/utils.py", line 262, in time_wrapper
W0629 11:52:50.259000 139012379337344 torch/_dynamo/convert_frame.py:824]     r = func(*args, **kwargs)
W0629 11:52:50.259000 139012379337344 torch/_dynamo/convert_frame.py:824]   File "/usr/local/lib/python3.10/dist-packages/torch/_inductor/graph.py", line 1254, in compile_to_module
W0629 11:52:50.259000 139012379337344 torch/_dynamo/convert_frame.py:824]     mod = PyCodeCache.load_by_key_path(
W0629 11:52:50.259000 139012379337344 torch/_dynamo/convert_frame.py:824]   File "/usr/local/lib/python3.10/dist-packages/torch/_inductor/codecache.py", line 2160, in load_by_key_path
W0629 11:52:50.259000 139012379337344 torch/_dynamo/convert_frame.py:824]     exec(code, mod.__dict__, mod.__dict__)
W0629 11:52:50.259000 139012379337344 torch/_dynamo/convert_frame.py:824]   File "/tmp/torchinductor_root/3c/c3c6zsdgckmnmg37zo6kwa6twnu6nrfqfprpb24pozwh3nm6caxf.py", line 219, in <module>
W0629 11:52:50.259000 139012379337344 torch/_dynamo/convert_frame.py:824]     async_compile.wait(globals())
W0629 11:52:50.259000 139012379337344 torch/_dynamo/convert_frame.py:824]   File "/usr/local/lib/python3.10/dist-packages/torch/_inductor/codecache.py", line 2715, in wait
W0629 11:52:50.259000 139012379337344 torch/_dynamo/convert_frame.py:824]     scope[key] = result.result()
W0629 11:52:50.259000 139012379337344 torch/_dynamo/convert_frame.py:824]   File "/usr/local/lib/python3.10/dist-packages/torch/_inductor/codecache.py", line 2522, in result
W0629 11:52:50.259000 139012379337344 torch/_dynamo/convert_frame.py:824]     self.future.result()
W0629 11:52:50.259000 139012379337344 torch/_dynamo/convert_frame.py:824]   File "/usr/lib/python3.10/concurrent/futures/_base.py", line 458, in result
W0629 11:52:50.259000 139012379337344 torch/_dynamo/convert_frame.py:824]     return self.__get_result()
W0629 11:52:50.259000 139012379337344 torch/_dynamo/convert_frame.py:824]   File "/usr/lib/python3.10/concurrent/futures/_base.py", line 403, in __get_result
W0629 11:52:50.259000 139012379337344 torch/_dynamo/convert_frame.py:824]     raise self._exception
W0629 11:52:50.259000 139012379337344 torch/_dynamo/convert_frame.py:824] torch._dynamo.exc.BackendCompilerFailed: backend='inductor' raised:
W0629 11:52:50.259000 139012379337344 torch/_dynamo/convert_frame.py:824] RuntimeError: Internal Triton PTX codegen error:
W0629 11:52:50.259000 139012379337344 torch/_dynamo/convert_frame.py:824] ptxas /tmp/compile-ptx-src-db8149, line 59; error   : Feature '.bf16' requires .target sm_80 or higher
W0629 11:52:50.259000 139012379337344 torch/_dynamo/convert_frame.py:824] ptxas /tmp/compile-ptx-src-db8149, line 59; error   : Feature 'cvt.bf16.f32' requires .target sm_80 or higher
W0629 11:52:50.259000 139012379337344 torch/_dynamo/convert_frame.py:824] ptxas /tmp/compile-ptx-src-db8149, line 60; error   : Feature '.bf16' requires .target sm_80 or higher
W0629 11:52:50.259000 139012379337344 torch/_dynamo/convert_frame.py:824] ptxas /tmp/compile-ptx-src-db8149, line 60; error   : Feature 'cvt.bf16.f32' requires .target sm_80 or higher
W0629 11:52:50.259000 139012379337344 torch/_dynamo/convert_frame.py:824] ptxas /tmp/compile-ptx-src-db8149, line 61; error   : Feature '.bf16' requires .target sm_80 or higher
W0629 11:52:50.259000 139012379337344 torch/_dynamo/convert_frame.py:824] ptxas /tmp/compile-ptx-src-db8149, line 61; error   : Feature 'cvt.bf16.f32' requires .target sm_80 or higher
W0629 11:52:50.259000 139012379337344 torch/_dynamo/convert_frame.py:824] ptxas /tmp/compile-ptx-src-db8149, line 62; error   : Feature '.bf16' requires .target sm_80 or higher
W0629 11:52:50.259000 139012379337344 torch/_dynamo/convert_frame.py:824] ptxas /tmp/compile-ptx-src-db8149, line 62; error   : Feature 'cvt.bf16.f32' requires .target sm_80 or higher
W0629 11:52:50.259000 139012379337344 torch/_dynamo/convert_frame.py:824] ptxas /tmp/compile-ptx-src-db8149, line 63; error   : Feature '.bf16' requires .target sm_80 or higher
W0629 11:52:50.259000 139012379337344 torch/_dynamo/convert_frame.py:824] ptxas /tmp/compile-ptx-src-db8149, line 63; error   : Feature 'cvt.bf16.f32' requires .target sm_80 or higher
W0629 11:52:50.259000 139012379337344 torch/_dynamo/convert_frame.py:824] ptxas /tmp/compile-ptx-src-db8149, line 64; error   : Feature '.bf16' requires .target sm_80 or higher
W0629 11:52:50.259000 139012379337344 torch/_dynamo/convert_frame.py:824] ptxas /tmp/compile-ptx-src-db8149, line 64; error   : Feature 'cvt.bf16.f32' requires .target sm_80 or higher
W0629 11:52:50.259000 139012379337344 torch/_dynamo/convert_frame.py:824] ptxas /tmp/compile-ptx-src-db8149, line 65; error   : Feature '.bf16' requires .target sm_80 or higher
W0629 11:52:50.259000 139012379337344 torch/_dynamo/convert_frame.py:824] ptxas /tmp/compile-ptx-src-db8149, line 65; error   : Feature 'cvt.bf16.f32' requires .target sm_80 or higher
W0629 11:52:50.259000 139012379337344 torch/_dynamo/convert_frame.py:824] ptxas /tmp/compile-ptx-src-db8149, line 66; error   : Feature '.bf16' requires .target sm_80 or higher
W0629 11:52:50.259000 139012379337344 torch/_dynamo/convert_frame.py:824] ptxas /tmp/compile-ptx-src-db8149, line 66; error   : Feature 'cvt.bf16.f32' requires .target sm_80 or higher
W0629 11:52:50.259000 139012379337344 torch/_dynamo/convert_frame.py:824] ptxas fatal   : Ptx assembly aborted due to errors
W0629 11:52:50.259000 139012379337344 torch/_dynamo/convert_frame.py:824]
W0629 11:52:50.259000 139012379337344 torch/_dynamo/convert_frame.py:824]
W0629 11:52:50.259000 139012379337344 torch/_dynamo/convert_frame.py:824] Set TORCH_LOGS="+dynamo" and TORCHDYNAMO_VERBOSE=1 for more information
W0629 11:52:50.259000 139012379337344 torch/_dynamo/convert_frame.py:824]
W0629 11:52:50.639000 139012379337344 torch/_dynamo/convert_frame.py:824] WON'T CONVERT feed_forward_chunk /usr/local/lib/python3.10/dist-packages/transformers/models/bert/modeling_bert.py line 633
W0629 11:52:50.639000 139012379337344 torch/_dynamo/convert_frame.py:824] due to:
W0629 11:52:50.639000 139012379337344 torch/_dynamo/convert_frame.py:824] Traceback (most recent call last):
W0629 11:52:50.639000 139012379337344 torch/_dynamo/convert_frame.py:824]   File "/usr/local/lib/python3.10/dist-packages/torch/_dynamo/convert_frame.py", line 786, in _convert_frame
W0629 11:52:50.639000 139012379337344 torch/_dynamo/convert_frame.py:824]     result = inner_convert(
W0629 11:52:50.639000 139012379337344 torch/_dynamo/convert_frame.py:824]   File "/usr/local/lib/python3.10/dist-packages/torch/_dynamo/convert_frame.py", line 400, in _convert_frame_assert
W0629 11:52:50.639000 139012379337344 torch/_dynamo/convert_frame.py:824]     return _compile(
W0629 11:52:50.639000 139012379337344 torch/_dynamo/convert_frame.py:824]   File "/usr/lib/python3.10/contextlib.py", line 79, in inner
W0629 11:52:50.639000 139012379337344 torch/_dynamo/convert_frame.py:824]     return func(*args, **kwds)
W0629 11:52:50.639000 139012379337344 torch/_dynamo/convert_frame.py:824]   File "/usr/local/lib/python3.10/dist-packages/torch/_dynamo/convert_frame.py", line 676, in _compile
W0629 11:52:50.639000 139012379337344 torch/_dynamo/convert_frame.py:824]     guarded_code = compile_inner(code, one_graph, hooks, transform)
W0629 11:52:50.639000 139012379337344 torch/_dynamo/convert_frame.py:824]   File "/usr/local/lib/python3.10/dist-packages/torch/_dynamo/utils.py", line 262, in time_wrapper
W0629 11:52:50.639000 139012379337344 torch/_dynamo/convert_frame.py:824]     r = func(*args, **kwargs)
W0629 11:52:50.639000 139012379337344 torch/_dynamo/convert_frame.py:824]   File "/usr/local/lib/python3.10/dist-packages/torch/_dynamo/convert_frame.py", line 535, in compile_inner
W0629 11:52:50.639000 139012379337344 torch/_dynamo/convert_frame.py:824]     out_code = transform_code_object(code, transform)
W0629 11:52:50.639000 139012379337344 torch/_dynamo/convert_frame.py:824]   File "/usr/local/lib/python3.10/dist-packages/torch/_dynamo/bytecode_transformation.py", line 1036, in transform_code_object
W0629 11:52:50.639000 139012379337344 torch/_dynamo/convert_frame.py:824]     transformations(instructions, code_options)
W0629 11:52:50.639000 139012379337344 torch/_dynamo/convert_frame.py:824]   File "/usr/local/lib/python3.10/dist-packages/torch/_dynamo/convert_frame.py", line 165, in _fn
W0629 11:52:50.639000 139012379337344 torch/_dynamo/convert_frame.py:824]     return fn(*args, **kwargs)
W0629 11:52:50.639000 139012379337344 torch/_dynamo/convert_frame.py:824]   File "/usr/local/lib/python3.10/dist-packages/torch/_dynamo/convert_frame.py", line 500, in transform
W0629 11:52:50.639000 139012379337344 torch/_dynamo/convert_frame.py:824]     tracer.run()
W0629 11:52:50.639000 139012379337344 torch/_dynamo/convert_frame.py:824]   File "/usr/local/lib/python3.10/dist-packages/torch/_dynamo/symbolic_convert.py", line 2149, in run
W0629 11:52:50.639000 139012379337344 torch/_dynamo/convert_frame.py:824]     super().run()
W0629 11:52:50.639000 139012379337344 torch/_dynamo/convert_frame.py:824]   File "/usr/local/lib/python3.10/dist-packages/torch/_dynamo/symbolic_convert.py", line 810, in run
W0629 11:52:50.639000 139012379337344 torch/_dynamo/convert_frame.py:824]     and self.step()
W0629 11:52:50.639000 139012379337344 torch/_dynamo/convert_frame.py:824]   File "/usr/local/lib/python3.10/dist-packages/torch/_dynamo/symbolic_convert.py", line 773, in step
W0629 11:52:50.639000 139012379337344 torch/_dynamo/convert_frame.py:824]     getattr(self, inst.opname)(inst)
W0629 11:52:50.639000 139012379337344 torch/_dynamo/convert_frame.py:824]   File "/usr/local/lib/python3.10/dist-packages/torch/_dynamo/symbolic_convert.py", line 2268, in RETURN_VALUE
W0629 11:52:50.639000 139012379337344 torch/_dynamo/convert_frame.py:824]     self.output.compile_subgraph(
W0629 11:52:50.639000 139012379337344 torch/_dynamo/convert_frame.py:824]   File "/usr/local/lib/python3.10/dist-packages/torch/_dynamo/output_graph.py", line 971, in compile_subgraph
W0629 11:52:50.639000 139012379337344 torch/_dynamo/convert_frame.py:824]     self.compile_and_call_fx_graph(tx, list(reversed(stack_values)), root)
W0629 11:52:50.639000 139012379337344 torch/_dynamo/convert_frame.py:824]   File "/usr/lib/python3.10/contextlib.py", line 79, in inner
W0629 11:52:50.639000 139012379337344 torch/_dynamo/convert_frame.py:824]     return func(*args, **kwds)
W0629 11:52:50.639000 139012379337344 torch/_dynamo/convert_frame.py:824]   File "/usr/local/lib/python3.10/dist-packages/torch/_dynamo/output_graph.py", line 1168, in compile_and_call_fx_graph
W0629 11:52:50.639000 139012379337344 torch/_dynamo/convert_frame.py:824]     compiled_fn = self.call_user_compiler(gm)
W0629 11:52:50.639000 139012379337344 torch/_dynamo/convert_frame.py:824]   File "/usr/local/lib/python3.10/dist-packages/torch/_dynamo/utils.py", line 262, in time_wrapper
W0629 11:52:50.639000 139012379337344 torch/_dynamo/convert_frame.py:824]     r = func(*args, **kwargs)
W0629 11:52:50.639000 139012379337344 torch/_dynamo/convert_frame.py:824]   File "/usr/local/lib/python3.10/dist-packages/torch/_dynamo/output_graph.py", line 1241, in call_user_compiler
W0629 11:52:50.639000 139012379337344 torch/_dynamo/convert_frame.py:824]     raise BackendCompilerFailed(self.compiler_fn, e).with_traceback(
W0629 11:52:50.639000 139012379337344 torch/_dynamo/convert_frame.py:824]   File "/usr/local/lib/python3.10/dist-packages/torch/_dynamo/output_graph.py", line 1222, in call_user_compiler
W0629 11:52:50.639000 139012379337344 torch/_dynamo/convert_frame.py:824]     compiled_fn = compiler_fn(gm, self.example_inputs())
W0629 11:52:50.639000 139012379337344 torch/_dynamo/convert_frame.py:824]   File "/usr/local/lib/python3.10/dist-packages/torch/_dynamo/repro/after_dynamo.py", line 117, in debug_wrapper
W0629 11:52:50.639000 139012379337344 torch/_dynamo/convert_frame.py:824]     compiled_gm = compiler_fn(gm, example_inputs)
W0629 11:52:50.639000 139012379337344 torch/_dynamo/convert_frame.py:824]   File "/usr/local/lib/python3.10/dist-packages/torch/__init__.py", line 1729, in __call__
W0629 11:52:50.639000 139012379337344 torch/_dynamo/convert_frame.py:824]     return compile_fx(model_, inputs_, config_patches=self.config)
W0629 11:52:50.639000 139012379337344 torch/_dynamo/convert_frame.py:824]   File "/usr/lib/python3.10/contextlib.py", line 79, in inner
W0629 11:52:50.639000 139012379337344 torch/_dynamo/convert_frame.py:824]     return func(*args, **kwds)
W0629 11:52:50.639000 139012379337344 torch/_dynamo/convert_frame.py:824]   File "/usr/local/lib/python3.10/dist-packages/torch/_inductor/compile_fx.py", line 1330, in compile_fx
W0629 11:52:50.639000 139012379337344 torch/_dynamo/convert_frame.py:824]     return aot_autograd(
W0629 11:52:50.639000 139012379337344 torch/_dynamo/convert_frame.py:824]   File "/usr/local/lib/python3.10/dist-packages/torch/_dynamo/backends/common.py", line 58, in compiler_fn
W0629 11:52:50.639000 139012379337344 torch/_dynamo/convert_frame.py:824]     cg = aot_module_simplified(gm, example_inputs, **kwargs)
W0629 11:52:50.639000 139012379337344 torch/_dynamo/convert_frame.py:824]   File "/usr/local/lib/python3.10/dist-packages/torch/_functorch/aot_autograd.py", line 903, in aot_module_simplified
W0629 11:52:50.639000 139012379337344 torch/_dynamo/convert_frame.py:824]     compiled_fn = create_aot_dispatcher_function(
W0629 11:52:50.639000 139012379337344 torch/_dynamo/convert_frame.py:824]   File "/usr/local/lib/python3.10/dist-packages/torch/_dynamo/utils.py", line 262, in time_wrapper
W0629 11:52:50.639000 139012379337344 torch/_dynamo/convert_frame.py:824]     r = func(*args, **kwargs)
W0629 11:52:50.639000 139012379337344 torch/_dynamo/convert_frame.py:824]   File "/usr/local/lib/python3.10/dist-packages/torch/_functorch/aot_autograd.py", line 628, in create_aot_dispatcher_function
W0629 11:52:50.639000 139012379337344 torch/_dynamo/convert_frame.py:824]     compiled_fn = compiler_fn(flat_fn, fake_flat_args, aot_config, fw_metadata=fw_metadata)
W0629 11:52:50.639000 139012379337344 torch/_dynamo/convert_frame.py:824]   File "/usr/local/lib/python3.10/dist-packages/torch/_functorch/_aot_autograd/runtime_wrappers.py", line 443, in aot_wrapper_dedupe
W0629 11:52:50.639000 139012379337344 torch/_dynamo/convert_frame.py:824]     return compiler_fn(flat_fn, leaf_flat_args, aot_config, fw_metadata=fw_metadata)
W0629 11:52:50.639000 139012379337344 torch/_dynamo/convert_frame.py:824]   File "/usr/local/lib/python3.10/dist-packages/torch/_functorch/_aot_autograd/runtime_wrappers.py", line 648, in aot_wrapper_synthetic_base
W0629 11:52:50.639000 139012379337344 torch/_dynamo/convert_frame.py:824]     return compiler_fn(flat_fn, flat_args, aot_config, fw_metadata=fw_metadata)
W0629 11:52:50.639000 139012379337344 torch/_dynamo/convert_frame.py:824]   File "/usr/local/lib/python3.10/dist-packages/torch/_functorch/_aot_autograd/jit_compile_runtime_wrappers.py", line 119, in aot_dispatch_base
W0629 11:52:50.639000 139012379337344 torch/_dynamo/convert_frame.py:824]     compiled_fw = compiler(fw_module, updated_flat_args)
W0629 11:52:50.639000 139012379337344 torch/_dynamo/convert_frame.py:824]   File "/usr/local/lib/python3.10/dist-packages/torch/_dynamo/utils.py", line 262, in time_wrapper
W0629 11:52:50.639000 139012379337344 torch/_dynamo/convert_frame.py:824]     r = func(*args, **kwargs)
W0629 11:52:50.639000 139012379337344 torch/_dynamo/convert_frame.py:824]   File "/usr/local/lib/python3.10/dist-packages/torch/_inductor/compile_fx.py", line 1257, in fw_compiler_base
W0629 11:52:50.639000 139012379337344 torch/_dynamo/convert_frame.py:824]     return inner_compile(
W0629 11:52:50.639000 139012379337344 torch/_dynamo/convert_frame.py:824]   File "/usr/local/lib/python3.10/dist-packages/torch/_dynamo/repro/after_aot.py", line 83, in debug_wrapper
W0629 11:52:50.639000 139012379337344 torch/_dynamo/convert_frame.py:824]     inner_compiled_fn = compiler_fn(gm, example_inputs)
W0629 11:52:50.639000 139012379337344 torch/_dynamo/convert_frame.py:824]   File "/usr/local/lib/python3.10/dist-packages/torch/_inductor/debug.py", line 304, in inner
W0629 11:52:50.639000 139012379337344 torch/_dynamo/convert_frame.py:824]     return fn(*args, **kwargs)
W0629 11:52:50.639000 139012379337344 torch/_dynamo/convert_frame.py:824]   File "/usr/lib/python3.10/contextlib.py", line 79, in inner
W0629 11:52:50.639000 139012379337344 torch/_dynamo/convert_frame.py:824]     return func(*args, **kwds)
W0629 11:52:50.639000 139012379337344 torch/_dynamo/convert_frame.py:824]   File "/usr/lib/python3.10/contextlib.py", line 79, in inner
W0629 11:52:50.639000 139012379337344 torch/_dynamo/convert_frame.py:824]     return func(*args, **kwds)
W0629 11:52:50.639000 139012379337344 torch/_dynamo/convert_frame.py:824]   File "/usr/local/lib/python3.10/dist-packages/torch/_dynamo/utils.py", line 262, in time_wrapper
W0629 11:52:50.639000 139012379337344 torch/_dynamo/convert_frame.py:824]     r = func(*args, **kwargs)
W0629 11:52:50.639000 139012379337344 torch/_dynamo/convert_frame.py:824]   File "/usr/local/lib/python3.10/dist-packages/torch/_inductor/compile_fx.py", line 438, in compile_fx_inner
W0629 11:52:50.639000 139012379337344 torch/_dynamo/convert_frame.py:824]     compiled_graph = fx_codegen_and_compile(
W0629 11:52:50.639000 139012379337344 torch/_dynamo/convert_frame.py:824]   File "/usr/local/lib/python3.10/dist-packages/torch/_inductor/compile_fx.py", line 714, in fx_codegen_and_compile
W0629 11:52:50.639000 139012379337344 torch/_dynamo/convert_frame.py:824]     compiled_fn = graph.compile_to_fn()
W0629 11:52:50.639000 139012379337344 torch/_dynamo/convert_frame.py:824]   File "/usr/local/lib/python3.10/dist-packages/torch/_inductor/graph.py", line 1307, in compile_to_fn
W0629 11:52:50.639000 139012379337344 torch/_dynamo/convert_frame.py:824]     return self.compile_to_module().call
W0629 11:52:50.639000 139012379337344 torch/_dynamo/convert_frame.py:824]   File "/usr/local/lib/python3.10/dist-packages/torch/_dynamo/utils.py", line 262, in time_wrapper
W0629 11:52:50.639000 139012379337344 torch/_dynamo/convert_frame.py:824]     r = func(*args, **kwargs)
W0629 11:52:50.639000 139012379337344 torch/_dynamo/convert_frame.py:824]   File "/usr/local/lib/python3.10/dist-packages/torch/_inductor/graph.py", line 1254, in compile_to_module
W0629 11:52:50.639000 139012379337344 torch/_dynamo/convert_frame.py:824]     mod = PyCodeCache.load_by_key_path(
W0629 11:52:50.639000 139012379337344 torch/_dynamo/convert_frame.py:824]   File "/usr/local/lib/python3.10/dist-packages/torch/_inductor/codecache.py", line 2160, in load_by_key_path
W0629 11:52:50.639000 139012379337344 torch/_dynamo/convert_frame.py:824]     exec(code, mod.__dict__, mod.__dict__)
W0629 11:52:50.639000 139012379337344 torch/_dynamo/convert_frame.py:824]   File "/tmp/torchinductor_root/3c/c3c6zsdgckmnmg37zo6kwa6twnu6nrfqfprpb24pozwh3nm6caxf.py", line 219, in <module>
W0629 11:52:50.639000 139012379337344 torch/_dynamo/convert_frame.py:824]     async_compile.wait(globals())
W0629 11:52:50.639000 139012379337344 torch/_dynamo/convert_frame.py:824]   File "/usr/local/lib/python3.10/dist-packages/torch/_inductor/codecache.py", line 2715, in wait
W0629 11:52:50.639000 139012379337344 torch/_dynamo/convert_frame.py:824]     scope[key] = result.result()
W0629 11:52:50.639000 139012379337344 torch/_dynamo/convert_frame.py:824]   File "/usr/local/lib/python3.10/dist-packages/torch/_inductor/codecache.py", line 2522, in result
W0629 11:52:50.639000 139012379337344 torch/_dynamo/convert_frame.py:824]     self.future.result()
W0629 11:52:50.639000 139012379337344 torch/_dynamo/convert_frame.py:824]   File "/usr/lib/python3.10/concurrent/futures/_base.py", line 458, in result
W0629 11:52:50.639000 139012379337344 torch/_dynamo/convert_frame.py:824]     return self.__get_result()
W0629 11:52:50.639000 139012379337344 torch/_dynamo/convert_frame.py:824]   File "/usr/lib/python3.10/concurrent/futures/_base.py", line 403, in __get_result
W0629 11:52:50.639000 139012379337344 torch/_dynamo/convert_frame.py:824]     raise self._exception
W0629 11:52:50.639000 139012379337344 torch/_dynamo/convert_frame.py:824] torch._dynamo.exc.BackendCompilerFailed: backend='inductor' raised:
W0629 11:52:50.639000 139012379337344 torch/_dynamo/convert_frame.py:824] RuntimeError: Internal Triton PTX codegen error:
W0629 11:52:50.639000 139012379337344 torch/_dynamo/convert_frame.py:824] ptxas /tmp/compile-ptx-src-814b22, line 59; error   : Feature '.bf16' requires .target sm_80 or higher
W0629 11:52:50.639000 139012379337344 torch/_dynamo/convert_frame.py:824] ptxas /tmp/compile-ptx-src-814b22, line 59; error   : Feature 'cvt.bf16.f32' requires .target sm_80 or higher
W0629 11:52:50.639000 139012379337344 torch/_dynamo/convert_frame.py:824] ptxas /tmp/compile-ptx-src-814b22, line 60; error   : Feature '.bf16' requires .target sm_80 or higher
W0629 11:52:50.639000 139012379337344 torch/_dynamo/convert_frame.py:824] ptxas /tmp/compile-ptx-src-814b22, line 60; error   : Feature 'cvt.bf16.f32' requires .target sm_80 or higher
W0629 11:52:50.639000 139012379337344 torch/_dynamo/convert_frame.py:824] ptxas /tmp/compile-ptx-src-814b22, line 61; error   : Feature '.bf16' requires .target sm_80 or higher
W0629 11:52:50.639000 139012379337344 torch/_dynamo/convert_frame.py:824] ptxas /tmp/compile-ptx-src-814b22, line 61; error   : Feature 'cvt.bf16.f32' requires .target sm_80 or higher
W0629 11:52:50.639000 139012379337344 torch/_dynamo/convert_frame.py:824] ptxas /tmp/compile-ptx-src-814b22, line 62; error   : Feature '.bf16' requires .target sm_80 or higher
W0629 11:52:50.639000 139012379337344 torch/_dynamo/convert_frame.py:824] ptxas /tmp/compile-ptx-src-814b22, line 62; error   : Feature 'cvt.bf16.f32' requires .target sm_80 or higher
W0629 11:52:50.639000 139012379337344 torch/_dynamo/convert_frame.py:824] ptxas /tmp/compile-ptx-src-814b22, line 63; error   : Feature '.bf16' requires .target sm_80 or higher
W0629 11:52:50.639000 139012379337344 torch/_dynamo/convert_frame.py:824] ptxas /tmp/compile-ptx-src-814b22, line 63; error   : Feature 'cvt.bf16.f32' requires .target sm_80 or higher
W0629 11:52:50.639000 139012379337344 torch/_dynamo/convert_frame.py:824] ptxas /tmp/compile-ptx-src-814b22, line 64; error   : Feature '.bf16' requires .target sm_80 or higher
W0629 11:52:50.639000 139012379337344 torch/_dynamo/convert_frame.py:824] ptxas /tmp/compile-ptx-src-814b22, line 64; error   : Feature 'cvt.bf16.f32' requires .target sm_80 or higher
W0629 11:52:50.639000 139012379337344 torch/_dynamo/convert_frame.py:824] ptxas /tmp/compile-ptx-src-814b22, line 65; error   : Feature '.bf16' requires .target sm_80 or higher
W0629 11:52:50.639000 139012379337344 torch/_dynamo/convert_frame.py:824] ptxas /tmp/compile-ptx-src-814b22, line 65; error   : Feature 'cvt.bf16.f32' requires .target sm_80 or higher
W0629 11:52:50.639000 139012379337344 torch/_dynamo/convert_frame.py:824] ptxas /tmp/compile-ptx-src-814b22, line 66; error   : Feature '.bf16' requires .target sm_80 or higher
W0629 11:52:50.639000 139012379337344 torch/_dynamo/convert_frame.py:824] ptxas /tmp/compile-ptx-src-814b22, line 66; error   : Feature 'cvt.bf16.f32' requires .target sm_80 or higher
W0629 11:52:50.639000 139012379337344 torch/_dynamo/convert_frame.py:824] ptxas fatal   : Ptx assembly aborted due to errors
W0629 11:52:50.639000 139012379337344 torch/_dynamo/convert_frame.py:824]
W0629 11:52:50.639000 139012379337344 torch/_dynamo/convert_frame.py:824]
W0629 11:52:50.639000 139012379337344 torch/_dynamo/convert_frame.py:824] Set TORCH_LOGS="+dynamo" and TORCHDYNAMO_VERBOSE=1 for more information
W0629 11:52:50.639000 139012379337344 torch/_dynamo/convert_frame.py:824]
W0629 11:52:50.639000 139012379337344 torch/_dynamo/convert_frame.py:824] Traceback (most recent call last):
W0629 11:52:50.639000 139012379337344 torch/_dynamo/convert_frame.py:824]   File "/usr/local/lib/python3.10/dist-packages/torch/_dynamo/convert_frame.py", line 786, in _convert_frame
W0629 11:52:50.639000 139012379337344 torch/_dynamo/convert_frame.py:824]     result = inner_convert(
W0629 11:52:50.639000 139012379337344 torch/_dynamo/convert_frame.py:824]   File "/usr/local/lib/python3.10/dist-packages/torch/_dynamo/convert_frame.py", line 400, in _convert_frame_assert
W0629 11:52:50.639000 139012379337344 torch/_dynamo/convert_frame.py:824]     return _compile(
W0629 11:52:50.639000 139012379337344 torch/_dynamo/convert_frame.py:824]   File "/usr/lib/python3.10/contextlib.py", line 79, in inner
W0629 11:52:50.639000 139012379337344 torch/_dynamo/convert_frame.py:824]     return func(*args, **kwds)
W0629 11:52:50.639000 139012379337344 torch/_dynamo/convert_frame.py:824]   File "/usr/local/lib/python3.10/dist-packages/torch/_dynamo/convert_frame.py", line 676, in _compile
W0629 11:52:50.639000 139012379337344 torch/_dynamo/convert_frame.py:824]     guarded_code = compile_inner(code, one_graph, hooks, transform)
W0629 11:52:50.639000 139012379337344 torch/_dynamo/convert_frame.py:824]   File "/usr/local/lib/python3.10/dist-packages/torch/_dynamo/utils.py", line 262, in time_wrapper
W0629 11:52:50.639000 139012379337344 torch/_dynamo/convert_frame.py:824]     r = func(*args, **kwargs)
W0629 11:52:50.639000 139012379337344 torch/_dynamo/convert_frame.py:824]   File "/usr/local/lib/python3.10/dist-packages/torch/_dynamo/convert_frame.py", line 535, in compile_inner
W0629 11:52:50.639000 139012379337344 torch/_dynamo/convert_frame.py:824]     out_code = transform_code_object(code, transform)
W0629 11:52:50.639000 139012379337344 torch/_dynamo/convert_frame.py:824]   File "/usr/local/lib/python3.10/dist-packages/torch/_dynamo/bytecode_transformation.py", line 1036, in transform_code_object
W0629 11:52:50.639000 139012379337344 torch/_dynamo/convert_frame.py:824]     transformations(instructions, code_options)
W0629 11:52:50.639000 139012379337344 torch/_dynamo/convert_frame.py:824]   File "/usr/local/lib/python3.10/dist-packages/torch/_dynamo/convert_frame.py", line 165, in _fn
W0629 11:52:50.639000 139012379337344 torch/_dynamo/convert_frame.py:824]     return fn(*args, **kwargs)
W0629 11:52:50.639000 139012379337344 torch/_dynamo/convert_frame.py:824]   File "/usr/local/lib/python3.10/dist-packages/torch/_dynamo/convert_frame.py", line 500, in transform
W0629 11:52:50.639000 139012379337344 torch/_dynamo/convert_frame.py:824]     tracer.run()
W0629 11:52:50.639000 139012379337344 torch/_dynamo/convert_frame.py:824]   File "/usr/local/lib/python3.10/dist-packages/torch/_dynamo/symbolic_convert.py", line 2149, in run
W0629 11:52:50.639000 139012379337344 torch/_dynamo/convert_frame.py:824]     super().run()
W0629 11:52:50.639000 139012379337344 torch/_dynamo/convert_frame.py:824]   File "/usr/local/lib/python3.10/dist-packages/torch/_dynamo/symbolic_convert.py", line 810, in run
W0629 11:52:50.639000 139012379337344 torch/_dynamo/convert_frame.py:824]     and self.step()
W0629 11:52:50.639000 139012379337344 torch/_dynamo/convert_frame.py:824]   File "/usr/local/lib/python3.10/dist-packages/torch/_dynamo/symbolic_convert.py", line 773, in step
W0629 11:52:50.639000 139012379337344 torch/_dynamo/convert_frame.py:824]     getattr(self, inst.opname)(inst)
W0629 11:52:50.639000 139012379337344 torch/_dynamo/convert_frame.py:824]   File "/usr/local/lib/python3.10/dist-packages/torch/_dynamo/symbolic_convert.py", line 2268, in RETURN_VALUE
W0629 11:52:50.639000 139012379337344 torch/_dynamo/convert_frame.py:824]     self.output.compile_subgraph(
W0629 11:52:50.639000 139012379337344 torch/_dynamo/convert_frame.py:824]   File "/usr/local/lib/python3.10/dist-packages/torch/_dynamo/output_graph.py", line 971, in compile_subgraph
W0629 11:52:50.639000 139012379337344 torch/_dynamo/convert_frame.py:824]     self.compile_and_call_fx_graph(tx, list(reversed(stack_values)), root)
W0629 11:52:50.639000 139012379337344 torch/_dynamo/convert_frame.py:824]   File "/usr/lib/python3.10/contextlib.py", line 79, in inner
W0629 11:52:50.639000 139012379337344 torch/_dynamo/convert_frame.py:824]     return func(*args, **kwds)
W0629 11:52:50.639000 139012379337344 torch/_dynamo/convert_frame.py:824]   File "/usr/local/lib/python3.10/dist-packages/torch/_dynamo/output_graph.py", line 1168, in compile_and_call_fx_graph
W0629 11:52:50.639000 139012379337344 torch/_dynamo/convert_frame.py:824]     compiled_fn = self.call_user_compiler(gm)
W0629 11:52:50.639000 139012379337344 torch/_dynamo/convert_frame.py:824]   File "/usr/local/lib/python3.10/dist-packages/torch/_dynamo/utils.py", line 262, in time_wrapper
W0629 11:52:50.639000 139012379337344 torch/_dynamo/convert_frame.py:824]     r = func(*args, **kwargs)
W0629 11:52:50.639000 139012379337344 torch/_dynamo/convert_frame.py:824]   File "/usr/local/lib/python3.10/dist-packages/torch/_dynamo/output_graph.py", line 1241, in call_user_compiler
W0629 11:52:50.639000 139012379337344 torch/_dynamo/convert_frame.py:824]     raise BackendCompilerFailed(self.compiler_fn, e).with_traceback(
W0629 11:52:50.639000 139012379337344 torch/_dynamo/convert_frame.py:824]   File "/usr/local/lib/python3.10/dist-packages/torch/_dynamo/output_graph.py", line 1222, in call_user_compiler
W0629 11:52:50.639000 139012379337344 torch/_dynamo/convert_frame.py:824]     compiled_fn = compiler_fn(gm, self.example_inputs())
W0629 11:52:50.639000 139012379337344 torch/_dynamo/convert_frame.py:824]   File "/usr/local/lib/python3.10/dist-packages/torch/_dynamo/repro/after_dynamo.py", line 117, in debug_wrapper
W0629 11:52:50.639000 139012379337344 torch/_dynamo/convert_frame.py:824]     compiled_gm = compiler_fn(gm, example_inputs)
W0629 11:52:50.639000 139012379337344 torch/_dynamo/convert_frame.py:824]   File "/usr/local/lib/python3.10/dist-packages/torch/__init__.py", line 1729, in __call__
W0629 11:52:50.639000 139012379337344 torch/_dynamo/convert_frame.py:824]     return compile_fx(model_, inputs_, config_patches=self.config)
W0629 11:52:50.639000 139012379337344 torch/_dynamo/convert_frame.py:824]   File "/usr/lib/python3.10/contextlib.py", line 79, in inner
W0629 11:52:50.639000 139012379337344 torch/_dynamo/convert_frame.py:824]     return func(*args, **kwds)
W0629 11:52:50.639000 139012379337344 torch/_dynamo/convert_frame.py:824]   File "/usr/local/lib/python3.10/dist-packages/torch/_inductor/compile_fx.py", line 1330, in compile_fx
W0629 11:52:50.639000 139012379337344 torch/_dynamo/convert_frame.py:824]     return aot_autograd(
W0629 11:52:50.639000 139012379337344 torch/_dynamo/convert_frame.py:824]   File "/usr/local/lib/python3.10/dist-packages/torch/_dynamo/backends/common.py", line 58, in compiler_fn
W0629 11:52:50.639000 139012379337344 torch/_dynamo/convert_frame.py:824]     cg = aot_module_simplified(gm, example_inputs, **kwargs)
W0629 11:52:50.639000 139012379337344 torch/_dynamo/convert_frame.py:824]   File "/usr/local/lib/python3.10/dist-packages/torch/_functorch/aot_autograd.py", line 903, in aot_module_simplified
W0629 11:52:50.639000 139012379337344 torch/_dynamo/convert_frame.py:824]     compiled_fn = create_aot_dispatcher_function(
W0629 11:52:50.639000 139012379337344 torch/_dynamo/convert_frame.py:824]   File "/usr/local/lib/python3.10/dist-packages/torch/_dynamo/utils.py", line 262, in time_wrapper
W0629 11:52:50.639000 139012379337344 torch/_dynamo/convert_frame.py:824]     r = func(*args, **kwargs)
W0629 11:52:50.639000 139012379337344 torch/_dynamo/convert_frame.py:824]   File "/usr/local/lib/python3.10/dist-packages/torch/_functorch/aot_autograd.py", line 628, in create_aot_dispatcher_function
W0629 11:52:50.639000 139012379337344 torch/_dynamo/convert_frame.py:824]     compiled_fn = compiler_fn(flat_fn, fake_flat_args, aot_config, fw_metadata=fw_metadata)
W0629 11:52:50.639000 139012379337344 torch/_dynamo/convert_frame.py:824]   File "/usr/local/lib/python3.10/dist-packages/torch/_functorch/_aot_autograd/runtime_wrappers.py", line 443, in aot_wrapper_dedupe
W0629 11:52:50.639000 139012379337344 torch/_dynamo/convert_frame.py:824]     return compiler_fn(flat_fn, leaf_flat_args, aot_config, fw_metadata=fw_metadata)
W0629 11:52:50.639000 139012379337344 torch/_dynamo/convert_frame.py:824]   File "/usr/local/lib/python3.10/dist-packages/torch/_functorch/_aot_autograd/runtime_wrappers.py", line 648, in aot_wrapper_synthetic_base
W0629 11:52:50.639000 139012379337344 torch/_dynamo/convert_frame.py:824]     return compiler_fn(flat_fn, flat_args, aot_config, fw_metadata=fw_metadata)
W0629 11:52:50.639000 139012379337344 torch/_dynamo/convert_frame.py:824]   File "/usr/local/lib/python3.10/dist-packages/torch/_functorch/_aot_autograd/jit_compile_runtime_wrappers.py", line 119, in aot_dispatch_base
W0629 11:52:50.639000 139012379337344 torch/_dynamo/convert_frame.py:824]     compiled_fw = compiler(fw_module, updated_flat_args)
W0629 11:52:50.639000 139012379337344 torch/_dynamo/convert_frame.py:824]   File "/usr/local/lib/python3.10/dist-packages/torch/_dynamo/utils.py", line 262, in time_wrapper
W0629 11:52:50.639000 139012379337344 torch/_dynamo/convert_frame.py:824]     r = func(*args, **kwargs)
W0629 11:52:50.639000 139012379337344 torch/_dynamo/convert_frame.py:824]   File "/usr/local/lib/python3.10/dist-packages/torch/_inductor/compile_fx.py", line 1257, in fw_compiler_base
W0629 11:52:50.639000 139012379337344 torch/_dynamo/convert_frame.py:824]     return inner_compile(
W0629 11:52:50.639000 139012379337344 torch/_dynamo/convert_frame.py:824]   File "/usr/local/lib/python3.10/dist-packages/torch/_dynamo/repro/after_aot.py", line 83, in debug_wrapper
W0629 11:52:50.639000 139012379337344 torch/_dynamo/convert_frame.py:824]     inner_compiled_fn = compiler_fn(gm, example_inputs)
W0629 11:52:50.639000 139012379337344 torch/_dynamo/convert_frame.py:824]   File "/usr/local/lib/python3.10/dist-packages/torch/_inductor/debug.py", line 304, in inner
W0629 11:52:50.639000 139012379337344 torch/_dynamo/convert_frame.py:824]     return fn(*args, **kwargs)
W0629 11:52:50.639000 139012379337344 torch/_dynamo/convert_frame.py:824]   File "/usr/lib/python3.10/contextlib.py", line 79, in inner
W0629 11:52:50.639000 139012379337344 torch/_dynamo/convert_frame.py:824]     return func(*args, **kwds)
W0629 11:52:50.639000 139012379337344 torch/_dynamo/convert_frame.py:824]   File "/usr/lib/python3.10/contextlib.py", line 79, in inner
W0629 11:52:50.639000 139012379337344 torch/_dynamo/convert_frame.py:824]     return func(*args, **kwds)
W0629 11:52:50.639000 139012379337344 torch/_dynamo/convert_frame.py:824]   File "/usr/local/lib/python3.10/dist-packages/torch/_dynamo/utils.py", line 262, in time_wrapper
W0629 11:52:50.639000 139012379337344 torch/_dynamo/convert_frame.py:824]     r = func(*args, **kwargs)
W0629 11:52:50.639000 139012379337344 torch/_dynamo/convert_frame.py:824]   File "/usr/local/lib/python3.10/dist-packages/torch/_inductor/compile_fx.py", line 438, in compile_fx_inner
W0629 11:52:50.639000 139012379337344 torch/_dynamo/convert_frame.py:824]     compiled_graph = fx_codegen_and_compile(
W0629 11:52:50.639000 139012379337344 torch/_dynamo/convert_frame.py:824]   File "/usr/local/lib/python3.10/dist-packages/torch/_inductor/compile_fx.py", line 714, in fx_codegen_and_compile
W0629 11:52:50.639000 139012379337344 torch/_dynamo/convert_frame.py:824]     compiled_fn = graph.compile_to_fn()
W0629 11:52:50.639000 139012379337344 torch/_dynamo/convert_frame.py:824]   File "/usr/local/lib/python3.10/dist-packages/torch/_inductor/graph.py", line 1307, in compile_to_fn
W0629 11:52:50.639000 139012379337344 torch/_dynamo/convert_frame.py:824]     return self.compile_to_module().call
W0629 11:52:50.639000 139012379337344 torch/_dynamo/convert_frame.py:824]   File "/usr/local/lib/python3.10/dist-packages/torch/_dynamo/utils.py", line 262, in time_wrapper
W0629 11:52:50.639000 139012379337344 torch/_dynamo/convert_frame.py:824]     r = func(*args, **kwargs)
W0629 11:52:50.639000 139012379337344 torch/_dynamo/convert_frame.py:824]   File "/usr/local/lib/python3.10/dist-packages/torch/_inductor/graph.py", line 1254, in compile_to_module
W0629 11:52:50.639000 139012379337344 torch/_dynamo/convert_frame.py:824]     mod = PyCodeCache.load_by_key_path(
W0629 11:52:50.639000 139012379337344 torch/_dynamo/convert_frame.py:824]   File "/usr/local/lib/python3.10/dist-packages/torch/_inductor/codecache.py", line 2160, in load_by_key_path
W0629 11:52:50.639000 139012379337344 torch/_dynamo/convert_frame.py:824]     exec(code, mod.__dict__, mod.__dict__)
W0629 11:52:50.639000 139012379337344 torch/_dynamo/convert_frame.py:824]   File "/tmp/torchinductor_root/3c/c3c6zsdgckmnmg37zo6kwa6twnu6nrfqfprpb24pozwh3nm6caxf.py", line 219, in <module>
W0629 11:52:50.639000 139012379337344 torch/_dynamo/convert_frame.py:824]     async_compile.wait(globals())
W0629 11:52:50.639000 139012379337344 torch/_dynamo/convert_frame.py:824]   File "/usr/local/lib/python3.10/dist-packages/torch/_inductor/codecache.py", line 2715, in wait
W0629 11:52:50.639000 139012379337344 torch/_dynamo/convert_frame.py:824]     scope[key] = result.result()
W0629 11:52:50.639000 139012379337344 torch/_dynamo/convert_frame.py:824]   File "/usr/local/lib/python3.10/dist-packages/torch/_inductor/codecache.py", line 2522, in result
W0629 11:52:50.639000 139012379337344 torch/_dynamo/convert_frame.py:824]     self.future.result()
W0629 11:52:50.639000 139012379337344 torch/_dynamo/convert_frame.py:824]   File "/usr/lib/python3.10/concurrent/futures/_base.py", line 458, in result
W0629 11:52:50.639000 139012379337344 torch/_dynamo/convert_frame.py:824]     return self.__get_result()
W0629 11:52:50.639000 139012379337344 torch/_dynamo/convert_frame.py:824]   File "/usr/lib/python3.10/concurrent/futures/_base.py", line 403, in __get_result
W0629 11:52:50.639000 139012379337344 torch/_dynamo/convert_frame.py:824]     raise self._exception
W0629 11:52:50.639000 139012379337344 torch/_dynamo/convert_frame.py:824] torch._dynamo.exc.BackendCompilerFailed: backend='inductor' raised:
W0629 11:52:50.639000 139012379337344 torch/_dynamo/convert_frame.py:824] RuntimeError: Internal Triton PTX codegen error:
W0629 11:52:50.639000 139012379337344 torch/_dynamo/convert_frame.py:824] ptxas /tmp/compile-ptx-src-814b22, line 59; error   : Feature '.bf16' requires .target sm_80 or higher
W0629 11:52:50.639000 139012379337344 torch/_dynamo/convert_frame.py:824] ptxas /tmp/compile-ptx-src-814b22, line 59; error   : Feature 'cvt.bf16.f32' requires .target sm_80 or higher
W0629 11:52:50.639000 139012379337344 torch/_dynamo/convert_frame.py:824] ptxas /tmp/compile-ptx-src-814b22, line 60; error   : Feature '.bf16' requires .target sm_80 or higher
W0629 11:52:50.639000 139012379337344 torch/_dynamo/convert_frame.py:824] ptxas /tmp/compile-ptx-src-814b22, line 60; error   : Feature 'cvt.bf16.f32' requires .target sm_80 or higher
W0629 11:52:50.639000 139012379337344 torch/_dynamo/convert_frame.py:824] ptxas /tmp/compile-ptx-src-814b22, line 61; error   : Feature '.bf16' requires .target sm_80 or higher
W0629 11:52:50.639000 139012379337344 torch/_dynamo/convert_frame.py:824] ptxas /tmp/compile-ptx-src-814b22, line 61; error   : Feature 'cvt.bf16.f32' requires .target sm_80 or higher
W0629 11:52:50.639000 139012379337344 torch/_dynamo/convert_frame.py:824] ptxas /tmp/compile-ptx-src-814b22, line 62; error   : Feature '.bf16' requires .target sm_80 or higher
W0629 11:52:50.639000 139012379337344 torch/_dynamo/convert_frame.py:824] ptxas /tmp/compile-ptx-src-814b22, line 62; error   : Feature 'cvt.bf16.f32' requires .target sm_80 or higher
W0629 11:52:50.639000 139012379337344 torch/_dynamo/convert_frame.py:824] ptxas /tmp/compile-ptx-src-814b22, line 63; error   : Feature '.bf16' requires .target sm_80 or higher
W0629 11:52:50.639000 139012379337344 torch/_dynamo/convert_frame.py:824] ptxas /tmp/compile-ptx-src-814b22, line 63; error   : Feature 'cvt.bf16.f32' requires .target sm_80 or higher
W0629 11:52:50.639000 139012379337344 torch/_dynamo/convert_frame.py:824] ptxas /tmp/compile-ptx-src-814b22, line 64; error   : Feature '.bf16' requires .target sm_80 or higher
W0629 11:52:50.639000 139012379337344 torch/_dynamo/convert_frame.py:824] ptxas /tmp/compile-ptx-src-814b22, line 64; error   : Feature 'cvt.bf16.f32' requires .target sm_80 or higher
W0629 11:52:50.639000 139012379337344 torch/_dynamo/convert_frame.py:824] ptxas /tmp/compile-ptx-src-814b22, line 65; error   : Feature '.bf16' requires .target sm_80 or higher
W0629 11:52:50.639000 139012379337344 torch/_dynamo/convert_frame.py:824] ptxas /tmp/compile-ptx-src-814b22, line 65; error   : Feature 'cvt.bf16.f32' requires .target sm_80 or higher
W0629 11:52:50.639000 139012379337344 torch/_dynamo/convert_frame.py:824] ptxas /tmp/compile-ptx-src-814b22, line 66; error   : Feature '.bf16' requires .target sm_80 or higher
W0629 11:52:50.639000 139012379337344 torch/_dynamo/convert_frame.py:824] ptxas /tmp/compile-ptx-src-814b22, line 66; error   : Feature 'cvt.bf16.f32' requires .target sm_80 or higher
W0629 11:52:50.639000 139012379337344 torch/_dynamo/convert_frame.py:824] ptxas fatal   : Ptx assembly aborted due to errors
W0629 11:52:50.639000 139012379337344 torch/_dynamo/convert_frame.py:824]
W0629 11:52:50.639000 139012379337344 torch/_dynamo/convert_frame.py:824]
W0629 11:52:50.639000 139012379337344 torch/_dynamo/convert_frame.py:824] Set TORCH_LOGS="+dynamo" and TORCHDYNAMO_VERBOSE=1 for more information
W0629 11:52:50.639000 139012379337344 torch/_dynamo/convert_frame.py:824]
W0629 11:52:50.837000 139012379337344 torch/_dynamo/convert_frame.py:824] WON'T CONVERT forward /usr/local/lib/python3.10/dist-packages/transformers/models/bert/modeling_bert.py line 533
W0629 11:52:50.837000 139012379337344 torch/_dynamo/convert_frame.py:824] due to:
W0629 11:52:50.837000 139012379337344 torch/_dynamo/convert_frame.py:824] Traceback (most recent call last):
W0629 11:52:50.837000 139012379337344 torch/_dynamo/convert_frame.py:824]   File "/usr/local/lib/python3.10/dist-packages/torch/_dynamo/convert_frame.py", line 786, in _convert_frame
W0629 11:52:50.837000 139012379337344 torch/_dynamo/convert_frame.py:824]     result = inner_convert(
W0629 11:52:50.837000 139012379337344 torch/_dynamo/convert_frame.py:824]   File "/usr/local/lib/python3.10/dist-packages/torch/_dynamo/convert_frame.py", line 400, in _convert_frame_assert
W0629 11:52:50.837000 139012379337344 torch/_dynamo/convert_frame.py:824]     return _compile(
W0629 11:52:50.837000 139012379337344 torch/_dynamo/convert_frame.py:824]   File "/usr/lib/python3.10/contextlib.py", line 79, in inner
W0629 11:52:50.837000 139012379337344 torch/_dynamo/convert_frame.py:824]     return func(*args, **kwds)
W0629 11:52:50.837000 139012379337344 torch/_dynamo/convert_frame.py:824]   File "/usr/local/lib/python3.10/dist-packages/torch/_dynamo/convert_frame.py", line 676, in _compile
W0629 11:52:50.837000 139012379337344 torch/_dynamo/convert_frame.py:824]     guarded_code = compile_inner(code, one_graph, hooks, transform)
W0629 11:52:50.837000 139012379337344 torch/_dynamo/convert_frame.py:824]   File "/usr/local/lib/python3.10/dist-packages/torch/_dynamo/utils.py", line 262, in time_wrapper
W0629 11:52:50.837000 139012379337344 torch/_dynamo/convert_frame.py:824]     r = func(*args, **kwargs)
W0629 11:52:50.837000 139012379337344 torch/_dynamo/convert_frame.py:824]   File "/usr/local/lib/python3.10/dist-packages/torch/_dynamo/convert_frame.py", line 535, in compile_inner
W0629 11:52:50.837000 139012379337344 torch/_dynamo/convert_frame.py:824]     out_code = transform_code_object(code, transform)
W0629 11:52:50.837000 139012379337344 torch/_dynamo/convert_frame.py:824]   File "/usr/local/lib/python3.10/dist-packages/torch/_dynamo/bytecode_transformation.py", line 1036, in transform_code_object
W0629 11:52:50.837000 139012379337344 torch/_dynamo/convert_frame.py:824]     transformations(instructions, code_options)
W0629 11:52:50.837000 139012379337344 torch/_dynamo/convert_frame.py:824]   File "/usr/local/lib/python3.10/dist-packages/torch/_dynamo/convert_frame.py", line 165, in _fn
W0629 11:52:50.837000 139012379337344 torch/_dynamo/convert_frame.py:824]     return fn(*args, **kwargs)
W0629 11:52:50.837000 139012379337344 torch/_dynamo/convert_frame.py:824]   File "/usr/local/lib/python3.10/dist-packages/torch/_dynamo/convert_frame.py", line 500, in transform
W0629 11:52:50.837000 139012379337344 torch/_dynamo/convert_frame.py:824]     tracer.run()
W0629 11:52:50.837000 139012379337344 torch/_dynamo/convert_frame.py:824]   File "/usr/local/lib/python3.10/dist-packages/torch/_dynamo/symbolic_convert.py", line 2149, in run
W0629 11:52:50.837000 139012379337344 torch/_dynamo/convert_frame.py:824]     super().run()
W0629 11:52:50.837000 139012379337344 torch/_dynamo/convert_frame.py:824]   File "/usr/local/lib/python3.10/dist-packages/torch/_dynamo/symbolic_convert.py", line 810, in run
W0629 11:52:50.837000 139012379337344 torch/_dynamo/convert_frame.py:824]     and self.step()
W0629 11:52:50.837000 139012379337344 torch/_dynamo/convert_frame.py:824]   File "/usr/local/lib/python3.10/dist-packages/torch/_dynamo/symbolic_convert.py", line 773, in step
W0629 11:52:50.837000 139012379337344 torch/_dynamo/convert_frame.py:824]     getattr(self, inst.opname)(inst)
W0629 11:52:50.837000 139012379337344 torch/_dynamo/convert_frame.py:824]   File "/usr/local/lib/python3.10/dist-packages/torch/_dynamo/symbolic_convert.py", line 2268, in RETURN_VALUE
W0629 11:52:50.837000 139012379337344 torch/_dynamo/convert_frame.py:824]     self.output.compile_subgraph(
W0629 11:52:50.837000 139012379337344 torch/_dynamo/convert_frame.py:824]   File "/usr/local/lib/python3.10/dist-packages/torch/_dynamo/output_graph.py", line 971, in compile_subgraph
W0629 11:52:50.837000 139012379337344 torch/_dynamo/convert_frame.py:824]     self.compile_and_call_fx_graph(tx, list(reversed(stack_values)), root)
W0629 11:52:50.837000 139012379337344 torch/_dynamo/convert_frame.py:824]   File "/usr/lib/python3.10/contextlib.py", line 79, in inner
W0629 11:52:50.837000 139012379337344 torch/_dynamo/convert_frame.py:824]     return func(*args, **kwds)
W0629 11:52:50.837000 139012379337344 torch/_dynamo/convert_frame.py:824]   File "/usr/local/lib/python3.10/dist-packages/torch/_dynamo/output_graph.py", line 1168, in compile_and_call_fx_graph
W0629 11:52:50.837000 139012379337344 torch/_dynamo/convert_frame.py:824]     compiled_fn = self.call_user_compiler(gm)
W0629 11:52:50.837000 139012379337344 torch/_dynamo/convert_frame.py:824]   File "/usr/local/lib/python3.10/dist-packages/torch/_dynamo/utils.py", line 262, in time_wrapper
W0629 11:52:50.837000 139012379337344 torch/_dynamo/convert_frame.py:824]     r = func(*args, **kwargs)
W0629 11:52:50.837000 139012379337344 torch/_dynamo/convert_frame.py:824]   File "/usr/local/lib/python3.10/dist-packages/torch/_dynamo/output_graph.py", line 1241, in call_user_compiler
W0629 11:52:50.837000 139012379337344 torch/_dynamo/convert_frame.py:824]     raise BackendCompilerFailed(self.compiler_fn, e).with_traceback(
W0629 11:52:50.837000 139012379337344 torch/_dynamo/convert_frame.py:824]   File "/usr/local/lib/python3.10/dist-packages/torch/_dynamo/output_graph.py", line 1222, in call_user_compiler
W0629 11:52:50.837000 139012379337344 torch/_dynamo/convert_frame.py:824]     compiled_fn = compiler_fn(gm, self.example_inputs())
W0629 11:52:50.837000 139012379337344 torch/_dynamo/convert_frame.py:824]   File "/usr/local/lib/python3.10/dist-packages/torch/_dynamo/repro/after_dynamo.py", line 117, in debug_wrapper
W0629 11:52:50.837000 139012379337344 torch/_dynamo/convert_frame.py:824]     compiled_gm = compiler_fn(gm, example_inputs)
W0629 11:52:50.837000 139012379337344 torch/_dynamo/convert_frame.py:824]   File "/usr/local/lib/python3.10/dist-packages/torch/__init__.py", line 1729, in __call__
W0629 11:52:50.837000 139012379337344 torch/_dynamo/convert_frame.py:824]     return compile_fx(model_, inputs_, config_patches=self.config)
W0629 11:52:50.837000 139012379337344 torch/_dynamo/convert_frame.py:824]   File "/usr/lib/python3.10/contextlib.py", line 79, in inner
W0629 11:52:50.837000 139012379337344 torch/_dynamo/convert_frame.py:824]     return func(*args, **kwds)
W0629 11:52:50.837000 139012379337344 torch/_dynamo/convert_frame.py:824]   File "/usr/local/lib/python3.10/dist-packages/torch/_inductor/compile_fx.py", line 1330, in compile_fx
W0629 11:52:50.837000 139012379337344 torch/_dynamo/convert_frame.py:824]     return aot_autograd(
W0629 11:52:50.837000 139012379337344 torch/_dynamo/convert_frame.py:824]   File "/usr/local/lib/python3.10/dist-packages/torch/_dynamo/backends/common.py", line 58, in compiler_fn
W0629 11:52:50.837000 139012379337344 torch/_dynamo/convert_frame.py:824]     cg = aot_module_simplified(gm, example_inputs, **kwargs)
W0629 11:52:50.837000 139012379337344 torch/_dynamo/convert_frame.py:824]   File "/usr/local/lib/python3.10/dist-packages/torch/_functorch/aot_autograd.py", line 903, in aot_module_simplified
W0629 11:52:50.837000 139012379337344 torch/_dynamo/convert_frame.py:824]     compiled_fn = create_aot_dispatcher_function(
W0629 11:52:50.837000 139012379337344 torch/_dynamo/convert_frame.py:824]   File "/usr/local/lib/python3.10/dist-packages/torch/_dynamo/utils.py", line 262, in time_wrapper
W0629 11:52:50.837000 139012379337344 torch/_dynamo/convert_frame.py:824]     r = func(*args, **kwargs)
W0629 11:52:50.837000 139012379337344 torch/_dynamo/convert_frame.py:824]   File "/usr/local/lib/python3.10/dist-packages/torch/_functorch/aot_autograd.py", line 628, in create_aot_dispatcher_function
W0629 11:52:50.837000 139012379337344 torch/_dynamo/convert_frame.py:824]     compiled_fn = compiler_fn(flat_fn, fake_flat_args, aot_config, fw_metadata=fw_metadata)
W0629 11:52:50.837000 139012379337344 torch/_dynamo/convert_frame.py:824]   File "/usr/local/lib/python3.10/dist-packages/torch/_functorch/_aot_autograd/runtime_wrappers.py", line 443, in aot_wrapper_dedupe
W0629 11:52:50.837000 139012379337344 torch/_dynamo/convert_frame.py:824]     return compiler_fn(flat_fn, leaf_flat_args, aot_config, fw_metadata=fw_metadata)
W0629 11:52:50.837000 139012379337344 torch/_dynamo/convert_frame.py:824]   File "/usr/local/lib/python3.10/dist-packages/torch/_functorch/_aot_autograd/runtime_wrappers.py", line 648, in aot_wrapper_synthetic_base
W0629 11:52:50.837000 139012379337344 torch/_dynamo/convert_frame.py:824]     return compiler_fn(flat_fn, flat_args, aot_config, fw_metadata=fw_metadata)
W0629 11:52:50.837000 139012379337344 torch/_dynamo/convert_frame.py:824]   File "/usr/local/lib/python3.10/dist-packages/torch/_functorch/_aot_autograd/jit_compile_runtime_wrappers.py", line 119, in aot_dispatch_base
W0629 11:52:50.837000 139012379337344 torch/_dynamo/convert_frame.py:824]     compiled_fw = compiler(fw_module, updated_flat_args)
W0629 11:52:50.837000 139012379337344 torch/_dynamo/convert_frame.py:824]   File "/usr/local/lib/python3.10/dist-packages/torch/_dynamo/utils.py", line 262, in time_wrapper
W0629 11:52:50.837000 139012379337344 torch/_dynamo/convert_frame.py:824]     r = func(*args, **kwargs)
W0629 11:52:50.837000 139012379337344 torch/_dynamo/convert_frame.py:824]   File "/usr/local/lib/python3.10/dist-packages/torch/_inductor/compile_fx.py", line 1257, in fw_compiler_base
W0629 11:52:50.837000 139012379337344 torch/_dynamo/convert_frame.py:824]     return inner_compile(
W0629 11:52:50.837000 139012379337344 torch/_dynamo/convert_frame.py:824]   File "/usr/local/lib/python3.10/dist-packages/torch/_dynamo/repro/after_aot.py", line 83, in debug_wrapper
W0629 11:52:50.837000 139012379337344 torch/_dynamo/convert_frame.py:824]     inner_compiled_fn = compiler_fn(gm, example_inputs)
W0629 11:52:50.837000 139012379337344 torch/_dynamo/convert_frame.py:824]   File "/usr/local/lib/python3.10/dist-packages/torch/_inductor/debug.py", line 304, in inner
W0629 11:52:50.837000 139012379337344 torch/_dynamo/convert_frame.py:824]     return fn(*args, **kwargs)
W0629 11:52:50.837000 139012379337344 torch/_dynamo/convert_frame.py:824]   File "/usr/lib/python3.10/contextlib.py", line 79, in inner
W0629 11:52:50.837000 139012379337344 torch/_dynamo/convert_frame.py:824]     return func(*args, **kwds)
W0629 11:52:50.837000 139012379337344 torch/_dynamo/convert_frame.py:824]   File "/usr/lib/python3.10/contextlib.py", line 79, in inner
W0629 11:52:50.837000 139012379337344 torch/_dynamo/convert_frame.py:824]     return func(*args, **kwds)
W0629 11:52:50.837000 139012379337344 torch/_dynamo/convert_frame.py:824]   File "/usr/local/lib/python3.10/dist-packages/torch/_dynamo/utils.py", line 262, in time_wrapper
W0629 11:52:50.837000 139012379337344 torch/_dynamo/convert_frame.py:824]     r = func(*args, **kwargs)
W0629 11:52:50.837000 139012379337344 torch/_dynamo/convert_frame.py:824]   File "/usr/local/lib/python3.10/dist-packages/torch/_inductor/compile_fx.py", line 438, in compile_fx_inner
W0629 11:52:50.837000 139012379337344 torch/_dynamo/convert_frame.py:824]     compiled_graph = fx_codegen_and_compile(
W0629 11:52:50.837000 139012379337344 torch/_dynamo/convert_frame.py:824]   File "/usr/local/lib/python3.10/dist-packages/torch/_inductor/compile_fx.py", line 714, in fx_codegen_and_compile
W0629 11:52:50.837000 139012379337344 torch/_dynamo/convert_frame.py:824]     compiled_fn = graph.compile_to_fn()
W0629 11:52:50.837000 139012379337344 torch/_dynamo/convert_frame.py:824]   File "/usr/local/lib/python3.10/dist-packages/torch/_inductor/graph.py", line 1307, in compile_to_fn
W0629 11:52:50.837000 139012379337344 torch/_dynamo/convert_frame.py:824]     return self.compile_to_module().call
W0629 11:52:50.837000 139012379337344 torch/_dynamo/convert_frame.py:824]   File "/usr/local/lib/python3.10/dist-packages/torch/_dynamo/utils.py", line 262, in time_wrapper
W0629 11:52:50.837000 139012379337344 torch/_dynamo/convert_frame.py:824]     r = func(*args, **kwargs)
W0629 11:52:50.837000 139012379337344 torch/_dynamo/convert_frame.py:824]   File "/usr/local/lib/python3.10/dist-packages/torch/_inductor/graph.py", line 1254, in compile_to_module
W0629 11:52:50.837000 139012379337344 torch/_dynamo/convert_frame.py:824]     mod = PyCodeCache.load_by_key_path(
W0629 11:52:50.837000 139012379337344 torch/_dynamo/convert_frame.py:824]   File "/usr/local/lib/python3.10/dist-packages/torch/_inductor/codecache.py", line 2160, in load_by_key_path
W0629 11:52:50.837000 139012379337344 torch/_dynamo/convert_frame.py:824]     exec(code, mod.__dict__, mod.__dict__)
W0629 11:52:50.837000 139012379337344 torch/_dynamo/convert_frame.py:824]   File "/tmp/torchinductor_root/uv/cuvr35tdh5iar7pawk4uy7oo77roav6rsrfsxbkfuxepd242zjn5.py", line 148, in <module>
W0629 11:52:50.837000 139012379337344 torch/_dynamo/convert_frame.py:824]     async_compile.wait(globals())
W0629 11:52:50.837000 139012379337344 torch/_dynamo/convert_frame.py:824]   File "/usr/local/lib/python3.10/dist-packages/torch/_inductor/codecache.py", line 2715, in wait
W0629 11:52:50.837000 139012379337344 torch/_dynamo/convert_frame.py:824]     scope[key] = result.result()
W0629 11:52:50.837000 139012379337344 torch/_dynamo/convert_frame.py:824]   File "/usr/local/lib/python3.10/dist-packages/torch/_inductor/codecache.py", line 2522, in result
W0629 11:52:50.837000 139012379337344 torch/_dynamo/convert_frame.py:824]     self.future.result()
W0629 11:52:50.837000 139012379337344 torch/_dynamo/convert_frame.py:824]   File "/usr/lib/python3.10/concurrent/futures/_base.py", line 458, in result
W0629 11:52:50.837000 139012379337344 torch/_dynamo/convert_frame.py:824]     return self.__get_result()
W0629 11:52:50.837000 139012379337344 torch/_dynamo/convert_frame.py:824]   File "/usr/lib/python3.10/concurrent/futures/_base.py", line 403, in __get_result
W0629 11:52:50.837000 139012379337344 torch/_dynamo/convert_frame.py:824]     raise self._exception
W0629 11:52:50.837000 139012379337344 torch/_dynamo/convert_frame.py:824] torch._dynamo.exc.BackendCompilerFailed: backend='inductor' raised:
W0629 11:52:50.837000 139012379337344 torch/_dynamo/convert_frame.py:824] RuntimeError: Internal Triton PTX codegen error:
W0629 11:52:50.837000 139012379337344 torch/_dynamo/convert_frame.py:824] ptxas /tmp/compile-ptx-src-52c445, line 59; error   : Feature '.bf16' requires .target sm_80 or higher
W0629 11:52:50.837000 139012379337344 torch/_dynamo/convert_frame.py:824] ptxas /tmp/compile-ptx-src-52c445, line 59; error   : Feature 'cvt.bf16.f32' requires .target sm_80 or higher
W0629 11:52:50.837000 139012379337344 torch/_dynamo/convert_frame.py:824] ptxas /tmp/compile-ptx-src-52c445, line 60; error   : Feature '.bf16' requires .target sm_80 or higher
W0629 11:52:50.837000 139012379337344 torch/_dynamo/convert_frame.py:824] ptxas /tmp/compile-ptx-src-52c445, line 60; error   : Feature 'cvt.bf16.f32' requires .target sm_80 or higher
W0629 11:52:50.837000 139012379337344 torch/_dynamo/convert_frame.py:824] ptxas /tmp/compile-ptx-src-52c445, line 61; error   : Feature '.bf16' requires .target sm_80 or higher
W0629 11:52:50.837000 139012379337344 torch/_dynamo/convert_frame.py:824] ptxas /tmp/compile-ptx-src-52c445, line 61; error   : Feature 'cvt.bf16.f32' requires .target sm_80 or higher
W0629 11:52:50.837000 139012379337344 torch/_dynamo/convert_frame.py:824] ptxas /tmp/compile-ptx-src-52c445, line 62; error   : Feature '.bf16' requires .target sm_80 or higher
W0629 11:52:50.837000 139012379337344 torch/_dynamo/convert_frame.py:824] ptxas /tmp/compile-ptx-src-52c445, line 62; error   : Feature 'cvt.bf16.f32' requires .target sm_80 or higher
W0629 11:52:50.837000 139012379337344 torch/_dynamo/convert_frame.py:824] ptxas /tmp/compile-ptx-src-52c445, line 63; error   : Feature '.bf16' requires .target sm_80 or higher
W0629 11:52:50.837000 139012379337344 torch/_dynamo/convert_frame.py:824] ptxas /tmp/compile-ptx-src-52c445, line 63; error   : Feature 'cvt.bf16.f32' requires .target sm_80 or higher
W0629 11:52:50.837000 139012379337344 torch/_dynamo/convert_frame.py:824] ptxas /tmp/compile-ptx-src-52c445, line 64; error   : Feature '.bf16' requires .target sm_80 or higher
W0629 11:52:50.837000 139012379337344 torch/_dynamo/convert_frame.py:824] ptxas /tmp/compile-ptx-src-52c445, line 64; error   : Feature 'cvt.bf16.f32' requires .target sm_80 or higher
W0629 11:52:50.837000 139012379337344 torch/_dynamo/convert_frame.py:824] ptxas /tmp/compile-ptx-src-52c445, line 65; error   : Feature '.bf16' requires .target sm_80 or higher
W0629 11:52:50.837000 139012379337344 torch/_dynamo/convert_frame.py:824] ptxas /tmp/compile-ptx-src-52c445, line 65; error   : Feature 'cvt.bf16.f32' requires .target sm_80 or higher
W0629 11:52:50.837000 139012379337344 torch/_dynamo/convert_frame.py:824] ptxas /tmp/compile-ptx-src-52c445, line 66; error   : Feature '.bf16' requires .target sm_80 or higher
W0629 11:52:50.837000 139012379337344 torch/_dynamo/convert_frame.py:824] ptxas /tmp/compile-ptx-src-52c445, line 66; error   : Feature 'cvt.bf16.f32' requires .target sm_80 or higher
W0629 11:52:50.837000 139012379337344 torch/_dynamo/convert_frame.py:824] ptxas fatal   : Ptx assembly aborted due to errors
W0629 11:52:50.837000 139012379337344 torch/_dynamo/convert_frame.py:824]
W0629 11:52:50.837000 139012379337344 torch/_dynamo/convert_frame.py:824]
W0629 11:52:50.837000 139012379337344 torch/_dynamo/convert_frame.py:824] Set TORCH_LOGS="+dynamo" and TORCHDYNAMO_VERBOSE=1 for more information
W0629 11:52:50.837000 139012379337344 torch/_dynamo/convert_frame.py:824]
W0629 11:52:50.837000 139012379337344 torch/_dynamo/convert_frame.py:824] Traceback (most recent call last):
W0629 11:52:50.837000 139012379337344 torch/_dynamo/convert_frame.py:824]   File "/usr/local/lib/python3.10/dist-packages/torch/_dynamo/convert_frame.py", line 786, in _convert_frame
W0629 11:52:50.837000 139012379337344 torch/_dynamo/convert_frame.py:824]     result = inner_convert(
W0629 11:52:50.837000 139012379337344 torch/_dynamo/convert_frame.py:824]   File "/usr/local/lib/python3.10/dist-packages/torch/_dynamo/convert_frame.py", line 400, in _convert_frame_assert
W0629 11:52:50.837000 139012379337344 torch/_dynamo/convert_frame.py:824]     return _compile(
W0629 11:52:50.837000 139012379337344 torch/_dynamo/convert_frame.py:824]   File "/usr/lib/python3.10/contextlib.py", line 79, in inner
W0629 11:52:50.837000 139012379337344 torch/_dynamo/convert_frame.py:824]     return func(*args, **kwds)
W0629 11:52:50.837000 139012379337344 torch/_dynamo/convert_frame.py:824]   File "/usr/local/lib/python3.10/dist-packages/torch/_dynamo/convert_frame.py", line 676, in _compile
W0629 11:52:50.837000 139012379337344 torch/_dynamo/convert_frame.py:824]     guarded_code = compile_inner(code, one_graph, hooks, transform)
W0629 11:52:50.837000 139012379337344 torch/_dynamo/convert_frame.py:824]   File "/usr/local/lib/python3.10/dist-packages/torch/_dynamo/utils.py", line 262, in time_wrapper
W0629 11:52:50.837000 139012379337344 torch/_dynamo/convert_frame.py:824]     r = func(*args, **kwargs)
W0629 11:52:50.837000 139012379337344 torch/_dynamo/convert_frame.py:824]   File "/usr/local/lib/python3.10/dist-packages/torch/_dynamo/convert_frame.py", line 535, in compile_inner
W0629 11:52:50.837000 139012379337344 torch/_dynamo/convert_frame.py:824]     out_code = transform_code_object(code, transform)
W0629 11:52:50.837000 139012379337344 torch/_dynamo/convert_frame.py:824]   File "/usr/local/lib/python3.10/dist-packages/torch/_dynamo/bytecode_transformation.py", line 1036, in transform_code_object
W0629 11:52:50.837000 139012379337344 torch/_dynamo/convert_frame.py:824]     transformations(instructions, code_options)
W0629 11:52:50.837000 139012379337344 torch/_dynamo/convert_frame.py:824]   File "/usr/local/lib/python3.10/dist-packages/torch/_dynamo/convert_frame.py", line 165, in _fn
W0629 11:52:50.837000 139012379337344 torch/_dynamo/convert_frame.py:824]     return fn(*args, **kwargs)
W0629 11:52:50.837000 139012379337344 torch/_dynamo/convert_frame.py:824]   File "/usr/local/lib/python3.10/dist-packages/torch/_dynamo/convert_frame.py", line 500, in transform
W0629 11:52:50.837000 139012379337344 torch/_dynamo/convert_frame.py:824]     tracer.run()
W0629 11:52:50.837000 139012379337344 torch/_dynamo/convert_frame.py:824]   File "/usr/local/lib/python3.10/dist-packages/torch/_dynamo/symbolic_convert.py", line 2149, in run
W0629 11:52:50.837000 139012379337344 torch/_dynamo/convert_frame.py:824]     super().run()
W0629 11:52:50.837000 139012379337344 torch/_dynamo/convert_frame.py:824]   File "/usr/local/lib/python3.10/dist-packages/torch/_dynamo/symbolic_convert.py", line 810, in run
W0629 11:52:50.837000 139012379337344 torch/_dynamo/convert_frame.py:824]     and self.step()
W0629 11:52:50.837000 139012379337344 torch/_dynamo/convert_frame.py:824]   File "/usr/local/lib/python3.10/dist-packages/torch/_dynamo/symbolic_convert.py", line 773, in step
W0629 11:52:50.837000 139012379337344 torch/_dynamo/convert_frame.py:824]     getattr(self, inst.opname)(inst)
W0629 11:52:50.837000 139012379337344 torch/_dynamo/convert_frame.py:824]   File "/usr/local/lib/python3.10/dist-packages/torch/_dynamo/symbolic_convert.py", line 2268, in RETURN_VALUE
W0629 11:52:50.837000 139012379337344 torch/_dynamo/convert_frame.py:824]     self.output.compile_subgraph(
W0629 11:52:50.837000 139012379337344 torch/_dynamo/convert_frame.py:824]   File "/usr/local/lib/python3.10/dist-packages/torch/_dynamo/output_graph.py", line 971, in compile_subgraph
W0629 11:52:50.837000 139012379337344 torch/_dynamo/convert_frame.py:824]     self.compile_and_call_fx_graph(tx, list(reversed(stack_values)), root)
W0629 11:52:50.837000 139012379337344 torch/_dynamo/convert_frame.py:824]   File "/usr/lib/python3.10/contextlib.py", line 79, in inner
W0629 11:52:50.837000 139012379337344 torch/_dynamo/convert_frame.py:824]     return func(*args, **kwds)
W0629 11:52:50.837000 139012379337344 torch/_dynamo/convert_frame.py:824]   File "/usr/local/lib/python3.10/dist-packages/torch/_dynamo/output_graph.py", line 1168, in compile_and_call_fx_graph
W0629 11:52:50.837000 139012379337344 torch/_dynamo/convert_frame.py:824]     compiled_fn = self.call_user_compiler(gm)
W0629 11:52:50.837000 139012379337344 torch/_dynamo/convert_frame.py:824]   File "/usr/local/lib/python3.10/dist-packages/torch/_dynamo/utils.py", line 262, in time_wrapper
W0629 11:52:50.837000 139012379337344 torch/_dynamo/convert_frame.py:824]     r = func(*args, **kwargs)
W0629 11:52:50.837000 139012379337344 torch/_dynamo/convert_frame.py:824]   File "/usr/local/lib/python3.10/dist-packages/torch/_dynamo/output_graph.py", line 1241, in call_user_compiler
W0629 11:52:50.837000 139012379337344 torch/_dynamo/convert_frame.py:824]     raise BackendCompilerFailed(self.compiler_fn, e).with_traceback(
W0629 11:52:50.837000 139012379337344 torch/_dynamo/convert_frame.py:824]   File "/usr/local/lib/python3.10/dist-packages/torch/_dynamo/output_graph.py", line 1222, in call_user_compiler
W0629 11:52:50.837000 139012379337344 torch/_dynamo/convert_frame.py:824]     compiled_fn = compiler_fn(gm, self.example_inputs())
W0629 11:52:50.837000 139012379337344 torch/_dynamo/convert_frame.py:824]   File "/usr/local/lib/python3.10/dist-packages/torch/_dynamo/repro/after_dynamo.py", line 117, in debug_wrapper
W0629 11:52:50.837000 139012379337344 torch/_dynamo/convert_frame.py:824]     compiled_gm = compiler_fn(gm, example_inputs)
W0629 11:52:50.837000 139012379337344 torch/_dynamo/convert_frame.py:824]   File "/usr/local/lib/python3.10/dist-packages/torch/__init__.py", line 1729, in __call__
W0629 11:52:50.837000 139012379337344 torch/_dynamo/convert_frame.py:824]     return compile_fx(model_, inputs_, config_patches=self.config)
W0629 11:52:50.837000 139012379337344 torch/_dynamo/convert_frame.py:824]   File "/usr/lib/python3.10/contextlib.py", line 79, in inner
W0629 11:52:50.837000 139012379337344 torch/_dynamo/convert_frame.py:824]     return func(*args, **kwds)
W0629 11:52:50.837000 139012379337344 torch/_dynamo/convert_frame.py:824]   File "/usr/local/lib/python3.10/dist-packages/torch/_inductor/compile_fx.py", line 1330, in compile_fx
W0629 11:52:50.837000 139012379337344 torch/_dynamo/convert_frame.py:824]     return aot_autograd(
W0629 11:52:50.837000 139012379337344 torch/_dynamo/convert_frame.py:824]   File "/usr/local/lib/python3.10/dist-packages/torch/_dynamo/backends/common.py", line 58, in compiler_fn
W0629 11:52:50.837000 139012379337344 torch/_dynamo/convert_frame.py:824]     cg = aot_module_simplified(gm, example_inputs, **kwargs)
W0629 11:52:50.837000 139012379337344 torch/_dynamo/convert_frame.py:824]   File "/usr/local/lib/python3.10/dist-packages/torch/_functorch/aot_autograd.py", line 903, in aot_module_simplified
W0629 11:52:50.837000 139012379337344 torch/_dynamo/convert_frame.py:824]     compiled_fn = create_aot_dispatcher_function(
W0629 11:52:50.837000 139012379337344 torch/_dynamo/convert_frame.py:824]   File "/usr/local/lib/python3.10/dist-packages/torch/_dynamo/utils.py", line 262, in time_wrapper
W0629 11:52:50.837000 139012379337344 torch/_dynamo/convert_frame.py:824]     r = func(*args, **kwargs)
W0629 11:52:50.837000 139012379337344 torch/_dynamo/convert_frame.py:824]   File "/usr/local/lib/python3.10/dist-packages/torch/_functorch/aot_autograd.py", line 628, in create_aot_dispatcher_function
W0629 11:52:50.837000 139012379337344 torch/_dynamo/convert_frame.py:824]     compiled_fn = compiler_fn(flat_fn, fake_flat_args, aot_config, fw_metadata=fw_metadata)
W0629 11:52:50.837000 139012379337344 torch/_dynamo/convert_frame.py:824]   File "/usr/local/lib/python3.10/dist-packages/torch/_functorch/_aot_autograd/runtime_wrappers.py", line 443, in aot_wrapper_dedupe
W0629 11:52:50.837000 139012379337344 torch/_dynamo/convert_frame.py:824]     return compiler_fn(flat_fn, leaf_flat_args, aot_config, fw_metadata=fw_metadata)
W0629 11:52:50.837000 139012379337344 torch/_dynamo/convert_frame.py:824]   File "/usr/local/lib/python3.10/dist-packages/torch/_functorch/_aot_autograd/runtime_wrappers.py", line 648, in aot_wrapper_synthetic_base
W0629 11:52:50.837000 139012379337344 torch/_dynamo/convert_frame.py:824]     return compiler_fn(flat_fn, flat_args, aot_config, fw_metadata=fw_metadata)
W0629 11:52:50.837000 139012379337344 torch/_dynamo/convert_frame.py:824]   File "/usr/local/lib/python3.10/dist-packages/torch/_functorch/_aot_autograd/jit_compile_runtime_wrappers.py", line 119, in aot_dispatch_base
W0629 11:52:50.837000 139012379337344 torch/_dynamo/convert_frame.py:824]     compiled_fw = compiler(fw_module, updated_flat_args)
W0629 11:52:50.837000 139012379337344 torch/_dynamo/convert_frame.py:824]   File "/usr/local/lib/python3.10/dist-packages/torch/_dynamo/utils.py", line 262, in time_wrapper
W0629 11:52:50.837000 139012379337344 torch/_dynamo/convert_frame.py:824]     r = func(*args, **kwargs)
W0629 11:52:50.837000 139012379337344 torch/_dynamo/convert_frame.py:824]   File "/usr/local/lib/python3.10/dist-packages/torch/_inductor/compile_fx.py", line 1257, in fw_compiler_base
W0629 11:52:50.837000 139012379337344 torch/_dynamo/convert_frame.py:824]     return inner_compile(
W0629 11:52:50.837000 139012379337344 torch/_dynamo/convert_frame.py:824]   File "/usr/local/lib/python3.10/dist-packages/torch/_dynamo/repro/after_aot.py", line 83, in debug_wrapper
W0629 11:52:50.837000 139012379337344 torch/_dynamo/convert_frame.py:824]     inner_compiled_fn = compiler_fn(gm, example_inputs)
W0629 11:52:50.837000 139012379337344 torch/_dynamo/convert_frame.py:824]   File "/usr/local/lib/python3.10/dist-packages/torch/_inductor/debug.py", line 304, in inner
W0629 11:52:50.837000 139012379337344 torch/_dynamo/convert_frame.py:824]     return fn(*args, **kwargs)
W0629 11:52:50.837000 139012379337344 torch/_dynamo/convert_frame.py:824]   File "/usr/lib/python3.10/contextlib.py", line 79, in inner
W0629 11:52:50.837000 139012379337344 torch/_dynamo/convert_frame.py:824]     return func(*args, **kwds)
W0629 11:52:50.837000 139012379337344 torch/_dynamo/convert_frame.py:824]   File "/usr/lib/python3.10/contextlib.py", line 79, in inner
W0629 11:52:50.837000 139012379337344 torch/_dynamo/convert_frame.py:824]     return func(*args, **kwds)
W0629 11:52:50.837000 139012379337344 torch/_dynamo/convert_frame.py:824]   File "/usr/local/lib/python3.10/dist-packages/torch/_dynamo/utils.py", line 262, in time_wrapper
W0629 11:52:50.837000 139012379337344 torch/_dynamo/convert_frame.py:824]     r = func(*args, **kwargs)
W0629 11:52:50.837000 139012379337344 torch/_dynamo/convert_frame.py:824]   File "/usr/local/lib/python3.10/dist-packages/torch/_inductor/compile_fx.py", line 438, in compile_fx_inner
W0629 11:52:50.837000 139012379337344 torch/_dynamo/convert_frame.py:824]     compiled_graph = fx_codegen_and_compile(
W0629 11:52:50.837000 139012379337344 torch/_dynamo/convert_frame.py:824]   File "/usr/local/lib/python3.10/dist-packages/torch/_inductor/compile_fx.py", line 714, in fx_codegen_and_compile
W0629 11:52:50.837000 139012379337344 torch/_dynamo/convert_frame.py:824]     compiled_fn = graph.compile_to_fn()
W0629 11:52:50.837000 139012379337344 torch/_dynamo/convert_frame.py:824]   File "/usr/local/lib/python3.10/dist-packages/torch/_inductor/graph.py", line 1307, in compile_to_fn
W0629 11:52:50.837000 139012379337344 torch/_dynamo/convert_frame.py:824]     return self.compile_to_module().call
W0629 11:52:50.837000 139012379337344 torch/_dynamo/convert_frame.py:824]   File "/usr/local/lib/python3.10/dist-packages/torch/_dynamo/utils.py", line 262, in time_wrapper
W0629 11:52:50.837000 139012379337344 torch/_dynamo/convert_frame.py:824]     r = func(*args, **kwargs)
W0629 11:52:50.837000 139012379337344 torch/_dynamo/convert_frame.py:824]   File "/usr/local/lib/python3.10/dist-packages/torch/_inductor/graph.py", line 1254, in compile_to_module
W0629 11:52:50.837000 139012379337344 torch/_dynamo/convert_frame.py:824]     mod = PyCodeCache.load_by_key_path(
W0629 11:52:50.837000 139012379337344 torch/_dynamo/convert_frame.py:824]   File "/usr/local/lib/python3.10/dist-packages/torch/_inductor/codecache.py", line 2160, in load_by_key_path
W0629 11:52:50.837000 139012379337344 torch/_dynamo/convert_frame.py:824]     exec(code, mod.__dict__, mod.__dict__)
W0629 11:52:50.837000 139012379337344 torch/_dynamo/convert_frame.py:824]   File "/tmp/torchinductor_root/uv/cuvr35tdh5iar7pawk4uy7oo77roav6rsrfsxbkfuxepd242zjn5.py", line 148, in <module>
W0629 11:52:50.837000 139012379337344 torch/_dynamo/convert_frame.py:824]     async_compile.wait(globals())
W0629 11:52:50.837000 139012379337344 torch/_dynamo/convert_frame.py:824]   File "/usr/local/lib/python3.10/dist-packages/torch/_inductor/codecache.py", line 2715, in wait
W0629 11:52:50.837000 139012379337344 torch/_dynamo/convert_frame.py:824]     scope[key] = result.result()
W0629 11:52:50.837000 139012379337344 torch/_dynamo/convert_frame.py:824]   File "/usr/local/lib/python3.10/dist-packages/torch/_inductor/codecache.py", line 2522, in result
W0629 11:52:50.837000 139012379337344 torch/_dynamo/convert_frame.py:824]     self.future.result()
W0629 11:52:50.837000 139012379337344 torch/_dynamo/convert_frame.py:824]   File "/usr/lib/python3.10/concurrent/futures/_base.py", line 458, in result
W0629 11:52:50.837000 139012379337344 torch/_dynamo/convert_frame.py:824]     return self.__get_result()
W0629 11:52:50.837000 139012379337344 torch/_dynamo/convert_frame.py:824]   File "/usr/lib/python3.10/concurrent/futures/_base.py", line 403, in __get_result
W0629 11:52:50.837000 139012379337344 torch/_dynamo/convert_frame.py:824]     raise self._exception
W0629 11:52:50.837000 139012379337344 torch/_dynamo/convert_frame.py:824] torch._dynamo.exc.BackendCompilerFailed: backend='inductor' raised:
W0629 11:52:50.837000 139012379337344 torch/_dynamo/convert_frame.py:824] RuntimeError: Internal Triton PTX codegen error:
W0629 11:52:50.837000 139012379337344 torch/_dynamo/convert_frame.py:824] ptxas /tmp/compile-ptx-src-52c445, line 59; error   : Feature '.bf16' requires .target sm_80 or higher
W0629 11:52:50.837000 139012379337344 torch/_dynamo/convert_frame.py:824] ptxas /tmp/compile-ptx-src-52c445, line 59; error   : Feature 'cvt.bf16.f32' requires .target sm_80 or higher
W0629 11:52:50.837000 139012379337344 torch/_dynamo/convert_frame.py:824] ptxas /tmp/compile-ptx-src-52c445, line 60; error   : Feature '.bf16' requires .target sm_80 or higher
W0629 11:52:50.837000 139012379337344 torch/_dynamo/convert_frame.py:824] ptxas /tmp/compile-ptx-src-52c445, line 60; error   : Feature 'cvt.bf16.f32' requires .target sm_80 or higher
W0629 11:52:50.837000 139012379337344 torch/_dynamo/convert_frame.py:824] ptxas /tmp/compile-ptx-src-52c445, line 61; error   : Feature '.bf16' requires .target sm_80 or higher
W0629 11:52:50.837000 139012379337344 torch/_dynamo/convert_frame.py:824] ptxas /tmp/compile-ptx-src-52c445, line 61; error   : Feature 'cvt.bf16.f32' requires .target sm_80 or higher
W0629 11:52:50.837000 139012379337344 torch/_dynamo/convert_frame.py:824] ptxas /tmp/compile-ptx-src-52c445, line 62; error   : Feature '.bf16' requires .target sm_80 or higher
W0629 11:52:50.837000 139012379337344 torch/_dynamo/convert_frame.py:824] ptxas /tmp/compile-ptx-src-52c445, line 62; error   : Feature 'cvt.bf16.f32' requires .target sm_80 or higher
W0629 11:52:50.837000 139012379337344 torch/_dynamo/convert_frame.py:824] ptxas /tmp/compile-ptx-src-52c445, line 63; error   : Feature '.bf16' requires .target sm_80 or higher
W0629 11:52:50.837000 139012379337344 torch/_dynamo/convert_frame.py:824] ptxas /tmp/compile-ptx-src-52c445, line 63; error   : Feature 'cvt.bf16.f32' requires .target sm_80 or higher
W0629 11:52:50.837000 139012379337344 torch/_dynamo/convert_frame.py:824] ptxas /tmp/compile-ptx-src-52c445, line 64; error   : Feature '.bf16' requires .target sm_80 or higher
W0629 11:52:50.837000 139012379337344 torch/_dynamo/convert_frame.py:824] ptxas /tmp/compile-ptx-src-52c445, line 64; error   : Feature 'cvt.bf16.f32' requires .target sm_80 or higher
W0629 11:52:50.837000 139012379337344 torch/_dynamo/convert_frame.py:824] ptxas /tmp/compile-ptx-src-52c445, line 65; error   : Feature '.bf16' requires .target sm_80 or higher
W0629 11:52:50.837000 139012379337344 torch/_dynamo/convert_frame.py:824] ptxas /tmp/compile-ptx-src-52c445, line 65; error   : Feature 'cvt.bf16.f32' requires .target sm_80 or higher
W0629 11:52:50.837000 139012379337344 torch/_dynamo/convert_frame.py:824] ptxas /tmp/compile-ptx-src-52c445, line 66; error   : Feature '.bf16' requires .target sm_80 or higher
W0629 11:52:50.837000 139012379337344 torch/_dynamo/convert_frame.py:824] ptxas /tmp/compile-ptx-src-52c445, line 66; error   : Feature 'cvt.bf16.f32' requires .target sm_80 or higher
W0629 11:52:50.837000 139012379337344 torch/_dynamo/convert_frame.py:824] ptxas fatal   : Ptx assembly aborted due to errors
W0629 11:52:50.837000 139012379337344 torch/_dynamo/convert_frame.py:824]
W0629 11:52:50.837000 139012379337344 torch/_dynamo/convert_frame.py:824]
W0629 11:52:50.837000 139012379337344 torch/_dynamo/convert_frame.py:824] Set TORCH_LOGS="+dynamo" and TORCHDYNAMO_VERBOSE=1 for more information
W0629 11:52:50.837000 139012379337344 torch/_dynamo/convert_frame.py:824]
W0629 11:52:51.020000 139012379337344 torch/_dynamo/convert_frame.py:824] WON'T CONVERT forward /usr/local/lib/python3.10/dist-packages/transformers/activations.py line 77
W0629 11:52:51.020000 139012379337344 torch/_dynamo/convert_frame.py:824] due to:
W0629 11:52:51.020000 139012379337344 torch/_dynamo/convert_frame.py:824] Traceback (most recent call last):
W0629 11:52:51.020000 139012379337344 torch/_dynamo/convert_frame.py:824]   File "/usr/local/lib/python3.10/dist-packages/torch/_dynamo/convert_frame.py", line 786, in _convert_frame
W0629 11:52:51.020000 139012379337344 torch/_dynamo/convert_frame.py:824]     result = inner_convert(
W0629 11:52:51.020000 139012379337344 torch/_dynamo/convert_frame.py:824]   File "/usr/local/lib/python3.10/dist-packages/torch/_dynamo/convert_frame.py", line 400, in _convert_frame_assert
W0629 11:52:51.020000 139012379337344 torch/_dynamo/convert_frame.py:824]     return _compile(
W0629 11:52:51.020000 139012379337344 torch/_dynamo/convert_frame.py:824]   File "/usr/lib/python3.10/contextlib.py", line 79, in inner
W0629 11:52:51.020000 139012379337344 torch/_dynamo/convert_frame.py:824]     return func(*args, **kwds)
W0629 11:52:51.020000 139012379337344 torch/_dynamo/convert_frame.py:824]   File "/usr/local/lib/python3.10/dist-packages/torch/_dynamo/convert_frame.py", line 676, in _compile
W0629 11:52:51.020000 139012379337344 torch/_dynamo/convert_frame.py:824]     guarded_code = compile_inner(code, one_graph, hooks, transform)
W0629 11:52:51.020000 139012379337344 torch/_dynamo/convert_frame.py:824]   File "/usr/local/lib/python3.10/dist-packages/torch/_dynamo/utils.py", line 262, in time_wrapper
W0629 11:52:51.020000 139012379337344 torch/_dynamo/convert_frame.py:824]     r = func(*args, **kwargs)
W0629 11:52:51.020000 139012379337344 torch/_dynamo/convert_frame.py:824]   File "/usr/local/lib/python3.10/dist-packages/torch/_dynamo/convert_frame.py", line 535, in compile_inner
W0629 11:52:51.020000 139012379337344 torch/_dynamo/convert_frame.py:824]     out_code = transform_code_object(code, transform)
W0629 11:52:51.020000 139012379337344 torch/_dynamo/convert_frame.py:824]   File "/usr/local/lib/python3.10/dist-packages/torch/_dynamo/bytecode_transformation.py", line 1036, in transform_code_object
W0629 11:52:51.020000 139012379337344 torch/_dynamo/convert_frame.py:824]     transformations(instructions, code_options)
W0629 11:52:51.020000 139012379337344 torch/_dynamo/convert_frame.py:824]   File "/usr/local/lib/python3.10/dist-packages/torch/_dynamo/convert_frame.py", line 165, in _fn
W0629 11:52:51.020000 139012379337344 torch/_dynamo/convert_frame.py:824]     return fn(*args, **kwargs)
W0629 11:52:51.020000 139012379337344 torch/_dynamo/convert_frame.py:824]   File "/usr/local/lib/python3.10/dist-packages/torch/_dynamo/convert_frame.py", line 500, in transform
W0629 11:52:51.020000 139012379337344 torch/_dynamo/convert_frame.py:824]     tracer.run()
W0629 11:52:51.020000 139012379337344 torch/_dynamo/convert_frame.py:824]   File "/usr/local/lib/python3.10/dist-packages/torch/_dynamo/symbolic_convert.py", line 2149, in run
W0629 11:52:51.020000 139012379337344 torch/_dynamo/convert_frame.py:824]     super().run()
W0629 11:52:51.020000 139012379337344 torch/_dynamo/convert_frame.py:824]   File "/usr/local/lib/python3.10/dist-packages/torch/_dynamo/symbolic_convert.py", line 810, in run
W0629 11:52:51.020000 139012379337344 torch/_dynamo/convert_frame.py:824]     and self.step()
W0629 11:52:51.020000 139012379337344 torch/_dynamo/convert_frame.py:824]   File "/usr/local/lib/python3.10/dist-packages/torch/_dynamo/symbolic_convert.py", line 773, in step
W0629 11:52:51.020000 139012379337344 torch/_dynamo/convert_frame.py:824]     getattr(self, inst.opname)(inst)
W0629 11:52:51.020000 139012379337344 torch/_dynamo/convert_frame.py:824]   File "/usr/local/lib/python3.10/dist-packages/torch/_dynamo/symbolic_convert.py", line 2268, in RETURN_VALUE
W0629 11:52:51.020000 139012379337344 torch/_dynamo/convert_frame.py:824]     self.output.compile_subgraph(
W0629 11:52:51.020000 139012379337344 torch/_dynamo/convert_frame.py:824]   File "/usr/local/lib/python3.10/dist-packages/torch/_dynamo/output_graph.py", line 971, in compile_subgraph
W0629 11:52:51.020000 139012379337344 torch/_dynamo/convert_frame.py:824]     self.compile_and_call_fx_graph(tx, list(reversed(stack_values)), root)
W0629 11:52:51.020000 139012379337344 torch/_dynamo/convert_frame.py:824]   File "/usr/lib/python3.10/contextlib.py", line 79, in inner
W0629 11:52:51.020000 139012379337344 torch/_dynamo/convert_frame.py:824]     return func(*args, **kwds)
W0629 11:52:51.020000 139012379337344 torch/_dynamo/convert_frame.py:824]   File "/usr/local/lib/python3.10/dist-packages/torch/_dynamo/output_graph.py", line 1168, in compile_and_call_fx_graph
W0629 11:52:51.020000 139012379337344 torch/_dynamo/convert_frame.py:824]     compiled_fn = self.call_user_compiler(gm)
W0629 11:52:51.020000 139012379337344 torch/_dynamo/convert_frame.py:824]   File "/usr/local/lib/python3.10/dist-packages/torch/_dynamo/utils.py", line 262, in time_wrapper
W0629 11:52:51.020000 139012379337344 torch/_dynamo/convert_frame.py:824]     r = func(*args, **kwargs)
W0629 11:52:51.020000 139012379337344 torch/_dynamo/convert_frame.py:824]   File "/usr/local/lib/python3.10/dist-packages/torch/_dynamo/output_graph.py", line 1241, in call_user_compiler
W0629 11:52:51.020000 139012379337344 torch/_dynamo/convert_frame.py:824]     raise BackendCompilerFailed(self.compiler_fn, e).with_traceback(
W0629 11:52:51.020000 139012379337344 torch/_dynamo/convert_frame.py:824]   File "/usr/local/lib/python3.10/dist-packages/torch/_dynamo/output_graph.py", line 1222, in call_user_compiler
W0629 11:52:51.020000 139012379337344 torch/_dynamo/convert_frame.py:824]     compiled_fn = compiler_fn(gm, self.example_inputs())
W0629 11:52:51.020000 139012379337344 torch/_dynamo/convert_frame.py:824]   File "/usr/local/lib/python3.10/dist-packages/torch/_dynamo/repro/after_dynamo.py", line 117, in debug_wrapper
W0629 11:52:51.020000 139012379337344 torch/_dynamo/convert_frame.py:824]     compiled_gm = compiler_fn(gm, example_inputs)
W0629 11:52:51.020000 139012379337344 torch/_dynamo/convert_frame.py:824]   File "/usr/local/lib/python3.10/dist-packages/torch/__init__.py", line 1729, in __call__
W0629 11:52:51.020000 139012379337344 torch/_dynamo/convert_frame.py:824]     return compile_fx(model_, inputs_, config_patches=self.config)
W0629 11:52:51.020000 139012379337344 torch/_dynamo/convert_frame.py:824]   File "/usr/lib/python3.10/contextlib.py", line 79, in inner
W0629 11:52:51.020000 139012379337344 torch/_dynamo/convert_frame.py:824]     return func(*args, **kwds)
W0629 11:52:51.020000 139012379337344 torch/_dynamo/convert_frame.py:824]   File "/usr/local/lib/python3.10/dist-packages/torch/_inductor/compile_fx.py", line 1330, in compile_fx
W0629 11:52:51.020000 139012379337344 torch/_dynamo/convert_frame.py:824]     return aot_autograd(
W0629 11:52:51.020000 139012379337344 torch/_dynamo/convert_frame.py:824]   File "/usr/local/lib/python3.10/dist-packages/torch/_dynamo/backends/common.py", line 58, in compiler_fn
W0629 11:52:51.020000 139012379337344 torch/_dynamo/convert_frame.py:824]     cg = aot_module_simplified(gm, example_inputs, **kwargs)
W0629 11:52:51.020000 139012379337344 torch/_dynamo/convert_frame.py:824]   File "/usr/local/lib/python3.10/dist-packages/torch/_functorch/aot_autograd.py", line 903, in aot_module_simplified
W0629 11:52:51.020000 139012379337344 torch/_dynamo/convert_frame.py:824]     compiled_fn = create_aot_dispatcher_function(
W0629 11:52:51.020000 139012379337344 torch/_dynamo/convert_frame.py:824]   File "/usr/local/lib/python3.10/dist-packages/torch/_dynamo/utils.py", line 262, in time_wrapper
W0629 11:52:51.020000 139012379337344 torch/_dynamo/convert_frame.py:824]     r = func(*args, **kwargs)
W0629 11:52:51.020000 139012379337344 torch/_dynamo/convert_frame.py:824]   File "/usr/local/lib/python3.10/dist-packages/torch/_functorch/aot_autograd.py", line 628, in create_aot_dispatcher_function
W0629 11:52:51.020000 139012379337344 torch/_dynamo/convert_frame.py:824]     compiled_fn = compiler_fn(flat_fn, fake_flat_args, aot_config, fw_metadata=fw_metadata)
W0629 11:52:51.020000 139012379337344 torch/_dynamo/convert_frame.py:824]   File "/usr/local/lib/python3.10/dist-packages/torch/_functorch/_aot_autograd/runtime_wrappers.py", line 443, in aot_wrapper_dedupe
W0629 11:52:51.020000 139012379337344 torch/_dynamo/convert_frame.py:824]     return compiler_fn(flat_fn, leaf_flat_args, aot_config, fw_metadata=fw_metadata)
W0629 11:52:51.020000 139012379337344 torch/_dynamo/convert_frame.py:824]   File "/usr/local/lib/python3.10/dist-packages/torch/_functorch/_aot_autograd/runtime_wrappers.py", line 648, in aot_wrapper_synthetic_base
W0629 11:52:51.020000 139012379337344 torch/_dynamo/convert_frame.py:824]     return compiler_fn(flat_fn, flat_args, aot_config, fw_metadata=fw_metadata)
W0629 11:52:51.020000 139012379337344 torch/_dynamo/convert_frame.py:824]   File "/usr/local/lib/python3.10/dist-packages/torch/_functorch/_aot_autograd/jit_compile_runtime_wrappers.py", line 119, in aot_dispatch_base
W0629 11:52:51.020000 139012379337344 torch/_dynamo/convert_frame.py:824]     compiled_fw = compiler(fw_module, updated_flat_args)
W0629 11:52:51.020000 139012379337344 torch/_dynamo/convert_frame.py:824]   File "/usr/local/lib/python3.10/dist-packages/torch/_dynamo/utils.py", line 262, in time_wrapper
W0629 11:52:51.020000 139012379337344 torch/_dynamo/convert_frame.py:824]     r = func(*args, **kwargs)
W0629 11:52:51.020000 139012379337344 torch/_dynamo/convert_frame.py:824]   File "/usr/local/lib/python3.10/dist-packages/torch/_inductor/compile_fx.py", line 1257, in fw_compiler_base
W0629 11:52:51.020000 139012379337344 torch/_dynamo/convert_frame.py:824]     return inner_compile(
W0629 11:52:51.020000 139012379337344 torch/_dynamo/convert_frame.py:824]   File "/usr/local/lib/python3.10/dist-packages/torch/_dynamo/repro/after_aot.py", line 83, in debug_wrapper
W0629 11:52:51.020000 139012379337344 torch/_dynamo/convert_frame.py:824]     inner_compiled_fn = compiler_fn(gm, example_inputs)
W0629 11:52:51.020000 139012379337344 torch/_dynamo/convert_frame.py:824]   File "/usr/local/lib/python3.10/dist-packages/torch/_inductor/debug.py", line 304, in inner
W0629 11:52:51.020000 139012379337344 torch/_dynamo/convert_frame.py:824]     return fn(*args, **kwargs)
W0629 11:52:51.020000 139012379337344 torch/_dynamo/convert_frame.py:824]   File "/usr/lib/python3.10/contextlib.py", line 79, in inner
W0629 11:52:51.020000 139012379337344 torch/_dynamo/convert_frame.py:824]     return func(*args, **kwds)
W0629 11:52:51.020000 139012379337344 torch/_dynamo/convert_frame.py:824]   File "/usr/lib/python3.10/contextlib.py", line 79, in inner
W0629 11:52:51.020000 139012379337344 torch/_dynamo/convert_frame.py:824]     return func(*args, **kwds)
W0629 11:52:51.020000 139012379337344 torch/_dynamo/convert_frame.py:824]   File "/usr/local/lib/python3.10/dist-packages/torch/_dynamo/utils.py", line 262, in time_wrapper
W0629 11:52:51.020000 139012379337344 torch/_dynamo/convert_frame.py:824]     r = func(*args, **kwargs)
W0629 11:52:51.020000 139012379337344 torch/_dynamo/convert_frame.py:824]   File "/usr/local/lib/python3.10/dist-packages/torch/_inductor/compile_fx.py", line 438, in compile_fx_inner
W0629 11:52:51.020000 139012379337344 torch/_dynamo/convert_frame.py:824]     compiled_graph = fx_codegen_and_compile(
W0629 11:52:51.020000 139012379337344 torch/_dynamo/convert_frame.py:824]   File "/usr/local/lib/python3.10/dist-packages/torch/_inductor/compile_fx.py", line 714, in fx_codegen_and_compile
W0629 11:52:51.020000 139012379337344 torch/_dynamo/convert_frame.py:824]     compiled_fn = graph.compile_to_fn()
W0629 11:52:51.020000 139012379337344 torch/_dynamo/convert_frame.py:824]   File "/usr/local/lib/python3.10/dist-packages/torch/_inductor/graph.py", line 1307, in compile_to_fn
W0629 11:52:51.020000 139012379337344 torch/_dynamo/convert_frame.py:824]     return self.compile_to_module().call
W0629 11:52:51.020000 139012379337344 torch/_dynamo/convert_frame.py:824]   File "/usr/local/lib/python3.10/dist-packages/torch/_dynamo/utils.py", line 262, in time_wrapper
W0629 11:52:51.020000 139012379337344 torch/_dynamo/convert_frame.py:824]     r = func(*args, **kwargs)
W0629 11:52:51.020000 139012379337344 torch/_dynamo/convert_frame.py:824]   File "/usr/local/lib/python3.10/dist-packages/torch/_inductor/graph.py", line 1254, in compile_to_module
W0629 11:52:51.020000 139012379337344 torch/_dynamo/convert_frame.py:824]     mod = PyCodeCache.load_by_key_path(
W0629 11:52:51.020000 139012379337344 torch/_dynamo/convert_frame.py:824]   File "/usr/local/lib/python3.10/dist-packages/torch/_inductor/codecache.py", line 2160, in load_by_key_path
W0629 11:52:51.020000 139012379337344 torch/_dynamo/convert_frame.py:824]     exec(code, mod.__dict__, mod.__dict__)
W0629 11:52:51.020000 139012379337344 torch/_dynamo/convert_frame.py:824]   File "/tmp/torchinductor_root/bt/cbtkblrdqbi2b37bc5xm4pdebphs3tppyr3ldwqujym2auick5ms.py", line 76, in <module>
W0629 11:52:51.020000 139012379337344 torch/_dynamo/convert_frame.py:824]     async_compile.wait(globals())
W0629 11:52:51.020000 139012379337344 torch/_dynamo/convert_frame.py:824]   File "/usr/local/lib/python3.10/dist-packages/torch/_inductor/codecache.py", line 2715, in wait
W0629 11:52:51.020000 139012379337344 torch/_dynamo/convert_frame.py:824]     scope[key] = result.result()
W0629 11:52:51.020000 139012379337344 torch/_dynamo/convert_frame.py:824]   File "/usr/local/lib/python3.10/dist-packages/torch/_inductor/codecache.py", line 2522, in result
W0629 11:52:51.020000 139012379337344 torch/_dynamo/convert_frame.py:824]     self.future.result()
W0629 11:52:51.020000 139012379337344 torch/_dynamo/convert_frame.py:824]   File "/usr/lib/python3.10/concurrent/futures/_base.py", line 458, in result
W0629 11:52:51.020000 139012379337344 torch/_dynamo/convert_frame.py:824]     return self.__get_result()
W0629 11:52:51.020000 139012379337344 torch/_dynamo/convert_frame.py:824]   File "/usr/lib/python3.10/concurrent/futures/_base.py", line 403, in __get_result
W0629 11:52:51.020000 139012379337344 torch/_dynamo/convert_frame.py:824]     raise self._exception
W0629 11:52:51.020000 139012379337344 torch/_dynamo/convert_frame.py:824] torch._dynamo.exc.BackendCompilerFailed: backend='inductor' raised:
W0629 11:52:51.020000 139012379337344 torch/_dynamo/convert_frame.py:824] RuntimeError: Internal Triton PTX codegen error:
W0629 11:52:51.020000 139012379337344 torch/_dynamo/convert_frame.py:824] ptxas /tmp/compile-ptx-src-4c8ef5, line 54; error   : Feature '.bf16' requires .target sm_80 or higher
W0629 11:52:51.020000 139012379337344 torch/_dynamo/convert_frame.py:824] ptxas /tmp/compile-ptx-src-4c8ef5, line 54; error   : Feature 'cvt with .f32.bf16' requires .target sm_80 or higher
W0629 11:52:51.020000 139012379337344 torch/_dynamo/convert_frame.py:824] ptxas /tmp/compile-ptx-src-4c8ef5, line 56; error   : Feature '.bf16' requires .target sm_80 or higher
W0629 11:52:51.020000 139012379337344 torch/_dynamo/convert_frame.py:824] ptxas /tmp/compile-ptx-src-4c8ef5, line 56; error   : Feature 'cvt with .f32.bf16' requires .target sm_80 or higher
W0629 11:52:51.020000 139012379337344 torch/_dynamo/convert_frame.py:824] ptxas /tmp/compile-ptx-src-4c8ef5, line 84; error   : Feature '.bf16' requires .target sm_80 or higher
W0629 11:52:51.020000 139012379337344 torch/_dynamo/convert_frame.py:824] ptxas /tmp/compile-ptx-src-4c8ef5, line 84; error   : Feature 'cvt with .f32.bf16' requires .target sm_80 or higher
W0629 11:52:51.020000 139012379337344 torch/_dynamo/convert_frame.py:824] ptxas /tmp/compile-ptx-src-4c8ef5, line 132; error   : Feature '.bf16' requires .target sm_80 or higher
W0629 11:52:51.020000 139012379337344 torch/_dynamo/convert_frame.py:824] ptxas /tmp/compile-ptx-src-4c8ef5, line 132; error   : Feature 'cvt with .f32.bf16' requires .target sm_80 or higher
W0629 11:52:51.020000 139012379337344 torch/_dynamo/convert_frame.py:824] ptxas /tmp/compile-ptx-src-4c8ef5, line 179; error   : Feature '.bf16' requires .target sm_80 or higher
W0629 11:52:51.020000 139012379337344 torch/_dynamo/convert_frame.py:824] ptxas /tmp/compile-ptx-src-4c8ef5, line 179; error   : Feature 'cvt with .f32.bf16' requires .target sm_80 or higher
W0629 11:52:51.020000 139012379337344 torch/_dynamo/convert_frame.py:824] ptxas /tmp/compile-ptx-src-4c8ef5, line 226; error   : Feature '.bf16' requires .target sm_80 or higher
W0629 11:52:51.020000 139012379337344 torch/_dynamo/convert_frame.py:824] ptxas /tmp/compile-ptx-src-4c8ef5, line 226; error   : Feature 'cvt with .f32.bf16' requires .target sm_80 or higher
W0629 11:52:51.020000 139012379337344 torch/_dynamo/convert_frame.py:824] ptxas /tmp/compile-ptx-src-4c8ef5, line 273; error   : Feature '.bf16' requires .target sm_80 or higher
W0629 11:52:51.020000 139012379337344 torch/_dynamo/convert_frame.py:824] ptxas /tmp/compile-ptx-src-4c8ef5, line 273; error   : Feature 'cvt with .f32.bf16' requires .target sm_80 or higher
W0629 11:52:51.020000 139012379337344 torch/_dynamo/convert_frame.py:824] ptxas /tmp/compile-ptx-src-4c8ef5, line 320; error   : Feature '.bf16' requires .target sm_80 or higher
W0629 11:52:51.020000 139012379337344 torch/_dynamo/convert_frame.py:824] ptxas /tmp/compile-ptx-src-4c8ef5, line 320; error   : Feature 'cvt with .f32.bf16' requires .target sm_80 or higher
W0629 11:52:51.020000 139012379337344 torch/_dynamo/convert_frame.py:824] ptxas /tmp/compile-ptx-src-4c8ef5, line 462; error   : Feature '.bf16' requires .target sm_80 or higher
W0629 11:52:51.020000 139012379337344 torch/_dynamo/convert_frame.py:824] ptxas /tmp/compile-ptx-src-4c8ef5, line 462; error   : Feature 'cvt.bf16.f32' requires .target sm_80 or higher
W0629 11:52:51.020000 139012379337344 torch/_dynamo/convert_frame.py:824] ptxas /tmp/compile-ptx-src-4c8ef5, line 464; error   : Feature '.bf16' requires .target sm_80 or higher
W0629 11:52:51.020000 139012379337344 torch/_dynamo/convert_frame.py:824] ptxas /tmp/compile-ptx-src-4c8ef5, line 464; error   : Feature 'cvt.bf16.f32' requires .target sm_80 or higher
W0629 11:52:51.020000 139012379337344 torch/_dynamo/convert_frame.py:824] ptxas /tmp/compile-ptx-src-4c8ef5, line 466; error   : Feature '.bf16' requires .target sm_80 or higher
W0629 11:52:51.020000 139012379337344 torch/_dynamo/convert_frame.py:824] ptxas /tmp/compile-ptx-src-4c8ef5, line 466; error   : Feature 'cvt.bf16.f32' requires .target sm_80 or higher
W0629 11:52:51.020000 139012379337344 torch/_dynamo/convert_frame.py:824] ptxas /tmp/compile-ptx-src-4c8ef5, line 468; error   : Feature '.bf16' requires .target sm_80 or higher
W0629 11:52:51.020000 139012379337344 torch/_dynamo/convert_frame.py:824] ptxas /tmp/compile-ptx-src-4c8ef5, line 468; error   : Feature 'cvt.bf16.f32' requires .target sm_80 or higher
W0629 11:52:51.020000 139012379337344 torch/_dynamo/convert_frame.py:824] ptxas /tmp/compile-ptx-src-4c8ef5, line 470; error   : Feature '.bf16' requires .target sm_80 or higher
W0629 11:52:51.020000 139012379337344 torch/_dynamo/convert_frame.py:824] ptxas /tmp/compile-ptx-src-4c8ef5, line 470; error   : Feature 'cvt.bf16.f32' requires .target sm_80 or higher
W0629 11:52:51.020000 139012379337344 torch/_dynamo/convert_frame.py:824] ptxas /tmp/compile-ptx-src-4c8ef5, line 472; error   : Feature '.bf16' requires .target sm_80 or higher
W0629 11:52:51.020000 139012379337344 torch/_dynamo/convert_frame.py:824] ptxas /tmp/compile-ptx-src-4c8ef5, line 472; error   : Feature 'cvt.bf16.f32' requires .target sm_80 or higher
W0629 11:52:51.020000 139012379337344 torch/_dynamo/convert_frame.py:824] ptxas /tmp/compile-ptx-src-4c8ef5, line 474; error   : Feature '.bf16' requires .target sm_80 or higher
W0629 11:52:51.020000 139012379337344 torch/_dynamo/convert_frame.py:824] ptxas /tmp/compile-ptx-src-4c8ef5, line 474; error   : Feature 'cvt.bf16.f32' requires .target sm_80 or higher
W0629 11:52:51.020000 139012379337344 torch/_dynamo/convert_frame.py:824] ptxas /tmp/compile-ptx-src-4c8ef5, line 476; error   : Feature '.bf16' requires .target sm_80 or higher
W0629 11:52:51.020000 139012379337344 torch/_dynamo/convert_frame.py:824] ptxas /tmp/compile-ptx-src-4c8ef5, line 476; error   : Feature 'cvt.bf16.f32' requires .target sm_80 or higher
W0629 11:52:51.020000 139012379337344 torch/_dynamo/convert_frame.py:824] ptxas fatal   : Ptx assembly aborted due to errors
W0629 11:52:51.020000 139012379337344 torch/_dynamo/convert_frame.py:824]
W0629 11:52:51.020000 139012379337344 torch/_dynamo/convert_frame.py:824]
W0629 11:52:51.020000 139012379337344 torch/_dynamo/convert_frame.py:824] Set TORCH_LOGS="+dynamo" and TORCHDYNAMO_VERBOSE=1 for more information
W0629 11:52:51.020000 139012379337344 torch/_dynamo/convert_frame.py:824]
W0629 11:52:51.020000 139012379337344 torch/_dynamo/convert_frame.py:824] Traceback (most recent call last):
W0629 11:52:51.020000 139012379337344 torch/_dynamo/convert_frame.py:824]   File "/usr/local/lib/python3.10/dist-packages/torch/_dynamo/convert_frame.py", line 786, in _convert_frame
W0629 11:52:51.020000 139012379337344 torch/_dynamo/convert_frame.py:824]     result = inner_convert(
W0629 11:52:51.020000 139012379337344 torch/_dynamo/convert_frame.py:824]   File "/usr/local/lib/python3.10/dist-packages/torch/_dynamo/convert_frame.py", line 400, in _convert_frame_assert
W0629 11:52:51.020000 139012379337344 torch/_dynamo/convert_frame.py:824]     return _compile(
W0629 11:52:51.020000 139012379337344 torch/_dynamo/convert_frame.py:824]   File "/usr/lib/python3.10/contextlib.py", line 79, in inner
W0629 11:52:51.020000 139012379337344 torch/_dynamo/convert_frame.py:824]     return func(*args, **kwds)
W0629 11:52:51.020000 139012379337344 torch/_dynamo/convert_frame.py:824]   File "/usr/local/lib/python3.10/dist-packages/torch/_dynamo/convert_frame.py", line 676, in _compile
W0629 11:52:51.020000 139012379337344 torch/_dynamo/convert_frame.py:824]     guarded_code = compile_inner(code, one_graph, hooks, transform)
W0629 11:52:51.020000 139012379337344 torch/_dynamo/convert_frame.py:824]   File "/usr/local/lib/python3.10/dist-packages/torch/_dynamo/utils.py", line 262, in time_wrapper
W0629 11:52:51.020000 139012379337344 torch/_dynamo/convert_frame.py:824]     r = func(*args, **kwargs)
W0629 11:52:51.020000 139012379337344 torch/_dynamo/convert_frame.py:824]   File "/usr/local/lib/python3.10/dist-packages/torch/_dynamo/convert_frame.py", line 535, in compile_inner
W0629 11:52:51.020000 139012379337344 torch/_dynamo/convert_frame.py:824]     out_code = transform_code_object(code, transform)
W0629 11:52:51.020000 139012379337344 torch/_dynamo/convert_frame.py:824]   File "/usr/local/lib/python3.10/dist-packages/torch/_dynamo/bytecode_transformation.py", line 1036, in transform_code_object
W0629 11:52:51.020000 139012379337344 torch/_dynamo/convert_frame.py:824]     transformations(instructions, code_options)
W0629 11:52:51.020000 139012379337344 torch/_dynamo/convert_frame.py:824]   File "/usr/local/lib/python3.10/dist-packages/torch/_dynamo/convert_frame.py", line 165, in _fn
W0629 11:52:51.020000 139012379337344 torch/_dynamo/convert_frame.py:824]     return fn(*args, **kwargs)
W0629 11:52:51.020000 139012379337344 torch/_dynamo/convert_frame.py:824]   File "/usr/local/lib/python3.10/dist-packages/torch/_dynamo/convert_frame.py", line 500, in transform
W0629 11:52:51.020000 139012379337344 torch/_dynamo/convert_frame.py:824]     tracer.run()
W0629 11:52:51.020000 139012379337344 torch/_dynamo/convert_frame.py:824]   File "/usr/local/lib/python3.10/dist-packages/torch/_dynamo/symbolic_convert.py", line 2149, in run
W0629 11:52:51.020000 139012379337344 torch/_dynamo/convert_frame.py:824]     super().run()
W0629 11:52:51.020000 139012379337344 torch/_dynamo/convert_frame.py:824]   File "/usr/local/lib/python3.10/dist-packages/torch/_dynamo/symbolic_convert.py", line 810, in run
W0629 11:52:51.020000 139012379337344 torch/_dynamo/convert_frame.py:824]     and self.step()
W0629 11:52:51.020000 139012379337344 torch/_dynamo/convert_frame.py:824]   File "/usr/local/lib/python3.10/dist-packages/torch/_dynamo/symbolic_convert.py", line 773, in step
W0629 11:52:51.020000 139012379337344 torch/_dynamo/convert_frame.py:824]     getattr(self, inst.opname)(inst)
W0629 11:52:51.020000 139012379337344 torch/_dynamo/convert_frame.py:824]   File "/usr/local/lib/python3.10/dist-packages/torch/_dynamo/symbolic_convert.py", line 2268, in RETURN_VALUE
W0629 11:52:51.020000 139012379337344 torch/_dynamo/convert_frame.py:824]     self.output.compile_subgraph(
W0629 11:52:51.020000 139012379337344 torch/_dynamo/convert_frame.py:824]   File "/usr/local/lib/python3.10/dist-packages/torch/_dynamo/output_graph.py", line 971, in compile_subgraph
W0629 11:52:51.020000 139012379337344 torch/_dynamo/convert_frame.py:824]     self.compile_and_call_fx_graph(tx, list(reversed(stack_values)), root)
W0629 11:52:51.020000 139012379337344 torch/_dynamo/convert_frame.py:824]   File "/usr/lib/python3.10/contextlib.py", line 79, in inner
W0629 11:52:51.020000 139012379337344 torch/_dynamo/convert_frame.py:824]     return func(*args, **kwds)
W0629 11:52:51.020000 139012379337344 torch/_dynamo/convert_frame.py:824]   File "/usr/local/lib/python3.10/dist-packages/torch/_dynamo/output_graph.py", line 1168, in compile_and_call_fx_graph
W0629 11:52:51.020000 139012379337344 torch/_dynamo/convert_frame.py:824]     compiled_fn = self.call_user_compiler(gm)
W0629 11:52:51.020000 139012379337344 torch/_dynamo/convert_frame.py:824]   File "/usr/local/lib/python3.10/dist-packages/torch/_dynamo/utils.py", line 262, in time_wrapper
W0629 11:52:51.020000 139012379337344 torch/_dynamo/convert_frame.py:824]     r = func(*args, **kwargs)
W0629 11:52:51.020000 139012379337344 torch/_dynamo/convert_frame.py:824]   File "/usr/local/lib/python3.10/dist-packages/torch/_dynamo/output_graph.py", line 1241, in call_user_compiler
W0629 11:52:51.020000 139012379337344 torch/_dynamo/convert_frame.py:824]     raise BackendCompilerFailed(self.compiler_fn, e).with_traceback(
W0629 11:52:51.020000 139012379337344 torch/_dynamo/convert_frame.py:824]   File "/usr/local/lib/python3.10/dist-packages/torch/_dynamo/output_graph.py", line 1222, in call_user_compiler
W0629 11:52:51.020000 139012379337344 torch/_dynamo/convert_frame.py:824]     compiled_fn = compiler_fn(gm, self.example_inputs())
W0629 11:52:51.020000 139012379337344 torch/_dynamo/convert_frame.py:824]   File "/usr/local/lib/python3.10/dist-packages/torch/_dynamo/repro/after_dynamo.py", line 117, in debug_wrapper
W0629 11:52:51.020000 139012379337344 torch/_dynamo/convert_frame.py:824]     compiled_gm = compiler_fn(gm, example_inputs)
W0629 11:52:51.020000 139012379337344 torch/_dynamo/convert_frame.py:824]   File "/usr/local/lib/python3.10/dist-packages/torch/__init__.py", line 1729, in __call__
W0629 11:52:51.020000 139012379337344 torch/_dynamo/convert_frame.py:824]     return compile_fx(model_, inputs_, config_patches=self.config)
W0629 11:52:51.020000 139012379337344 torch/_dynamo/convert_frame.py:824]   File "/usr/lib/python3.10/contextlib.py", line 79, in inner
W0629 11:52:51.020000 139012379337344 torch/_dynamo/convert_frame.py:824]     return func(*args, **kwds)
W0629 11:52:51.020000 139012379337344 torch/_dynamo/convert_frame.py:824]   File "/usr/local/lib/python3.10/dist-packages/torch/_inductor/compile_fx.py", line 1330, in compile_fx
W0629 11:52:51.020000 139012379337344 torch/_dynamo/convert_frame.py:824]     return aot_autograd(
W0629 11:52:51.020000 139012379337344 torch/_dynamo/convert_frame.py:824]   File "/usr/local/lib/python3.10/dist-packages/torch/_dynamo/backends/common.py", line 58, in compiler_fn
W0629 11:52:51.020000 139012379337344 torch/_dynamo/convert_frame.py:824]     cg = aot_module_simplified(gm, example_inputs, **kwargs)
W0629 11:52:51.020000 139012379337344 torch/_dynamo/convert_frame.py:824]   File "/usr/local/lib/python3.10/dist-packages/torch/_functorch/aot_autograd.py", line 903, in aot_module_simplified
W0629 11:52:51.020000 139012379337344 torch/_dynamo/convert_frame.py:824]     compiled_fn = create_aot_dispatcher_function(
W0629 11:52:51.020000 139012379337344 torch/_dynamo/convert_frame.py:824]   File "/usr/local/lib/python3.10/dist-packages/torch/_dynamo/utils.py", line 262, in time_wrapper
W0629 11:52:51.020000 139012379337344 torch/_dynamo/convert_frame.py:824]     r = func(*args, **kwargs)
W0629 11:52:51.020000 139012379337344 torch/_dynamo/convert_frame.py:824]   File "/usr/local/lib/python3.10/dist-packages/torch/_functorch/aot_autograd.py", line 628, in create_aot_dispatcher_function
W0629 11:52:51.020000 139012379337344 torch/_dynamo/convert_frame.py:824]     compiled_fn = compiler_fn(flat_fn, fake_flat_args, aot_config, fw_metadata=fw_metadata)
W0629 11:52:51.020000 139012379337344 torch/_dynamo/convert_frame.py:824]   File "/usr/local/lib/python3.10/dist-packages/torch/_functorch/_aot_autograd/runtime_wrappers.py", line 443, in aot_wrapper_dedupe
W0629 11:52:51.020000 139012379337344 torch/_dynamo/convert_frame.py:824]     return compiler_fn(flat_fn, leaf_flat_args, aot_config, fw_metadata=fw_metadata)
W0629 11:52:51.020000 139012379337344 torch/_dynamo/convert_frame.py:824]   File "/usr/local/lib/python3.10/dist-packages/torch/_functorch/_aot_autograd/runtime_wrappers.py", line 648, in aot_wrapper_synthetic_base
W0629 11:52:51.020000 139012379337344 torch/_dynamo/convert_frame.py:824]     return compiler_fn(flat_fn, flat_args, aot_config, fw_metadata=fw_metadata)
W0629 11:52:51.020000 139012379337344 torch/_dynamo/convert_frame.py:824]   File "/usr/local/lib/python3.10/dist-packages/torch/_functorch/_aot_autograd/jit_compile_runtime_wrappers.py", line 119, in aot_dispatch_base
W0629 11:52:51.020000 139012379337344 torch/_dynamo/convert_frame.py:824]     compiled_fw = compiler(fw_module, updated_flat_args)
W0629 11:52:51.020000 139012379337344 torch/_dynamo/convert_frame.py:824]   File "/usr/local/lib/python3.10/dist-packages/torch/_dynamo/utils.py", line 262, in time_wrapper
W0629 11:52:51.020000 139012379337344 torch/_dynamo/convert_frame.py:824]     r = func(*args, **kwargs)
W0629 11:52:51.020000 139012379337344 torch/_dynamo/convert_frame.py:824]   File "/usr/local/lib/python3.10/dist-packages/torch/_inductor/compile_fx.py", line 1257, in fw_compiler_base
W0629 11:52:51.020000 139012379337344 torch/_dynamo/convert_frame.py:824]     return inner_compile(
W0629 11:52:51.020000 139012379337344 torch/_dynamo/convert_frame.py:824]   File "/usr/local/lib/python3.10/dist-packages/torch/_dynamo/repro/after_aot.py", line 83, in debug_wrapper
W0629 11:52:51.020000 139012379337344 torch/_dynamo/convert_frame.py:824]     inner_compiled_fn = compiler_fn(gm, example_inputs)
W0629 11:52:51.020000 139012379337344 torch/_dynamo/convert_frame.py:824]   File "/usr/local/lib/python3.10/dist-packages/torch/_inductor/debug.py", line 304, in inner
W0629 11:52:51.020000 139012379337344 torch/_dynamo/convert_frame.py:824]     return fn(*args, **kwargs)
W0629 11:52:51.020000 139012379337344 torch/_dynamo/convert_frame.py:824]   File "/usr/lib/python3.10/contextlib.py", line 79, in inner
W0629 11:52:51.020000 139012379337344 torch/_dynamo/convert_frame.py:824]     return func(*args, **kwds)
W0629 11:52:51.020000 139012379337344 torch/_dynamo/convert_frame.py:824]   File "/usr/lib/python3.10/contextlib.py", line 79, in inner
W0629 11:52:51.020000 139012379337344 torch/_dynamo/convert_frame.py:824]     return func(*args, **kwds)
W0629 11:52:51.020000 139012379337344 torch/_dynamo/convert_frame.py:824]   File "/usr/local/lib/python3.10/dist-packages/torch/_dynamo/utils.py", line 262, in time_wrapper
W0629 11:52:51.020000 139012379337344 torch/_dynamo/convert_frame.py:824]     r = func(*args, **kwargs)
W0629 11:52:51.020000 139012379337344 torch/_dynamo/convert_frame.py:824]   File "/usr/local/lib/python3.10/dist-packages/torch/_inductor/compile_fx.py", line 438, in compile_fx_inner
W0629 11:52:51.020000 139012379337344 torch/_dynamo/convert_frame.py:824]     compiled_graph = fx_codegen_and_compile(
W0629 11:52:51.020000 139012379337344 torch/_dynamo/convert_frame.py:824]   File "/usr/local/lib/python3.10/dist-packages/torch/_inductor/compile_fx.py", line 714, in fx_codegen_and_compile
W0629 11:52:51.020000 139012379337344 torch/_dynamo/convert_frame.py:824]     compiled_fn = graph.compile_to_fn()
W0629 11:52:51.020000 139012379337344 torch/_dynamo/convert_frame.py:824]   File "/usr/local/lib/python3.10/dist-packages/torch/_inductor/graph.py", line 1307, in compile_to_fn
W0629 11:52:51.020000 139012379337344 torch/_dynamo/convert_frame.py:824]     return self.compile_to_module().call
W0629 11:52:51.020000 139012379337344 torch/_dynamo/convert_frame.py:824]   File "/usr/local/lib/python3.10/dist-packages/torch/_dynamo/utils.py", line 262, in time_wrapper
W0629 11:52:51.020000 139012379337344 torch/_dynamo/convert_frame.py:824]     r = func(*args, **kwargs)
W0629 11:52:51.020000 139012379337344 torch/_dynamo/convert_frame.py:824]   File "/usr/local/lib/python3.10/dist-packages/torch/_inductor/graph.py", line 1254, in compile_to_module
W0629 11:52:51.020000 139012379337344 torch/_dynamo/convert_frame.py:824]     mod = PyCodeCache.load_by_key_path(
W0629 11:52:51.020000 139012379337344 torch/_dynamo/convert_frame.py:824]   File "/usr/local/lib/python3.10/dist-packages/torch/_inductor/codecache.py", line 2160, in load_by_key_path
W0629 11:52:51.020000 139012379337344 torch/_dynamo/convert_frame.py:824]     exec(code, mod.__dict__, mod.__dict__)
W0629 11:52:51.020000 139012379337344 torch/_dynamo/convert_frame.py:824]   File "/tmp/torchinductor_root/bt/cbtkblrdqbi2b37bc5xm4pdebphs3tppyr3ldwqujym2auick5ms.py", line 76, in <module>
W0629 11:52:51.020000 139012379337344 torch/_dynamo/convert_frame.py:824]     async_compile.wait(globals())
W0629 11:52:51.020000 139012379337344 torch/_dynamo/convert_frame.py:824]   File "/usr/local/lib/python3.10/dist-packages/torch/_inductor/codecache.py", line 2715, in wait
W0629 11:52:51.020000 139012379337344 torch/_dynamo/convert_frame.py:824]     scope[key] = result.result()
W0629 11:52:51.020000 139012379337344 torch/_dynamo/convert_frame.py:824]   File "/usr/local/lib/python3.10/dist-packages/torch/_inductor/codecache.py", line 2522, in result
W0629 11:52:51.020000 139012379337344 torch/_dynamo/convert_frame.py:824]     self.future.result()
W0629 11:52:51.020000 139012379337344 torch/_dynamo/convert_frame.py:824]   File "/usr/lib/python3.10/concurrent/futures/_base.py", line 458, in result
W0629 11:52:51.020000 139012379337344 torch/_dynamo/convert_frame.py:824]     return self.__get_result()
W0629 11:52:51.020000 139012379337344 torch/_dynamo/convert_frame.py:824]   File "/usr/lib/python3.10/concurrent/futures/_base.py", line 403, in __get_result
W0629 11:52:51.020000 139012379337344 torch/_dynamo/convert_frame.py:824]     raise self._exception
W0629 11:52:51.020000 139012379337344 torch/_dynamo/convert_frame.py:824] torch._dynamo.exc.BackendCompilerFailed: backend='inductor' raised:
W0629 11:52:51.020000 139012379337344 torch/_dynamo/convert_frame.py:824] RuntimeError: Internal Triton PTX codegen error:
W0629 11:52:51.020000 139012379337344 torch/_dynamo/convert_frame.py:824] ptxas /tmp/compile-ptx-src-4c8ef5, line 54; error   : Feature '.bf16' requires .target sm_80 or higher
W0629 11:52:51.020000 139012379337344 torch/_dynamo/convert_frame.py:824] ptxas /tmp/compile-ptx-src-4c8ef5, line 54; error   : Feature 'cvt with .f32.bf16' requires .target sm_80 or higher
W0629 11:52:51.020000 139012379337344 torch/_dynamo/convert_frame.py:824] ptxas /tmp/compile-ptx-src-4c8ef5, line 56; error   : Feature '.bf16' requires .target sm_80 or higher
W0629 11:52:51.020000 139012379337344 torch/_dynamo/convert_frame.py:824] ptxas /tmp/compile-ptx-src-4c8ef5, line 56; error   : Feature 'cvt with .f32.bf16' requires .target sm_80 or higher
W0629 11:52:51.020000 139012379337344 torch/_dynamo/convert_frame.py:824] ptxas /tmp/compile-ptx-src-4c8ef5, line 84; error   : Feature '.bf16' requires .target sm_80 or higher
W0629 11:52:51.020000 139012379337344 torch/_dynamo/convert_frame.py:824] ptxas /tmp/compile-ptx-src-4c8ef5, line 84; error   : Feature 'cvt with .f32.bf16' requires .target sm_80 or higher
W0629 11:52:51.020000 139012379337344 torch/_dynamo/convert_frame.py:824] ptxas /tmp/compile-ptx-src-4c8ef5, line 132; error   : Feature '.bf16' requires .target sm_80 or higher
W0629 11:52:51.020000 139012379337344 torch/_dynamo/convert_frame.py:824] ptxas /tmp/compile-ptx-src-4c8ef5, line 132; error   : Feature 'cvt with .f32.bf16' requires .target sm_80 or higher
W0629 11:52:51.020000 139012379337344 torch/_dynamo/convert_frame.py:824] ptxas /tmp/compile-ptx-src-4c8ef5, line 179; error   : Feature '.bf16' requires .target sm_80 or higher
W0629 11:52:51.020000 139012379337344 torch/_dynamo/convert_frame.py:824] ptxas /tmp/compile-ptx-src-4c8ef5, line 179; error   : Feature 'cvt with .f32.bf16' requires .target sm_80 or higher
W0629 11:52:51.020000 139012379337344 torch/_dynamo/convert_frame.py:824] ptxas /tmp/compile-ptx-src-4c8ef5, line 226; error   : Feature '.bf16' requires .target sm_80 or higher
W0629 11:52:51.020000 139012379337344 torch/_dynamo/convert_frame.py:824] ptxas /tmp/compile-ptx-src-4c8ef5, line 226; error   : Feature 'cvt with .f32.bf16' requires .target sm_80 or higher
W0629 11:52:51.020000 139012379337344 torch/_dynamo/convert_frame.py:824] ptxas /tmp/compile-ptx-src-4c8ef5, line 273; error   : Feature '.bf16' requires .target sm_80 or higher
W0629 11:52:51.020000 139012379337344 torch/_dynamo/convert_frame.py:824] ptxas /tmp/compile-ptx-src-4c8ef5, line 273; error   : Feature 'cvt with .f32.bf16' requires .target sm_80 or higher
W0629 11:52:51.020000 139012379337344 torch/_dynamo/convert_frame.py:824] ptxas /tmp/compile-ptx-src-4c8ef5, line 320; error   : Feature '.bf16' requires .target sm_80 or higher
W0629 11:52:51.020000 139012379337344 torch/_dynamo/convert_frame.py:824] ptxas /tmp/compile-ptx-src-4c8ef5, line 320; error   : Feature 'cvt with .f32.bf16' requires .target sm_80 or higher
W0629 11:52:51.020000 139012379337344 torch/_dynamo/convert_frame.py:824] ptxas /tmp/compile-ptx-src-4c8ef5, line 462; error   : Feature '.bf16' requires .target sm_80 or higher
W0629 11:52:51.020000 139012379337344 torch/_dynamo/convert_frame.py:824] ptxas /tmp/compile-ptx-src-4c8ef5, line 462; error   : Feature 'cvt.bf16.f32' requires .target sm_80 or higher
W0629 11:52:51.020000 139012379337344 torch/_dynamo/convert_frame.py:824] ptxas /tmp/compile-ptx-src-4c8ef5, line 464; error   : Feature '.bf16' requires .target sm_80 or higher
W0629 11:52:51.020000 139012379337344 torch/_dynamo/convert_frame.py:824] ptxas /tmp/compile-ptx-src-4c8ef5, line 464; error   : Feature 'cvt.bf16.f32' requires .target sm_80 or higher
W0629 11:52:51.020000 139012379337344 torch/_dynamo/convert_frame.py:824] ptxas /tmp/compile-ptx-src-4c8ef5, line 466; error   : Feature '.bf16' requires .target sm_80 or higher
W0629 11:52:51.020000 139012379337344 torch/_dynamo/convert_frame.py:824] ptxas /tmp/compile-ptx-src-4c8ef5, line 466; error   : Feature 'cvt.bf16.f32' requires .target sm_80 or higher
W0629 11:52:51.020000 139012379337344 torch/_dynamo/convert_frame.py:824] ptxas /tmp/compile-ptx-src-4c8ef5, line 468; error   : Feature '.bf16' requires .target sm_80 or higher
W0629 11:52:51.020000 139012379337344 torch/_dynamo/convert_frame.py:824] ptxas /tmp/compile-ptx-src-4c8ef5, line 468; error   : Feature 'cvt.bf16.f32' requires .target sm_80 or higher
W0629 11:52:51.020000 139012379337344 torch/_dynamo/convert_frame.py:824] ptxas /tmp/compile-ptx-src-4c8ef5, line 470; error   : Feature '.bf16' requires .target sm_80 or higher
W0629 11:52:51.020000 139012379337344 torch/_dynamo/convert_frame.py:824] ptxas /tmp/compile-ptx-src-4c8ef5, line 470; error   : Feature 'cvt.bf16.f32' requires .target sm_80 or higher
W0629 11:52:51.020000 139012379337344 torch/_dynamo/convert_frame.py:824] ptxas /tmp/compile-ptx-src-4c8ef5, line 472; error   : Feature '.bf16' requires .target sm_80 or higher
W0629 11:52:51.020000 139012379337344 torch/_dynamo/convert_frame.py:824] ptxas /tmp/compile-ptx-src-4c8ef5, line 472; error   : Feature 'cvt.bf16.f32' requires .target sm_80 or higher
W0629 11:52:51.020000 139012379337344 torch/_dynamo/convert_frame.py:824] ptxas /tmp/compile-ptx-src-4c8ef5, line 474; error   : Feature '.bf16' requires .target sm_80 or higher
W0629 11:52:51.020000 139012379337344 torch/_dynamo/convert_frame.py:824] ptxas /tmp/compile-ptx-src-4c8ef5, line 474; error   : Feature 'cvt.bf16.f32' requires .target sm_80 or higher
W0629 11:52:51.020000 139012379337344 torch/_dynamo/convert_frame.py:824] ptxas /tmp/compile-ptx-src-4c8ef5, line 476; error   : Feature '.bf16' requires .target sm_80 or higher
W0629 11:52:51.020000 139012379337344 torch/_dynamo/convert_frame.py:824] ptxas /tmp/compile-ptx-src-4c8ef5, line 476; error   : Feature 'cvt.bf16.f32' requires .target sm_80 or higher
W0629 11:52:51.020000 139012379337344 torch/_dynamo/convert_frame.py:824] ptxas fatal   : Ptx assembly aborted due to errors
W0629 11:52:51.020000 139012379337344 torch/_dynamo/convert_frame.py:824]
W0629 11:52:51.020000 139012379337344 torch/_dynamo/convert_frame.py:824]
W0629 11:52:51.020000 139012379337344 torch/_dynamo/convert_frame.py:824] Set TORCH_LOGS="+dynamo" and TORCHDYNAMO_VERBOSE=1 for more information
W0629 11:52:51.020000 139012379337344 torch/_dynamo/convert_frame.py:824]
W0629 11:52:51.286000 139012379337344 torch/_dynamo/convert_frame.py:824] WON'T CONVERT forward /usr/local/lib/python3.10/dist-packages/transformers/models/bert/modeling_bert.py line 546
W0629 11:52:51.286000 139012379337344 torch/_dynamo/convert_frame.py:824] due to:
W0629 11:52:51.286000 139012379337344 torch/_dynamo/convert_frame.py:824] Traceback (most recent call last):
W0629 11:52:51.286000 139012379337344 torch/_dynamo/convert_frame.py:824]   File "/usr/local/lib/python3.10/dist-packages/torch/_dynamo/convert_frame.py", line 786, in _convert_frame
W0629 11:52:51.286000 139012379337344 torch/_dynamo/convert_frame.py:824]     result = inner_convert(
W0629 11:52:51.286000 139012379337344 torch/_dynamo/convert_frame.py:824]   File "/usr/local/lib/python3.10/dist-packages/torch/_dynamo/convert_frame.py", line 400, in _convert_frame_assert
W0629 11:52:51.286000 139012379337344 torch/_dynamo/convert_frame.py:824]     return _compile(
W0629 11:52:51.286000 139012379337344 torch/_dynamo/convert_frame.py:824]   File "/usr/lib/python3.10/contextlib.py", line 79, in inner
W0629 11:52:51.286000 139012379337344 torch/_dynamo/convert_frame.py:824]     return func(*args, **kwds)
W0629 11:52:51.286000 139012379337344 torch/_dynamo/convert_frame.py:824]   File "/usr/local/lib/python3.10/dist-packages/torch/_dynamo/convert_frame.py", line 676, in _compile
W0629 11:52:51.286000 139012379337344 torch/_dynamo/convert_frame.py:824]     guarded_code = compile_inner(code, one_graph, hooks, transform)
W0629 11:52:51.286000 139012379337344 torch/_dynamo/convert_frame.py:824]   File "/usr/local/lib/python3.10/dist-packages/torch/_dynamo/utils.py", line 262, in time_wrapper
W0629 11:52:51.286000 139012379337344 torch/_dynamo/convert_frame.py:824]     r = func(*args, **kwargs)
W0629 11:52:51.286000 139012379337344 torch/_dynamo/convert_frame.py:824]   File "/usr/local/lib/python3.10/dist-packages/torch/_dynamo/convert_frame.py", line 535, in compile_inner
W0629 11:52:51.286000 139012379337344 torch/_dynamo/convert_frame.py:824]     out_code = transform_code_object(code, transform)
W0629 11:52:51.286000 139012379337344 torch/_dynamo/convert_frame.py:824]   File "/usr/local/lib/python3.10/dist-packages/torch/_dynamo/bytecode_transformation.py", line 1036, in transform_code_object
W0629 11:52:51.286000 139012379337344 torch/_dynamo/convert_frame.py:824]     transformations(instructions, code_options)
W0629 11:52:51.286000 139012379337344 torch/_dynamo/convert_frame.py:824]   File "/usr/local/lib/python3.10/dist-packages/torch/_dynamo/convert_frame.py", line 165, in _fn
W0629 11:52:51.286000 139012379337344 torch/_dynamo/convert_frame.py:824]     return fn(*args, **kwargs)
W0629 11:52:51.286000 139012379337344 torch/_dynamo/convert_frame.py:824]   File "/usr/local/lib/python3.10/dist-packages/torch/_dynamo/convert_frame.py", line 500, in transform
W0629 11:52:51.286000 139012379337344 torch/_dynamo/convert_frame.py:824]     tracer.run()
W0629 11:52:51.286000 139012379337344 torch/_dynamo/convert_frame.py:824]   File "/usr/local/lib/python3.10/dist-packages/torch/_dynamo/symbolic_convert.py", line 2149, in run
W0629 11:52:51.286000 139012379337344 torch/_dynamo/convert_frame.py:824]     super().run()
W0629 11:52:51.286000 139012379337344 torch/_dynamo/convert_frame.py:824]   File "/usr/local/lib/python3.10/dist-packages/torch/_dynamo/symbolic_convert.py", line 810, in run
W0629 11:52:51.286000 139012379337344 torch/_dynamo/convert_frame.py:824]     and self.step()
W0629 11:52:51.286000 139012379337344 torch/_dynamo/convert_frame.py:824]   File "/usr/local/lib/python3.10/dist-packages/torch/_dynamo/symbolic_convert.py", line 773, in step
W0629 11:52:51.286000 139012379337344 torch/_dynamo/convert_frame.py:824]     getattr(self, inst.opname)(inst)
W0629 11:52:51.286000 139012379337344 torch/_dynamo/convert_frame.py:824]   File "/usr/local/lib/python3.10/dist-packages/torch/_dynamo/symbolic_convert.py", line 2268, in RETURN_VALUE
W0629 11:52:51.286000 139012379337344 torch/_dynamo/convert_frame.py:824]     self.output.compile_subgraph(
W0629 11:52:51.286000 139012379337344 torch/_dynamo/convert_frame.py:824]   File "/usr/local/lib/python3.10/dist-packages/torch/_dynamo/output_graph.py", line 971, in compile_subgraph
W0629 11:52:51.286000 139012379337344 torch/_dynamo/convert_frame.py:824]     self.compile_and_call_fx_graph(tx, list(reversed(stack_values)), root)
W0629 11:52:51.286000 139012379337344 torch/_dynamo/convert_frame.py:824]   File "/usr/lib/python3.10/contextlib.py", line 79, in inner
W0629 11:52:51.286000 139012379337344 torch/_dynamo/convert_frame.py:824]     return func(*args, **kwds)
W0629 11:52:51.286000 139012379337344 torch/_dynamo/convert_frame.py:824]   File "/usr/local/lib/python3.10/dist-packages/torch/_dynamo/output_graph.py", line 1168, in compile_and_call_fx_graph
W0629 11:52:51.286000 139012379337344 torch/_dynamo/convert_frame.py:824]     compiled_fn = self.call_user_compiler(gm)
W0629 11:52:51.286000 139012379337344 torch/_dynamo/convert_frame.py:824]   File "/usr/local/lib/python3.10/dist-packages/torch/_dynamo/utils.py", line 262, in time_wrapper
W0629 11:52:51.286000 139012379337344 torch/_dynamo/convert_frame.py:824]     r = func(*args, **kwargs)
W0629 11:52:51.286000 139012379337344 torch/_dynamo/convert_frame.py:824]   File "/usr/local/lib/python3.10/dist-packages/torch/_dynamo/output_graph.py", line 1241, in call_user_compiler
W0629 11:52:51.286000 139012379337344 torch/_dynamo/convert_frame.py:824]     raise BackendCompilerFailed(self.compiler_fn, e).with_traceback(
W0629 11:52:51.286000 139012379337344 torch/_dynamo/convert_frame.py:824]   File "/usr/local/lib/python3.10/dist-packages/torch/_dynamo/output_graph.py", line 1222, in call_user_compiler
W0629 11:52:51.286000 139012379337344 torch/_dynamo/convert_frame.py:824]     compiled_fn = compiler_fn(gm, self.example_inputs())
W0629 11:52:51.286000 139012379337344 torch/_dynamo/convert_frame.py:824]   File "/usr/local/lib/python3.10/dist-packages/torch/_dynamo/repro/after_dynamo.py", line 117, in debug_wrapper
W0629 11:52:51.286000 139012379337344 torch/_dynamo/convert_frame.py:824]     compiled_gm = compiler_fn(gm, example_inputs)
W0629 11:52:51.286000 139012379337344 torch/_dynamo/convert_frame.py:824]   File "/usr/local/lib/python3.10/dist-packages/torch/__init__.py", line 1729, in __call__
W0629 11:52:51.286000 139012379337344 torch/_dynamo/convert_frame.py:824]     return compile_fx(model_, inputs_, config_patches=self.config)
W0629 11:52:51.286000 139012379337344 torch/_dynamo/convert_frame.py:824]   File "/usr/lib/python3.10/contextlib.py", line 79, in inner
W0629 11:52:51.286000 139012379337344 torch/_dynamo/convert_frame.py:824]     return func(*args, **kwds)
W0629 11:52:51.286000 139012379337344 torch/_dynamo/convert_frame.py:824]   File "/usr/local/lib/python3.10/dist-packages/torch/_inductor/compile_fx.py", line 1330, in compile_fx
W0629 11:52:51.286000 139012379337344 torch/_dynamo/convert_frame.py:824]     return aot_autograd(
W0629 11:52:51.286000 139012379337344 torch/_dynamo/convert_frame.py:824]   File "/usr/local/lib/python3.10/dist-packages/torch/_dynamo/backends/common.py", line 58, in compiler_fn
W0629 11:52:51.286000 139012379337344 torch/_dynamo/convert_frame.py:824]     cg = aot_module_simplified(gm, example_inputs, **kwargs)
W0629 11:52:51.286000 139012379337344 torch/_dynamo/convert_frame.py:824]   File "/usr/local/lib/python3.10/dist-packages/torch/_functorch/aot_autograd.py", line 903, in aot_module_simplified
W0629 11:52:51.286000 139012379337344 torch/_dynamo/convert_frame.py:824]     compiled_fn = create_aot_dispatcher_function(
W0629 11:52:51.286000 139012379337344 torch/_dynamo/convert_frame.py:824]   File "/usr/local/lib/python3.10/dist-packages/torch/_dynamo/utils.py", line 262, in time_wrapper
W0629 11:52:51.286000 139012379337344 torch/_dynamo/convert_frame.py:824]     r = func(*args, **kwargs)
W0629 11:52:51.286000 139012379337344 torch/_dynamo/convert_frame.py:824]   File "/usr/local/lib/python3.10/dist-packages/torch/_functorch/aot_autograd.py", line 628, in create_aot_dispatcher_function
W0629 11:52:51.286000 139012379337344 torch/_dynamo/convert_frame.py:824]     compiled_fn = compiler_fn(flat_fn, fake_flat_args, aot_config, fw_metadata=fw_metadata)
W0629 11:52:51.286000 139012379337344 torch/_dynamo/convert_frame.py:824]   File "/usr/local/lib/python3.10/dist-packages/torch/_functorch/_aot_autograd/runtime_wrappers.py", line 443, in aot_wrapper_dedupe
W0629 11:52:51.286000 139012379337344 torch/_dynamo/convert_frame.py:824]     return compiler_fn(flat_fn, leaf_flat_args, aot_config, fw_metadata=fw_metadata)
W0629 11:52:51.286000 139012379337344 torch/_dynamo/convert_frame.py:824]   File "/usr/local/lib/python3.10/dist-packages/torch/_functorch/_aot_autograd/runtime_wrappers.py", line 648, in aot_wrapper_synthetic_base
W0629 11:52:51.286000 139012379337344 torch/_dynamo/convert_frame.py:824]     return compiler_fn(flat_fn, flat_args, aot_config, fw_metadata=fw_metadata)
W0629 11:52:51.286000 139012379337344 torch/_dynamo/convert_frame.py:824]   File "/usr/local/lib/python3.10/dist-packages/torch/_functorch/_aot_autograd/jit_compile_runtime_wrappers.py", line 119, in aot_dispatch_base
W0629 11:52:51.286000 139012379337344 torch/_dynamo/convert_frame.py:824]     compiled_fw = compiler(fw_module, updated_flat_args)
W0629 11:52:51.286000 139012379337344 torch/_dynamo/convert_frame.py:824]   File "/usr/local/lib/python3.10/dist-packages/torch/_dynamo/utils.py", line 262, in time_wrapper
W0629 11:52:51.286000 139012379337344 torch/_dynamo/convert_frame.py:824]     r = func(*args, **kwargs)
W0629 11:52:51.286000 139012379337344 torch/_dynamo/convert_frame.py:824]   File "/usr/local/lib/python3.10/dist-packages/torch/_inductor/compile_fx.py", line 1257, in fw_compiler_base
W0629 11:52:51.286000 139012379337344 torch/_dynamo/convert_frame.py:824]     return inner_compile(
W0629 11:52:51.286000 139012379337344 torch/_dynamo/convert_frame.py:824]   File "/usr/local/lib/python3.10/dist-packages/torch/_dynamo/repro/after_aot.py", line 83, in debug_wrapper
W0629 11:52:51.286000 139012379337344 torch/_dynamo/convert_frame.py:824]     inner_compiled_fn = compiler_fn(gm, example_inputs)
W0629 11:52:51.286000 139012379337344 torch/_dynamo/convert_frame.py:824]   File "/usr/local/lib/python3.10/dist-packages/torch/_inductor/debug.py", line 304, in inner
W0629 11:52:51.286000 139012379337344 torch/_dynamo/convert_frame.py:824]     return fn(*args, **kwargs)
W0629 11:52:51.286000 139012379337344 torch/_dynamo/convert_frame.py:824]   File "/usr/lib/python3.10/contextlib.py", line 79, in inner
W0629 11:52:51.286000 139012379337344 torch/_dynamo/convert_frame.py:824]     return func(*args, **kwds)
W0629 11:52:51.286000 139012379337344 torch/_dynamo/convert_frame.py:824]   File "/usr/lib/python3.10/contextlib.py", line 79, in inner
W0629 11:52:51.286000 139012379337344 torch/_dynamo/convert_frame.py:824]     return func(*args, **kwds)
W0629 11:52:51.286000 139012379337344 torch/_dynamo/convert_frame.py:824]   File "/usr/local/lib/python3.10/dist-packages/torch/_dynamo/utils.py", line 262, in time_wrapper
W0629 11:52:51.286000 139012379337344 torch/_dynamo/convert_frame.py:824]     r = func(*args, **kwargs)
W0629 11:52:51.286000 139012379337344 torch/_dynamo/convert_frame.py:824]   File "/usr/local/lib/python3.10/dist-packages/torch/_inductor/compile_fx.py", line 438, in compile_fx_inner
W0629 11:52:51.286000 139012379337344 torch/_dynamo/convert_frame.py:824]     compiled_graph = fx_codegen_and_compile(
W0629 11:52:51.286000 139012379337344 torch/_dynamo/convert_frame.py:824]   File "/usr/local/lib/python3.10/dist-packages/torch/_inductor/compile_fx.py", line 714, in fx_codegen_and_compile
W0629 11:52:51.286000 139012379337344 torch/_dynamo/convert_frame.py:824]     compiled_fn = graph.compile_to_fn()
W0629 11:52:51.286000 139012379337344 torch/_dynamo/convert_frame.py:824]   File "/usr/local/lib/python3.10/dist-packages/torch/_inductor/graph.py", line 1307, in compile_to_fn
W0629 11:52:51.286000 139012379337344 torch/_dynamo/convert_frame.py:824]     return self.compile_to_module().call
W0629 11:52:51.286000 139012379337344 torch/_dynamo/convert_frame.py:824]   File "/usr/local/lib/python3.10/dist-packages/torch/_dynamo/utils.py", line 262, in time_wrapper
W0629 11:52:51.286000 139012379337344 torch/_dynamo/convert_frame.py:824]     r = func(*args, **kwargs)
W0629 11:52:51.286000 139012379337344 torch/_dynamo/convert_frame.py:824]   File "/usr/local/lib/python3.10/dist-packages/torch/_inductor/graph.py", line 1254, in compile_to_module
W0629 11:52:51.286000 139012379337344 torch/_dynamo/convert_frame.py:824]     mod = PyCodeCache.load_by_key_path(
W0629 11:52:51.286000 139012379337344 torch/_dynamo/convert_frame.py:824]   File "/usr/local/lib/python3.10/dist-packages/torch/_inductor/codecache.py", line 2160, in load_by_key_path
W0629 11:52:51.286000 139012379337344 torch/_dynamo/convert_frame.py:824]     exec(code, mod.__dict__, mod.__dict__)
W0629 11:52:51.286000 139012379337344 torch/_dynamo/convert_frame.py:824]   File "/tmp/torchinductor_root/lh/clhbch42q7qymhc4rliwboxfar33te5n67jkrksggxhpdncnazwn.py", line 138, in <module>
W0629 11:52:51.286000 139012379337344 torch/_dynamo/convert_frame.py:824]     async_compile.wait(globals())
W0629 11:52:51.286000 139012379337344 torch/_dynamo/convert_frame.py:824]   File "/usr/local/lib/python3.10/dist-packages/torch/_inductor/codecache.py", line 2715, in wait
W0629 11:52:51.286000 139012379337344 torch/_dynamo/convert_frame.py:824]     scope[key] = result.result()
W0629 11:52:51.286000 139012379337344 torch/_dynamo/convert_frame.py:824]   File "/usr/local/lib/python3.10/dist-packages/torch/_inductor/codecache.py", line 2522, in result
W0629 11:52:51.286000 139012379337344 torch/_dynamo/convert_frame.py:824]     self.future.result()
W0629 11:52:51.286000 139012379337344 torch/_dynamo/convert_frame.py:824]   File "/usr/lib/python3.10/concurrent/futures/_base.py", line 458, in result
W0629 11:52:51.286000 139012379337344 torch/_dynamo/convert_frame.py:824]     return self.__get_result()
W0629 11:52:51.286000 139012379337344 torch/_dynamo/convert_frame.py:824]   File "/usr/lib/python3.10/concurrent/futures/_base.py", line 403, in __get_result
W0629 11:52:51.286000 139012379337344 torch/_dynamo/convert_frame.py:824]     raise self._exception
W0629 11:52:51.286000 139012379337344 torch/_dynamo/convert_frame.py:824] torch._dynamo.exc.BackendCompilerFailed: backend='inductor' raised:
W0629 11:52:51.286000 139012379337344 torch/_dynamo/convert_frame.py:824] RuntimeError: Internal Triton PTX codegen error:
W0629 11:52:51.286000 139012379337344 torch/_dynamo/convert_frame.py:824] ptxas /tmp/compile-ptx-src-c2f441, line 59; error   : Feature '.bf16' requires .target sm_80 or higher
W0629 11:52:51.286000 139012379337344 torch/_dynamo/convert_frame.py:824] ptxas /tmp/compile-ptx-src-c2f441, line 59; error   : Feature 'cvt.bf16.f32' requires .target sm_80 or higher
W0629 11:52:51.286000 139012379337344 torch/_dynamo/convert_frame.py:824] ptxas /tmp/compile-ptx-src-c2f441, line 60; error   : Feature '.bf16' requires .target sm_80 or higher
W0629 11:52:51.286000 139012379337344 torch/_dynamo/convert_frame.py:824] ptxas /tmp/compile-ptx-src-c2f441, line 60; error   : Feature 'cvt.bf16.f32' requires .target sm_80 or higher
W0629 11:52:51.286000 139012379337344 torch/_dynamo/convert_frame.py:824] ptxas /tmp/compile-ptx-src-c2f441, line 61; error   : Feature '.bf16' requires .target sm_80 or higher
W0629 11:52:51.286000 139012379337344 torch/_dynamo/convert_frame.py:824] ptxas /tmp/compile-ptx-src-c2f441, line 61; error   : Feature 'cvt.bf16.f32' requires .target sm_80 or higher
W0629 11:52:51.286000 139012379337344 torch/_dynamo/convert_frame.py:824] ptxas /tmp/compile-ptx-src-c2f441, line 62; error   : Feature '.bf16' requires .target sm_80 or higher
W0629 11:52:51.286000 139012379337344 torch/_dynamo/convert_frame.py:824] ptxas /tmp/compile-ptx-src-c2f441, line 62; error   : Feature 'cvt.bf16.f32' requires .target sm_80 or higher
W0629 11:52:51.286000 139012379337344 torch/_dynamo/convert_frame.py:824] ptxas /tmp/compile-ptx-src-c2f441, line 63; error   : Feature '.bf16' requires .target sm_80 or higher
W0629 11:52:51.286000 139012379337344 torch/_dynamo/convert_frame.py:824] ptxas /tmp/compile-ptx-src-c2f441, line 63; error   : Feature 'cvt.bf16.f32' requires .target sm_80 or higher
W0629 11:52:51.286000 139012379337344 torch/_dynamo/convert_frame.py:824] ptxas /tmp/compile-ptx-src-c2f441, line 64; error   : Feature '.bf16' requires .target sm_80 or higher
W0629 11:52:51.286000 139012379337344 torch/_dynamo/convert_frame.py:824] ptxas /tmp/compile-ptx-src-c2f441, line 64; error   : Feature 'cvt.bf16.f32' requires .target sm_80 or higher
W0629 11:52:51.286000 139012379337344 torch/_dynamo/convert_frame.py:824] ptxas /tmp/compile-ptx-src-c2f441, line 65; error   : Feature '.bf16' requires .target sm_80 or higher
W0629 11:52:51.286000 139012379337344 torch/_dynamo/convert_frame.py:824] ptxas /tmp/compile-ptx-src-c2f441, line 65; error   : Feature 'cvt.bf16.f32' requires .target sm_80 or higher
W0629 11:52:51.286000 139012379337344 torch/_dynamo/convert_frame.py:824] ptxas /tmp/compile-ptx-src-c2f441, line 66; error   : Feature '.bf16' requires .target sm_80 or higher
W0629 11:52:51.286000 139012379337344 torch/_dynamo/convert_frame.py:824] ptxas /tmp/compile-ptx-src-c2f441, line 66; error   : Feature 'cvt.bf16.f32' requires .target sm_80 or higher
W0629 11:52:51.286000 139012379337344 torch/_dynamo/convert_frame.py:824] ptxas fatal   : Ptx assembly aborted due to errors
W0629 11:52:51.286000 139012379337344 torch/_dynamo/convert_frame.py:824]
W0629 11:52:51.286000 139012379337344 torch/_dynamo/convert_frame.py:824]
W0629 11:52:51.286000 139012379337344 torch/_dynamo/convert_frame.py:824] Set TORCH_LOGS="+dynamo" and TORCHDYNAMO_VERBOSE=1 for more information
W0629 11:52:51.286000 139012379337344 torch/_dynamo/convert_frame.py:824]
W0629 11:52:51.286000 139012379337344 torch/_dynamo/convert_frame.py:824] Traceback (most recent call last):
W0629 11:52:51.286000 139012379337344 torch/_dynamo/convert_frame.py:824]   File "/usr/local/lib/python3.10/dist-packages/torch/_dynamo/convert_frame.py", line 786, in _convert_frame
W0629 11:52:51.286000 139012379337344 torch/_dynamo/convert_frame.py:824]     result = inner_convert(
W0629 11:52:51.286000 139012379337344 torch/_dynamo/convert_frame.py:824]   File "/usr/local/lib/python3.10/dist-packages/torch/_dynamo/convert_frame.py", line 400, in _convert_frame_assert
W0629 11:52:51.286000 139012379337344 torch/_dynamo/convert_frame.py:824]     return _compile(
W0629 11:52:51.286000 139012379337344 torch/_dynamo/convert_frame.py:824]   File "/usr/lib/python3.10/contextlib.py", line 79, in inner
W0629 11:52:51.286000 139012379337344 torch/_dynamo/convert_frame.py:824]     return func(*args, **kwds)
W0629 11:52:51.286000 139012379337344 torch/_dynamo/convert_frame.py:824]   File "/usr/local/lib/python3.10/dist-packages/torch/_dynamo/convert_frame.py", line 676, in _compile
W0629 11:52:51.286000 139012379337344 torch/_dynamo/convert_frame.py:824]     guarded_code = compile_inner(code, one_graph, hooks, transform)
W0629 11:52:51.286000 139012379337344 torch/_dynamo/convert_frame.py:824]   File "/usr/local/lib/python3.10/dist-packages/torch/_dynamo/utils.py", line 262, in time_wrapper
W0629 11:52:51.286000 139012379337344 torch/_dynamo/convert_frame.py:824]     r = func(*args, **kwargs)
W0629 11:52:51.286000 139012379337344 torch/_dynamo/convert_frame.py:824]   File "/usr/local/lib/python3.10/dist-packages/torch/_dynamo/convert_frame.py", line 535, in compile_inner
W0629 11:52:51.286000 139012379337344 torch/_dynamo/convert_frame.py:824]     out_code = transform_code_object(code, transform)
W0629 11:52:51.286000 139012379337344 torch/_dynamo/convert_frame.py:824]   File "/usr/local/lib/python3.10/dist-packages/torch/_dynamo/bytecode_transformation.py", line 1036, in transform_code_object
W0629 11:52:51.286000 139012379337344 torch/_dynamo/convert_frame.py:824]     transformations(instructions, code_options)
W0629 11:52:51.286000 139012379337344 torch/_dynamo/convert_frame.py:824]   File "/usr/local/lib/python3.10/dist-packages/torch/_dynamo/convert_frame.py", line 165, in _fn
W0629 11:52:51.286000 139012379337344 torch/_dynamo/convert_frame.py:824]     return fn(*args, **kwargs)
W0629 11:52:51.286000 139012379337344 torch/_dynamo/convert_frame.py:824]   File "/usr/local/lib/python3.10/dist-packages/torch/_dynamo/convert_frame.py", line 500, in transform
W0629 11:52:51.286000 139012379337344 torch/_dynamo/convert_frame.py:824]     tracer.run()
W0629 11:52:51.286000 139012379337344 torch/_dynamo/convert_frame.py:824]   File "/usr/local/lib/python3.10/dist-packages/torch/_dynamo/symbolic_convert.py", line 2149, in run
W0629 11:52:51.286000 139012379337344 torch/_dynamo/convert_frame.py:824]     super().run()
W0629 11:52:51.286000 139012379337344 torch/_dynamo/convert_frame.py:824]   File "/usr/local/lib/python3.10/dist-packages/torch/_dynamo/symbolic_convert.py", line 810, in run
W0629 11:52:51.286000 139012379337344 torch/_dynamo/convert_frame.py:824]     and self.step()
W0629 11:52:51.286000 139012379337344 torch/_dynamo/convert_frame.py:824]   File "/usr/local/lib/python3.10/dist-packages/torch/_dynamo/symbolic_convert.py", line 773, in step
W0629 11:52:51.286000 139012379337344 torch/_dynamo/convert_frame.py:824]     getattr(self, inst.opname)(inst)
W0629 11:52:51.286000 139012379337344 torch/_dynamo/convert_frame.py:824]   File "/usr/local/lib/python3.10/dist-packages/torch/_dynamo/symbolic_convert.py", line 2268, in RETURN_VALUE
W0629 11:52:51.286000 139012379337344 torch/_dynamo/convert_frame.py:824]     self.output.compile_subgraph(
W0629 11:52:51.286000 139012379337344 torch/_dynamo/convert_frame.py:824]   File "/usr/local/lib/python3.10/dist-packages/torch/_dynamo/output_graph.py", line 971, in compile_subgraph
W0629 11:52:51.286000 139012379337344 torch/_dynamo/convert_frame.py:824]     self.compile_and_call_fx_graph(tx, list(reversed(stack_values)), root)
W0629 11:52:51.286000 139012379337344 torch/_dynamo/convert_frame.py:824]   File "/usr/lib/python3.10/contextlib.py", line 79, in inner
W0629 11:52:51.286000 139012379337344 torch/_dynamo/convert_frame.py:824]     return func(*args, **kwds)
W0629 11:52:51.286000 139012379337344 torch/_dynamo/convert_frame.py:824]   File "/usr/local/lib/python3.10/dist-packages/torch/_dynamo/output_graph.py", line 1168, in compile_and_call_fx_graph
W0629 11:52:51.286000 139012379337344 torch/_dynamo/convert_frame.py:824]     compiled_fn = self.call_user_compiler(gm)
W0629 11:52:51.286000 139012379337344 torch/_dynamo/convert_frame.py:824]   File "/usr/local/lib/python3.10/dist-packages/torch/_dynamo/utils.py", line 262, in time_wrapper
W0629 11:52:51.286000 139012379337344 torch/_dynamo/convert_frame.py:824]     r = func(*args, **kwargs)
W0629 11:52:51.286000 139012379337344 torch/_dynamo/convert_frame.py:824]   File "/usr/local/lib/python3.10/dist-packages/torch/_dynamo/output_graph.py", line 1241, in call_user_compiler
W0629 11:52:51.286000 139012379337344 torch/_dynamo/convert_frame.py:824]     raise BackendCompilerFailed(self.compiler_fn, e).with_traceback(
W0629 11:52:51.286000 139012379337344 torch/_dynamo/convert_frame.py:824]   File "/usr/local/lib/python3.10/dist-packages/torch/_dynamo/output_graph.py", line 1222, in call_user_compiler
W0629 11:52:51.286000 139012379337344 torch/_dynamo/convert_frame.py:824]     compiled_fn = compiler_fn(gm, self.example_inputs())
W0629 11:52:51.286000 139012379337344 torch/_dynamo/convert_frame.py:824]   File "/usr/local/lib/python3.10/dist-packages/torch/_dynamo/repro/after_dynamo.py", line 117, in debug_wrapper
W0629 11:52:51.286000 139012379337344 torch/_dynamo/convert_frame.py:824]     compiled_gm = compiler_fn(gm, example_inputs)
W0629 11:52:51.286000 139012379337344 torch/_dynamo/convert_frame.py:824]   File "/usr/local/lib/python3.10/dist-packages/torch/__init__.py", line 1729, in __call__
W0629 11:52:51.286000 139012379337344 torch/_dynamo/convert_frame.py:824]     return compile_fx(model_, inputs_, config_patches=self.config)
W0629 11:52:51.286000 139012379337344 torch/_dynamo/convert_frame.py:824]   File "/usr/lib/python3.10/contextlib.py", line 79, in inner
W0629 11:52:51.286000 139012379337344 torch/_dynamo/convert_frame.py:824]     return func(*args, **kwds)
W0629 11:52:51.286000 139012379337344 torch/_dynamo/convert_frame.py:824]   File "/usr/local/lib/python3.10/dist-packages/torch/_inductor/compile_fx.py", line 1330, in compile_fx
W0629 11:52:51.286000 139012379337344 torch/_dynamo/convert_frame.py:824]     return aot_autograd(
W0629 11:52:51.286000 139012379337344 torch/_dynamo/convert_frame.py:824]   File "/usr/local/lib/python3.10/dist-packages/torch/_dynamo/backends/common.py", line 58, in compiler_fn
W0629 11:52:51.286000 139012379337344 torch/_dynamo/convert_frame.py:824]     cg = aot_module_simplified(gm, example_inputs, **kwargs)
W0629 11:52:51.286000 139012379337344 torch/_dynamo/convert_frame.py:824]   File "/usr/local/lib/python3.10/dist-packages/torch/_functorch/aot_autograd.py", line 903, in aot_module_simplified
W0629 11:52:51.286000 139012379337344 torch/_dynamo/convert_frame.py:824]     compiled_fn = create_aot_dispatcher_function(
W0629 11:52:51.286000 139012379337344 torch/_dynamo/convert_frame.py:824]   File "/usr/local/lib/python3.10/dist-packages/torch/_dynamo/utils.py", line 262, in time_wrapper
W0629 11:52:51.286000 139012379337344 torch/_dynamo/convert_frame.py:824]     r = func(*args, **kwargs)
W0629 11:52:51.286000 139012379337344 torch/_dynamo/convert_frame.py:824]   File "/usr/local/lib/python3.10/dist-packages/torch/_functorch/aot_autograd.py", line 628, in create_aot_dispatcher_function
W0629 11:52:51.286000 139012379337344 torch/_dynamo/convert_frame.py:824]     compiled_fn = compiler_fn(flat_fn, fake_flat_args, aot_config, fw_metadata=fw_metadata)
W0629 11:52:51.286000 139012379337344 torch/_dynamo/convert_frame.py:824]   File "/usr/local/lib/python3.10/dist-packages/torch/_functorch/_aot_autograd/runtime_wrappers.py", line 443, in aot_wrapper_dedupe
W0629 11:52:51.286000 139012379337344 torch/_dynamo/convert_frame.py:824]     return compiler_fn(flat_fn, leaf_flat_args, aot_config, fw_metadata=fw_metadata)
W0629 11:52:51.286000 139012379337344 torch/_dynamo/convert_frame.py:824]   File "/usr/local/lib/python3.10/dist-packages/torch/_functorch/_aot_autograd/runtime_wrappers.py", line 648, in aot_wrapper_synthetic_base
W0629 11:52:51.286000 139012379337344 torch/_dynamo/convert_frame.py:824]     return compiler_fn(flat_fn, flat_args, aot_config, fw_metadata=fw_metadata)
W0629 11:52:51.286000 139012379337344 torch/_dynamo/convert_frame.py:824]   File "/usr/local/lib/python3.10/dist-packages/torch/_functorch/_aot_autograd/jit_compile_runtime_wrappers.py", line 119, in aot_dispatch_base
W0629 11:52:51.286000 139012379337344 torch/_dynamo/convert_frame.py:824]     compiled_fw = compiler(fw_module, updated_flat_args)
W0629 11:52:51.286000 139012379337344 torch/_dynamo/convert_frame.py:824]   File "/usr/local/lib/python3.10/dist-packages/torch/_dynamo/utils.py", line 262, in time_wrapper
W0629 11:52:51.286000 139012379337344 torch/_dynamo/convert_frame.py:824]     r = func(*args, **kwargs)
W0629 11:52:51.286000 139012379337344 torch/_dynamo/convert_frame.py:824]   File "/usr/local/lib/python3.10/dist-packages/torch/_inductor/compile_fx.py", line 1257, in fw_compiler_base
W0629 11:52:51.286000 139012379337344 torch/_dynamo/convert_frame.py:824]     return inner_compile(
W0629 11:52:51.286000 139012379337344 torch/_dynamo/convert_frame.py:824]   File "/usr/local/lib/python3.10/dist-packages/torch/_dynamo/repro/after_aot.py", line 83, in debug_wrapper
W0629 11:52:51.286000 139012379337344 torch/_dynamo/convert_frame.py:824]     inner_compiled_fn = compiler_fn(gm, example_inputs)
W0629 11:52:51.286000 139012379337344 torch/_dynamo/convert_frame.py:824]   File "/usr/local/lib/python3.10/dist-packages/torch/_inductor/debug.py", line 304, in inner
W0629 11:52:51.286000 139012379337344 torch/_dynamo/convert_frame.py:824]     return fn(*args, **kwargs)
W0629 11:52:51.286000 139012379337344 torch/_dynamo/convert_frame.py:824]   File "/usr/lib/python3.10/contextlib.py", line 79, in inner
W0629 11:52:51.286000 139012379337344 torch/_dynamo/convert_frame.py:824]     return func(*args, **kwds)
W0629 11:52:51.286000 139012379337344 torch/_dynamo/convert_frame.py:824]   File "/usr/lib/python3.10/contextlib.py", line 79, in inner
W0629 11:52:51.286000 139012379337344 torch/_dynamo/convert_frame.py:824]     return func(*args, **kwds)
W0629 11:52:51.286000 139012379337344 torch/_dynamo/convert_frame.py:824]   File "/usr/local/lib/python3.10/dist-packages/torch/_dynamo/utils.py", line 262, in time_wrapper
W0629 11:52:51.286000 139012379337344 torch/_dynamo/convert_frame.py:824]     r = func(*args, **kwargs)
W0629 11:52:51.286000 139012379337344 torch/_dynamo/convert_frame.py:824]   File "/usr/local/lib/python3.10/dist-packages/torch/_inductor/compile_fx.py", line 438, in compile_fx_inner
W0629 11:52:51.286000 139012379337344 torch/_dynamo/convert_frame.py:824]     compiled_graph = fx_codegen_and_compile(
W0629 11:52:51.286000 139012379337344 torch/_dynamo/convert_frame.py:824]   File "/usr/local/lib/python3.10/dist-packages/torch/_inductor/compile_fx.py", line 714, in fx_codegen_and_compile
W0629 11:52:51.286000 139012379337344 torch/_dynamo/convert_frame.py:824]     compiled_fn = graph.compile_to_fn()
W0629 11:52:51.286000 139012379337344 torch/_dynamo/convert_frame.py:824]   File "/usr/local/lib/python3.10/dist-packages/torch/_inductor/graph.py", line 1307, in compile_to_fn
W0629 11:52:51.286000 139012379337344 torch/_dynamo/convert_frame.py:824]     return self.compile_to_module().call
W0629 11:52:51.286000 139012379337344 torch/_dynamo/convert_frame.py:824]   File "/usr/local/lib/python3.10/dist-packages/torch/_dynamo/utils.py", line 262, in time_wrapper
W0629 11:52:51.286000 139012379337344 torch/_dynamo/convert_frame.py:824]     r = func(*args, **kwargs)
W0629 11:52:51.286000 139012379337344 torch/_dynamo/convert_frame.py:824]   File "/usr/local/lib/python3.10/dist-packages/torch/_inductor/graph.py", line 1254, in compile_to_module
W0629 11:52:51.286000 139012379337344 torch/_dynamo/convert_frame.py:824]     mod = PyCodeCache.load_by_key_path(
W0629 11:52:51.286000 139012379337344 torch/_dynamo/convert_frame.py:824]   File "/usr/local/lib/python3.10/dist-packages/torch/_inductor/codecache.py", line 2160, in load_by_key_path
W0629 11:52:51.286000 139012379337344 torch/_dynamo/convert_frame.py:824]     exec(code, mod.__dict__, mod.__dict__)
W0629 11:52:51.286000 139012379337344 torch/_dynamo/convert_frame.py:824]   File "/tmp/torchinductor_root/lh/clhbch42q7qymhc4rliwboxfar33te5n67jkrksggxhpdncnazwn.py", line 138, in <module>
W0629 11:52:51.286000 139012379337344 torch/_dynamo/convert_frame.py:824]     async_compile.wait(globals())
W0629 11:52:51.286000 139012379337344 torch/_dynamo/convert_frame.py:824]   File "/usr/local/lib/python3.10/dist-packages/torch/_inductor/codecache.py", line 2715, in wait
W0629 11:52:51.286000 139012379337344 torch/_dynamo/convert_frame.py:824]     scope[key] = result.result()
W0629 11:52:51.286000 139012379337344 torch/_dynamo/convert_frame.py:824]   File "/usr/local/lib/python3.10/dist-packages/torch/_inductor/codecache.py", line 2522, in result
W0629 11:52:51.286000 139012379337344 torch/_dynamo/convert_frame.py:824]     self.future.result()
W0629 11:52:51.286000 139012379337344 torch/_dynamo/convert_frame.py:824]   File "/usr/lib/python3.10/concurrent/futures/_base.py", line 458, in result
W0629 11:52:51.286000 139012379337344 torch/_dynamo/convert_frame.py:824]     return self.__get_result()
W0629 11:52:51.286000 139012379337344 torch/_dynamo/convert_frame.py:824]   File "/usr/lib/python3.10/concurrent/futures/_base.py", line 403, in __get_result
W0629 11:52:51.286000 139012379337344 torch/_dynamo/convert_frame.py:824]     raise self._exception
W0629 11:52:51.286000 139012379337344 torch/_dynamo/convert_frame.py:824] torch._dynamo.exc.BackendCompilerFailed: backend='inductor' raised:
W0629 11:52:51.286000 139012379337344 torch/_dynamo/convert_frame.py:824] RuntimeError: Internal Triton PTX codegen error:
W0629 11:52:51.286000 139012379337344 torch/_dynamo/convert_frame.py:824] ptxas /tmp/compile-ptx-src-c2f441, line 59; error   : Feature '.bf16' requires .target sm_80 or higher
W0629 11:52:51.286000 139012379337344 torch/_dynamo/convert_frame.py:824] ptxas /tmp/compile-ptx-src-c2f441, line 59; error   : Feature 'cvt.bf16.f32' requires .target sm_80 or higher
W0629 11:52:51.286000 139012379337344 torch/_dynamo/convert_frame.py:824] ptxas /tmp/compile-ptx-src-c2f441, line 60; error   : Feature '.bf16' requires .target sm_80 or higher
W0629 11:52:51.286000 139012379337344 torch/_dynamo/convert_frame.py:824] ptxas /tmp/compile-ptx-src-c2f441, line 60; error   : Feature 'cvt.bf16.f32' requires .target sm_80 or higher
W0629 11:52:51.286000 139012379337344 torch/_dynamo/convert_frame.py:824] ptxas /tmp/compile-ptx-src-c2f441, line 61; error   : Feature '.bf16' requires .target sm_80 or higher
W0629 11:52:51.286000 139012379337344 torch/_dynamo/convert_frame.py:824] ptxas /tmp/compile-ptx-src-c2f441, line 61; error   : Feature 'cvt.bf16.f32' requires .target sm_80 or higher
W0629 11:52:51.286000 139012379337344 torch/_dynamo/convert_frame.py:824] ptxas /tmp/compile-ptx-src-c2f441, line 62; error   : Feature '.bf16' requires .target sm_80 or higher
W0629 11:52:51.286000 139012379337344 torch/_dynamo/convert_frame.py:824] ptxas /tmp/compile-ptx-src-c2f441, line 62; error   : Feature 'cvt.bf16.f32' requires .target sm_80 or higher
W0629 11:52:51.286000 139012379337344 torch/_dynamo/convert_frame.py:824] ptxas /tmp/compile-ptx-src-c2f441, line 63; error   : Feature '.bf16' requires .target sm_80 or higher
W0629 11:52:51.286000 139012379337344 torch/_dynamo/convert_frame.py:824] ptxas /tmp/compile-ptx-src-c2f441, line 63; error   : Feature 'cvt.bf16.f32' requires .target sm_80 or higher
W0629 11:52:51.286000 139012379337344 torch/_dynamo/convert_frame.py:824] ptxas /tmp/compile-ptx-src-c2f441, line 64; error   : Feature '.bf16' requires .target sm_80 or higher
W0629 11:52:51.286000 139012379337344 torch/_dynamo/convert_frame.py:824] ptxas /tmp/compile-ptx-src-c2f441, line 64; error   : Feature 'cvt.bf16.f32' requires .target sm_80 or higher
W0629 11:52:51.286000 139012379337344 torch/_dynamo/convert_frame.py:824] ptxas /tmp/compile-ptx-src-c2f441, line 65; error   : Feature '.bf16' requires .target sm_80 or higher
W0629 11:52:51.286000 139012379337344 torch/_dynamo/convert_frame.py:824] ptxas /tmp/compile-ptx-src-c2f441, line 65; error   : Feature 'cvt.bf16.f32' requires .target sm_80 or higher
W0629 11:52:51.286000 139012379337344 torch/_dynamo/convert_frame.py:824] ptxas /tmp/compile-ptx-src-c2f441, line 66; error   : Feature '.bf16' requires .target sm_80 or higher
W0629 11:52:51.286000 139012379337344 torch/_dynamo/convert_frame.py:824] ptxas /tmp/compile-ptx-src-c2f441, line 66; error   : Feature 'cvt.bf16.f32' requires .target sm_80 or higher
W0629 11:52:51.286000 139012379337344 torch/_dynamo/convert_frame.py:824] ptxas fatal   : Ptx assembly aborted due to errors
W0629 11:52:51.286000 139012379337344 torch/_dynamo/convert_frame.py:824]
W0629 11:52:51.286000 139012379337344 torch/_dynamo/convert_frame.py:824]
W0629 11:52:51.286000 139012379337344 torch/_dynamo/convert_frame.py:824] Set TORCH_LOGS="+dynamo" and TORCHDYNAMO_VERBOSE=1 for more information
W0629 11:52:51.286000 139012379337344 torch/_dynamo/convert_frame.py:824]
W0629 11:52:52.467000 139012379337344 torch/_dynamo/convert_frame.py:824]   File "/usr/local/lib/python3.10/dist-packages/torch/_dynamo/convert_frame.py", line 400, in _convert_frame_assert
W0629 11:52:52.467000 139012379337344 torch/_dynamo/convert_frame.py:824]     return _compile(
W0629 11:52:52.467000 139012379337344 torch/_dynamo/convert_frame.py:824]   File "/usr/lib/python3.10/contextlib.py", line 79, in inner
W0629 11:52:52.467000 139012379337344 torch/_dynamo/convert_frame.py:824]     return func(*args, **kwds)
W0629 11:52:52.467000 139012379337344 torch/_dynamo/convert_frame.py:824]   File "/usr/local/lib/python3.10/dist-packages/torch/_dynamo/convert_frame.py", line 676, in _compile
W0629 11:52:52.467000 139012379337344 torch/_dynamo/convert_frame.py:824]     guarded_code = compile_inner(code, one_graph, hooks, transform)
W0629 11:52:52.467000 139012379337344 torch/_dynamo/convert_frame.py:824]   File "/usr/local/lib/python3.10/dist-packages/torch/_dynamo/utils.py", line 262, in time_wrapper
W0629 11:52:52.467000 139012379337344 torch/_dynamo/convert_frame.py:824]     r = func(*args, **kwargs)
W0629 11:52:52.467000 139012379337344 torch/_dynamo/convert_frame.py:824]   File "/usr/local/lib/python3.10/dist-packages/torch/_dynamo/convert_frame.py", line 535, in compile_inner
W0629 11:52:52.467000 139012379337344 torch/_dynamo/convert_frame.py:824]     out_code = transform_code_object(code, transform)
W0629 11:52:52.467000 139012379337344 torch/_dynamo/convert_frame.py:824]   File "/usr/local/lib/python3.10/dist-packages/torch/_dynamo/bytecode_transformation.py", line 1036, in transform_code_object
W0629 11:52:52.467000 139012379337344 torch/_dynamo/convert_frame.py:824]     transformations(instructions, code_options)
W0629 11:52:52.467000 139012379337344 torch/_dynamo/convert_frame.py:824]   File "/usr/local/lib/python3.10/dist-packages/torch/_dynamo/convert_frame.py", line 165, in _fn
W0629 11:52:52.467000 139012379337344 torch/_dynamo/convert_frame.py:824]     return fn(*args, **kwargs)
W0629 11:52:52.467000 139012379337344 torch/_dynamo/convert_frame.py:824]   File "/usr/local/lib/python3.10/dist-packages/torch/_dynamo/convert_frame.py", line 500, in transform
W0629 11:52:52.467000 139012379337344 torch/_dynamo/convert_frame.py:824]     tracer.run()
W0629 11:52:52.467000 139012379337344 torch/_dynamo/convert_frame.py:824]   File "/usr/local/lib/python3.10/dist-packages/torch/_dynamo/symbolic_convert.py", line 2149, in run
W0629 11:52:52.467000 139012379337344 torch/_dynamo/convert_frame.py:824]     super().run()
W0629 11:52:52.467000 139012379337344 torch/_dynamo/convert_frame.py:824]   File "/usr/local/lib/python3.10/dist-packages/torch/_dynamo/symbolic_convert.py", line 810, in run
W0629 11:52:52.467000 139012379337344 torch/_dynamo/convert_frame.py:824]     and self.step()
W0629 11:52:52.467000 139012379337344 torch/_dynamo/convert_frame.py:824]   File "/usr/local/lib/python3.10/dist-packages/torch/_dynamo/symbolic_convert.py", line 773, in step
W0629 11:52:52.467000 139012379337344 torch/_dynamo/convert_frame.py:824]     getattr(self, inst.opname)(inst)
W0629 11:52:52.467000 139012379337344 torch/_dynamo/convert_frame.py:824]   File "/usr/local/lib/python3.10/dist-packages/torch/_dynamo/symbolic_convert.py", line 2268, in RETURN_VALUE
W0629 11:52:52.467000 139012379337344 torch/_dynamo/convert_frame.py:824]     self.output.compile_subgraph(
W0629 11:52:52.467000 139012379337344 torch/_dynamo/convert_frame.py:824]   File "/usr/local/lib/python3.10/dist-packages/torch/_dynamo/output_graph.py", line 971, in compile_subgraph
W0629 11:52:52.467000 139012379337344 torch/_dynamo/convert_frame.py:824]     self.compile_and_call_fx_graph(tx, list(reversed(stack_values)), root)
W0629 11:52:52.467000 139012379337344 torch/_dynamo/convert_frame.py:824]   File "/usr/lib/python3.10/contextlib.py", line 79, in inner
W0629 11:52:52.467000 139012379337344 torch/_dynamo/convert_frame.py:824]     return func(*args, **kwds)
W0629 11:52:52.467000 139012379337344 torch/_dynamo/convert_frame.py:824]   File "/usr/local/lib/python3.10/dist-packages/torch/_dynamo/output_graph.py", line 1168, in compile_and_call_fx_graph
W0629 11:52:52.467000 139012379337344 torch/_dynamo/convert_frame.py:824]     compiled_fn = self.call_user_compiler(gm)
W0629 11:52:52.467000 139012379337344 torch/_dynamo/convert_frame.py:824]   File "/usr/local/lib/python3.10/dist-packages/torch/_dynamo/utils.py", line 262, in time_wrapper
W0629 11:52:52.467000 139012379337344 torch/_dynamo/convert_frame.py:824]     r = func(*args, **kwargs)
W0629 11:52:52.467000 139012379337344 torch/_dynamo/convert_frame.py:824]   File "/usr/local/lib/python3.10/dist-packages/torch/_dynamo/output_graph.py", line 1241, in call_user_compiler
W0629 11:52:52.467000 139012379337344 torch/_dynamo/convert_frame.py:824]     raise BackendCompilerFailed(self.compiler_fn, e).with_traceback(
W0629 11:52:52.467000 139012379337344 torch/_dynamo/convert_frame.py:824]   File "/usr/local/lib/python3.10/dist-packages/torch/_dynamo/output_graph.py", line 1222, in call_user_compiler
W0629 11:52:52.467000 139012379337344 torch/_dynamo/convert_frame.py:824]     compiled_fn = compiler_fn(gm, self.example_inputs())
W0629 11:52:52.467000 139012379337344 torch/_dynamo/convert_frame.py:824]   File "/usr/local/lib/python3.10/dist-packages/torch/_dynamo/repro/after_dynamo.py", line 117, in debug_wrapper
W0629 11:52:52.467000 139012379337344 torch/_dynamo/convert_frame.py:824]     compiled_gm = compiler_fn(gm, example_inputs)
W0629 11:52:52.467000 139012379337344 torch/_dynamo/convert_frame.py:824]   File "/usr/local/lib/python3.10/dist-packages/torch/__init__.py", line 1729, in __call__
W0629 11:52:52.467000 139012379337344 torch/_dynamo/convert_frame.py:824]     return compile_fx(model_, inputs_, config_patches=self.config)
W0629 11:52:52.467000 139012379337344 torch/_dynamo/convert_frame.py:824]   File "/usr/lib/python3.10/contextlib.py", line 79, in inner
W0629 11:52:52.467000 139012379337344 torch/_dynamo/convert_frame.py:824]     return func(*args, **kwds)
W0629 11:52:52.467000 139012379337344 torch/_dynamo/convert_frame.py:824]   File "/usr/local/lib/python3.10/dist-packages/torch/_inductor/compile_fx.py", line 1330, in compile_fx
W0629 11:52:52.467000 139012379337344 torch/_dynamo/convert_frame.py:824]     return aot_autograd(
W0629 11:52:52.467000 139012379337344 torch/_dynamo/convert_frame.py:824]   File "/usr/local/lib/python3.10/dist-packages/torch/_dynamo/backends/common.py", line 58, in compiler_fn
W0629 11:52:52.467000 139012379337344 torch/_dynamo/convert_frame.py:824]     cg = aot_module_simplified(gm, example_inputs, **kwargs)
W0629 11:52:52.467000 139012379337344 torch/_dynamo/convert_frame.py:824]   File "/usr/local/lib/python3.10/dist-packages/torch/_functorch/aot_autograd.py", line 903, in aot_module_simplified
W0629 11:52:52.467000 139012379337344 torch/_dynamo/convert_frame.py:824]     compiled_fn = create_aot_dispatcher_function(
W0629 11:52:52.467000 139012379337344 torch/_dynamo/convert_frame.py:824]   File "/usr/local/lib/python3.10/dist-packages/torch/_dynamo/utils.py", line 262, in time_wrapper
W0629 11:52:52.467000 139012379337344 torch/_dynamo/convert_frame.py:824]     r = func(*args, **kwargs)
W0629 11:52:52.467000 139012379337344 torch/_dynamo/convert_frame.py:824]   File "/usr/local/lib/python3.10/dist-packages/torch/_functorch/aot_autograd.py", line 628, in create_aot_dispatcher_function
W0629 11:52:52.467000 139012379337344 torch/_dynamo/convert_frame.py:824]     compiled_fn = compiler_fn(flat_fn, fake_flat_args, aot_config, fw_metadata=fw_metadata)
W0629 11:52:52.467000 139012379337344 torch/_dynamo/convert_frame.py:824]   File "/usr/local/lib/python3.10/dist-packages/torch/_functorch/_aot_autograd/runtime_wrappers.py", line 443, in aot_wrapper_dedupe
W0629 11:52:52.467000 139012379337344 torch/_dynamo/convert_frame.py:824]     return compiler_fn(flat_fn, leaf_flat_args, aot_config, fw_metadata=fw_metadata)
W0629 11:52:52.467000 139012379337344 torch/_dynamo/convert_frame.py:824]   File "/usr/local/lib/python3.10/dist-packages/torch/_functorch/_aot_autograd/runtime_wrappers.py", line 648, in aot_wrapper_synthetic_base
W0629 11:52:52.467000 139012379337344 torch/_dynamo/convert_frame.py:824]     return compiler_fn(flat_fn, flat_args, aot_config, fw_metadata=fw_metadata)
W0629 11:52:52.467000 139012379337344 torch/_dynamo/convert_frame.py:824]   File "/usr/local/lib/python3.10/dist-packages/torch/_functorch/_aot_autograd/jit_compile_runtime_wrappers.py", line 119, in aot_dispatch_base
W0629 11:52:52.467000 139012379337344 torch/_dynamo/convert_frame.py:824]     compiled_fw = compiler(fw_module, updated_flat_args)
W0629 11:52:52.467000 139012379337344 torch/_dynamo/convert_frame.py:824]   File "/usr/local/lib/python3.10/dist-packages/torch/_dynamo/utils.py", line 262, in time_wrapper
W0629 11:52:52.467000 139012379337344 torch/_dynamo/convert_frame.py:824]     r = func(*args, **kwargs)
W0629 11:52:52.467000 139012379337344 torch/_dynamo/convert_frame.py:824]   File "/usr/local/lib/python3.10/dist-packages/torch/_inductor/compile_fx.py", line 1257, in fw_compiler_base
W0629 11:52:52.467000 139012379337344 torch/_dynamo/convert_frame.py:824]     return inner_compile(
W0629 11:52:52.467000 139012379337344 torch/_dynamo/convert_frame.py:824]   File "/usr/local/lib/python3.10/dist-packages/torch/_dynamo/repro/after_aot.py", line 83, in debug_wrapper
W0629 11:52:52.467000 139012379337344 torch/_dynamo/convert_frame.py:824]     inner_compiled_fn = compiler_fn(gm, example_inputs)
W0629 11:52:52.467000 139012379337344 torch/_dynamo/convert_frame.py:824]   File "/usr/local/lib/python3.10/dist-packages/torch/_inductor/debug.py", line 304, in inner
W0629 11:52:52.467000 139012379337344 torch/_dynamo/convert_frame.py:824]     return fn(*args, **kwargs)
W0629 11:52:52.467000 139012379337344 torch/_dynamo/convert_frame.py:824]   File "/usr/lib/python3.10/contextlib.py", line 79, in inner
W0629 11:52:52.467000 139012379337344 torch/_dynamo/convert_frame.py:824]     return func(*args, **kwds)
W0629 11:52:52.467000 139012379337344 torch/_dynamo/convert_frame.py:824]   File "/usr/lib/python3.10/contextlib.py", line 79, in inner
W0629 11:52:52.467000 139012379337344 torch/_dynamo/convert_frame.py:824]     return func(*args, **kwds)
W0629 11:52:52.467000 139012379337344 torch/_dynamo/convert_frame.py:824]   File "/usr/local/lib/python3.10/dist-packages/torch/_dynamo/utils.py", line 262, in time_wrapper
W0629 11:52:52.467000 139012379337344 torch/_dynamo/convert_frame.py:824]     r = func(*args, **kwargs)
W0629 11:52:52.467000 139012379337344 torch/_dynamo/convert_frame.py:824]   File "/usr/local/lib/python3.10/dist-packages/torch/_inductor/compile_fx.py", line 438, in compile_fx_inner
W0629 11:52:52.467000 139012379337344 torch/_dynamo/convert_frame.py:824]     compiled_graph = fx_codegen_and_compile(
W0629 11:52:52.467000 139012379337344 torch/_dynamo/convert_frame.py:824]   File "/usr/local/lib/python3.10/dist-packages/torch/_inductor/compile_fx.py", line 714, in fx_codegen_and_compile
W0629 11:52:52.467000 139012379337344 torch/_dynamo/convert_frame.py:824]     compiled_fn = graph.compile_to_fn()
W0629 11:52:52.467000 139012379337344 torch/_dynamo/convert_frame.py:824]   File "/usr/local/lib/python3.10/dist-packages/torch/_inductor/graph.py", line 1307, in compile_to_fn
W0629 11:52:52.467000 139012379337344 torch/_dynamo/convert_frame.py:824]     return self.compile_to_module().call
W0629 11:52:52.467000 139012379337344 torch/_dynamo/convert_frame.py:824]   File "/usr/local/lib/python3.10/dist-packages/torch/_dynamo/utils.py", line 262, in time_wrapper
W0629 11:52:52.467000 139012379337344 torch/_dynamo/convert_frame.py:824]     r = func(*args, **kwargs)
W0629 11:52:52.467000 139012379337344 torch/_dynamo/convert_frame.py:824]   File "/usr/local/lib/python3.10/dist-packages/torch/_inductor/graph.py", line 1254, in compile_to_module
W0629 11:52:52.467000 139012379337344 torch/_dynamo/convert_frame.py:824]     mod = PyCodeCache.load_by_key_path(
W0629 11:52:52.467000 139012379337344 torch/_dynamo/convert_frame.py:824]   File "/usr/local/lib/python3.10/dist-packages/torch/_inductor/codecache.py", line 2160, in load_by_key_path
W0629 11:52:52.467000 139012379337344 torch/_dynamo/convert_frame.py:824]     exec(code, mod.__dict__, mod.__dict__)
W0629 11:52:52.467000 139012379337344 torch/_dynamo/convert_frame.py:824]   File "/tmp/torchinductor_root/q4/cq4nwrcp64c6u23yedbfawpak3m7k2hjlkgk5nzco4am3rvkxlaa.py", line 142, in <module>
W0629 11:52:52.467000 139012379337344 torch/_dynamo/convert_frame.py:824]     async_compile.wait(globals())
W0629 11:52:52.467000 139012379337344 torch/_dynamo/convert_frame.py:824]   File "/usr/local/lib/python3.10/dist-packages/torch/_inductor/codecache.py", line 2715, in wait
W0629 11:52:52.467000 139012379337344 torch/_dynamo/convert_frame.py:824]     scope[key] = result.result()
W0629 11:52:52.467000 139012379337344 torch/_dynamo/convert_frame.py:824]   File "/usr/local/lib/python3.10/dist-packages/torch/_inductor/codecache.py", line 2522, in result
W0629 11:52:52.467000 139012379337344 torch/_dynamo/convert_frame.py:824]     self.future.result()
W0629 11:52:52.467000 139012379337344 torch/_dynamo/convert_frame.py:824]   File "/usr/lib/python3.10/concurrent/futures/_base.py", line 458, in result
W0629 11:52:52.467000 139012379337344 torch/_dynamo/convert_frame.py:824]     return self.__get_result()
W0629 11:52:52.467000 139012379337344 torch/_dynamo/convert_frame.py:824]   File "/usr/lib/python3.10/concurrent/futures/_base.py", line 403, in __get_result
W0629 11:52:52.467000 139012379337344 torch/_dynamo/convert_frame.py:824]     raise self._exception
W0629 11:52:52.467000 139012379337344 torch/_dynamo/convert_frame.py:824] torch._dynamo.exc.BackendCompilerFailed: backend='inductor' raised:
W0629 11:52:52.467000 139012379337344 torch/_dynamo/convert_frame.py:824] RuntimeError: Internal Triton PTX codegen error:
W0629 11:52:52.467000 139012379337344 torch/_dynamo/convert_frame.py:824] ptxas /tmp/compile-ptx-src-dd7b75, line 60; error   : Feature '.bf16' requires .target sm_80 or higher
W0629 11:52:52.467000 139012379337344 torch/_dynamo/convert_frame.py:824] ptxas /tmp/compile-ptx-src-dd7b75, line 60; error   : Feature 'cvt.bf16.f32' requires .target sm_80 or higher
W0629 11:52:52.467000 139012379337344 torch/_dynamo/convert_frame.py:824] ptxas /tmp/compile-ptx-src-dd7b75, line 61; error   : Feature '.bf16' requires .target sm_80 or higher
W0629 11:52:52.467000 139012379337344 torch/_dynamo/convert_frame.py:824] ptxas /tmp/compile-ptx-src-dd7b75, line 61; error   : Feature 'cvt.bf16.f32' requires .target sm_80 or higher
W0629 11:52:52.467000 139012379337344 torch/_dynamo/convert_frame.py:824] ptxas fatal   : Ptx assembly aborted due to errors
W0629 11:52:52.467000 139012379337344 torch/_dynamo/convert_frame.py:824]
W0629 11:52:52.467000 139012379337344 torch/_dynamo/convert_frame.py:824]
W0629 11:52:52.467000 139012379337344 torch/_dynamo/convert_frame.py:824] Set TORCH_LOGS="+dynamo" and TORCHDYNAMO_VERBOSE=1 for more information
W0629 11:52:52.467000 139012379337344 torch/_dynamo/convert_frame.py:824]
W0629 11:52:52.467000 139012379337344 torch/_dynamo/convert_frame.py:824] Traceback (most recent call last):
W0629 11:52:52.467000 139012379337344 torch/_dynamo/convert_frame.py:824]   File "/usr/local/lib/python3.10/dist-packages/torch/_dynamo/convert_frame.py", line 786, in _convert_frame
W0629 11:52:52.467000 139012379337344 torch/_dynamo/convert_frame.py:824]     result = inner_convert(
W0629 11:52:52.467000 139012379337344 torch/_dynamo/convert_frame.py:824]   File "/usr/local/lib/python3.10/dist-packages/torch/_dynamo/convert_frame.py", line 400, in _convert_frame_assert
W0629 11:52:52.467000 139012379337344 torch/_dynamo/convert_frame.py:824]     return _compile(
W0629 11:52:52.467000 139012379337344 torch/_dynamo/convert_frame.py:824]   File "/usr/lib/python3.10/contextlib.py", line 79, in inner
W0629 11:52:52.467000 139012379337344 torch/_dynamo/convert_frame.py:824]     return func(*args, **kwds)
W0629 11:52:52.467000 139012379337344 torch/_dynamo/convert_frame.py:824]   File "/usr/local/lib/python3.10/dist-packages/torch/_dynamo/convert_frame.py", line 676, in _compile
W0629 11:52:52.467000 139012379337344 torch/_dynamo/convert_frame.py:824]     guarded_code = compile_inner(code, one_graph, hooks, transform)
W0629 11:52:52.467000 139012379337344 torch/_dynamo/convert_frame.py:824]   File "/usr/local/lib/python3.10/dist-packages/torch/_dynamo/utils.py", line 262, in time_wrapper
W0629 11:52:52.467000 139012379337344 torch/_dynamo/convert_frame.py:824]     r = func(*args, **kwargs)
W0629 11:52:52.467000 139012379337344 torch/_dynamo/convert_frame.py:824]   File "/usr/local/lib/python3.10/dist-packages/torch/_dynamo/convert_frame.py", line 535, in compile_inner
W0629 11:52:52.467000 139012379337344 torch/_dynamo/convert_frame.py:824]     out_code = transform_code_object(code, transform)
W0629 11:52:52.467000 139012379337344 torch/_dynamo/convert_frame.py:824]   File "/usr/local/lib/python3.10/dist-packages/torch/_dynamo/bytecode_transformation.py", line 1036, in transform_code_object
W0629 11:52:52.467000 139012379337344 torch/_dynamo/convert_frame.py:824]     transformations(instructions, code_options)
W0629 11:52:52.467000 139012379337344 torch/_dynamo/convert_frame.py:824]   File "/usr/local/lib/python3.10/dist-packages/torch/_dynamo/convert_frame.py", line 165, in _fn
W0629 11:52:52.467000 139012379337344 torch/_dynamo/convert_frame.py:824]     return fn(*args, **kwargs)
W0629 11:52:52.467000 139012379337344 torch/_dynamo/convert_frame.py:824]   File "/usr/local/lib/python3.10/dist-packages/torch/_dynamo/convert_frame.py", line 500, in transform
W0629 11:52:52.467000 139012379337344 torch/_dynamo/convert_frame.py:824]     tracer.run()
W0629 11:52:52.467000 139012379337344 torch/_dynamo/convert_frame.py:824]   File "/usr/local/lib/python3.10/dist-packages/torch/_dynamo/symbolic_convert.py", line 2149, in run
W0629 11:52:52.467000 139012379337344 torch/_dynamo/convert_frame.py:824]     super().run()
W0629 11:52:52.467000 139012379337344 torch/_dynamo/convert_frame.py:824]   File "/usr/local/lib/python3.10/dist-packages/torch/_dynamo/symbolic_convert.py", line 810, in run
W0629 11:52:52.467000 139012379337344 torch/_dynamo/convert_frame.py:824]     and self.step()
W0629 11:52:52.467000 139012379337344 torch/_dynamo/convert_frame.py:824]   File "/usr/local/lib/python3.10/dist-packages/torch/_dynamo/symbolic_convert.py", line 773, in step
W0629 11:52:52.467000 139012379337344 torch/_dynamo/convert_frame.py:824]     getattr(self, inst.opname)(inst)
W0629 11:52:52.467000 139012379337344 torch/_dynamo/convert_frame.py:824]   File "/usr/local/lib/python3.10/dist-packages/torch/_dynamo/symbolic_convert.py", line 2268, in RETURN_VALUE
W0629 11:52:52.467000 139012379337344 torch/_dynamo/convert_frame.py:824]     self.output.compile_subgraph(
W0629 11:52:52.467000 139012379337344 torch/_dynamo/convert_frame.py:824]   File "/usr/local/lib/python3.10/dist-packages/torch/_dynamo/output_graph.py", line 971, in compile_subgraph
W0629 11:52:52.467000 139012379337344 torch/_dynamo/convert_frame.py:824]     self.compile_and_call_fx_graph(tx, list(reversed(stack_values)), root)
W0629 11:52:52.467000 139012379337344 torch/_dynamo/convert_frame.py:824]   File "/usr/lib/python3.10/contextlib.py", line 79, in inner
W0629 11:52:52.467000 139012379337344 torch/_dynamo/convert_frame.py:824]     return func(*args, **kwds)
W0629 11:52:52.467000 139012379337344 torch/_dynamo/convert_frame.py:824]   File "/usr/local/lib/python3.10/dist-packages/torch/_dynamo/output_graph.py", line 1168, in compile_and_call_fx_graph
W0629 11:52:52.467000 139012379337344 torch/_dynamo/convert_frame.py:824]     compiled_fn = self.call_user_compiler(gm)
W0629 11:52:52.467000 139012379337344 torch/_dynamo/convert_frame.py:824]   File "/usr/local/lib/python3.10/dist-packages/torch/_dynamo/utils.py", line 262, in time_wrapper
W0629 11:52:52.467000 139012379337344 torch/_dynamo/convert_frame.py:824]     r = func(*args, **kwargs)
W0629 11:52:52.467000 139012379337344 torch/_dynamo/convert_frame.py:824]   File "/usr/local/lib/python3.10/dist-packages/torch/_dynamo/output_graph.py", line 1241, in call_user_compiler
W0629 11:52:52.467000 139012379337344 torch/_dynamo/convert_frame.py:824]     raise BackendCompilerFailed(self.compiler_fn, e).with_traceback(
W0629 11:52:52.467000 139012379337344 torch/_dynamo/convert_frame.py:824]   File "/usr/local/lib/python3.10/dist-packages/torch/_dynamo/output_graph.py", line 1222, in call_user_compiler
W0629 11:52:52.467000 139012379337344 torch/_dynamo/convert_frame.py:824]     compiled_fn = compiler_fn(gm, self.example_inputs())
W0629 11:52:52.467000 139012379337344 torch/_dynamo/convert_frame.py:824]   File "/usr/local/lib/python3.10/dist-packages/torch/_dynamo/repro/after_dynamo.py", line 117, in debug_wrapper
W0629 11:52:52.467000 139012379337344 torch/_dynamo/convert_frame.py:824]     compiled_gm = compiler_fn(gm, example_inputs)
W0629 11:52:52.467000 139012379337344 torch/_dynamo/convert_frame.py:824]   File "/usr/local/lib/python3.10/dist-packages/torch/__init__.py", line 1729, in __call__
W0629 11:52:52.467000 139012379337344 torch/_dynamo/convert_frame.py:824]     return compile_fx(model_, inputs_, config_patches=self.config)
W0629 11:52:52.467000 139012379337344 torch/_dynamo/convert_frame.py:824]   File "/usr/lib/python3.10/contextlib.py", line 79, in inner
W0629 11:52:52.467000 139012379337344 torch/_dynamo/convert_frame.py:824]     return func(*args, **kwds)
W0629 11:52:52.467000 139012379337344 torch/_dynamo/convert_frame.py:824]   File "/usr/local/lib/python3.10/dist-packages/torch/_inductor/compile_fx.py", line 1330, in compile_fx
W0629 11:52:52.467000 139012379337344 torch/_dynamo/convert_frame.py:824]     return aot_autograd(
W0629 11:52:52.467000 139012379337344 torch/_dynamo/convert_frame.py:824]   File "/usr/local/lib/python3.10/dist-packages/torch/_dynamo/backends/common.py", line 58, in compiler_fn
W0629 11:52:52.467000 139012379337344 torch/_dynamo/convert_frame.py:824]     cg = aot_module_simplified(gm, example_inputs, **kwargs)
W0629 11:52:52.467000 139012379337344 torch/_dynamo/convert_frame.py:824]   File "/usr/local/lib/python3.10/dist-packages/torch/_functorch/aot_autograd.py", line 903, in aot_module_simplified
W0629 11:52:52.467000 139012379337344 torch/_dynamo/convert_frame.py:824]     compiled_fn = create_aot_dispatcher_function(
W0629 11:52:52.467000 139012379337344 torch/_dynamo/convert_frame.py:824]   File "/usr/local/lib/python3.10/dist-packages/torch/_dynamo/utils.py", line 262, in time_wrapper
W0629 11:52:52.467000 139012379337344 torch/_dynamo/convert_frame.py:824]     r = func(*args, **kwargs)
W0629 11:52:52.467000 139012379337344 torch/_dynamo/convert_frame.py:824]   File "/usr/local/lib/python3.10/dist-packages/torch/_functorch/aot_autograd.py", line 628, in create_aot_dispatcher_function
W0629 11:52:52.467000 139012379337344 torch/_dynamo/convert_frame.py:824]     compiled_fn = compiler_fn(flat_fn, fake_flat_args, aot_config, fw_metadata=fw_metadata)
W0629 11:52:52.467000 139012379337344 torch/_dynamo/convert_frame.py:824]   File "/usr/local/lib/python3.10/dist-packages/torch/_functorch/_aot_autograd/runtime_wrappers.py", line 443, in aot_wrapper_dedupe
W0629 11:52:52.467000 139012379337344 torch/_dynamo/convert_frame.py:824]     return compiler_fn(flat_fn, leaf_flat_args, aot_config, fw_metadata=fw_metadata)
W0629 11:52:52.467000 139012379337344 torch/_dynamo/convert_frame.py:824]   File "/usr/local/lib/python3.10/dist-packages/torch/_functorch/_aot_autograd/runtime_wrappers.py", line 648, in aot_wrapper_synthetic_base
W0629 11:52:52.467000 139012379337344 torch/_dynamo/convert_frame.py:824]     return compiler_fn(flat_fn, flat_args, aot_config, fw_metadata=fw_metadata)
W0629 11:52:52.467000 139012379337344 torch/_dynamo/convert_frame.py:824]   File "/usr/local/lib/python3.10/dist-packages/torch/_functorch/_aot_autograd/jit_compile_runtime_wrappers.py", line 119, in aot_dispatch_base
W0629 11:52:52.467000 139012379337344 torch/_dynamo/convert_frame.py:824]     compiled_fw = compiler(fw_module, updated_flat_args)
W0629 11:52:52.467000 139012379337344 torch/_dynamo/convert_frame.py:824]   File "/usr/local/lib/python3.10/dist-packages/torch/_dynamo/utils.py", line 262, in time_wrapper
W0629 11:52:52.467000 139012379337344 torch/_dynamo/convert_frame.py:824]     r = func(*args, **kwargs)
W0629 11:52:52.467000 139012379337344 torch/_dynamo/convert_frame.py:824]   File "/usr/local/lib/python3.10/dist-packages/torch/_inductor/compile_fx.py", line 1257, in fw_compiler_base
W0629 11:52:52.467000 139012379337344 torch/_dynamo/convert_frame.py:824]     return inner_compile(
W0629 11:52:52.467000 139012379337344 torch/_dynamo/convert_frame.py:824]   File "/usr/local/lib/python3.10/dist-packages/torch/_dynamo/repro/after_aot.py", line 83, in debug_wrapper
W0629 11:52:52.467000 139012379337344 torch/_dynamo/convert_frame.py:824]     inner_compiled_fn = compiler_fn(gm, example_inputs)
W0629 11:52:52.467000 139012379337344 torch/_dynamo/convert_frame.py:824]   File "/usr/local/lib/python3.10/dist-packages/torch/_inductor/debug.py", line 304, in inner
W0629 11:52:52.467000 139012379337344 torch/_dynamo/convert_frame.py:824]     return fn(*args, **kwargs)
W0629 11:52:52.467000 139012379337344 torch/_dynamo/convert_frame.py:824]   File "/usr/lib/python3.10/contextlib.py", line 79, in inner
W0629 11:52:52.467000 139012379337344 torch/_dynamo/convert_frame.py:824]     return func(*args, **kwds)
W0629 11:52:52.467000 139012379337344 torch/_dynamo/convert_frame.py:824]   File "/usr/lib/python3.10/contextlib.py", line 79, in inner
W0629 11:52:52.467000 139012379337344 torch/_dynamo/convert_frame.py:824]     return func(*args, **kwds)
W0629 11:52:52.467000 139012379337344 torch/_dynamo/convert_frame.py:824]   File "/usr/local/lib/python3.10/dist-packages/torch/_dynamo/utils.py", line 262, in time_wrapper
W0629 11:52:52.467000 139012379337344 torch/_dynamo/convert_frame.py:824]     r = func(*args, **kwargs)
W0629 11:52:52.467000 139012379337344 torch/_dynamo/convert_frame.py:824]   File "/usr/local/lib/python3.10/dist-packages/torch/_inductor/compile_fx.py", line 438, in compile_fx_inner
W0629 11:52:52.467000 139012379337344 torch/_dynamo/convert_frame.py:824]     compiled_graph = fx_codegen_and_compile(
W0629 11:52:52.467000 139012379337344 torch/_dynamo/convert_frame.py:824]   File "/usr/local/lib/python3.10/dist-packages/torch/_inductor/compile_fx.py", line 714, in fx_codegen_and_compile
W0629 11:52:52.467000 139012379337344 torch/_dynamo/convert_frame.py:824]     compiled_fn = graph.compile_to_fn()
W0629 11:52:52.467000 139012379337344 torch/_dynamo/convert_frame.py:824]   File "/usr/local/lib/python3.10/dist-packages/torch/_inductor/graph.py", line 1307, in compile_to_fn
W0629 11:52:52.467000 139012379337344 torch/_dynamo/convert_frame.py:824]     return self.compile_to_module().call
W0629 11:52:52.467000 139012379337344 torch/_dynamo/convert_frame.py:824]   File "/usr/local/lib/python3.10/dist-packages/torch/_dynamo/utils.py", line 262, in time_wrapper
W0629 11:52:52.467000 139012379337344 torch/_dynamo/convert_frame.py:824]     r = func(*args, **kwargs)
W0629 11:52:52.467000 139012379337344 torch/_dynamo/convert_frame.py:824]   File "/usr/local/lib/python3.10/dist-packages/torch/_inductor/graph.py", line 1254, in compile_to_module
W0629 11:52:52.467000 139012379337344 torch/_dynamo/convert_frame.py:824]     mod = PyCodeCache.load_by_key_path(
W0629 11:52:52.467000 139012379337344 torch/_dynamo/convert_frame.py:824]   File "/usr/local/lib/python3.10/dist-packages/torch/_inductor/codecache.py", line 2160, in load_by_key_path
W0629 11:52:52.467000 139012379337344 torch/_dynamo/convert_frame.py:824]     exec(code, mod.__dict__, mod.__dict__)
W0629 11:52:52.467000 139012379337344 torch/_dynamo/convert_frame.py:824]   File "/tmp/torchinductor_root/q4/cq4nwrcp64c6u23yedbfawpak3m7k2hjlkgk5nzco4am3rvkxlaa.py", line 142, in <module>
W0629 11:52:52.467000 139012379337344 torch/_dynamo/convert_frame.py:824]     async_compile.wait(globals())
W0629 11:52:52.467000 139012379337344 torch/_dynamo/convert_frame.py:824]   File "/usr/local/lib/python3.10/dist-packages/torch/_inductor/codecache.py", line 2715, in wait
W0629 11:52:52.467000 139012379337344 torch/_dynamo/convert_frame.py:824]     scope[key] = result.result()
W0629 11:52:52.467000 139012379337344 torch/_dynamo/convert_frame.py:824]   File "/usr/local/lib/python3.10/dist-packages/torch/_inductor/codecache.py", line 2522, in result
W0629 11:52:52.467000 139012379337344 torch/_dynamo/convert_frame.py:824]     self.future.result()
W0629 11:52:52.467000 139012379337344 torch/_dynamo/convert_frame.py:824]   File "/usr/lib/python3.10/concurrent/futures/_base.py", line 458, in result
W0629 11:52:52.467000 139012379337344 torch/_dynamo/convert_frame.py:824]     return self.__get_result()
W0629 11:52:52.467000 139012379337344 torch/_dynamo/convert_frame.py:824]   File "/usr/lib/python3.10/concurrent/futures/_base.py", line 403, in __get_result
W0629 11:52:52.467000 139012379337344 torch/_dynamo/convert_frame.py:824]     raise self._exception
W0629 11:52:52.467000 139012379337344 torch/_dynamo/convert_frame.py:824] torch._dynamo.exc.BackendCompilerFailed: backend='inductor' raised:
W0629 11:52:52.467000 139012379337344 torch/_dynamo/convert_frame.py:824] RuntimeError: Internal Triton PTX codegen error:
W0629 11:52:52.467000 139012379337344 torch/_dynamo/convert_frame.py:824] ptxas /tmp/compile-ptx-src-dd7b75, line 60; error   : Feature '.bf16' requires .target sm_80 or higher
W0629 11:52:52.467000 139012379337344 torch/_dynamo/convert_frame.py:824] ptxas /tmp/compile-ptx-src-dd7b75, line 60; error   : Feature 'cvt.bf16.f32' requires .target sm_80 or higher
W0629 11:52:52.467000 139012379337344 torch/_dynamo/convert_frame.py:824] ptxas /tmp/compile-ptx-src-dd7b75, line 61; error   : Feature '.bf16' requires .target sm_80 or higher
W0629 11:52:52.467000 139012379337344 torch/_dynamo/convert_frame.py:824] ptxas /tmp/compile-ptx-src-dd7b75, line 61; error   : Feature 'cvt.bf16.f32' requires .target sm_80 or higher
W0629 11:52:52.467000 139012379337344 torch/_dynamo/convert_frame.py:824] ptxas fatal   : Ptx assembly aborted due to errors
W0629 11:52:52.467000 139012379337344 torch/_dynamo/convert_frame.py:824]
W0629 11:52:52.467000 139012379337344 torch/_dynamo/convert_frame.py:824]
W0629 11:52:52.467000 139012379337344 torch/_dynamo/convert_frame.py:824] Set TORCH_LOGS="+dynamo" and TORCHDYNAMO_VERBOSE=1 for more information
W0629 11:52:52.467000 139012379337344 torch/_dynamo/convert_frame.py:824]
  9%|         | 1/11 [00:59<09:51, 59.18s/it]
W0629 11:52:52.467000 139012379337344 torch/_dynamo/convert_frame.py:824] WON'T CONVERT forward /usr/local/lib/python3.10/dist-packages/transformers/models/bert/modeling_bert.py line 738
W0629 11:52:52.467000 139012379337344 torch/_dynamo/convert_frame.py:824] due to:
W0629 11:52:52.467000 139012379337344 torch/_dynamo/convert_frame.py:824] Traceback (most recent call last):
W0629 11:52:52.467000 139012379337344 torch/_dynamo/convert_frame.py:824]   File "/usr/local/lib/python3.10/dist-packages/torch/_dynamo/convert_frame.py", line 786, in _convert_frame
W0629 11:52:52.467000 139012379337344 torch/_dynamo/convert_frame.py:824]     result = inner_convert(
W0629 11:52:52.467000 139012379337344 torch/_dynamo/convert_frame.py:824]     result = inner_convert(
W0629 11:52:52.467000 139012379337344 torch/_dynamo/convert_frame.py:824]     result = inner_convert(
epoch: None 	loss: 0.065260   	accuracy: 0.2575 	precision: 0.1848 	recall: 0.2304 	f1: 0.1233
W0629 11:52:52.467000 139012379337344 torch/_dynamo/convert_frame.py:824]     result = inner_convert(
out(train): SequenceClassifierOutput(loss=tensor(1.7158, device='cuda:0', grad_fn=<NllLossBackward0>), logits=tensor([[ 0.2119,  0.2139,  0.5117,  0.1216, -0.4727],
        [ 0.3184, -0.1260, -0.6289, -0.4844,  0.3262],
        [-0.4648, -0.2412,  0.2402, -0.3086, -0.6016],
        [-0.3750, -0.1631, -0.0498, -0.2197, -1.1719],
        [-0.1201, -0.1108, -0.3301, -0.1650,  0.2148],
        [-0.2256, -0.0938,  0.4902, -0.2871, -0.4453],
        [-0.0742, -0.2139, -0.0835, -0.3203, -0.2930],
        [ 0.1602,  0.1543, -0.0544, -0.4609, -0.3008],
        [-0.1758, -0.1621,  0.1777, -0.2451, -0.3164],
        [-0.3301,  0.2480,  0.2070, -0.1445, -0.2617],
        [-0.2949, -0.0349,  0.4629,  0.1099, -0.2812],
        [-0.2266,  0.1289,  0.3359, -0.1533, -0.4707],
        [ 0.2871,  0.0664, -0.1514, -0.3848,  0.1318],
        [-0.1250, -0.1338, -0.3398, -0.5547, -0.4004],
        [-0.1826,  0.2422, -0.0088, -0.2793, -0.4492],
        [ 0.0520,  0.5625,  0.3359, -0.0928, -0.3438]], device='cuda:0',
       dtype=torch.bfloat16, grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)
out(train): SequenceClassifierOutput(loss=tensor(1.6978, device='cuda:0', grad_fn=<NllLossBackward0>), logits=tensor([[ 0.2852,  0.1670,  0.1226, -0.2285, -0.3457],
        [-0.5195, -0.0344,  0.5039, -0.3438, -0.2715],
        [ 0.0410,  0.0815,  0.1050,  0.0073, -0.2246],
        [-0.2266, -0.0192, -0.0267, -0.2695, -0.3555],
        [-0.0757,  0.1299,  0.1641,  0.0786, -0.4570],
        [ 0.0188, -0.3164, -0.1133, -0.3945, -0.3105],
        [-0.3184,  0.1187,  0.2383, -0.1885, -0.3340],
        [ 0.1328,  0.2168,  0.2598, -0.4395, -0.4668],
        [-0.1094, -0.1279,  0.0386, -0.4355, -0.2930],
        [-0.3633, -0.1235,  0.0139, -0.3730, -0.3652],
        [-0.3848,  0.0210,  0.0520, -0.2754, -0.0388],
        [-0.4492,  0.3457,  0.4453, -0.2412, -0.3184],
        [ 0.2109, -0.5234, -0.6758, -0.1650,  0.2656],
        [ 0.2891, -0.2520, -0.4043, -0.3730,  0.0894],
        [-0.2041,  0.0117,  0.4941,  0.1553, -0.1777],
        [-0.3086,  0.1562,  0.5117, -0.1006, -0.0854]], device='cuda:0',
       dtype=torch.bfloat16, grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)
out(train): SequenceClassifierOutput(loss=tensor(1.5737, device='cuda:0', grad_fn=<NllLossBackward0>), logits=tensor([[ 0.1299, -0.2207, -0.3652, -0.3613, -0.0554],
        [-0.4219,  0.3398,  0.0542, -0.2002, -0.2988],
        [ 0.0840, -0.1650,  0.0474, -0.3809, -0.1348],
        [-0.0635, -0.0221,  0.2500, -0.1143,  0.0251],
        [-0.0698,  0.2373,  0.2217, -0.4199, -0.2354],
        [ 0.4785, -0.2451, -0.5039, -0.3164,  0.2891],
        [-0.2930,  0.3320,  0.0771, -0.0542, -0.4629],
        [-0.0913,  0.2285,  0.6797,  0.2139, -0.2793],
        [-0.1216, -0.3555,  0.3379,  0.2910, -0.5586],
        [-0.6055,  0.1187,  0.7266,  0.0845, -0.7461],
        [-0.2520,  0.0454,  0.2061, -0.0674, -0.1445],
        [ 0.2188, -0.5859, -0.3242, -0.3711,  0.3027],
        [ 0.0635,  0.0505,  0.4609, -0.0605, -0.2773],
        [-0.2812, -0.1455,  0.2285, -0.2617, -0.3535],
        [ 0.0410, -0.0117,  0.5859, -0.4395, -0.3574],
        [ 0.5469, -0.2773, -0.6602, -0.1777,  0.3574]], device='cuda:0',
       dtype=torch.bfloat16, grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)
out(train): SequenceClassifierOutput(loss=tensor(1.6963, device='cuda:0', grad_fn=<NllLossBackward0>), logits=tensor([[-4.3945e-01, -7.8613e-02,  1.5137e-01, -3.8672e-01, -1.7773e-01],
        [-2.5391e-01,  1.0400e-01,  3.5352e-01, -2.0508e-01,  7.1289e-02],
        [-1.2012e-01, -3.3203e-02, -7.7820e-04, -2.4414e-01, -2.2852e-01],
        [-1.6211e-01,  1.5625e-01, -8.0078e-02, -6.0303e-02, -2.4121e-01],
        [-6.4844e-01, -7.1777e-02,  4.6875e-01, -3.4375e-01, -3.1445e-01],
        [ 7.8125e-02,  2.7930e-01,  5.0391e-01, -1.4258e-01, -7.1484e-01],
        [ 5.5908e-02,  1.1182e-01,  1.9434e-01, -2.3145e-01, -1.4746e-01],
        [-3.0469e-01,  4.0039e-02,  7.8125e-02, -2.6953e-01, -1.4355e-01],
        [ 8.5938e-02, -3.9648e-01, -2.4292e-02, -1.6895e-01,  1.8848e-01],
        [ 1.3965e-01,  8.5449e-02,  2.5391e-01, -2.0703e-01, -1.6895e-01],
        [-3.7305e-01,  2.1191e-01, -2.6758e-01, -3.3789e-01,  1.0059e-01],
        [ 3.5553e-03, -1.0449e-01, -1.3184e-01, -1.9531e-01, -8.4766e-01],
        [-8.4473e-02, -2.6562e-01, -2.1484e-01,  7.7820e-03,  6.2988e-02],
        [-1.2695e-01, -1.2573e-02,  4.1211e-01,  9.2773e-02, -2.8711e-01],
        [ 3.8086e-02, -9.4727e-02,  3.5938e-01, -2.5000e-01, -6.3281e-01],
        [ 6.6895e-02, -2.6172e-01, -6.5918e-02, -1.3086e-01, -2.8516e-01]],
       device='cuda:0', dtype=torch.bfloat16, grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)
W0629 11:52:52.467000 139012379337344 torch/_dynamo/convert_frame.py:824]     result = inner_convert(
out(train): SequenceClassifierOutput(loss=tensor(1.6802, device='cuda:0', grad_fn=<NllLossBackward0>), logits=tensor([[-0.3281,  0.0972,  0.3555, -0.2598, -0.1973],
        [ 0.0410, -0.0593,  0.4805,  0.0850, -0.1060],
        [-0.2051, -0.0718,  0.2012, -0.2500, -0.2695],
        [-0.3398, -0.0070,  0.4238, -0.2061, -0.2715],
        [-0.2158, -0.2852,  0.4980, -0.2119, -0.3164],
        [ 0.3926, -0.3008, -0.6094, -0.1719,  0.2969],
        [-0.2422,  0.1118,  0.0011, -0.6211, -0.1572],
        [ 0.2734, -0.3906, -0.5352, -0.0259,  0.5078],
        [ 0.2031, -0.4414, -0.4121, -0.3125,  0.5898],
        [ 0.0264, -0.0708,  0.0172, -0.3828, -0.6406],
        [ 0.0014, -0.3516,  0.2559, -0.2734, -0.5859],
        [-0.0630, -0.1543, -0.0239, -0.1914, -0.5586],
        [-0.2178,  0.5078,  0.3301, -0.3652, -0.2754],
        [ 0.0188,  0.0708,  0.1504, -0.4277, -0.2656],
        [-0.3223, -0.1816,  0.1475, -0.4219, -0.4336],
        [-0.2891,  0.1074,  0.0121, -0.1270, -0.6133]], device='cuda:0',
       dtype=torch.bfloat16, grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)
out(train): SequenceClassifierOutput(loss=tensor(1.6553, device='cuda:0', grad_fn=<NllLossBackward0>), logits=tensor([[-0.1787, -0.0559,  0.5508,  0.0693, -0.4395],
        [ 0.0100,  0.1621,  0.2373, -0.1846, -0.1562],
        [ 0.1533,  0.1245,  0.1914, -0.0605, -0.0286],
        [-0.2031,  0.2109, -0.0270, -0.3750, -0.4570],
        [-0.2930, -0.1680,  0.0718,  0.1455, -0.3789],
        [-0.0422,  0.0243,  0.2402, -0.3574, -0.1377],
        [ 0.0221, -0.1099,  0.2598,  0.0771, -0.8008],
        [-0.3047,  0.2305,  0.5664, -0.0918, -0.6523],
        [-0.3164, -0.1074, -0.0811, -0.1807, -0.8672],
        [-0.2285, -0.1396,  0.4434, -0.1021, -0.7109],
        [ 0.1123, -0.5156, -0.0327, -0.4531, -0.3242],
        [-0.5430,  0.1611,  0.3770, -0.1719, -0.5820],
        [ 0.1074,  0.0967,  0.7812, -0.0391, -0.6602],
        [-0.2266, -0.1807,  0.0718, -0.4238, -0.7031],
        [-0.5547, -0.4062,  0.2021, -0.2393, -0.6953],
        [-0.4707,  0.1592,  0.2988, -0.2119, -0.7500]], device='cuda:0',
       dtype=torch.bfloat16, grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)
out(train): SequenceClassifierOutput(loss=tensor(1.6768, device='cuda:0', grad_fn=<NllLossBackward0>), logits=tensor([[-0.1660, -0.3633,  0.1235, -0.4199, -0.2031],
        [-0.5000,  0.3262,  0.1396, -0.6016, -1.1094],
        [-0.4766, -0.0693,  0.6289, -0.1377, -0.6719],
        [ 0.0184,  0.0120,  0.4238, -0.1196, -0.6445],
        [-0.1982, -0.1621,  0.4102, -0.2422, -0.3613],
        [-0.1787,  0.3145,  0.5820,  0.2188, -0.9062],
        [ 0.0259, -0.1650,  0.7812,  0.0544, -0.9141],
        [-0.5625,  0.0583,  0.3457,  0.1094, -0.7695],
        [-0.2129, -0.2178,  0.3418, -0.0986, -0.6445],
        [-0.3047,  0.1709,  0.6094, -0.1201, -0.8789],
        [-0.1943, -0.0618,  0.2490, -0.2041, -0.5430],
        [-0.4512,  0.1553,  0.2949, -0.4551, -0.7227],
        [-0.4199, -0.0276,  0.2695, -0.0447, -0.6758],
        [-0.2188, -0.0132,  0.4707, -0.4570, -0.6055],
        [-0.6875, -0.2402, -0.0952, -0.4023, -0.6602],
        [-0.4688,  0.0854,  0.5039, -0.0688, -0.2715]], device='cuda:0',
       dtype=torch.bfloat16, grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)
out(train): SequenceClassifierOutput(loss=tensor(1.5708, device='cuda:0', grad_fn=<NllLossBackward0>), logits=tensor([[-0.1172, -0.2441,  0.3945,  0.0291, -0.5859],
        [-0.5078,  0.2363,  0.4473, -0.3652, -0.6289],
        [-0.3008, -0.3672,  0.5156, -0.0996, -0.7852],
        [-0.3672, -0.1270,  0.2871,  0.2715, -0.9570],
        [-0.0986, -0.1416,  0.4473,  0.0082, -0.4844],
        [-0.2148,  0.4512, -0.4082, -0.2832, -0.3926],
        [-0.1523,  0.3770,  0.6133, -0.5938, -0.4316],
        [-0.5039,  0.2793,  0.2080, -0.3379, -0.8320],
        [-0.2109,  0.1631,  0.5664, -0.1836, -0.5781],
        [-0.0898,  0.2275,  0.2090, -0.0679, -0.8945],
        [ 0.1270, -0.1240,  0.6094, -0.3555, -0.5078],
        [-0.3301, -0.3438,  0.3047, -0.2393, -0.2598],
        [-0.3418, -0.2891,  0.3125, -0.2471, -1.0078],
        [-0.3516, -0.0981,  0.5859, -0.1709, -0.7930],
        [-0.4180,  0.0928,  0.4023, -0.2305, -0.7305],
        [-0.1030, -0.1328,  0.2773, -0.1396, -1.1719]], device='cuda:0',
       dtype=torch.bfloat16, grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)
out(train): SequenceClassifierOutput(loss=tensor(1.6460, device='cuda:0', grad_fn=<NllLossBackward0>), logits=tensor([[-0.1357,  0.0084,  0.4023, -0.3145, -0.8125],
        [ 0.0015,  0.4551,  0.6836, -0.1982, -0.5820],
        [-0.2520, -0.2041,  0.9336, -0.1758, -0.4590],
        [-0.3691, -0.1016,  0.3652, -0.2090, -0.7031],
        [-0.4219, -0.5508,  0.3477, -0.0845, -0.6211],
        [ 0.0981, -0.1445,  0.8594, -0.4434, -0.9844],
        [-0.3574, -0.0408,  0.5508, -0.3340, -0.7773],
        [-0.1235,  0.0767,  0.1309, -0.4082, -0.6328],
        [-0.2461,  0.2354,  0.5742, -0.1611, -0.3691],
        [-0.1021, -0.2148, -0.0044, -0.3145, -0.6680],
        [-0.3164, -0.1211,  0.5273, -0.2891, -0.3711],
        [-0.1484, -0.0147,  0.3750, -0.3340, -0.6836],
        [-0.2207,  0.1904,  0.4922, -0.2344, -0.1143],
        [-0.1289, -0.1289,  0.4570, -0.4863, -0.4043],
        [-0.5312,  0.0430,  0.3828, -0.1797, -0.4648],
        [-0.1436, -0.4551,  0.0304, -0.1836, -0.4961]], device='cuda:0',
       dtype=torch.bfloat16, grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)
out(train): SequenceClassifierOutput(loss=tensor(1.5200, device='cuda:0', grad_fn=<NllLossBackward0>), logits=tensor([[-0.6914,  0.1816,  0.3086, -0.1240, -0.6562],
        [-0.3320, -0.0781,  0.3867, -0.3613, -0.7891],
        [-0.1196,  0.0981, -0.0238, -0.1406, -0.3164],
        [-0.2402,  0.0864,  0.4395, -0.2422, -0.5078],
        [-0.5000, -0.0308,  0.3770, -0.3262, -0.5781],
        [-0.4453,  0.4023,  0.1768, -0.0737, -0.8164],
        [ 0.2432, -0.1445,  0.1904, -0.3145, -0.2148],
        [-0.6523, -0.1074,  0.1235, -0.2422, -0.5859],
        [-0.6523, -0.0938,  0.2871, -0.1494, -0.7695],
        [-0.5547,  0.1309,  0.2891, -0.0581, -0.4902],
        [-0.1436, -0.2715,  0.3340, -0.4219, -1.0312],
        [-0.3887, -0.0535,  0.2734, -0.1768, -0.3477],
        [-0.2578,  0.1172,  0.1807, -0.4238, -0.8203],
        [-0.2520,  0.2344, -0.0454,  0.0605, -0.2539],
        [-0.3926,  0.0210, -0.0791, -0.3008, -0.5312],
        [-0.2617, -0.0889,  0.5312, -0.6172, -0.5977]], device='cuda:0',
       dtype=torch.bfloat16, grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)
out(train): SequenceClassifierOutput(loss=tensor(1.6182, device='cuda:0', grad_fn=<NllLossBackward0>), logits=tensor([[-0.5156,  0.0103,  0.1367, -0.4238, -0.9688],
        [-0.2490, -0.1582,  0.5391, -0.1621, -0.8164],
        [-0.3906, -0.5078,  0.2266, -0.0820, -0.9531],
        [-0.1133,  0.2871,  0.5156, -0.3281, -0.7734],
        [-0.3555,  0.4473,  0.3711, -0.1309, -0.8086],
        [-0.4062,  0.1338,  0.4531, -0.3457, -0.6758],
        [-0.1924, -0.2432,  0.3906,  0.0591, -0.8672],
        [-0.3086, -0.1299,  0.1748, -0.3750, -0.7812],
        [-0.3145, -0.0388,  0.6953, -0.1416, -0.9180],
        [-0.1562,  0.2539,  0.5508, -0.2793, -0.3145],
        [-0.0271, -0.0238,  0.7461, -0.0130, -0.7773],
        [ 0.0334, -0.0417,  0.4258, -0.2266, -0.6641],
        [-0.2734,  0.0289,  0.5352,  0.0286, -0.3418],
        [-0.0864, -0.0439,  0.1729,  0.1396, -0.3008],
        [-0.1562,  0.2051,  0.3242, -0.3926, -0.8281],
        [-0.3789, -0.5352,  0.0015, -0.1592, -0.4668]], device='cuda:0',
       dtype=torch.bfloat16, grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)
out(train): SequenceClassifierOutput(loss=tensor(1.6255, device='cuda:0', grad_fn=<NllLossBackward0>), logits=tensor([[-0.2539, -0.3281,  0.0131, -0.0977, -0.6797],
        [-0.1826,  0.0118,  0.3281, -0.0012, -0.8906],
        [-0.3066, -0.1943,  0.4023, -0.6797, -0.7422],
        [-0.4121, -0.1172,  0.0334, -0.3770, -0.3262],
        [-0.2832,  0.0515,  0.1689, -0.3047, -0.3848],
        [-0.3652, -0.1455,  0.4746, -0.2471, -0.5078],
        [-0.2793, -0.2949,  0.3086, -0.3008, -0.4395],
        [-0.4043,  0.0854,  0.1001, -0.2393, -0.5938],
        [-0.3047,  0.1738,  0.4551, -0.0811, -0.4727],
        [-0.2832, -0.1221,  0.3477, -0.1660, -0.8750],
        [-0.5391, -0.0981,  0.5195, -0.1748, -0.7578],
        [-0.2559, -0.1992,  0.4492, -0.1143, -0.4102],
        [ 0.0869, -0.6836, -0.3418, -0.4961, -0.1113],
        [ 0.0603,  0.2363,  0.6367, -0.3867, -0.5117],
        [-0.0894,  0.0605,  0.2305, -0.5234, -0.3477],
        [-0.0669, -0.2988,  0.4062, -0.2891, -0.7695]], device='cuda:0',
       dtype=torch.bfloat16, grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)
W0629 11:52:52.467000 139012379337344 torch/_dynamo/convert_frame.py:824]     result = inner_convert(
out(train): SequenceClassifierOutput(loss=tensor(1.6279, device='cuda:0', grad_fn=<NllLossBackward0>), logits=tensor([[-0.3535,  0.2930,  0.3184, -0.1885, -0.2949],
        [-0.0227, -0.1084,  0.7969, -0.2275, -0.5938],
        [-0.4219, -0.1650,  0.1279, -0.1504, -0.5078],
        [ 0.0312,  0.0112,  0.4590, -0.3066, -0.8281],
        [-0.2812, -0.1426,  0.6953, -0.1592, -0.8047],
        [-0.2969,  0.0952,  0.5430, -0.2754, -0.6562],
        [-0.4180,  0.0459,  0.5078, -0.1357, -0.7578],
        [-0.2871, -0.0217,  0.3496, -0.1152, -0.7266],
        [-0.0608,  0.2148,  0.3848,  0.2119, -0.3203],
        [-0.4023, -0.1846,  0.2520, -0.6211, -0.9414],
        [-0.4141,  0.0261,  0.8047, -0.2617, -0.5547],
        [-0.4727, -0.3164,  0.1797, -0.2773, -0.0496],
        [-0.2949, -0.1338,  0.2988, -0.3555, -0.6836],
        [-0.1738,  0.2197, -0.0315, -0.4375, -0.5430],
        [-0.5547,  0.2422,  0.4297, -0.5078, -0.5039],
        [-0.1069, -0.1250,  0.9141, -0.2090, -0.4805]], device='cuda:0',
       dtype=torch.bfloat16, grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)
out(train): SequenceClassifierOutput(loss=tensor(1.6025, device='cuda:0', grad_fn=<NllLossBackward0>), logits=tensor([[ 0.1245, -0.2656, -0.0630, -0.2158, -0.4102],
        [-0.1748,  0.0447, -0.0352, -0.2617, -0.2832],
        [-0.3613, -0.3477,  0.5234,  0.2598, -0.8242],
        [-0.2930, -0.0918,  0.3359, -0.0972, -0.3730],
        [-0.1768, -0.0236,  0.3730, -0.1973, -0.5430],
        [-0.2100, -0.1328,  0.4824,  0.1094, -0.4531],
        [-0.0879, -0.1846,  0.3867, -0.2812, -0.5391],
        [-0.4629,  0.1367,  0.5000, -0.4961, -0.5117],
        [-0.2266, -0.3613,  0.1270, -0.4062, -0.5195],
        [-0.2402, -0.0479,  0.2080, -0.0928, -0.3184],
        [-0.0806, -0.0703,  0.1855, -0.4121, -0.3516],
        [-0.2832,  0.0898, -0.0693, -0.3457, -0.6484],
        [-0.6836,  0.2080,  0.4141, -0.1523, -0.5938],
        [ 0.2266,  0.0781,  0.1475, -0.4473, -0.5195],
        [-0.0903,  0.3711, -0.1592, -0.2969, -0.2598],
        [-0.1357,  0.0747,  0.2754, -0.3828, -0.6484]], device='cuda:0',
       dtype=torch.bfloat16, grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)
out(train): SequenceClassifierOutput(loss=tensor(1.5513, device='cuda:0', grad_fn=<NllLossBackward0>), logits=tensor([[-0.3867,  0.0437,  0.4102, -0.2930, -0.5391],
        [-0.1436, -0.1611,  0.5586, -0.2637, -0.7500],
        [-0.3418,  0.0374,  0.5742,  0.0110, -0.6914],
        [-0.1494,  0.2695,  0.3613,  0.0540, -0.8086],
        [-0.6016, -0.0256,  0.8242, -0.3164, -0.3945],
        [-0.4199,  0.0967,  0.2871, -0.3008, -0.5781],
        [-0.0493, -0.3809,  0.3438, -0.0076, -0.6250],
        [-0.1309, -0.2852,  0.2988, -0.2734, -0.2070],
        [-0.4590,  0.2676,  0.1523, -0.5312, -1.0781],
        [-0.2500,  0.1562,  0.4902, -0.0679, -0.9336],
        [-0.0544,  0.1348,  0.3027, -0.4980, -0.4707],
        [-0.2520,  0.0361,  0.5234, -0.1226, -0.2080],
        [-0.2773, -0.4199,  0.0688, -0.2559, -0.4961],
        [-0.4023,  0.5195,  0.3828, -0.3613, -0.7031],
        [-0.3984, -0.0908,  0.1865, -0.1562, -0.7148],
        [-0.1660,  0.2236,  0.0981, -0.1729, -0.8164]], device='cuda:0',
       dtype=torch.bfloat16, grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)
out(train): SequenceClassifierOutput(loss=tensor(1.5024, device='cuda:0', grad_fn=<NllLossBackward0>), logits=tensor([[-0.1826,  0.2891,  0.1943, -0.3086, -0.3672],
        [-0.2344,  0.0898,  0.7773, -0.4082, -0.6875],
        [-0.5742, -0.0278,  0.4551, -0.4824, -0.7070],
        [-0.3281,  0.1738,  0.0752, -0.0806, -0.8320],
        [-0.2754,  0.0330,  0.2969, -0.1719, -0.6758],
        [-0.2129, -0.0300,  0.4395, -0.1523, -0.5117],
        [-0.2812, -0.2773,  0.1592, -0.5117, -0.7383],
        [-0.3418,  0.1660,  0.3457, -0.1011, -0.5156],
        [-0.2676,  0.3496,  0.5898, -0.6680, -0.8672],
        [-0.2246, -0.0366,  0.1895, -0.3223, -0.5547],
        [-0.3027, -0.1426,  0.2363, -0.3223, -0.8516],
        [-0.1318, -0.2559,  0.2217, -0.3379, -0.4434],
        [-0.2051, -0.1128,  0.4785,  0.0623, -0.2383],
        [-0.2930,  0.0527,  0.2832, -0.4668, -0.8320],
        [-0.7891,  0.2344,  0.4512, -0.2852, -0.6406],
        [-0.4043,  0.3066,  0.4414, -0.1016, -1.0469]], device='cuda:0',
       dtype=torch.bfloat16, grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)
out(train): SequenceClassifierOutput(loss=tensor(1.6323, device='cuda:0', grad_fn=<NllLossBackward0>), logits=tensor([[-0.3750,  0.0459,  0.1006, -0.4824, -0.4746],
        [-0.1807,  0.0510,  0.2432, -0.0211, -0.5938],
        [ 0.0437,  0.1758,  0.5195, -0.5938, -0.2617],
        [-0.1953, -0.3047,  0.4590, -0.2441, -0.1895],
        [ 0.2832, -0.1196, -0.5703, -0.2422,  0.4434],
        [-0.1973,  0.0967,  0.4141, -0.3965, -0.8398],
        [-0.2832, -0.1836,  0.1738,  0.0962, -0.6875],
        [-0.3320, -0.2949,  0.0315, -0.3008, -0.5938],
        [-0.4043,  0.5000,  0.2949, -0.1436, -0.1201],
        [-0.4570,  0.1768,  0.5625, -0.3047, -0.6484],
        [-0.4199, -0.0011,  0.7461, -0.3047, -0.7852],
        [-0.2812, -0.0173,  0.2402, -0.1387, -0.4043],
        [-0.5039,  0.0400,  0.1826,  0.0645, -0.9062],
        [-0.1855,  0.0674,  0.1260, -0.4004, -0.1797],
        [-0.4160,  0.0052,  0.3164, -0.1436, -0.5078],
        [-0.0400, -0.1807,  0.3496, -0.2422, -0.5625]], device='cuda:0',
       dtype=torch.bfloat16, grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)
out(train): SequenceClassifierOutput(loss=tensor(1.4619, device='cuda:0', grad_fn=<NllLossBackward0>), logits=tensor([[-0.2441, -0.2305,  0.0479, -0.1709, -0.4082],
        [-0.2871,  0.2393,  0.4707, -0.2637, -1.0000],
        [-0.2969,  0.1729,  0.6875, -0.0503, -0.7148],
        [-0.1650, -0.0054,  0.2002,  0.0415, -0.6797],
        [ 0.0923, -0.0393, -0.2969, -0.4258, -0.1816],
        [ 0.0437, -0.0278,  0.3496, -0.2715, -0.2314],
        [-0.0623, -0.0171,  0.1206, -0.3086,  0.2227],
        [-0.4551,  0.2432,  0.1758, -0.5430, -0.3652],
        [ 0.0708,  0.2451,  0.1777, -0.5195, -0.4609],
        [ 0.1260,  0.0505,  0.1709, -0.3086, -0.0908],
        [ 0.1006,  0.3398, -0.2363, -0.3223, -0.3691],
        [-0.1396,  0.1494,  0.0591, -0.4180, -0.3320],
        [ 0.2354,  0.3008,  0.3887, -0.2500, -0.6094],
        [-0.3379, -0.1865, -0.0417, -0.3672, -0.3672],
        [-0.1602,  0.1094,  0.1572, -0.3730, -0.9062],
        [-0.2773,  0.1045, -0.0266,  0.0391, -0.2334]], device='cuda:0',
       dtype=torch.bfloat16, grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)
out(train): SequenceClassifierOutput(loss=tensor(1.4526, device='cuda:0', grad_fn=<NllLossBackward0>), logits=tensor([[-0.2002,  0.3281, -0.1455, -0.5508, -0.3535],
        [-0.0369,  0.0186,  0.2793, -0.5156, -0.4629],
        [-0.5469,  0.2344, -0.0410, -0.5508, -0.5664],
        [-0.2109, -0.0265, -0.0845, -0.0483, -0.6445],
        [ 0.0659, -0.3184,  0.7227, -0.4062, -0.6172],
        [-0.0581,  0.1270,  0.0342, -0.4160, -0.2109],
        [ 0.1235, -0.1216,  0.4668,  0.0126, -0.8516],
        [ 0.0947,  0.1758,  0.0356,  0.0649, -0.2275],
        [-0.3750, -0.1289,  0.0762, -0.1187, -0.2539],
        [-0.2441,  0.4102,  0.2275, -0.5742, -0.0767],
        [ 0.0119,  0.1719,  0.0216,  0.1406, -0.8125],
        [-0.3984,  0.3633, -0.3047, -0.2598, -0.1523],
        [-0.3555,  0.4570, -0.0112, -0.3535, -0.2412],
        [-0.3613,  0.2305, -0.2080, -0.0503, -0.6445],
        [-0.0762, -0.4648,  0.0796, -0.0014, -0.5547],
        [-0.2422,  0.1025,  0.0192, -0.4141, -0.4961]], device='cuda:0',
       dtype=torch.bfloat16, grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)
out(train): SequenceClassifierOutput(loss=tensor(1.4482, device='cuda:0', grad_fn=<NllLossBackward0>), logits=tensor([[-0.2617, -0.0879,  0.4434, -0.4316, -0.8594],
        [-0.1982,  0.3770,  0.3398, -0.1719, -0.3867],
        [-0.3789,  0.0918,  0.1768, -0.4023, -0.5234],
        [-0.2500,  0.1426,  0.3672, -0.4355, -0.0054],
        [-0.1006, -0.1699,  0.0713, -0.2578, -0.5234],
        [-0.0806, -0.2227, -0.0337, -0.3027, -0.0566],
        [ 0.4102, -0.0728,  0.2949, -0.3359, -0.0513],
        [-0.3613,  0.1748,  0.2734, -0.1016, -0.6953],
        [-0.0084, -0.0488,  0.1660, -0.4414, -0.6758],
        [-0.0835,  0.1816,  0.1621, -0.5898, -0.3984],
        [-0.1201, -0.1787,  0.0640, -0.2051, -0.8477],
        [ 0.1270,  0.0056, -0.0737, -0.1621, -0.2354],
        [ 0.0048,  0.5859,  0.2812, -0.1523, -0.4629],
        [-0.2256,  0.3945,  0.3203, -0.3281, -0.2715],
        [ 0.0825,  0.2109,  0.2100, -0.3516,  0.0081],
        [-0.3984,  0.1670,  0.2334, -0.3262, -0.4023]], device='cuda:0',
       dtype=torch.bfloat16, grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)
out(train): SequenceClassifierOutput(loss=tensor(1.5220, device='cuda:0', grad_fn=<NllLossBackward0>), logits=tensor([[-0.0952, -0.1040,  0.3438, -0.1328, -0.8672],
        [-0.1484,  0.1299,  0.4707, -0.4746, -0.8047],
        [-0.0640, -0.1104,  0.3477, -0.4492, -0.1982],
        [-0.2373,  0.2539,  0.4492, -0.3711, -0.7227],
        [-0.1699, -0.0869,  0.2539, -0.4609, -1.0391],
        [ 0.0130,  0.2676, -0.0669, -0.2236, -0.4238],
        [-0.3965,  0.0466,  0.1699, -0.1807, -0.8672],
        [ 0.1079,  0.1465,  0.0435,  0.0525, -0.2695],
        [ 0.1855,  0.2246,  0.3926, -0.2520, -0.4531],
        [-0.1846,  0.1846,  0.1504, -0.3730, -0.6523],
        [-0.2314, -0.1270,  0.3398, -0.3828, -0.9766],
        [-0.1250,  0.2598,  0.5312, -0.0442, -0.4023],
        [-0.5898,  0.2637,  0.4434, -0.0123, -0.2617],
        [-0.3965,  0.1001,  0.2891, -0.1807, -0.2051],
        [-0.4004, -0.1953,  0.3574, -0.4199, -0.4492],
        [-0.2236,  0.4883,  0.5352, -0.5312,  0.0198]], device='cuda:0',
       dtype=torch.bfloat16, grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)
W0629 11:52:52.467000 139012379337344 torch/_dynamo/convert_frame.py:824]     result = inner_convert(
out(train): SequenceClassifierOutput(loss=tensor(1.5024, device='cuda:0', grad_fn=<NllLossBackward0>), logits=tensor([[-0.0693,  0.2246,  0.0317, -0.2021, -0.2236],
        [-0.1230,  0.2617,  0.0840, -0.1465, -0.4082],
        [-0.4766, -0.0508, -0.0315, -0.1177, -1.0547],
        [-0.3066,  0.2148, -0.0522, -0.3438, -0.4102],
        [ 0.1943,  0.2461,  0.5586, -0.2598, -1.0469],
        [-0.3516,  0.2100, -0.0430, -0.3594, -0.1895],
        [-0.2949, -0.0630, -0.0552, -0.1523, -0.2969],
        [-0.1465, -0.1250,  0.5156, -0.0403, -0.7500],
        [ 0.0542, -0.0801,  0.1016, -0.1348, -0.2520],
        [-0.0262,  0.1855,  0.7148, -0.2090, -0.6953],
        [ 0.0383,  0.0879,  0.2197, -0.3047, -0.2461],
        [-0.2178,  0.1758,  0.2266, -0.5273, -0.6719],
        [ 0.1953, -0.1836, -0.3516, -0.2070,  0.3848],
        [-0.3340,  0.3828,  0.2207, -0.2314, -0.3242],
        [-0.0249, -0.1504,  0.1963,  0.3398, -0.2852],
        [-0.0684,  0.1191,  0.3359, -0.2041, -0.0603]], device='cuda:0',
       dtype=torch.bfloat16, grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)
out(train): SequenceClassifierOutput(loss=tensor(1.4463, device='cuda:0', grad_fn=<NllLossBackward0>), logits=tensor([[-0.2002,  0.5586,  0.6406, -0.5273, -0.3477],
        [-0.0155,  0.2070,  0.3711, -0.1523, -0.5469],
        [-0.2988,  0.3262,  0.1152, -0.2422, -0.2930],
        [-0.4180,  0.2314, -0.1660, -0.2256, -0.1060],
        [-0.2090,  0.2373,  0.3848, -0.4160, -0.7461],
        [ 0.1582, -0.0250,  0.1206, -0.1816,  0.0142],
        [-0.0737,  0.1904, -0.2520, -0.2559, -0.0757],
        [-0.1436,  0.3398, -0.0908, -0.2422, -0.3496],
        [-0.1631,  0.1030,  0.4355, -0.1514, -0.3906],
        [-0.3066, -0.0132,  0.1167, -0.1504, -0.2490],
        [-0.5273, -0.0430, -0.1816, -0.0471, -0.2773],
        [-0.2070,  0.3730,  0.0598, -0.4258, -0.4648],
        [-0.1768,  0.3203,  0.1826, -0.1523, -0.1021],
        [ 0.1602,  0.2402,  0.0762, -0.2168, -0.1108],
        [-0.0359,  0.1660, -0.2012, -0.4355, -0.0889],
        [ 0.1045, -0.0410,  0.4336,  0.2080, -0.7148]], device='cuda:0',
       dtype=torch.bfloat16, grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)
out(train): SequenceClassifierOutput(loss=tensor(1.3809, device='cuda:0', grad_fn=<NllLossBackward0>), logits=tensor([[-0.0278,  0.1104,  0.0991, -0.2275, -0.3203],
        [-0.1338, -0.1875,  0.4551, -0.0623, -0.4785],
        [ 0.0889,  0.3027,  0.4414, -0.1904, -0.3613],
        [-0.1973,  0.2207,  0.5039, -0.2988, -0.7227],
        [-0.0845,  0.3516,  0.2988, -0.1719, -0.3867],
        [-0.1230,  0.1787,  0.4023, -0.1245, -1.0391],
        [-0.3320,  0.1758,  0.1553, -0.2734, -0.1924],
        [-0.4961,  0.3770,  0.0923,  0.0077, -0.4121],
        [ 0.2158,  0.3535,  0.3359, -0.4395, -0.2656],
        [-0.5391,  0.2012,  0.0033, -0.4219, -0.1318],
        [ 0.0674, -0.0493,  0.0356,  0.0781, -0.5234],
        [-0.3359,  0.3223, -0.0190, -0.1836, -0.2422],
        [-0.2715, -0.3672,  0.0815, -0.5586, -0.2471],
        [-0.1787,  0.6836,  0.2852, -0.1426, -0.4609],
        [-0.3730,  0.1650,  0.3008, -0.1191, -0.4316],
        [-0.1582,  0.3535,  0.1299, -0.1221, -0.3379]], device='cuda:0',
       dtype=torch.bfloat16, grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)
out(train): SequenceClassifierOutput(loss=tensor(1.6113, device='cuda:0', grad_fn=<NllLossBackward0>), logits=tensor([[-0.3809,  0.0118,  0.5234, -0.0481, -1.0938],
        [-0.2988, -0.4414,  0.2393, -0.2012, -0.5703],
        [-0.0547, -0.0288,  0.3457, -0.2949, -1.0312],
        [-0.1084,  0.1416,  0.0237, -0.3516, -0.4492],
        [ 0.0027, -0.1289,  0.5977,  0.0109, -0.6328],
        [ 0.0422,  0.1699,  0.1973,  0.0129, -0.3438],
        [ 0.1641,  0.0148, -0.1221,  0.0084, -0.1367],
        [ 0.5586, -0.2520, -0.6367,  0.2148,  0.4961],
        [-0.1855,  0.3164,  0.1104, -0.1279, -0.3320],
        [-0.3691,  0.1680,  0.6992, -0.1807, -0.7188],
        [ 0.3301, -0.0227, -0.1494, -0.2461,  0.0576],
        [-0.3340,  0.3965,  0.1855, -0.0786, -0.3965],
        [-0.1855,  0.2539, -0.3828, -0.3496, -0.3574],
        [-0.0947,  0.0530,  0.1934, -0.0270, -0.4609],
        [-0.0850,  0.1660,  0.2676, -0.0708,  0.0287],
        [-0.1113,  0.4727, -0.0732, -0.0698, -0.3770]], device='cuda:0',
       dtype=torch.bfloat16, grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)
out(train): SequenceClassifierOutput(loss=tensor(1.4492, device='cuda:0', grad_fn=<NllLossBackward0>), logits=tensor([[-0.3984,  0.3262,  0.3418,  0.3242, -0.7930],
        [-0.0474,  0.3984,  0.1582, -0.1299, -0.6016],
        [-0.5078,  0.1875,  0.4434, -0.1758, -0.8828],
        [ 0.0698,  0.3008,  0.1113, -0.0806, -0.2910],
        [-0.2637,  0.3672,  0.3906, -0.2148, -0.1270],
        [ 0.2461, -0.0977, -0.0962, -0.3008, -0.3301],
        [-0.5781,  0.2852, -0.0581,  0.0535, -0.2891],
        [-0.3887,  0.2539,  0.0698, -0.2070, -0.5312],
        [ 0.2002, -0.0175, -0.2930, -0.5430,  0.0542],
        [-0.0449, -0.0325,  0.6797, -0.2363, -0.7148],
        [-0.4355,  0.2949,  0.0728, -0.2852, -0.0430],
        [-0.1167,  0.2793,  0.3516, -0.4199, -0.3555],
        [-0.0518,  0.3340,  0.4727, -0.3965, -0.5234],
        [-0.1152, -0.0022,  0.2217, -0.4883, -0.7148],
        [ 0.2119,  0.1260, -0.0576, -0.0830, -0.3516],
        [-0.2480,  0.1680,  0.1138, -0.0884, -0.2471]], device='cuda:0',
       dtype=torch.bfloat16, grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)
out(train): SequenceClassifierOutput(loss=tensor(1.4707, device='cuda:0', grad_fn=<NllLossBackward0>), logits=tensor([[-0.1113,  0.3789,  0.3066, -0.2734, -0.4492],
        [-0.4121,  0.1387,  0.3535, -0.3887, -1.4609],
        [-0.2656, -0.1157,  0.7070, -0.4062, -0.2061],
        [-0.0435,  0.2539,  0.3730, -0.0654, -0.4473],
        [ 0.1021,  0.1572, -0.2148, -0.1621, -0.1011],
        [-0.1133, -0.0693,  0.6172,  0.0986, -0.5273],
        [ 0.0674,  0.1387,  0.3809, -0.1211, -0.3086],
        [-0.1226,  0.3066,  0.2393,  0.1118, -0.7422],
        [-0.3652,  0.1279,  0.3320, -0.4062, -0.1992],
        [-0.0525,  0.2402,  0.4062, -0.3242, -0.1689],
        [-0.2207,  0.2793,  0.3145, -0.1855, -0.3027],
        [ 0.1494, -0.0688,  0.2578, -0.1367, -0.1250],
        [-0.3379,  0.3145,  0.3320, -0.4004, -0.3652],
        [ 0.0347,  0.5469,  0.3457, -0.5469, -0.1992],
        [-0.1309,  0.0820,  0.6211, -0.1006, -0.5625],
        [-0.3184, -0.0364,  0.2617, -0.4570, -0.5859]], device='cuda:0',
       dtype=torch.bfloat16, grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)
out(train): SequenceClassifierOutput(loss=tensor(1.4468, device='cuda:0', grad_fn=<NllLossBackward0>), logits=tensor([[-0.2930,  0.2490, -0.0120, -0.0811, -0.2480],
        [-0.0967,  0.3672,  0.3281, -0.0742, -0.3145],
        [-0.4082,  0.3242,  0.0864, -0.0437, -0.2734],
        [-0.4336,  0.2656,  0.4043, -0.4766, -0.7695],
        [ 0.3418,  0.1553, -0.3945, -0.4414, -0.1816],
        [-0.3125,  0.1670,  0.2305, -0.4160,  0.0089],
        [-0.2070,  0.3496,  0.4883, -0.2227, -0.1338],
        [-0.1147,  0.2188,  0.2461, -0.0889, -0.7344],
        [-0.2148,  0.2021,  0.0830, -0.2393, -0.2422],
        [-0.3730, -0.4121,  0.1216, -0.1455, -0.3730],
        [-0.5312,  0.2188,  0.2090, -0.2324, -0.5352],
        [-0.3789,  0.1982, -0.0334,  0.1621, -0.5000],
        [ 0.1641,  0.0422,  0.0708, -0.2617, -0.4297],
        [-0.4551,  0.0452,  0.4766,  0.0781, -0.8438],
        [-0.4824,  0.2188, -0.0513, -0.0564, -1.0547],
        [-0.4805,  0.1123,  0.3242, -0.6328, -0.9297]], device='cuda:0',
       dtype=torch.bfloat16, grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)
out(train): SequenceClassifierOutput(loss=tensor(1.3496, device='cuda:0', grad_fn=<NllLossBackward0>), logits=tensor([[-0.2217,  0.2070,  0.0598, -0.3848, -0.4785],
        [-0.1206,  0.2520, -0.0010,  0.3555, -0.6641],
        [ 0.0869, -0.3066,  0.0742, -0.3398, -0.3633],
        [ 0.0835,  0.0981,  0.0405, -0.3613, -0.2051],
        [-0.2715,  0.2715,  0.2041, -0.3574, -0.3809],
        [-0.3086,  0.2930, -0.0064, -0.5820, -0.2715],
        [-0.4453,  0.0459,  0.3320, -0.2617, -0.3691],
        [-0.2988,  0.4707,  0.4980, -0.0500, -0.2715],
        [-0.2617,  0.1465,  0.1895, -0.0139, -0.3066],
        [-0.0933,  0.0474,  0.2393, -0.3438, -0.3496],
        [-0.2275,  0.5312,  0.0967, -0.5859, -0.3535],
        [-0.1123,  0.4805, -0.1260, -0.2715, -0.1157],
        [ 0.3477,  0.2539,  0.3945, -0.5430,  0.3457],
        [-0.0271,  0.2334, -0.1328, -0.3203, -0.4336],
        [-0.0304,  0.2969,  0.0737, -0.1689, -0.0664],
        [-0.2891,  0.3105,  0.0513, -0.4082, -0.3613]], device='cuda:0',
       dtype=torch.bfloat16, grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)
out(train): SequenceClassifierOutput(loss=tensor(1.4619, device='cuda:0', grad_fn=<NllLossBackward0>), logits=tensor([[-0.0859,  0.1494,  0.5000, -0.3105, -0.6641],
        [ 0.0369,  0.1963, -0.1445, -0.1611, -0.1660],
        [ 0.1206,  0.1309,  0.2217, -0.3379, -0.1406],
        [-0.5078,  0.3105,  0.2139, -0.5156, -0.8398],
        [ 0.0286, -0.3262, -0.2305, -0.4395, -0.1299],
        [-0.0378, -0.1357,  0.6680, -0.1992, -0.6328],
        [-0.1943, -0.0840,  0.7070, -0.1660, -0.8320],
        [ 0.1357,  0.0928,  0.1621, -0.3730, -0.6367],
        [-0.1768, -0.2910,  0.0320, -0.2031, -0.6562],
        [-0.1816,  0.5859,  0.2910, -0.3164, -0.3770],
        [-0.1523,  0.2393,  0.0908, -0.0986, -0.2637],
        [ 0.1797,  0.5156, -0.0082, -0.3438,  0.3574],
        [-0.0618,  0.3418,  0.1406, -0.1299, -0.5312],
        [-0.3711,  0.4453,  0.2734, -0.2539, -0.2969],
        [-0.1377,  0.3535,  0.1533, -0.0603, -0.5508],
        [-0.1074, -0.0481,  0.1230, -0.3672, -0.2520]], device='cuda:0',
       dtype=torch.bfloat16, grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)
out(train): SequenceClassifierOutput(loss=tensor(1.5073, device='cuda:0', grad_fn=<NllLossBackward0>), logits=tensor([[-0.0046, -0.2031,  0.4355, -0.1147, -0.0923],
        [-0.2480, -0.0444,  0.2217, -0.4160, -0.2559],
        [-0.0640,  0.4062,  0.2734, -0.2676, -0.2988],
        [-0.0820,  0.1504,  0.3555, -0.3672, -0.5273],
        [ 0.2109,  0.4062,  0.0732,  0.2480, -0.3516],
        [-0.3516, -0.0967,  0.1904, -0.1543, -0.3555],
        [-0.0376,  0.3789,  0.1641, -0.2295, -0.1206],
        [ 0.1118,  0.0576,  0.0728, -0.4219, -0.4160],
        [-0.5430,  0.2754,  0.2578, -0.1045, -0.5703],
        [-0.0038,  0.1689, -0.0222, -0.6016, -0.3027],
        [-0.1035,  0.2578,  0.2295, -0.2285, -0.2949],
        [-0.0596,  0.5156,  0.5781,  0.1445, -0.5391],
        [-0.2734, -0.1221,  0.0708,  0.0947, -0.1226],
        [-0.1865,  0.0732,  0.4141, -0.0124, -0.2354],
        [-0.0576,  0.2451, -0.0767, -0.1719, -0.2217],
        [-0.3535,  0.3613,  0.6406, -0.2471, -0.6641]], device='cuda:0',
       dtype=torch.bfloat16, grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)
W0629 11:52:52.467000 139012379337344 torch/_dynamo/convert_frame.py:824]     result = inner_convert(
out(train): SequenceClassifierOutput(loss=tensor(1.5684, device='cuda:0', grad_fn=<NllLossBackward0>), logits=tensor([[-0.2363,  0.1904,  0.0025, -0.5742, -1.1016],
        [ 0.1260,  0.2451,  0.3066, -0.3203, -0.6406],
        [ 0.0359,  0.3594,  0.3652, -0.1680, -0.6172],
        [-0.2656,  0.0405,  0.1504, -0.1641, -0.1914],
        [-0.1318, -0.0928,  0.0287,  0.0208, -0.3320],
        [-0.1494,  0.2891,  0.1250, -0.2871, -0.4707],
        [ 0.0825,  0.1641,  0.3477, -0.3809, -0.6094],
        [-0.3223, -0.0679, -0.0474, -0.0967, -0.6133],
        [-0.2852, -0.3281,  0.3027, -0.4648, -0.8789],
        [-0.4219,  0.5000,  0.0703, -0.2373, -0.2432],
        [-0.2852,  0.1128,  0.6094, -0.1709, -0.2754],
        [ 0.2354,  0.3672, -0.2695,  0.0123, -0.3691],
        [-0.1211, -0.0457,  0.1050, -0.0325, -0.1611],
        [-0.1289,  0.1973,  0.4004, -0.0649, -0.0591],
        [ 0.0598, -0.0366, -0.2217, -0.3262, -0.1260],
        [ 0.0356, -0.0420,  0.7031,  0.1235, -0.6484]], device='cuda:0',
       dtype=torch.bfloat16, grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)
out(train): SequenceClassifierOutput(loss=tensor(1.6001, device='cuda:0', grad_fn=<NllLossBackward0>), logits=tensor([[ 0.0908,  0.0659,  0.3125, -0.1758, -0.0171],
        [-0.2148, -0.1216,  0.1689, -0.3574, -0.4277],
        [ 0.0869,  0.1309,  0.1660, -0.0674, -0.1582],
        [-0.1318,  0.0415, -0.3223, -0.1416, -0.2334],
        [ 0.1533,  0.3203,  0.2080, -0.2129, -0.3262],
        [-0.2090,  0.5039,  0.0859, -0.0544, -0.1924],
        [ 0.1025,  0.1572,  0.4570, -0.3711, -0.1914],
        [-0.1162,  0.0053, -0.0593, -0.0583, -0.5508],
        [-0.0315,  0.0200,  0.3906, -0.4102, -0.2227],
        [-0.3281,  0.2520, -0.1689,  0.0513, -0.2695],
        [ 0.2500,  0.0757, -0.0198, -0.0479, -0.1875],
        [ 0.2188, -0.1963,  0.3633,  0.1045, -0.1924],
        [-0.4473,  0.4160,  0.4746, -0.0107, -0.4180],
        [-0.2471,  0.1729,  0.2969, -0.4141, -0.4102],
        [-0.0491,  0.3242,  0.1758, -0.1885,  0.0294],
        [-0.1104, -0.1299,  0.1055, -0.1328, -0.3496]], device='cuda:0',
       dtype=torch.bfloat16, grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)
out(train): SequenceClassifierOutput(loss=tensor(1.4175, device='cuda:0', grad_fn=<NllLossBackward0>), logits=tensor([[-0.5508,  0.5273,  0.1279, -0.1963, -0.3945],
        [-0.1846,  0.1826,  0.1455, -0.0659, -0.2432],
        [-0.2256,  0.4688,  0.2832, -0.3125, -0.2891],
        [-0.3613,  0.3008,  0.1475, -0.3594, -0.6211],
        [-0.1147,  0.1245,  0.3203, -0.1650, -0.5625],
        [-0.4746,  0.0115,  0.2148, -0.1001, -0.3496],
        [-0.2178, -0.0496,  0.5664, -0.0562, -0.2559],
        [ 0.2100,  0.3164,  0.1465, -0.4355, -0.2754],
        [-0.0583,  0.2031, -0.1177, -0.4121, -0.6367],
        [-0.3770,  0.1777, -0.0874, -0.2363, -0.3398],
        [-0.3926, -0.2715, -0.1855, -0.3750, -0.3008],
        [-0.1436,  0.2812,  0.2656, -0.3926, -0.3711],
        [-0.3086, -0.1177,  0.1826,  0.3672, -1.0078],
        [-0.3008,  0.5742,  0.2520, -0.1709, -0.0603],
        [ 0.0493,  0.1030, -0.1260, -0.4023,  0.2969],
        [-0.3086, -0.1543,  0.0210, -0.1523, -0.6680]], device='cuda:0',
       dtype=torch.bfloat16, grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)
out(train): SequenceClassifierOutput(loss=tensor(1.5532, device='cuda:0', grad_fn=<NllLossBackward0>), logits=tensor([[-0.1875,  0.4824,  0.3848, -0.0454, -0.1846],
        [-0.0172,  0.1250,  0.4941, -0.3926, -0.3809],
        [-0.2275, -0.0947, -0.1309, -0.3184, -0.3457],
        [-0.1592,  0.1172,  0.3457, -0.1895, -0.3359],
        [ 0.2080,  0.1729, -0.0071, -0.4199, -0.1787],
        [ 0.0608,  0.0830,  0.4355, -0.1846, -0.2158],
        [-0.2930,  0.3457, -0.1172, -0.0898, -0.2354],
        [-0.4512,  0.1494,  0.3105, -0.4395, -0.4004],
        [ 0.0027,  0.1094,  0.2256, -0.2373,  0.0194],
        [-0.0635,  0.1177, -0.2930, -0.5234, -0.2578],
        [ 0.0850, -0.0393,  0.3633,  0.0291, -0.5469],
        [-0.1045,  0.4902,  0.1436, -0.0742, -0.2461],
        [-0.5781,  0.2676,  0.1553, -0.1660, -0.1846],
        [-0.3320,  0.2090,  0.1113, -0.2246, -0.2178],
        [-0.3535,  0.0806, -0.2490, -0.1816,  0.1738],
        [-0.5039,  0.0232,  0.3691, -0.4609, -0.9648]], device='cuda:0',
       dtype=torch.bfloat16, grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)
out(train): SequenceClassifierOutput(loss=tensor(1.4976, device='cuda:0', grad_fn=<NllLossBackward0>), logits=tensor([[-0.3750,  0.4785,  0.0374, -0.1060, -0.5156],
        [ 0.0281, -0.2520, -0.1270, -0.4863,  0.0879],
        [ 0.0075,  0.3730,  0.2168, -0.4863, -0.2246],
        [-0.6484,  0.4551,  0.4043, -0.1875, -0.4297],
        [-0.2295,  0.2246,  0.3594, -0.0767, -0.2695],
        [-0.3535,  0.1436,  0.1943, -0.3359, -0.8672],
        [ 0.0815,  0.4766,  0.1113, -0.4766, -0.0442],
        [ 0.0161,  0.3613,  0.0359, -0.1729, -0.2676],
        [-0.0447,  0.1924,  0.3652,  0.2080, -0.3926],
        [-0.0898,  0.1543,  0.4434, -0.3145, -0.3984],
        [ 0.1157, -0.2100, -0.1069, -0.3789, -0.1001],
        [-0.3105,  0.2930,  0.6250, -0.5000, -0.6797],
        [-0.2578, -0.0188,  0.4785, -0.1797, -0.5039],
        [-0.2930,  0.1523,  0.4434, -0.2871, -0.4902],
        [-0.0520,  0.0742,  0.3613, -0.0962,  0.0564],
        [-0.3164,  0.2314,  0.3281, -0.1191, -0.3379]], device='cuda:0',
       dtype=torch.bfloat16, grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)
out(train): SequenceClassifierOutput(loss=tensor(1.4458, device='cuda:0', grad_fn=<NllLossBackward0>), logits=tensor([[ 0.3477, -0.1118,  0.1719, -0.1787, -0.1416],
        [-0.0835,  0.3594,  0.0253, -0.1436, -0.2188],
        [-0.2451, -0.1147,  0.7578, -0.3555, -0.8828],
        [-0.5195,  0.1699,  0.2031, -0.2490, -0.1309],
        [ 0.4160, -0.0762, -0.4199, -0.0045,  0.2422],
        [-0.4199,  0.3438,  0.0503, -0.4844, -0.6406],
        [ 0.2285,  0.2393, -0.0540, -0.0371, -0.0767],
        [-0.2168,  0.0054,  0.2461, -0.3828, -0.3047],
        [ 0.0557, -0.0645,  0.2051, -0.1475, -0.3613],
        [-0.2832,  0.1318, -0.0583, -0.0972, -0.4902],
        [-0.2695, -0.1689,  0.2266,  0.1299, -0.9961],
        [-0.0437,  0.2061,  0.1592, -0.3379, -0.6719],
        [-0.0967,  0.1846, -0.2500, -0.0496, -0.1875],
        [-0.0201,  0.0155,  0.3945, -0.3574,  0.1211],
        [ 0.2354,  0.3184, -0.2217, -0.2695,  0.0776],
        [ 0.0469,  0.3320,  0.3418, -0.1904, -0.3203]], device='cuda:0',
       dtype=torch.bfloat16, grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)
out(train): SequenceClassifierOutput(loss=tensor(1.4756, device='cuda:0', grad_fn=<NllLossBackward0>), logits=tensor([[ 0.0928,  0.5742,  0.3027, -0.2168, -0.2539],
        [-0.2539, -0.0309,  0.3164, -0.2090, -0.8633],
        [-0.1680,  0.1348,  0.4375, -0.1689, -0.4629],
        [-0.4258, -0.3477,  0.4590, -0.4141, -0.8828],
        [ 0.1377,  0.4629,  0.5820, -0.2070, -0.2197],
        [-0.1572,  0.2139,  0.0045,  0.0618, -0.4434],
        [ 0.0238,  0.1348,  0.2695, -0.2383, -0.0302],
        [-0.1396,  0.2617, -0.0226,  0.0172, -0.3496],
        [ 0.0583,  0.1621, -0.2988, -0.4336,  0.0315],
        [-0.1069, -0.0608,  0.5547, -0.1855, -0.2236],
        [-0.4336,  0.4785,  0.1445,  0.0259, -0.1895],
        [-0.0977,  0.3379, -0.1201, -0.4961, -0.5820],
        [-0.0145,  0.2676,  0.2988, -0.1855, -0.5859],
        [-0.3359,  0.3770,  0.2754, -0.5273, -0.5039],
        [-0.1875, -0.0723,  0.2314,  0.0593, -0.3652],
        [-0.0547,  0.3984,  0.1226, -0.4238, -0.0447]], device='cuda:0',
       dtype=torch.bfloat16, grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)
out(train): SequenceClassifierOutput(loss=tensor(1.6353, device='cuda:0', grad_fn=<NllLossBackward0>), logits=tensor([[ 0.0908,  0.1719,  0.2109, -0.5000, -0.2832],
        [-0.2871, -0.1221,  0.5195, -0.2793, -0.9570],
        [ 0.0342, -0.1157,  0.3730, -0.3574, -0.2500],
        [ 0.2539, -0.1631,  0.5820,  0.0845, -0.8672],
        [-0.2734,  0.1377,  0.1187,  0.1250, -0.0216],
        [ 0.0143,  0.1128, -0.0757, -0.4395, -0.3164],
        [-0.2969,  0.4336, -0.1396, -0.0889, -0.2793],
        [-0.2285,  0.3066,  0.3867, -0.1211, -0.2988],
        [ 0.0278,  0.0679,  0.4102, -0.2227, -0.1514],
        [-0.3730,  0.3594,  0.2520,  0.1050, -0.4941],
        [-0.0972,  0.2188,  0.1514, -0.1992, -0.3984],
        [ 0.1040, -0.2168, -0.1191, -0.2676,  0.1641],
        [ 0.0396,  0.2793,  0.2178, -0.2852, -0.2412],
        [-0.1748, -0.2598,  0.7578, -0.0537, -0.4863],
        [ 0.0459,  0.3770,  0.1572, -0.0525,  0.0840],
        [-0.0369,  0.2314,  0.2520, -0.1807, -0.8867]], device='cuda:0',
       dtype=torch.bfloat16, grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)
out(train): SequenceClassifierOutput(loss=tensor(1.5337, device='cuda:0', grad_fn=<NllLossBackward0>), logits=tensor([[ 0.1396,  0.3750, -0.2949, -0.5352,  0.0742],
        [-0.0146,  0.1387,  0.1611,  0.2002, -0.5391],
        [ 0.0073,  0.2207,  0.6367, -0.2930, -0.5195],
        [ 0.0801,  0.2598,  0.0669, -0.2930, -0.3418],
        [ 0.0488,  0.1318, -0.0356, -0.4043, -0.2031],
        [ 0.2695,  0.0928,  0.3926, -0.0588, -0.1680],
        [ 0.1777, -0.1768, -0.4160, -0.4258,  0.4375],
        [-0.1709,  0.1260,  0.5352, -0.5234, -0.7539],
        [ 0.0806,  0.2793,  0.3301, -0.4316, -0.7422],
        [-0.1641,  0.0986,  0.0077, -0.5117, -0.3535],
        [ 0.1455,  0.4395,  0.0640, -0.5469, -0.2129],
        [-0.4531, -0.4082,  0.4512, -0.3125, -1.3828],
        [-0.1006, -0.0427,  0.3359, -0.2031, -0.7969],
        [-0.0054, -0.0106,  0.3945, -0.3477, -0.4004],
        [-0.1934,  0.2930,  0.2412, -0.2520, -0.9297],
        [-0.2539, -0.0356,  0.2754,  0.1621, -0.4180]], device='cuda:0',
       dtype=torch.bfloat16, grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)
W0629 11:52:52.467000 139012379337344 torch/_dynamo/convert_frame.py:824]     result = inner_convert(
out(train): SequenceClassifierOutput(loss=tensor(1.5430, device='cuda:0', grad_fn=<NllLossBackward0>), logits=tensor([[ 7.0801e-02,  1.7188e-01, -1.1230e-01, -4.7363e-02, -5.8984e-01],
        [-2.2736e-03, -6.0303e-02, -9.5215e-02, -3.1055e-01, -1.1484e+00],
        [-1.9922e-01,  1.3086e-01, -1.8082e-03, -4.7119e-02, -1.7578e-01],
        [-2.3047e-01,  2.4219e-01,  2.1973e-01, -3.1641e-01, -9.1406e-01],
        [ 2.5024e-02,  1.4844e-01, -1.8066e-01, -1.6699e-01, -2.1582e-01],
        [-6.9336e-02,  1.1133e-01,  2.0898e-01, -2.2754e-01, -3.0859e-01],
        [ 2.9102e-01, -1.8848e-01, -6.3672e-01, -4.5312e-01,  5.0000e-01],
        [ 5.8105e-02, -1.4844e-01, -1.5918e-01, -4.6094e-01,  2.1875e-01],
        [-1.1230e-01,  5.1172e-01,  6.8750e-01, -2.2168e-01, -5.9766e-01],
        [-1.7871e-01, -4.2969e-01,  8.2031e-02, -9.9609e-02, -4.2578e-01],
        [-2.5391e-01, -1.4746e-01,  1.3281e-01, -4.1211e-01, -5.4688e-01],
        [-1.4355e-01, -9.7656e-04,  5.2344e-01, -1.1035e-01, -5.3516e-01],
        [ 8.2031e-02, -2.1362e-03,  1.6211e-01, -1.8945e-01, -2.8516e-01],
        [-1.7578e-01, -7.1411e-03,  3.3398e-01,  6.3965e-02, -4.9609e-01],
        [ 6.5918e-02,  1.9238e-01, -1.4062e-01,  7.1289e-02, -1.6406e-01],
        [-4.0820e-01,  4.4922e-01,  1.3574e-01, -5.8984e-01, -6.9922e-01]],
       device='cuda:0', dtype=torch.bfloat16, grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)
out(train): SequenceClassifierOutput(loss=tensor(1.5625, device='cuda:0', grad_fn=<NllLossBackward0>), logits=tensor([[-0.2305,  0.1494,  0.1187, -0.2461, -0.2578],
        [-0.2275,  0.0111,  0.0427, -0.0374, -0.2139],
        [-0.1826,  0.1045,  0.3887, -0.2266, -0.6992],
        [-0.1904,  0.3457,  0.1182, -0.3848, -0.2197],
        [ 0.1699,  0.0854, -0.0583, -0.4844, -0.1514],
        [-0.5859, -0.0325,  0.0791,  0.0947, -0.1885],
        [-0.0981,  0.2676,  0.0415, -0.1030, -0.5898],
        [-0.3438,  0.1455,  0.1216, -0.3164, -0.1475],
        [-0.3848,  0.5195,  0.1494, -0.1201, -0.4512],
        [-0.5938, -0.0396,  0.0583, -0.4023, -0.5664],
        [ 0.0796,  0.1099, -0.0330, -0.1768, -0.2412],
        [-0.1328,  0.3477, -0.0830, -0.6445, -0.6367],
        [-0.1943, -0.0302,  0.2412,  0.2227, -0.7188],
        [ 0.0354,  0.2373,  0.4297, -0.1992,  0.3164],
        [-0.1523,  0.2578, -0.0376, -0.4004, -0.3320],
        [-0.0144, -0.3750,  0.5000, -0.2490, -0.6172]], device='cuda:0',
       dtype=torch.bfloat16, grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)
out(train): SequenceClassifierOutput(loss=tensor(1.5996, device='cuda:0', grad_fn=<NllLossBackward0>), logits=tensor([[-0.2217, -0.0347,  0.2324, -0.2852, -0.8867],
        [-0.2041,  0.2314,  0.1138, -0.2412, -0.3672],
        [-0.1357,  0.0442,  0.4609, -0.0684, -0.3613],
        [ 0.0708,  0.1387,  0.0869, -0.0033, -0.2246],
        [ 0.1094, -0.1040,  0.1689, -0.1963,  0.1553],
        [-0.1758,  0.0569,  0.2158,  0.0500, -0.2246],
        [-0.3047,  0.6211,  0.2598, -0.0552, -0.6016],
        [ 0.1631,  0.0253,  0.2305, -0.3008, -0.2285],
        [-0.0728, -0.1226,  0.8711, -0.0400, -0.7383],
        [-0.0403,  0.0354,  0.1162, -0.3691, -0.0835],
        [-0.1074,  0.3887,  0.1465, -0.2871, -0.4199],
        [ 0.0471,  0.0176,  0.1777, -0.0053, -0.5859],
        [-0.1279, -0.0308,  0.0874, -0.0486, -0.4277],
        [-0.0840,  0.0361, -0.0608, -0.1475,  0.0261],
        [ 0.3301,  0.1533,  0.3320, -0.0476, -0.4297],
        [-0.1582, -0.0603, -0.0168, -0.2910, -0.3945]], device='cuda:0',
       dtype=torch.bfloat16, grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)
out(train): SequenceClassifierOutput(loss=tensor(1.4238, device='cuda:0', grad_fn=<NllLossBackward0>), logits=tensor([[ 0.0212,  0.3984,  0.0300, -0.3301, -0.4805],
        [-0.0776,  0.2617,  0.4961, -0.6133, -0.4102],
        [-0.3184,  0.2188,  0.2441, -0.4160, -0.0723],
        [ 0.1030,  0.3691,  0.1816,  0.1162, -0.3008],
        [ 0.0115,  0.0248,  0.0204, -0.2188, -0.8789],
        [ 0.2637, -0.0894, -0.0640,  0.0674, -0.1416],
        [-0.2812,  0.1357,  0.7383, -0.3105, -0.9336],
        [-0.0933,  0.1436,  0.4004, -0.3516, -0.2969],
        [ 0.2305,  0.2256,  0.3633, -0.4062, -0.2734],
        [ 0.1289,  0.1475,  0.0688, -0.3008, -0.6016],
        [-0.2480,  0.3730,  0.0986, -0.1084, -0.1348],
        [-0.1914,  0.2520,  0.3496, -0.1279, -0.2441],
        [-0.5547,  0.1396,  0.5742, -0.5195, -0.6953],
        [-0.1426,  0.2910, -0.0510,  0.1406, -0.6406],
        [-0.3184, -0.2354,  0.2012, -0.0869, -0.4648],
        [-0.4570,  0.3613,  0.5312, -0.2119, -0.2334]], device='cuda:0',
       dtype=torch.bfloat16, grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)
out(train): SequenceClassifierOutput(loss=tensor(1.4404, device='cuda:0', grad_fn=<NllLossBackward0>), logits=tensor([[-0.1699, -0.1060,  0.0012, -0.4512, -0.1396],
        [-0.1348,  0.2793,  0.4141, -0.2559, -0.9336],
        [ 0.0708, -0.1357,  0.1406, -0.5938, -0.7734],
        [-0.1055,  0.2129,  0.0762, -0.1162, -0.0579],
        [-0.3887,  0.5156,  0.3203, -0.4961, -0.6562],
        [-0.2578,  0.3320,  0.2559,  0.0845, -0.2910],
        [ 0.0527, -0.0386,  0.1279, -0.6992, -0.6641],
        [-0.0422,  0.2305, -0.1021, -0.2578, -0.4082],
        [ 0.0942,  0.0908,  0.1689, -0.3359, -0.4707],
        [-0.3203, -0.0698,  0.1582,  0.1475, -0.6016],
        [-0.2256,  0.2852,  0.1816, -0.0060, -0.0898],
        [-0.0165,  0.6992, -0.1777, -0.1797,  0.0146],
        [-0.1699,  0.5039,  0.3535, -0.2207, -0.3340],
        [-0.5312,  0.1177,  0.2598, -0.5742, -0.6680],
        [ 0.1729,  0.2197, -0.0400, -0.1680, -0.0708],
        [-0.1221, -0.1089, -0.0182, -0.2373, -0.7656]], device='cuda:0',
       dtype=torch.bfloat16, grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)
out(train): SequenceClassifierOutput(loss=tensor(1.4902, device='cuda:0', grad_fn=<NllLossBackward0>), logits=tensor([[ 3.2227e-01, -4.8340e-02, -6.8359e-02,  1.4746e-01,  3.3203e-01],
        [-2.9883e-01,  1.2500e-01,  6.5430e-02, -2.9102e-01, -4.4336e-01],
        [-2.4902e-02,  3.3398e-01,  1.2012e-01, -1.8457e-01, -3.4961e-01],
        [-3.0859e-01, -3.8330e-02, -1.2061e-01, -2.7539e-01, -7.5000e-01],
        [-3.2617e-01,  2.8320e-01, -2.2095e-02, -3.9062e-02, -4.1797e-01],
        [-9.3262e-02,  5.4688e-01,  3.9453e-01, -1.4062e-01, -5.2734e-01],
        [ 5.8746e-04,  3.6914e-01,  3.9844e-01, -7.0312e-01, -3.9062e-02],
        [-2.0386e-02,  2.1191e-01,  5.0391e-01, -9.8633e-02, -9.1406e-01],
        [-2.6978e-02,  4.5312e-01, -3.7354e-02, -1.6113e-01, -3.9453e-01],
        [ 2.2559e-01, -1.2207e-01, -2.2070e-01, -5.3125e-01,  2.7539e-01],
        [-3.0469e-01, -2.9907e-02,  7.5195e-02, -2.3145e-01, -3.8281e-01],
        [ 7.4707e-02,  2.5586e-01,  4.5508e-01, -3.7891e-01, -4.7656e-01],
        [ 1.1133e-01,  3.6719e-01,  2.7539e-01, -4.1016e-01, -4.1406e-01],
        [-6.1719e-01, -1.2207e-01,  3.2422e-01, -2.0508e-01, -6.2500e-01],
        [-2.7539e-01,  2.9492e-01,  9.7656e-02, -2.3438e-01, -2.2949e-01],
        [ 5.0391e-01, -1.3062e-02,  3.8818e-02, -9.7656e-02, -8.7402e-02]],
       device='cuda:0', dtype=torch.bfloat16, grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)
out(train): SequenceClassifierOutput(loss=tensor(1.5259, device='cuda:0', grad_fn=<NllLossBackward0>), logits=tensor([[-0.0498,  0.3398, -0.0178, -0.1309, -0.4766],
        [-0.2402,  0.0143, -0.3789, -0.4121, -0.3867],
        [ 0.2422,  0.1387, -0.0815, -0.0991, -0.3164],
        [-0.1592,  0.2656,  0.0008, -0.3711, -0.4121],
        [-0.0051,  0.1572,  0.0422, -0.1104, -0.4883],
        [ 0.1289,  0.1768,  0.1060,  0.0762, -0.6367],
        [ 0.1963,  0.4707,  0.1328, -0.4277, -0.7695],
        [-0.1084, -0.1270, -0.0430, -0.3848, -0.5000],
        [-0.2441,  0.1387,  0.4805, -0.3594, -0.6133],
        [-0.0688,  0.2695,  0.0190, -0.0306, -0.6602],
        [-0.0015,  0.0781, -0.2461, -0.4941, -0.0266],
        [-0.1748,  0.2246,  0.1406, -0.0664, -0.7070],
        [-0.3613,  0.2676,  0.7773, -0.2100, -0.6328],
        [-0.1030,  0.3359,  0.2539, -0.2969, -0.3438],
        [-0.2188,  0.1089,  0.2021,  0.1973, -0.3691],
        [ 0.0089,  0.1445, -0.0056, -0.6250,  0.0189]], device='cuda:0',
       dtype=torch.bfloat16, grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)
out(train): SequenceClassifierOutput(loss=tensor(1.4541, device='cuda:0', grad_fn=<NllLossBackward0>), logits=tensor([[-0.3535,  0.5898, -0.0933, -0.3184, -0.0640],
        [ 0.1514, -0.0481, -0.2500, -0.1982, -0.6875],
        [-0.0693,  0.2559,  0.3340, -0.4004, -0.0610],
        [-0.0664,  0.4668,  0.0747, -0.4062, -0.2559],
        [ 0.3359,  0.0566,  0.0330, -0.4434, -0.4121],
        [-0.3320,  0.1279,  0.2393, -0.0195, -0.1289],
        [ 0.1562,  0.3105,  0.0845, -0.3613, -0.2969],
        [-0.0593,  0.1113,  0.1963,  0.0815, -0.4316],
        [-0.0294,  0.1494,  0.3340, -0.1045, -0.4141],
        [ 0.1631, -0.1504,  0.3848, -0.1592, -0.5078],
        [-0.2461,  0.2793,  0.1260,  0.1221, -0.4277],
        [-0.0840,  0.3574, -0.0383, -0.4258, -0.2227],
        [-0.3906,  0.0559,  0.3652, -0.3828, -0.6680],
        [-0.0698,  0.1348,  0.3398, -0.0728, -0.3555],
        [-0.0564, -0.1797, -0.2334, -0.4473, -0.2969],
        [-0.2480,  0.1904,  0.4648, -0.0796, -0.3711]], device='cuda:0',
       dtype=torch.bfloat16, grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)
out(train): SequenceClassifierOutput(loss=tensor(1.4683, device='cuda:0', grad_fn=<NllLossBackward0>), logits=tensor([[-0.1436, -0.2217,  0.2354, -0.3398, -0.4219],
        [ 0.0277,  0.2578,  0.1602, -0.3477, -0.8633],
        [ 0.0277,  0.3262,  0.2070,  0.1777, -0.4258],
        [-0.0640, -0.0447,  0.4375,  0.0330, -0.6641],
        [-0.1045, -0.0923,  0.0493, -0.6406, -0.1147],
        [ 0.0292, -0.2129,  0.0352, -0.6719, -0.3535],
        [-0.1240, -0.0474, -0.1196, -0.3945, -0.5156],
        [-0.1021,  0.4023,  0.1787, -0.2695, -0.1050],
        [ 0.2021,  0.2715,  0.5039, -0.7617, -0.1777],
        [-0.2637,  0.2148,  0.2852, -0.3438, -0.5586],
        [-0.0403,  0.0708,  0.4863, -0.0364, -0.5312],
        [-0.3223,  0.5117,  0.3926, -0.4629, -0.4766],
        [-0.3926,  0.0913, -0.0608, -0.3711, -0.4492],
        [-0.6172,  0.2119, -0.0918, -0.1494, -0.5508],
        [ 0.2314,  0.2676, -0.2168, -0.1582, -0.2461],
        [ 0.0165,  0.0254,  0.1641, -0.0840, -0.2461]], device='cuda:0',
       dtype=torch.bfloat16, grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)
W0629 11:52:52.467000 139012379337344 torch/_dynamo/convert_frame.py:824]     result = inner_convert(
out(train): SequenceClassifierOutput(loss=tensor(1.3525, device='cuda:0', grad_fn=<NllLossBackward0>), logits=tensor([[-0.4434,  0.0669,  0.1250, -0.5703, -0.7656],
        [-0.2197,  0.2246,  0.1260, -0.3496, -0.9258],
        [-0.0674,  0.3281, -0.0294, -0.0608, -0.2393],
        [ 0.3320,  0.3789,  0.1846,  0.0231, -0.2227],
        [-0.2402,  0.2227,  0.2100, -0.2305, -0.6484],
        [-0.1494, -0.0588,  0.2236,  0.0256, -0.0610],
        [-0.0211,  0.3496, -0.0054, -0.3457, -0.3066],
        [-0.3203,  0.2539, -0.0811, -0.2832, -0.4570],
        [-0.5391,  0.1885, -0.1572, -0.0454, -0.3516],
        [-0.0737,  0.0708,  0.0121, -0.4043, -0.1089],
        [-0.2197,  0.3594,  0.2393, -0.2344, -0.2891],
        [-0.2012,  0.5430,  0.5742, -0.2520, -0.3574],
        [-0.0157,  0.4395,  0.3047, -0.2539, -0.2305],
        [-0.2471,  0.3652,  0.1729, -0.2373, -0.4238],
        [-0.0187,  0.2598,  0.3789, -0.3848, -0.9180],
        [-0.2500,  0.0688,  0.2305, -0.0620, -0.6016]], device='cuda:0',
       dtype=torch.bfloat16, grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)
out(train): SequenceClassifierOutput(loss=tensor(1.4365, device='cuda:0', grad_fn=<NllLossBackward0>), logits=tensor([[-2.2095e-02,  3.7891e-01,  6.5430e-02, -3.4375e-01, -2.6367e-02],
        [-4.6387e-02, -1.2207e-01, -1.6797e-01, -4.2383e-01,  2.8534e-03],
        [ 5.1270e-02,  1.6797e-01,  1.7676e-01, -1.7773e-01, -3.9062e-01],
        [-3.8867e-01,  4.0283e-02,  1.0352e-01, -8.1055e-02, -1.2354e-01],
        [-1.1865e-01,  3.6523e-01,  7.1289e-02, -4.7266e-01, -7.0312e-02],
        [-2.1289e-01,  2.3730e-01,  1.6895e-01, -6.1279e-02, -2.7344e-01],
        [-4.0771e-02,  8.7402e-02,  2.0508e-01, -4.9219e-01, -8.9062e-01],
        [-2.0801e-01,  1.3123e-02,  5.1514e-02, -1.4648e-01, -2.8516e-01],
        [-8.4961e-02,  2.8711e-01,  4.0820e-01,  4.1809e-03, -7.2266e-01],
        [-1.2695e-02,  6.3477e-02,  1.5015e-02, -6.0791e-02, -5.1953e-01],
        [-2.2461e-01,  3.4375e-01,  3.1641e-01, -5.6250e-01, -5.1172e-01],
        [-4.5898e-01,  7.8613e-02,  2.8125e-01, -4.9219e-01, -9.1797e-01],
        [ 6.5994e-04, -2.3315e-02,  1.7188e-01, -1.1816e-01, -3.9844e-01],
        [-4.2578e-01,  5.6641e-01,  5.0000e-01, -1.6992e-01, -5.0391e-01],
        [ 2.1484e-01,  6.2500e-02, -1.2598e-01, -5.9375e-01, -4.7266e-01],
        [-3.7695e-01,  3.5352e-01,  1.3965e-01, -2.8906e-01, -4.1016e-01]],
       device='cuda:0', dtype=torch.bfloat16, grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)
out(train): SequenceClassifierOutput(loss=tensor(1.4248, device='cuda:0', grad_fn=<NllLossBackward0>), logits=tensor([[-0.1279,  0.3125,  0.1729, -0.4297, -0.2715],
        [-0.2490,  0.4414, -0.2041, -0.0913, -0.1260],
        [-0.0811,  0.2090,  0.2021, -0.3008, -0.2988],
        [ 0.0791,  0.0591,  0.0947, -0.4668, -0.3867],
        [ 0.0840,  0.2314,  0.1885, -0.2354, -0.0928],
        [-0.2754,  0.2773,  0.4512, -0.0991, -0.6328],
        [ 0.0491,  0.0635, -0.1758, -0.4336, -0.1895],
        [-0.2891, -0.1216,  0.2393, -0.3242, -0.5898],
        [-0.5547,  0.0574,  0.3887, -0.0786, -0.5352],
        [-0.1709,  0.1416,  0.1338, -0.1455, -0.3965],
        [ 0.0044,  0.2188, -0.0396, -0.3242, -0.4082],
        [-0.2656,  0.2988,  0.4180, -0.2451, -0.3965],
        [ 0.2188,  0.0845,  0.2773, -0.2021, -0.2217],
        [-0.2637,  0.1494,  0.2412,  0.0952, -0.3789],
        [ 0.0098,  0.1895,  0.1797,  0.1055, -0.1299],
        [ 0.3945, -0.2754, -0.4160, -0.4727, -0.0894]], device='cuda:0',
       dtype=torch.bfloat16, grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)
out(train): SequenceClassifierOutput(loss=tensor(1.4023, device='cuda:0', grad_fn=<NllLossBackward0>), logits=tensor([[-0.0117,  0.3379, -0.1089, -0.2422,  0.1230],
        [-0.1357,  0.0571,  0.1846,  0.0427, -0.4590],
        [ 0.1270,  0.3301,  0.2100, -0.0679, -0.2178],
        [ 0.1797,  0.6133, -0.1445, -0.4238, -0.1660],
        [-0.1455, -0.1426,  0.2324, -0.1001, -0.2080],
        [ 0.0771,  0.3457, -0.1807, -0.2676, -0.1641],
        [-0.1030,  0.4453, -0.0835, -0.0170, -0.3340],
        [-0.1816,  0.3848,  0.0403, -0.2402, -0.3945],
        [-0.1650,  0.2432,  0.1230, -0.4648, -0.3477],
        [-0.0339, -0.1445, -0.0884, -0.4414, -0.1494],
        [-0.1426,  0.3359,  0.3027, -0.5273, -0.3633],
        [-0.3594, -0.0447, -0.2754, -0.0400, -0.3105],
        [-0.1118,  0.4707,  0.2539, -0.1328, -0.4180],
        [-0.1826, -0.1279,  0.4609, -0.6406, -0.9180],
        [ 0.0859,  0.1924,  0.4141, -0.1611, -0.5742],
        [-0.4492,  0.1133,  0.3516, -0.4023, -0.7578]], device='cuda:0',
       dtype=torch.bfloat16, grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)
out(train): SequenceClassifierOutput(loss=tensor(1.3745, device='cuda:0', grad_fn=<NllLossBackward0>), logits=tensor([[ 0.1299,  0.1357,  0.0752, -0.3242,  0.3770],
        [-0.1641,  0.2617, -0.0114, -0.1357, -0.3887],
        [-0.1074,  0.1582,  0.1006, -0.4492, -0.0640],
        [-0.2832,  0.3906,  0.1318, -0.3320, -0.5859],
        [-0.1040, -0.0066,  0.0518, -0.2012, -0.3652],
        [-0.1187,  0.2197,  0.1572, -0.1240, -0.3672],
        [-0.4922,  0.7227,  0.3398, -0.4453, -0.4160],
        [-0.2773,  0.3457,  0.0067, -0.1826, -0.2061],
        [ 0.1367,  0.1826, -0.0332, -0.4375,  0.2432],
        [-0.1226,  0.5078,  0.0513, -0.3105, -0.3828],
        [ 0.2578,  0.2227,  0.0908, -0.2012, -0.1396],
        [-0.1289,  0.2344,  0.3477, -0.0125, -0.3301],
        [ 0.1426,  0.5508,  0.3516, -0.1543, -0.3164],
        [ 0.1582,  0.4746,  0.0056, -0.5312, -0.3984],
        [-0.0349,  0.2598, -0.4453, -0.2383, -0.2754],
        [-0.2656,  0.4453,  0.5156, -0.4141, -0.4473]], device='cuda:0',
       dtype=torch.bfloat16, grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)
out(train): SequenceClassifierOutput(loss=tensor(1.4595, device='cuda:0', grad_fn=<NllLossBackward0>), logits=tensor([[-0.0757,  0.1816, -0.0417, -0.5195, -0.1963],
        [-0.1318,  0.2852, -0.0104, -0.3027, -0.4551],
        [-0.1953,  0.3574,  0.0095, -0.0222, -0.4199],
        [ 0.1289,  0.1533,  0.2051, -0.3887, -0.2178],
        [-0.2275,  0.5078, -0.0596, -0.2520, -0.0835],
        [ 0.0493,  0.5273,  0.1079, -0.0566, -0.3848],
        [-0.0618,  0.2773,  0.0017, -0.1338, -0.0986],
        [-0.0361,  0.3535,  0.1865, -0.3770, -0.1289],
        [ 0.0354,  0.0157,  0.2227, -0.0070, -0.3242],
        [-0.2324, -0.0564,  0.7930, -0.1650, -0.1777],
        [-0.2314,  0.1299,  0.1416, -0.4434, -0.2617],
        [-0.1797,  0.1191,  0.3164, -0.1426, -0.0762],
        [-0.2041,  0.3242,  0.1216, -0.1660, -0.2031],
        [-0.5195,  0.3477,  0.3066, -0.3418, -0.6836],
        [ 0.0361,  0.0181, -0.2031, -0.0742, -0.0669],
        [-0.6211,  0.3926, -0.0072, -0.0698, -0.5039]], device='cuda:0',
       dtype=torch.bfloat16, grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)
out(train): SequenceClassifierOutput(loss=tensor(1.4351, device='cuda:0', grad_fn=<NllLossBackward0>), logits=tensor([[-3.8086e-01,  3.4961e-01,  1.9531e-01, -2.5195e-01, -6.4844e-01],
        [ 8.5938e-02,  7.9590e-02,  2.3438e-02, -8.6914e-02, -2.2559e-01],
        [-3.5352e-01,  3.8867e-01,  1.5430e-01,  1.0889e-01, -1.6504e-01],
        [-3.7305e-01,  4.0625e-01, -1.0156e-01, -4.2773e-01, -4.4141e-01],
        [-4.8438e-01,  3.9648e-01,  3.8477e-01, -4.0430e-01, -3.2617e-01],
        [-1.5820e-01,  2.4121e-01,  3.4766e-01, -2.4414e-01, -6.1768e-02],
        [-3.4424e-02,  3.6328e-01,  9.7656e-02, -1.5625e-01, -4.9414e-01],
        [-2.9883e-01,  4.1406e-01,  3.1641e-01, -4.4531e-01, -5.1562e-01],
        [-3.4961e-01,  3.0078e-01,  4.1016e-01, -1.8164e-01, -5.3906e-01],
        [-2.0215e-01, -1.3770e-01,  3.7695e-01, -1.4160e-01, -9.9219e-01],
        [ 1.4160e-01,  5.6641e-01, -2.7734e-01, -3.6328e-01, -3.8818e-02],
        [-1.0864e-02,  2.0020e-01, -1.2329e-02, -9.5703e-02, -4.2383e-01],
        [-1.8311e-02,  3.8086e-01,  3.0859e-01, -2.4609e-01, -4.9805e-02],
        [-1.6797e-01,  4.2188e-01,  7.3730e-02, -8.7891e-02, -5.1025e-02],
        [ 2.6489e-02,  4.2773e-01,  3.1055e-01,  1.6499e-04, -1.0400e-01],
        [-4.4922e-01,  5.4297e-01,  1.0925e-02,  4.9561e-02, -1.8848e-01]],
       device='cuda:0', dtype=torch.bfloat16, grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)
out(train): SequenceClassifierOutput(loss=tensor(1.4438, device='cuda:0', grad_fn=<NllLossBackward0>), logits=tensor([[-0.1875,  0.2969, -0.4004, -0.0190, -0.4590],
        [-0.2383,  0.0894,  0.1216, -0.3555, -0.7773],
        [-0.0383,  0.3438,  0.2793,  0.0422, -0.2656],
        [-0.4961,  0.0737,  0.3262, -0.5195, -0.3008],
        [-0.2598,  0.4805,  0.3906, -0.2422, -0.3965],
        [-0.0496,  0.0620,  0.1245, -0.3496, -0.1270],
        [-0.3809,  0.0820,  0.2041, -0.2227, -0.4805],
        [-0.1426,  0.3652,  0.2793, -0.0991, -0.1064],
        [-0.5312,  0.0413,  0.4180, -0.2314, -0.5898],
        [-0.0825,  0.3164,  0.2197, -0.0972, -0.5391],
        [-0.1758,  0.2295,  0.1484, -0.3008, -0.8906],
        [-0.4062, -0.0459,  0.3047, -0.1748, -0.8047],
        [-0.1543,  0.1030, -0.4023, -0.1826, -0.0801],
        [-0.0413,  0.3047,  0.2715, -0.2812,  0.0645],
        [-0.1797,  0.2617,  0.3223, -0.2676, -0.2891],
        [ 0.2119, -0.0072,  0.1030, -0.1167, -0.6328]], device='cuda:0',
       dtype=torch.bfloat16, grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)
out(train): SequenceClassifierOutput(loss=tensor(1.4048, device='cuda:0', grad_fn=<NllLossBackward0>), logits=tensor([[-0.3145,  0.2949,  0.0908,  0.0742, -0.4766],
        [-0.2949,  0.1484,  0.2061, -0.4668, -0.9414],
        [ 0.3613,  0.1172, -0.3535, -0.5781, -0.0239],
        [-0.0056,  0.0962, -0.0620, -0.1562, -0.3242],
        [-0.0703,  0.5312,  0.1611, -0.2852, -0.4199],
        [-0.1055,  0.3965,  0.4141,  0.0277, -0.2754],
        [-0.0767,  0.0781,  0.0850, -0.0029, -0.6758],
        [-0.2520,  0.4414, -0.0820, -0.2383, -0.3984],
        [-0.3613, -0.3066,  0.4199, -0.1621, -0.2754],
        [-0.3828,  0.0571,  0.2656, -0.3086, -1.2344],
        [-0.1553,  0.3906,  0.3633, -0.2412, -1.1094],
        [-0.1807,  0.2969,  0.0332, -0.2490, -0.2734],
        [ 0.0439,  0.3457, -0.1924, -0.1426, -0.2539],
        [-0.1689,  0.4336, -0.1543, -0.1050, -0.5117],
        [ 0.1367, -0.0176, -0.1357, -0.2559, -0.2080],
        [-0.1387,  0.3652,  0.1396, -0.0400, -0.0464]], device='cuda:0',
       dtype=torch.bfloat16, grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)
out(train): SequenceClassifierOutput(loss=tensor(1.4009, device='cuda:0', grad_fn=<NllLossBackward0>), logits=tensor([[-0.1748,  0.4434, -0.1807, -0.2344, -0.2480],
        [-0.1777,  0.0972, -0.0693, -0.2168, -0.0962],
        [-0.1240,  0.0082, -0.1875,  0.0269, -0.5312],
        [-0.8867,  0.3359,  0.3945, -0.3730, -0.6680],
        [-0.2256,  0.5547,  0.0420, -0.3594, -0.4531],
        [-0.4297,  0.2637, -0.0347, -0.1240, -0.3984],
        [ 0.1025,  0.0187, -0.4609, -0.3242, -0.0146],
        [-0.3809,  0.5117, -0.2500, -0.4453, -0.4141],
        [ 0.1436,  0.2334,  0.5312,  0.0212, -0.3809],
        [ 0.2637,  0.5391,  0.7383, -0.3281, -0.4121],
        [-0.4004,  0.1245, -0.1641, -0.2793, -0.7734],
        [ 0.0297,  0.4805, -0.0879, -0.3711, -0.0996],
        [-0.0315,  0.3223, -0.1680, -0.6445, -0.4219],
        [-0.2129,  0.4043, -0.2695, -0.4277, -0.4492],
        [-0.1318,  0.0228, -0.1992, -0.3066, -0.2041],
        [-0.2217,  0.3770, -0.1240, -0.4980,  0.2100]], device='cuda:0',
       dtype=torch.bfloat16, grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)
W0629 11:52:52.467000 139012379337344 torch/_dynamo/convert_frame.py:824]     result = inner_convert(
out(train): SequenceClassifierOutput(loss=tensor(1.4463, device='cuda:0', grad_fn=<NllLossBackward0>), logits=tensor([[-0.0762,  0.1138, -0.1738, -0.3750, -0.4277],
        [-0.0554,  0.2559, -0.2539, -0.4473, -0.2871],
        [-0.1885, -0.2578,  0.6055,  0.2168, -0.1797],
        [ 0.1680,  0.3906,  0.0762, -0.3145, -0.3594],
        [ 0.0240,  0.3164, -0.0300, -0.4512, -0.6797],
        [-0.1484,  0.1826,  0.2852, -0.3477, -0.4082],
        [-0.4766, -0.0806, -0.1572, -0.1289, -0.4023],
        [-0.0112,  0.1865, -0.0147, -0.6484, -0.3574],
        [-0.1865,  0.4316,  0.3594, -0.3730, -0.3457],
        [-0.1250,  0.2061,  0.3594, -0.3086, -0.2002],
        [-0.0608,  0.5039,  0.2910, -0.4316,  0.0454],
        [ 0.0610, -0.3379,  0.8203, -0.3359, -0.4727],
        [-0.1030, -0.0308, -0.0325, -0.2812, -0.2715],
        [ 0.1182,  0.3984, -0.0679, -0.1758,  0.3281],
        [-0.3223,  0.4141, -0.2041, -0.5352, -0.4199],
        [-0.2812,  0.2695,  0.0879, -0.4277, -0.2949]], device='cuda:0',
       dtype=torch.bfloat16, grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)
out(train): SequenceClassifierOutput(loss=tensor(1.4092, device='cuda:0', grad_fn=<NllLossBackward0>), logits=tensor([[ 0.1670, -0.4570, -0.5039, -0.5859, -0.1709],
        [ 0.1230,  0.2637,  0.0820, -0.2676, -0.3691],
        [ 0.2773,  0.1934,  0.1719, -0.3535,  0.1465],
        [-0.3535,  0.4082, -0.0923,  0.0977, -0.6914],
        [-0.1128,  0.1338,  0.2422, -0.1748, -0.5586],
        [-0.0294,  0.0356,  0.1143, -0.5703, -0.5586],
        [-0.2148,  0.1211, -0.3320, -0.1719, -0.2402],
        [-0.0248,  0.4492, -0.0085,  0.0018, -0.3574],
        [-0.0347,  0.3867, -0.0186, -0.3340, -0.2637],
        [-0.3457,  0.5898, -0.0713, -0.1484, -0.3477],
        [-0.2715,  0.1436,  0.3945, -0.2617, -0.9180],
        [ 0.0874,  0.2852,  0.3125, -0.2061, -0.8281],
        [ 0.0035,  0.2930,  0.2910, -0.4160, -0.4629],
        [-0.4180,  0.5273,  0.0564,  0.1436, -0.0796],
        [-0.1777,  0.3184,  0.1069,  0.0325, -0.5391],
        [ 0.1855,  0.0547,  0.0439, -0.1582, -0.4922]], device='cuda:0',
       dtype=torch.bfloat16, grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)
out(train): SequenceClassifierOutput(loss=tensor(1.4194, device='cuda:0', grad_fn=<NllLossBackward0>), logits=tensor([[ 0.1836,  0.2930,  0.2441, -0.1699, -0.1074],
        [-0.2305,  0.0781,  0.0311, -0.1260, -0.2539],
        [ 0.3242,  0.2578, -0.4375, -0.5039, -0.0654],
        [ 0.1016,  0.3613, -0.0188, -0.2080, -0.4785],
        [-0.5898,  0.5156,  0.0815, -0.1621, -0.1846],
        [-0.3770,  0.0090,  0.4453, -0.6758, -0.4609],
        [-0.0361,  0.2539,  0.5352, -0.4121, -0.2988],
        [-0.2949,  0.4707,  0.1963, -0.6680, -0.6055],
        [-0.4590,  0.4199, -0.0977, -0.0996, -0.1680],
        [-0.3770,  0.1021,  0.1436,  0.0070, -0.7344],
        [-0.1016,  0.5156,  0.3008, -0.1787, -0.0571],
        [ 0.2041,  0.2227, -0.0278, -0.2451, -0.3418],
        [-0.0903,  0.3105, -0.0513, -0.1226, -0.3340],
        [-0.3008,  0.4609,  0.3984, -0.3652, -0.4844],
        [-0.2002,  0.6211,  0.1875, -0.2539, -0.2227],
        [-0.0562,  0.0752,  0.3887, -0.1128, -0.5586]], device='cuda:0',
       dtype=torch.bfloat16, grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)
out(train): SequenceClassifierOutput(loss=tensor(1.2974, device='cuda:0', grad_fn=<NllLossBackward0>), logits=tensor([[-0.4766, -0.2031,  0.0830, -0.2275, -0.4121],
        [ 0.0040,  0.2539,  0.3359, -0.3027, -0.3906],
        [ 0.0815,  0.4707,  0.2500, -0.3848, -0.4219],
        [-0.2383,  0.6016,  0.1953, -0.4883, -0.3379],
        [ 0.1533,  0.5352,  0.3027, -0.0693, -0.9727],
        [ 0.1562,  0.0996, -0.0388, -0.1846, -0.1221],
        [-0.4863,  0.6680,  0.2676, -0.2129, -0.5352],
        [ 0.1260,  0.5273,  0.0425, -0.2871, -0.3125],
        [ 0.1064,  0.3340, -0.0579, -0.2852, -0.2695],
        [-0.1055,  0.1016, -0.1167, -0.3418, -0.1187],
        [-0.2256,  0.3945,  0.1514,  0.1660,  0.0500],
        [-0.3242,  0.1357,  0.5039, -0.1035, -0.5781],
        [ 0.1089,  0.1494,  0.5898, -0.1201,  0.0437],
        [-0.1167,  0.5742,  0.1338, -0.2324, -0.4316],
        [-0.3145,  0.2129,  0.0044, -0.4043, -0.4531],
        [-0.0869,  0.7070,  0.1953, -0.4551, -0.1611]], device='cuda:0',
       dtype=torch.bfloat16, grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)
out(train): SequenceClassifierOutput(loss=tensor(1.2207, device='cuda:0', grad_fn=<NllLossBackward0>), logits=tensor([[-0.5312,  0.4707, -0.2021, -0.2500, -0.1982],
        [-0.0518,  0.6680,  0.2832, -0.4082, -0.6875],
        [-0.3223,  0.1631, -0.0640, -0.4785, -0.5586],
        [-0.4375,  0.4805,  0.0703, -0.4492, -0.5234],
        [ 0.2656,  0.3516, -0.2051, -0.6680,  0.0239],
        [ 0.1885,  0.4316,  0.0063, -0.3086, -0.1875],
        [-0.1377,  0.1377, -0.1650, -0.2715, -0.4570],
        [ 0.1826,  0.3613,  0.1514, -0.1650, -0.4043],
        [-0.3613,  0.1689,  0.1216, -0.1191, -0.2100],
        [-0.0845,  0.8242,  0.2471, -0.2715, -0.0027],
        [-0.1680,  0.7305,  0.2432, -0.2520, -0.0193],
        [-0.3555,  0.4961,  0.0476, -0.6836, -0.5430],
        [-0.2754,  0.5195,  0.1934, -0.1118, -0.4785],
        [-0.0610,  0.3730,  0.1592, -0.4609, -0.0654],
        [ 0.1167, -0.1016,  0.1172, -0.1318, -0.2812],
        [ 0.2246,  0.1807, -0.2090, -0.1992, -0.4375]], device='cuda:0',
       dtype=torch.bfloat16, grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)
out(train): SequenceClassifierOutput(loss=tensor(1.3081, device='cuda:0', grad_fn=<NllLossBackward0>), logits=tensor([[ 0.1543,  0.6250,  0.0359,  0.0996, -0.0708],
        [ 0.0378,  0.5820,  0.1416, -0.1406, -0.1128],
        [-0.4609,  0.2793, -0.1592, -0.0188, -0.4160],
        [ 0.1436, -0.2158,  0.3535, -0.4609, -0.4375],
        [ 0.3906,  0.1582, -0.1299, -0.4395, -0.0610],
        [-0.1758,  0.3516,  0.0640, -0.4941, -0.4727],
        [-0.0266,  0.5156, -0.0347, -0.2256, -0.0718],
        [-0.2578,  0.3320,  0.1855, -0.4141, -0.5430],
        [-0.2314,  0.5273,  0.2754, -0.2129, -0.2490],
        [-0.3828,  0.4355, -0.2754, -0.0359, -0.2637],
        [-0.0913,  0.4941,  0.2393, -0.2812, -0.5273],
        [-0.1201,  0.6328, -0.1475, -0.2852, -0.3633],
        [-0.3438,  0.3691,  0.0254, -0.5039, -0.2314],
        [ 0.0078,  0.1631,  0.4727,  0.1709, -0.6875],
        [ 0.1797,  0.1572, -0.0869, -0.2520, -0.3320],
        [-0.1787,  0.5938,  0.0972,  0.4160, -0.3926]], device='cuda:0',
       dtype=torch.bfloat16, grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)
out(train): SequenceClassifierOutput(loss=tensor(1.3301, device='cuda:0', grad_fn=<NllLossBackward0>), logits=tensor([[ 0.1416,  0.5117,  0.0566, -0.1484, -0.1211],
        [-0.3320,  0.0130,  0.2314, -0.0874, -0.3984],
        [-0.2891,  0.2129,  0.0933, -0.3691, -0.2637],
        [-0.0344,  0.2695,  0.5156, -0.0879, -0.0430],
        [-0.2188,  0.4668,  0.1680, -0.4336, -0.5664],
        [-0.2324,  0.4980,  0.2480, -0.0265, -0.4492],
        [-0.3027,  0.2500,  0.3984, -0.3281, -0.7578],
        [-0.3691,  0.1138,  0.4082, -0.1543, -0.5859],
        [-0.0752,  0.3730,  0.4766, -0.4141, -0.6992],
        [-0.0703,  0.7070,  0.1611, -0.2305, -0.0903],
        [ 0.0981,  0.4727,  0.0247,  0.0069, -0.0496],
        [-0.2617, -0.0972,  0.1396, -0.2432, -0.0464],
        [ 0.0752,  0.5156, -0.0732, -0.5234, -0.4668],
        [-0.1572,  0.3359, -0.1187, -0.0107, -0.2002],
        [-0.0977,  0.1035,  0.1758, -0.1943, -0.1943],
        [-0.0271,  0.1465, -0.1631, -0.3887,  0.0058]], device='cuda:0',
       dtype=torch.bfloat16, grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)
out(train): SequenceClassifierOutput(loss=tensor(1.4346, device='cuda:0', grad_fn=<NllLossBackward0>), logits=tensor([[-2.2754e-01,  3.2227e-01, -1.2695e-01, -5.5078e-01, -8.2812e-01],
        [ 1.2451e-01, -2.8320e-01, -4.8828e-01, -4.7852e-01,  6.0059e-02],
        [-4.5410e-02,  3.8672e-01,  1.2878e-02, -4.0234e-01,  1.3281e-01],
        [-3.2812e-01,  3.2227e-01,  2.0142e-02,  3.9062e-01, -1.4648e-01],
        [ 2.0898e-01,  5.8203e-01, -4.1580e-04, -4.0039e-01, -1.8945e-01],
        [-1.3965e-01,  5.1880e-03,  6.3672e-01, -3.3691e-02, -7.5000e-01],
        [ 4.8828e-01, -8.7402e-02, -3.5547e-01,  3.3008e-01,  5.5469e-01],
        [ 5.3711e-02,  2.0117e-01, -5.9326e-02, -3.1641e-01, -3.0078e-01],
        [ 6.0059e-02,  1.3281e-01,  3.5889e-02, -9.4238e-02, -2.4316e-01],
        [-4.5312e-01,  2.8516e-01,  1.5430e-01, -3.7891e-01, -2.5977e-01],
        [-3.1250e-01,  6.8750e-01,  2.4023e-01, -1.9043e-01, -4.0430e-01],
        [ 1.8652e-01,  3.1836e-01, -2.6367e-02,  5.2002e-02, -2.2168e-01],
        [-2.1387e-01,  3.4375e-01,  5.1270e-02, -2.5000e-01, -3.1250e-01],
        [ 4.9805e-02,  1.4062e-01, -6.3965e-02, -6.9922e-01,  2.3047e-01],
        [-1.8848e-01,  2.3145e-01,  1.8164e-01, -4.0283e-02, -7.4609e-01],
        [-7.3730e-02,  4.9414e-01,  4.6094e-01, -2.5391e-01, -7.3828e-01]],
       device='cuda:0', dtype=torch.bfloat16, grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)
out(train): SequenceClassifierOutput(loss=tensor(1.3835, device='cuda:0', grad_fn=<NllLossBackward0>), logits=tensor([[-0.1592,  0.4062,  0.2363, -0.1865, -0.3047],
        [-0.2148,  0.2021,  0.2812, -0.3906, -0.8633],
        [-0.3633,  0.1748, -0.1108, -0.0957, -0.4004],
        [-0.2715,  0.1729,  0.0623,  0.1680, -0.6914],
        [-0.2539,  0.0396, -0.0288, -0.4219, -0.6367],
        [-0.1230, -0.1348, -0.1777, -0.4004, -0.6523],
        [-0.3613,  0.2754,  0.0308, -0.0145, -0.4043],
        [-0.1865,  0.5508,  0.1299,  0.0942, -0.3242],
        [-0.0408,  0.8281,  0.1494, -0.3691, -0.1973],
        [-0.1719,  0.2021,  0.0089, -0.1187, -0.3848],
        [-0.4277,  0.4941,  0.0090, -0.3281, -0.1582],
        [-0.3359,  0.3750,  0.0781, -0.2754, -0.7109],
        [-0.4648,  0.4336, -0.0204, -0.3770, -0.1191],
        [-0.0513,  0.2178, -0.3672, -0.4492, -0.1895],
        [-0.3809,  0.3984, -0.0025, -0.4316, -0.3672],
        [-0.1738,  0.3477, -0.0280, -0.2422, -0.3828]], device='cuda:0',
       dtype=torch.bfloat16, grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)
out(train): SequenceClassifierOutput(loss=tensor(1.5083, device='cuda:0', grad_fn=<NllLossBackward0>), logits=tensor([[-0.1426,  0.4785,  0.0347, -0.1455, -0.1631],
        [-0.2383, -0.0806,  0.2715, -0.4785, -0.8516],
        [-0.3398,  0.3203,  0.2432, -0.3477, -0.7422],
        [-0.2100,  0.2217,  0.1768,  0.1240, -0.4727],
        [-0.0645,  0.1865, -0.0811,  0.0972, -0.2148],
        [ 0.2080,  0.3398,  0.1152, -0.0288, -0.0815],
        [-0.1650,  0.4551,  0.0240, -0.2910, -0.2578],
        [ 0.0630,  0.2754,  0.4824, -0.4453, -0.5352],
        [ 0.2539,  0.0737,  0.4609, -0.0981, -0.4961],
        [-0.1064,  0.4316, -0.0010, -0.0583, -0.0569],
        [-0.3496,  0.2734,  0.3691, -0.8008, -0.8398],
        [-0.0981,  0.6367,  0.2441, -0.5430, -0.1211],
        [-0.6484,  0.1631,  0.2559, -0.3066, -0.2285],
        [ 0.0498,  0.6172,  0.1113, -0.3828, -0.0894],
        [ 0.0374,  0.2363,  0.1143, -0.2656, -0.3145],
        [-0.0457, -0.0292, -0.2188, -0.4805, -0.3184]], device='cuda:0',
       dtype=torch.bfloat16, grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)
W0629 11:52:52.467000 139012379337344 torch/_dynamo/convert_frame.py:824]     result = inner_convert(
out(train): SequenceClassifierOutput(loss=tensor(1.4448, device='cuda:0', grad_fn=<NllLossBackward0>), logits=tensor([[-0.1465,  0.6562, -0.0410, -0.3770, -0.2158],
        [-0.2012,  0.4941, -0.0347, -0.4141, -0.3438],
        [ 0.0422,  0.2891,  0.2676, -0.3984, -0.7148],
        [ 0.1387,  0.0016, -0.1133, -0.4043, -0.1621],
        [ 0.5469, -0.1094, -0.4023, -0.1729,  0.5781],
        [-0.3848, -0.0194, -0.3320, -0.1235, -0.7031],
        [ 0.0330,  0.5391, -0.3027, -0.3066, -0.1504],
        [-0.0215,  0.3145,  0.1914, -0.2891, -0.8633],
        [-0.0129,  0.1670,  0.4355, -0.2891, -0.7383],
        [-0.0864,  0.2715,  0.1738, -0.3184, -0.0625],
        [-0.0762,  0.4590, -0.2715, -0.2266, -0.3730],
        [ 0.1777,  0.3203, -0.1113, -0.2910, -0.0505],
        [ 0.0015,  0.4590, -0.0237, -0.5625, -0.2949],
        [-0.2402,  0.1245,  0.0537, -0.4102, -0.7188],
        [ 0.0698,  0.1641,  0.0074, -0.3438, -0.2891],
        [ 0.0791,  0.2969,  0.2910, -0.2793, -0.9180]], device='cuda:0',
       dtype=torch.bfloat16, grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)
out(train): SequenceClassifierOutput(loss=tensor(1.2856, device='cuda:0', grad_fn=<NllLossBackward0>), logits=tensor([[-0.0559,  0.4512,  0.2119, -0.1309, -0.3066],
        [-0.1309,  0.4004, -0.1475, -0.1875, -0.1885],
        [-0.0491,  0.5859, -0.1875, -0.5664, -0.2109],
        [-0.3281,  0.3105,  0.2061, -0.5391, -0.5859],
        [-0.0771,  0.4902,  0.3008, -0.1602, -0.7656],
        [-0.1406,  0.4609,  0.0103, -0.4688, -0.6445],
        [-0.0530,  0.2617, -0.0439, -0.2988, -0.2051],
        [-0.1797,  0.6797,  0.1924, -0.3262, -0.5312],
        [-0.3203,  0.2539,  0.2949, -0.2285, -0.3086],
        [-0.0835,  0.3340, -0.0255, -0.3340, -0.1963],
        [ 0.0334,  0.6914,  0.2334, -0.3809, -0.1523],
        [ 0.2715,  0.0505,  0.0009, -0.2773, -0.4414],
        [ 0.0430,  0.3008,  0.0176, -0.1475, -0.3340],
        [ 0.3301,  0.6445,  0.0903, -0.5000, -0.3008],
        [-0.0236,  0.1182, -0.1299, -0.2871, -0.2256],
        [-0.2520,  0.6016,  0.1348, -0.3320, -0.2227]], device='cuda:0',
       dtype=torch.bfloat16, grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)
out(train): SequenceClassifierOutput(loss=tensor(1.4028, device='cuda:0', grad_fn=<NllLossBackward0>), logits=tensor([[-0.2295,  0.4297, -0.0757, -0.0610, -0.2578],
        [-0.3848, -0.1787,  0.0801, -0.3105, -0.6680],
        [-0.2852,  0.7070, -0.1504, -0.1494, -0.3340],
        [-0.0165,  0.5820, -0.0095, -0.2383, -0.1992],
        [-0.3730,  0.4414,  0.3242, -0.2090, -0.2188],
        [-0.2383,  0.1367,  0.4004, -0.5703, -1.0234],
        [-0.1328,  0.5469,  0.0115, -0.3887, -0.5000],
        [-0.1094,  0.5273, -0.0195, -0.3281, -0.1924],
        [-0.4785,  0.4316,  0.4863,  0.1406, -0.0452],
        [-0.1152,  0.7031,  0.0613, -0.1484, -0.4922],
        [ 0.1885,  0.5898,  0.2461,  0.0028, -0.0500],
        [ 0.0291,  0.4668,  0.1191, -0.0179, -0.0806],
        [-0.1562,  0.4062,  0.4883, -0.1992, -0.6680],
        [ 0.1211,  0.3203, -0.0286, -0.4629, -0.4512],
        [ 0.2891, -0.0206, -0.5039, -0.2061,  0.2539],
        [-0.3066,  0.1836, -0.0515, -0.5039, -1.1719]], device='cuda:0',
       dtype=torch.bfloat16, grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)
out(train): SequenceClassifierOutput(loss=tensor(1.2480, device='cuda:0', grad_fn=<NllLossBackward0>), logits=tensor([[ 0.1475,  0.4707, -0.2695, -0.4590,  0.1089],
        [ 0.3066,  0.5625,  0.2197, -0.3379, -0.2148],
        [ 0.1963,  0.3965, -0.1523, -0.4492, -0.3301],
        [-0.1099,  0.5352,  0.0608, -0.4219,  0.1079],
        [ 0.4160,  0.4395, -0.1797, -0.3320, -0.2754],
        [-0.0244,  0.4590, -0.1895, -0.5234, -0.4434],
        [-0.2852,  0.4648,  0.4512, -0.2324, -0.3457],
        [-0.2695,  0.2070,  0.1211, -0.3867, -0.9141],
        [-0.1055,  0.5898,  0.0815, -0.2891, -0.5742],
        [-0.3242,  0.4863,  0.1357, -0.4609, -0.2314],
        [ 0.1021,  0.4414, -0.0413, -0.0859, -0.2910],
        [-0.2236,  0.0461,  0.4902, -0.3730, -0.3613],
        [-0.1377, -0.0214,  0.0410, -0.3672, -0.4805],
        [ 0.0967,  0.7266, -0.2910, -0.4238, -0.2393],
        [ 0.3164,  0.4355, -0.2432, -0.2637, -0.4922],
        [-0.0552,  0.4219, -0.3887, -0.0014, -0.1475]], device='cuda:0',
       dtype=torch.bfloat16, grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)
out(train): SequenceClassifierOutput(loss=tensor(1.2295, device='cuda:0', grad_fn=<NllLossBackward0>), logits=tensor([[-0.0025,  0.5273,  0.1069, -0.0942, -0.3867],
        [-0.0513,  0.4688,  0.1436, -0.6484, -0.5703],
        [ 0.0258,  0.3359,  0.2354, -0.3359, -0.3848],
        [-0.1924,  0.4609,  0.4961, -0.2754, -0.3809],
        [ 0.1777,  0.3730,  0.0330, -0.4141, -0.0106],
        [-0.1348, -0.0400,  0.2891, -0.2812, -0.6836],
        [ 0.1934,  0.6562, -0.4648, -0.2520,  0.0173],
        [ 0.0574,  0.5820,  0.0830, -0.3672, -0.2275],
        [ 0.0850,  0.6211,  0.1465, -0.2451, -0.3848],
        [-0.2129,  0.6250,  0.2715,  0.0320, -0.4844],
        [-0.0981,  0.7461,  0.1514, -0.5938, -0.5625],
        [-0.1079,  0.3281, -0.3516, -0.2930, -0.4258],
        [ 0.3633, -0.2852, -0.5625, -0.4980,  0.4258],
        [ 0.0767,  0.4141,  0.1670, -0.3340, -0.6719],
        [-0.0918,  0.4688,  0.2490, -0.4277, -0.1865],
        [ 0.3125,  0.4746,  0.1128, -0.4316, -0.1768]], device='cuda:0',
       dtype=torch.bfloat16, grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)
out(train): SequenceClassifierOutput(loss=tensor(1.3755, device='cuda:0', grad_fn=<NllLossBackward0>), logits=tensor([[-0.2324,  0.4434,  0.0771, -0.2393, -0.0903],
        [-0.1367,  0.5625,  0.5039, -0.5469, -0.3418],
        [ 0.1943,  0.2441, -0.0635, -0.6445, -0.3027],
        [-0.2021,  0.5078,  0.5000, -0.4414, -0.3105],
        [-0.2070,  0.1270,  0.1641, -0.2539, -0.2158],
        [-0.1943,  0.3477,  0.1182, -0.3535, -0.1631],
        [ 0.0272,  0.4902, -0.0152, -0.3379, -0.1621],
        [ 0.1631,  0.0791,  0.3418,  0.0242, -0.3301],
        [-0.0835,  0.3770,  0.6172, -0.1914, -0.6289],
        [ 0.0928,  0.3828, -0.1030, -0.1533, -0.4551],
        [ 0.0111,  0.4785, -0.3789, -0.5781, -0.0102],
        [-0.0591,  0.5430,  0.2500, -0.4316, -0.5742],
        [-0.2734, -0.0055,  0.0315, -0.6367, -0.4531],
        [-0.0530,  0.1226,  0.1709, -0.2480, -0.2871],
        [-0.2852,  0.5078, -0.1074,  0.0043, -0.5234],
        [-0.3652,  0.4336,  0.0781, -0.2949, -0.1084]], device='cuda:0',
       dtype=torch.bfloat16, grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)
out(train): SequenceClassifierOutput(loss=tensor(1.4463, device='cuda:0', grad_fn=<NllLossBackward0>), logits=tensor([[-0.2617, -0.0505,  0.1504, -0.2930, -0.8516],
        [ 0.2217,  0.5117, -0.1787, -0.4453, -0.0713],
        [ 0.0674,  0.5273,  0.2930, -0.2256, -0.2852],
        [-0.2676,  0.5391, -0.0579, -0.1406, -0.4609],
        [-0.1079,  0.2461,  0.4180, -0.2266, -0.2227],
        [ 0.2178, -0.2021, -0.1836,  0.1260, -0.3359],
        [-0.2891,  0.7383,  0.2471,  0.0547,  0.1680],
        [ 0.2314,  0.1631,  0.3672, -0.2500, -0.7188],
        [-0.1152,  0.2852,  0.6523, -0.0405, -0.7695],
        [ 0.0311,  0.4316,  0.0022, -0.5898, -0.1641],
        [-0.1074,  0.2334,  0.0167, -0.6289, -0.3047],
        [ 0.1699,  0.3418,  0.0684, -0.6406, -0.4473],
        [-0.0164,  0.7031,  0.2324, -0.4102, -0.3730],
        [-0.1904,  0.4219,  0.2793,  0.1138, -0.3770],
        [ 0.0026,  0.1738,  0.2324, -0.1816, -0.6875],
        [-0.3379,  0.3867,  0.1709, -0.0874,  0.0300]], device='cuda:0',
       dtype=torch.bfloat16, grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)
out(train): SequenceClassifierOutput(loss=tensor(1.2444, device='cuda:0', grad_fn=<NllLossBackward0>), logits=tensor([[-0.1504,  0.0238,  0.2031, -0.2910, -0.4180],
        [ 0.0613,  0.4648, -0.1152, -0.2178, -0.3320],
        [ 0.0156,  0.8086, -0.0693, -0.1191, -0.2832],
        [-0.0884,  0.3828,  0.0786, -0.4414, -0.2490],
        [-0.3203,  0.2480,  0.1206, -0.3691, -0.8867],
        [ 0.2656,  0.2949,  0.0265, -0.3164, -0.3477],
        [-0.0869,  0.5078, -0.0356, -0.1514, -0.6758],
        [-0.0486,  0.4062,  0.3496, -0.5234, -0.6953],
        [ 0.1338,  0.3574, -0.3320, -0.5938, -0.1123],
        [-0.2480,  0.3965,  0.2275, -0.2500, -0.4785],
        [-0.3398,  0.2197, -0.1719, -0.2949, -0.3438],
        [ 0.1270,  0.2812,  0.0015, -0.4824, -0.1816],
        [-0.0388,  0.5156,  0.0815, -0.6133, -0.5781],
        [-0.2812,  0.3027,  0.2891, -0.2246, -0.4238],
        [-0.3203,  0.0454,  0.3477, -0.3652, -0.3594],
        [ 0.1328, -0.4102, -0.7539, -0.1729,  0.0957]], device='cuda:0',
       dtype=torch.bfloat16, grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)
out(train): SequenceClassifierOutput(loss=tensor(1.3489, device='cuda:0', grad_fn=<NllLossBackward0>), logits=tensor([[ 0.0349,  0.2988, -0.0234, -0.5742, -0.8281],
        [ 0.2676,  0.5039, -0.0728, -0.5078, -0.1196],
        [-0.0265,  0.0566,  0.4766, -0.0825, -0.2578],
        [-0.1729,  0.1396, -0.0554, -0.0806, -0.0986],
        [ 0.0464,  0.1963,  0.2559,  0.0532, -0.6133],
        [ 0.1875,  0.4766,  0.3203, -0.2695, -0.8203],
        [-0.3105,  0.1738,  0.2598, -0.4883, -0.0096],
        [-0.0337,  0.6992, -0.0776, -0.2910, -0.3047],
        [ 0.0137,  0.7383,  0.1289, -0.4570, -0.7070],
        [-0.3613, -0.0042,  0.0195,  0.0032, -0.5312],
        [-0.2256,  0.5039, -0.0588, -0.2334, -0.1816],
        [-0.0593,  0.5664, -0.1436, -0.4746, -0.4824],
        [ 0.1553,  0.3359, -0.0864, -0.1572, -0.4121],
        [ 0.0654,  0.3887,  0.0532,  0.1641, -0.1807],
        [ 0.1562,  0.2021, -0.2988, -0.1338, -0.1602],
        [-0.0620,  0.5273,  0.0972, -0.1089, -0.3086]], device='cuda:0',
       dtype=torch.bfloat16, grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)
out(train): SequenceClassifierOutput(loss=tensor(1.2778, device='cuda:0', grad_fn=<NllLossBackward0>), logits=tensor([[ 0.2520,  0.0199, -0.6875, -0.6289,  0.3418],
        [-0.3418,  0.3477, -0.0518, -0.3887, -0.3750],
        [-0.5469,  0.2500,  0.0256, -0.3164, -0.5391],
        [-0.2578,  0.3027,  0.1953, -0.5742, -0.4395],
        [ 0.1196,  0.3223, -0.1680, -0.2832,  0.0112],
        [-0.0884,  0.5039, -0.2422,  0.0554, -0.2539],
        [-0.4629,  0.5586,  0.1768, -0.1758, -0.6367],
        [-0.1631,  0.4062,  0.2754, -0.1699, -0.5156],
        [ 0.1011,  0.5898,  0.1250, -0.2715, -0.2344],
        [-0.4414,  0.5039,  0.4922,  0.0347, -0.7266],
        [-0.2002,  0.4941,  0.0234, -0.1631, -0.2949],
        [-0.2520,  0.3105, -0.0554, -0.2080, -0.0031],
        [ 0.0021,  0.5781,  0.1191, -0.2695, -0.2832],
        [-0.3145, -0.0688,  0.0317, -0.2402, -0.7773],
        [ 0.0124,  0.5195, -0.1465, -0.4395, -0.3418],
        [-0.2002,  0.2676, -0.0339, -0.1475, -0.6641]], device='cuda:0',
       dtype=torch.bfloat16, grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)
out(train): SequenceClassifierOutput(loss=tensor(1.3152, device='cuda:0', grad_fn=<NllLossBackward0>), logits=tensor([[-0.0060, -0.1030, -0.2520, -0.4141, -0.3750],
        [-0.2480,  0.0544,  0.0342, -0.6367, -0.7305],
        [ 0.1611,  0.5078,  0.2236, -0.2832, -0.1611],
        [-0.1147,  0.4629,  0.3242, -0.4277,  0.0094],
        [-0.1973,  0.3613, -0.1758, -0.3809, -0.3828],
        [-0.2021,  0.6914, -0.0530, -0.1309, -0.5508],
        [-0.2617,  0.6602, -0.1177, -0.1768, -0.4492],
        [ 0.2393,  0.6602,  0.2451, -0.3027, -0.0540],
        [ 0.2471,  0.0659, -0.1865, -0.4531, -0.0305],
        [-0.1045,  0.5938,  0.0352,  0.1079, -0.2197],
        [ 0.0118,  0.6914, -0.1113, -0.3359, -0.1973],
        [ 0.0270,  0.4863, -0.3340, -0.1279, -0.0977],
        [-0.5156,  0.3691,  0.2812, -0.3125, -0.3613],
        [-0.2891,  0.1855, -0.3594, -0.0913, -0.4453],
        [-0.6250,  0.6562,  0.1924, -0.2656, -0.5117],
        [-0.1128,  0.6016, -0.0063, -0.1328, -0.2637]], device='cuda:0',
W0629 11:52:52.467000 139012379337344 torch/_dynamo/convert_frame.py:824]     result = inner_convert(
out(train): SequenceClassifierOutput(loss=tensor(1.2300, device='cuda:0', grad_fn=<NllLossBackward0>), logits=tensor([[ 0.0039,  0.3711,  0.1045, -0.0874, -0.2793],
        [-0.2100,  0.4316, -0.0243, -0.4785, -0.2441],
        [-0.3828,  0.4668, -0.0186, -0.1836, -0.6562],
        [-0.1406,  0.6133,  0.3262, -0.1729, -0.2236],
        [-0.2539,  0.3184,  0.6562, -0.1396, -0.8633],
        [-0.0825,  0.3555, -0.3555, -0.3242, -0.5508],
        [-0.3711,  0.5547,  0.0352, -0.3828, -0.5234],
        [-0.3203,  0.0791, -0.2285, -0.2852, -0.4004],
        [-0.0069,  0.1719, -0.2090, -0.0664, -0.2695],
        [ 0.2275,  0.3223, -0.2500, -0.4473, -0.6016],
        [-0.3301,  0.6523, -0.0898, -0.3340, -0.1494],
        [-0.2354,  0.2031,  0.1455, -0.6602, -1.2031],
        [-0.1455,  0.1514,  0.3262, -0.2158, -0.9922],
        [-0.2656,  0.1069,  0.2139, -0.2002, -0.4551],
        [-0.0923,  0.6602,  0.0679, -0.4590, -0.2080],
        [-0.0684,  0.5820,  0.0674, -0.2617, -0.2139]], device='cuda:0',
       dtype=torch.bfloat16, grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)
out(train): SequenceClassifierOutput(loss=tensor(1.3247, device='cuda:0', grad_fn=<NllLossBackward0>), logits=tensor([[-0.1084,  0.0986,  0.5938, -0.0312, -0.7500],
        [ 0.1992,  0.5781, -0.2578, -0.3242, -0.3555],
        [-0.2119,  0.3457, -0.0088,  0.2412, -0.3652],
        [ 0.2246,  0.6445, -0.2090, -0.3594, -0.1562],
        [-0.0383,  0.6172, -0.1602, -0.5820, -0.4199],
        [-0.0276,  0.5977,  0.1191, -0.1934, -0.0664],
        [ 0.2344,  0.6250,  0.4805, -0.3379, -0.3262],
        [-0.1777,  0.2500,  0.1177, -0.3125, -0.2852],
        [-0.1904,  0.5156,  0.0806, -0.5547, -0.7891],
        [-0.1436,  0.1953,  0.3340,  0.1123, -0.0820],
        [-0.0649,  0.5391,  0.0371, -0.1426, -0.3789],
        [-0.0155,  0.4629, -0.1084, -0.2021, -0.3652],
        [-0.0189,  0.2119,  0.6641,  0.2236, -0.5625],
        [-0.3398,  0.6406,  0.1367, -0.2832, -0.3184],
        [-0.5898,  0.5508, -0.2988, -0.4102, -0.4512],
        [ 0.0038,  0.1221, -0.2695, -0.4570,  0.0286]], device='cuda:0',
       dtype=torch.bfloat16, grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)
out(train): SequenceClassifierOutput(loss=tensor(1.2822, device='cuda:0', grad_fn=<NllLossBackward0>), logits=tensor([[ 1.1035e-01,  3.4180e-01, -3.2227e-01, -1.5234e-01,  2.3145e-01],
        [-1.7480e-01,  5.0781e-01,  3.8867e-01, -6.7969e-01, -3.3398e-01],
        [-5.5176e-02,  6.6016e-01, -5.2246e-02, -1.0498e-01, -4.2969e-01],
        [ 5.7861e-02,  5.6641e-01, -8.9844e-02, -8.2520e-02, -1.2500e-01],
        [-1.2305e-01,  3.2617e-01,  3.5938e-01, -4.5312e-01, -6.7188e-01],
        [-2.9883e-01,  5.4688e-01, -2.4512e-01, -3.4180e-01, -2.5195e-01],
        [-2.3535e-01,  5.8203e-01, -9.7656e-02, -2.1875e-01, -2.0508e-01],
        [-9.3750e-02,  3.5156e-01,  9.8877e-03,  1.3867e-01, -1.1523e-01],
        [-3.9062e-01,  5.1953e-01,  7.0801e-02, -2.6172e-01, -3.6328e-01],
        [-3.2227e-01,  8.0859e-01, -1.4343e-02, -2.0801e-01, -3.3594e-01],
        [ 2.2168e-01,  1.6699e-01, -4.4531e-01, -4.2969e-01, -3.1836e-01],
        [ 8.1055e-02,  6.7578e-01,  7.2266e-02, -5.0781e-01, -3.4375e-01],
        [-7.2266e-02,  6.7188e-01,  2.5195e-01, -1.5625e-01, -3.8086e-02],
        [-1.3574e-01,  9.0332e-02,  1.8848e-01, -4.7852e-02, -7.5781e-01],
        [-8.9844e-02,  3.0859e-01, -2.0312e-01, -1.0352e-01, -4.4922e-01],
        [-2.4128e-04,  8.5938e-01, -3.8818e-02, -2.0117e-01, -8.2520e-02]],
       device='cuda:0', dtype=torch.bfloat16, grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)
out(train): SequenceClassifierOutput(loss=tensor(1.2924, device='cuda:0', grad_fn=<NllLossBackward0>), logits=tensor([[-0.2051,  0.5195,  0.2559, -0.0260, -0.6680],
        [ 0.0962,  0.3379,  0.0035, -0.0728, -0.2578],
        [-0.2539,  0.1855,  0.3145, -0.5781, -0.7617],
        [-0.0212,  0.3809, -0.4629, -0.2637, -0.0352],
        [-0.4883,  0.4062,  0.3926, -0.5703, -0.7812],
        [-0.0957,  0.4961,  0.0967, -0.2490, -0.3438],
        [-0.4609,  0.5742, -0.1465, -0.1416, -0.3184]], device='cuda:0',
       dtype=torch.bfloat16, grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)
       dtype=torch.bfloat16, grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)
W0629 11:52:52.467000 139012379337344 torch/_dynamo/convert_frame.py:824]     result = inner_convert(
W0629 11:52:52.467000 139012379337344 torch/_dynamo/convert_frame.py:824]     result = inner_convert(
W0629 11:52:52.467000 139012379337344 torch/_dynamo/convert_frame.py:824]     result = inner_convert(
W0629 11:52:52.467000 139012379337344 torch/_dynamo/convert_frame.py:824]     result = inner_convert(
W0629 11:52:52.467000 139012379337344 torch/_dynamo/convert_frame.py:824]     result = inner_convert(
W0629 11:52:52.467000 139012379337344 torch/_dynamo/convert_frame.py:824]     result = inner_convert(
W0629 11:52:52.467000 139012379337344 torch/_dynamo/convert_frame.py:824]     result = inner_convert(
       dtype=torch.bfloat16, grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)
        [-0.3652,  0.2676, -0.0918, -0.2969, -0.5039],, hidden_states=None, attentions=None)
        [-0.3125,  0.4766,  0.1396, -0.3340, -0.7773]], device='cuda:0',
       dtype=torch.bfloat16, grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)
out(train): SequenceClassifierOutput(loss=tensor(1.3035, device='cuda:0', grad_fn=<NllLossBackward0>), logits=tensor([[-0.1621,  0.5625, -0.0014, -0.7422, -0.9375],
        [ 0.2227,  0.5195,  0.4492, -0.5117, -0.4844],
        [-0.3691,  0.5859,  0.4160, -0.4707, -0.2715],
        [-0.3242,  0.0659, -0.2207, -0.6641, -0.7070],
        [-0.4512, -0.0659,  0.0752, -0.2754, -0.5977],
        [-0.3262,  0.4355,  0.4766, -0.1309, -0.7617],
        [-0.5859,  0.1084,  0.1611, -0.5859, -0.6602],
        [-0.4453,  0.1807,  0.1875, -0.4180, -0.8633],
        [-0.0835,  0.0159,  0.1611, -0.4590, -0.6953],
        [-0.1660,  0.4141, -0.3613, -0.1436, -0.5938],
        [-0.4199,  0.2715,  0.2793, -0.3438, -0.5586],
        [-0.3164,  0.2158, -0.0112, -0.2266, -0.6875],
        [-0.2402,  0.0608,  0.2852, -0.7266, -0.1250],
        [-0.3809,  0.1738,  0.1357, -0.5586, -0.7266],
        [-0.2285,  0.3496,  0.0439, -0.4805, -0.1436],
        [-0.1836, -0.0776,  0.0771, -0.3125, -0.6094]], device='cuda:0',
       dtype=torch.bfloat16, grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)
out(train): SequenceClassifierOutput(loss=tensor(1.3354, device='cuda:0', grad_fn=<NllLossBackward0>), logits=tensor([[-0.3320,  0.4180,  0.0413, -0.2002, -0.4883],
        [-0.4238,  0.0320,  0.4062, -0.3926, -0.8867],
        [ 0.0825,  0.2246, -0.0610, -0.3145, -0.4023],
        [-0.2109,  0.2617,  0.2012, -0.4824, -0.5078],
        [-0.3848,  0.1523,  0.1865, -0.1768, -0.5547],
        [-0.1592, -0.1001,  0.0737, -0.2344, -0.7578],
        [ 0.1514,  0.6875,  0.1406, -0.3047, -0.4609],
        [-0.2441,  0.6953,  0.0151, -0.0554, -0.2734],
        [-0.4141,  0.3398,  0.1270, -0.4082, -0.9258],
        [-0.3027, -0.2100,  0.2969, -0.1123, -0.9180],
        [-0.2148,  0.3184,  0.2070, -0.5898, -0.9531],
        [-0.1030,  0.1128,  0.1641, -0.1445, -0.5508],
        [-0.2148,  0.2383,  0.0708, -0.4531, -0.9570],
        [-0.0398,  0.5273, -0.0381, -0.1855, -0.2949],
        [-0.3359,  0.2852, -0.1279,  0.1797, -0.6758],
        [-0.5156,  0.2334,  0.1650, -0.1445, -0.3008]], device='cuda:0',
       dtype=torch.bfloat16, grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)
        [-0.3652,  0.2676, -0.0918, -0.2969, -0.5039],, hidden_states=None, attentions=None)
        [-0.5156,  0.4062,  0.1787, -0.2617, -0.6328],, hidden_states=None, attentions=None)
        [-0.3887,  0.6523,  0.0991, -0.4961, -0.7461]], device='cuda:0',
       dtype=torch.bfloat16, grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)
out(train): SequenceClassifierOutput(loss=tensor(1.3887, device='cuda:0', grad_fn=<NllLossBackward0>), logits=tensor([[-0.2461,  0.3223,  0.2852, -0.7344, -0.6289],
        [-0.2168,  0.0928,  0.0498, -0.1768, -0.7500],
        [-0.1069,  0.1387,  0.0347, -0.3945, -0.3867],
        [-0.2217,  0.0908,  0.2178, -0.4551, -0.4980],
        [ 0.4434, -0.0103, -0.3574, -0.0574,  0.3691],
        [-0.0952,  0.3867,  0.1973, -0.7031, -0.6445],
        [-0.1357,  0.3242,  0.2119, -0.2363, -0.8164],
        [-0.0173,  0.2236, -0.2031, -0.5859, -0.7500],
        [-0.2812,  0.3184,  0.1572, -0.5273, -0.1807],
        [-0.5000,  0.5625,  0.5234, -0.4492, -1.0625],
        [-0.1406,  0.1943, -0.0493, -0.1650, -0.6250],
        [-0.2500,  0.6367,  0.0344, -0.2480, -0.2305],
        [-0.4980,  0.3105,  0.3242, -0.0811, -1.0625],
        [-0.0962,  0.3242, -0.0391, -0.0464, -0.0542],
        [-0.2051,  0.3809,  0.1118, -0.4316, -0.6797],
        [ 0.1982,  0.3281,  0.0378, -0.2598, -0.2373]], device='cuda:0',
       dtype=torch.bfloat16, grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)
out(train): SequenceClassifierOutput(loss=tensor(1.1885, device='cuda:0', grad_fn=<NllLossBackward0>), logits=tensor([[-0.2188,  0.1060, -0.1904, -0.2754, -0.4434],
        [ 0.1777,  0.2754,  0.0564, -0.3496, -0.5820],
        [-0.4473,  0.3359,  0.3770, -0.5391, -0.9648],
        [-0.5117,  0.3516,  0.1553, -0.1826, -0.7695],
        [ 0.0981,  0.3867, -0.1279,  0.0908, -0.3184],
        [-0.0022,  0.0771,  0.1787, -0.4863, -0.5117],
        [ 0.1001,  0.5156, -0.1416, -0.1963, -0.1455],
        [-0.3516,  0.5586, -0.1328, -0.4629, -0.4941],
        [-0.0223,  0.4141,  0.0540, -0.6680, -0.6289],
        [ 0.0728,  0.6836, -0.0449, -0.1875, -0.3848],
        [-0.1436,  0.4160, -0.3008, -0.2520, -0.2129],
        [-0.0854,  0.3848, -0.3496, -0.3281, -0.3359],
        [-0.0845,  0.5859,  0.0325, -0.6797, -0.1963],
        [-0.4102,  0.5312, -0.0491, -0.4023, -0.3047],
        [-0.3633,  0.2871,  0.0298, -0.4688, -0.9414],
        [ 0.2539,  0.5586, -0.1826, -0.0967, -0.2754]], device='cuda:0',
       dtype=torch.bfloat16, grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)
        [-0.5156,  0.4062,  0.1787, -0.2617, -0.6328],, hidden_states=None, attentions=None)
        [-0.3301,  0.5312,  0.0859, -0.4004, -0.4258],, hidden_states=None, attentions=None)
        [-0.2480,  0.7656, -0.0444, -0.1416, -0.4199]], device='cuda:0',
       dtype=torch.bfloat16, grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)
out(train): SequenceClassifierOutput(loss=tensor(1.3540, device='cuda:0', grad_fn=<NllLossBackward0>), logits=tensor([[-0.3086,  0.4727,  0.3770, -0.5078, -0.9219],
        [-0.2500, -0.3750,  0.0100, -0.4316, -0.7188],
        [-0.3789,  0.5742,  0.0019, -0.4570, -1.0781],
        [-0.0522,  0.4746,  0.0835, -0.2393, -0.4863],
        [-0.0603,  0.1943,  0.0540, -0.4141, -0.5586],
        [-0.3477,  0.8828, -0.0591, -0.2100, -0.0913],
        [ 0.0505,  0.8750, -0.2217, -0.1885, -0.2158],
        [ 0.6641, -0.1118, -0.7812, -0.0352,  0.1055],
        [ 0.0981,  0.6992,  0.0596, -0.4785, -0.3848],
        [-0.1680,  0.1279,  0.3301,  0.0030, -0.5859],
        [ 0.2051,  0.5078,  0.0728, -0.0070, -0.3672],
        [-0.2910,  0.5000, -0.2188, -0.1309, -0.4492],
        [ 0.0295,  0.5508, -0.4043, -0.4492, -0.2812],
        [-0.2520,  0.1650, -0.2734,  0.0126, -0.4414],
        [-0.0583,  0.4531, -0.1699, -0.1084,  0.0923],
        [-0.1660,  0.4902, -0.0488, -0.0175, -0.4941]], device='cuda:0',
       dtype=torch.bfloat16, grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)
out(train): SequenceClassifierOutput(loss=tensor(1.1826, device='cuda:0', grad_fn=<NllLossBackward0>), logits=tensor([[-0.5000,  0.3926, -0.2246, -0.1855, -0.5820],
        [-0.4531,  0.5039,  0.1572, -0.3594, -0.4180],
        [-0.1680,  0.6016,  0.1406, -0.4160, -0.9219],
        [ 0.1011,  0.4199, -0.1992, -0.3457, -0.1836],
        [-0.1475,  0.5781, -0.2100, -0.4590, -0.1709],
        [ 0.0752,  0.4922, -0.1318, -0.3340, -0.4316],
        [-0.1147,  0.4238, -0.2012, -0.0149, -0.5312],
        [-0.2090,  0.2520, -0.2461, -0.4512, -0.1631],
        [ 0.2344,  0.6484, -0.1377, -0.4941, -0.1670],
        [ 0.1396,  0.4395,  0.4473, -0.3730, -0.5625],
        [-0.2598,  0.8086,  0.1279, -0.2969, -0.1191],
        [-0.0483,  0.5625,  0.2832, -0.1523, -0.4551],
        [ 0.2207,  0.5664, -0.1982,  0.0206, -0.1611],
        [-0.1426,  0.3516, -0.0127, -0.6289, -0.4180],
        [ 0.2539,  0.5078,  0.0713, -0.0874, -0.3750],
        [-0.3086,  0.4668, -0.3516, -0.2041, -0.3555]], device='cuda:0',
       dtype=torch.bfloat16, grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)
out(train): SequenceClassifierOutput(loss=tensor(1.1099, device='cuda:0', grad_fn=<NllLossBackward0>), logits=tensor([[-1.5332e-01,  5.7422e-01,  4.6082e-03, -6.4453e-01, -8.1641e-01],
        [-4.6680e-01,  4.9023e-01, -1.3281e-01, -3.8867e-01, -1.1484e+00],
        [ 8.3008e-02,  5.1953e-01,  1.6699e-01, -5.0391e-01, -3.8086e-01],
        [ 1.6992e-01,  4.5508e-01,  4.7852e-01, -1.5137e-02, -1.3086e-01],
        [-7.4219e-02,  6.7188e-01,  5.9814e-02,  9.7656e-03, -1.7676e-01],
        [-4.5898e-01,  3.9453e-01,  4.8828e-01, -9.1797e-02, -5.3125e-01],
        [ 3.8330e-02,  7.0703e-01, -7.8125e-02, -2.6562e-01, -3.1250e-01],
        [ 1.3657e-03,  4.9805e-01,  1.5527e-01, -2.6123e-02, -6.2500e-01],
        [-3.0859e-01,  8.9453e-01, -2.8516e-01, -3.4180e-01, -2.5000e-01],
        [ 1.2109e-01,  7.3438e-01,  9.9182e-04, -2.6758e-01, -3.1836e-01],
        [-3.5547e-01,  3.7500e-01,  2.5977e-01, -2.1484e-01, -4.3555e-01],
        [ 8.0078e-02,  8.6328e-01, -2.6562e-01, -1.4832e-02, -2.2461e-01],
        [-9.9609e-02,  7.7734e-01,  1.6895e-01, -3.8672e-01, -6.7969e-01],
        [ 1.7969e-01,  8.2812e-01,  2.5781e-01, -4.1211e-01, -3.1641e-01],
        [ 2.2461e-01,  3.9453e-01,  5.5078e-01, -2.7344e-01, -4.4922e-01],
        [-1.5918e-01,  3.8281e-01,  5.0537e-02, -5.2734e-01, -7.2656e-01]],
       device='cuda:0', dtype=torch.bfloat16, grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)
out(train): SequenceClassifierOutput(loss=tensor(1.1589, device='cuda:0', grad_fn=<NllLossBackward0>), logits=tensor([[-0.2969,  0.4238, -0.4414,  0.2041, -0.3613],
        [-0.3789,  0.4414,  0.0708, -0.5820, -0.5352],
        [-0.1875,  0.8398, -0.2432, -0.1406, -0.5352],
        [-0.3223,  0.5664, -0.3652, -0.3770, -0.6406],
        [ 0.1299,  0.5352, -0.2734, -0.6484, -0.4297],
        [ 0.0161,  0.6406, -0.0737, -0.3359, -0.1855],
        [-0.0087,  0.8984, -0.0544, -0.3047, -0.1074],
        [ 0.0139,  0.5000,  0.3516, -0.4160, -0.2539],
        [-0.2158,  0.7734,  0.0425, -0.3301, -0.3242],
        [-0.1836,  0.2852, -0.0344, -0.4316, -0.3281],
        [-0.1221,  0.4434,  0.2695, -0.5000, -0.7734],
        [-0.3418,  0.4980,  0.0204, -0.1504, -0.6484],
        [-0.0344,  0.5586, -0.0879, -0.0366, -0.6328],
        [-0.1396,  0.3906,  0.3770, -0.1201, -0.8516],
        [-0.0933,  0.3320, -0.5781,  0.0608, -0.7539],
        [-0.4473,  0.8398,  0.3535, -0.8633, -0.6484]], device='cuda:0',
       dtype=torch.bfloat16, grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)
        [-0.3301,  0.5312,  0.0859, -0.4004, -0.4258],, hidden_states=None, attentions=None)
        [ 7.6660e-02,  4.9609e-01, -4.9561e-02, -5.4932e-02, -3.1641e-01],, attentions=None)
        [ 3.3447e-02,  3.0859e-01, -1.4941e-01, -7.0801e-02, -3.1055e-01]],
       device='cuda:0', dtype=torch.bfloat16, grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)
out(train): SequenceClassifierOutput(loss=tensor(1.2625, device='cuda:0', grad_fn=<NllLossBackward0>), logits=tensor([[-0.0903,  0.5508,  0.1836, -0.1050, -0.3672],
        [ 0.1582,  0.6914,  0.1758, -0.4805, -0.2432],
        [-0.1338,  0.5391, -0.1904, -0.1396, -0.1060],
        [-0.2988,  0.4199,  0.0126, -0.4023, -0.3574],
        [ 0.1006,  0.4668,  0.0359, -0.5391, -0.6641],
        [ 0.1279,  0.4922,  0.0300, -0.0221, -0.1201],
        [-0.2334,  0.8789, -0.2109, -0.1377, -0.1279],
        [ 0.1060,  0.1719,  0.3125, -0.6797, -0.8750],
        [-0.0082,  0.6523, -0.2197, -0.2520, -0.1377],
        [-0.5156,  0.6797,  0.0175, -0.4551, -0.4668],
        [ 0.1875,  0.3730,  0.3789, -0.0205, -0.8594],
        [ 0.0522,  0.5156, -0.1934, -0.0520, -0.2969],
        [-0.0664,  0.3379,  0.0284, -0.1167, -0.5195],
        [ 0.2207,  0.3945, -0.1699, -0.2871, -0.3828],
        [-0.2285,  0.8711, -0.2100, -0.5625, -0.0532],
        [-0.4277,  0.4492,  0.1738, -0.5391, -1.0312]], device='cuda:0',
       dtype=torch.bfloat16, grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)
out(train): SequenceClassifierOutput(loss=tensor(1.1780, device='cuda:0', grad_fn=<NllLossBackward0>), logits=tensor([[-3.2031e-01,  6.5625e-01, -2.3730e-01, -6.9336e-02, -3.9453e-01],
        [ 1.8457e-01,  6.3281e-01,  1.7944e-02, -2.9297e-01, -6.5430e-02],
        [-5.0659e-03,  7.8125e-01,  1.5527e-01, -3.7695e-01, -2.6562e-01],
        [-3.7891e-01,  9.3359e-01,  4.7656e-01, -2.2266e-01, -4.4727e-01],
        [-8.3008e-02,  8.7109e-01, -6.8848e-02,  1.3574e-01, -2.7734e-01],
        [-4.7852e-01,  3.3398e-01,  3.9551e-02, -2.9492e-01, -9.4922e-01],
        [ 2.0508e-01,  9.3750e-01,  1.1182e-01, -3.2031e-01, -1.1816e-01],
        [-3.0859e-01,  8.0859e-01, -1.8945e-01, -3.3984e-01, -7.7637e-02],
        [-3.5938e-01,  6.7578e-01, -3.1445e-01, -4.3945e-01, -3.5547e-01],
        [-7.7148e-02,  1.0000e+00,  2.1875e-01, -2.9102e-01, -3.0273e-01],
        [ 2.9492e-01,  5.1562e-01, -3.1250e-01, -3.6133e-01, -2.5195e-01],
        [ 2.8931e-02,  3.2031e-01, -5.0293e-02, -2.9688e-01, -5.0781e-01],
        [-4.2383e-01,  4.4141e-01,  3.9307e-02, -4.6875e-01, -6.1719e-01],
        [ 3.7598e-02,  4.5312e-01,  9.9121e-02, -3.4961e-01, -6.5234e-01],
        [-1.6504e-01,  6.8359e-01,  6.2012e-02, -3.2812e-01, -4.4336e-01],
        [-2.2461e-02,  8.0078e-01, -2.6703e-04, -1.0547e-01, -4.1602e-01]],
       device='cuda:0', dtype=torch.bfloat16, grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)
out(train): SequenceClassifierOutput(loss=tensor(1.1279, device='cuda:0', grad_fn=<NllLossBackward0>), logits=tensor([[ 0.2812,  0.5859,  0.0845, -0.4648, -0.0203],
        [-0.5117,  0.7148, -0.0491, -0.4004, -0.3496],
        [ 0.0571,  0.4648,  0.3828, -0.0771, -0.6875],
        [-0.4297,  0.7617,  0.3145, -0.5352, -0.4902],
        [ 0.4941,  0.0698, -0.5703,  0.0618,  0.1514],
        [ 0.0396,  0.3887,  0.0153, -0.1562, -0.7188],
        [ 0.0058,  0.9727, -0.0601, -0.1152, -0.5078],
        [-0.2090,  0.3574,  0.1426, -0.2949, -0.6680],
        [-0.1147,  0.5859, -0.0679, -0.4824, -0.2324],
        [-0.2275,  0.2695, -0.0515, -0.3105, -0.4785],
        [-0.1758,  0.3164,  0.1245, -0.0977, -1.1250],
        [-0.1836,  0.7188,  0.2520, -0.7773, -0.4414],
        [ 0.0066,  0.6602, -0.2451, -0.2441, -0.2988],
        [ 0.0222,  0.5703,  0.2676, -0.3652, -0.0190],
        [-0.0845,  0.8438, -0.0981, -0.3008, -0.1670],
        [-0.0177,  0.6758,  0.0200, -0.1328, -0.6133]], device='cuda:0',
       dtype=torch.bfloat16, grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)
        [ 7.6660e-02,  4.9609e-01, -4.9561e-02, -5.4932e-02, -3.1641e-01],, attentions=None)
        [ 0.1206,  0.8281, -0.0928, -0.3555, -0.2334],2e-02, -3.1641e-01],, attentions=None)
        [-0.0386,  0.8438, -0.1426, -0.3047, -0.4258]], device='cuda:0',
       dtype=torch.bfloat16, grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)
out(train): SequenceClassifierOutput(loss=tensor(1.1091, device='cuda:0', grad_fn=<NllLossBackward0>), logits=tensor([[ 0.0413,  0.7734, -0.0083, -0.6484, -0.5820],
        [-0.1680,  0.8633, -0.2061, -0.8047, -0.7461],
        [-0.1245,  0.9141,  0.1357, -0.6172, -0.1934],
        [ 0.1094,  0.9922, -0.3828, -0.2275, -0.2969],
        [-0.1709,  0.6445, -0.0928, -0.4316, -0.7852],
        [ 0.1943,  0.5938, -0.3340, -0.4629, -0.4609],
        [-0.3652,  0.4453,  0.1201, -0.3340, -0.7617],
        [-0.2852,  0.9062, -0.1904, -0.2520, -0.2734],
        [ 0.1719,  0.8750,  0.0308, -0.2334, -0.1562],
        [-0.1689,  0.6289, -0.1641, -0.4180, -0.3242],
        [-0.0732,  0.9453, -0.0116, -0.2695, -0.4922],
        [-0.2598,  0.9727, -0.0732, -0.3438, -0.1934],
        [-0.4297,  0.5078,  0.0352, -0.4512, -0.6758],
        [-0.2217,  0.5000, -0.1846, -0.1138, -0.5078],
        [-0.3203,  0.1240,  0.2080, -0.2324, -0.8125],
        [-0.2637,  0.5859,  0.3125, -0.4258, -0.4102]], device='cuda:0',
       dtype=torch.bfloat16, grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)
        [ 0.1206,  0.8281, -0.0928, -0.3555, -0.2334],2e-02, -3.1641e-01],, attentions=None)
        [-0.4395,  0.3691,  0.0315, -0.4531, -0.9297],2e-02, -3.1641e-01],, attentions=None)
        [-0.1953,  0.2441,  0.0825, -0.1001, -0.6562]], device='cuda:0',
       dtype=torch.bfloat16, grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)
out(train): SequenceClassifierOutput(loss=tensor(1.1448, device='cuda:0', grad_fn=<NllLossBackward0>), logits=tensor([[ 0.0042,  0.7305, -0.1719, -0.3711, -0.2695],
        [ 0.0869,  0.4844, -0.2148, -0.1865, -0.1719],
        [ 0.0195,  0.6133, -0.1680, -0.5820, -0.3613],
        [-0.3652,  0.5234, -0.3145, -0.0688, -0.3496],
        [-0.0510,  0.8516, -0.2949, -0.6016, -0.3984],
        [ 0.0972,  0.7383, -0.1553, -0.3945, -0.3750],
        [-0.1875,  0.3086,  0.1963, -0.3887, -0.7617],
        [-0.1475,  0.4590, -0.5039, -0.0237, -0.2246],
        [-0.1582,  0.6758,  0.1240, -0.2402, -0.9375],
        [-0.1670,  0.7070, -0.2354, -0.2734, -0.2871],
        [-0.1992,  0.5273,  0.0068, -0.5508, -0.4629],
        [-0.1123,  0.4434, -0.4336, -0.5078, -0.5859],
        [-0.4414,  0.6523, -0.2354, -0.1079, -0.5938],
        [-0.0718,  0.4961, -0.1406, -0.5781, -0.5078],
        [ 0.1895,  0.9883, -0.3047, -0.2773, -0.3887],
        [-0.4277,  0.7383,  0.0486, -0.3750, -0.3887]], device='cuda:0',
       dtype=torch.bfloat16, grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)
out(train): SequenceClassifierOutput(loss=tensor(1.0901, device='cuda:0', grad_fn=<NllLossBackward0>), logits=tensor([[-3.9795e-02,  6.2500e-01, -8.0566e-03, -5.6250e-01, -4.1797e-01],
        [-6.5430e-02,  7.7734e-01, -4.1211e-01,  6.7444e-03, -1.1475e-01],
        [ 1.0254e-02,  4.9805e-01, -3.6523e-01, -2.9688e-01, -5.1172e-01],
        [ 1.6016e-01,  6.8750e-01, -7.9590e-02, -5.2344e-01, -1.2891e-01],
        [ 8.8867e-02,  3.8477e-01, -5.0293e-02, -2.5586e-01, -2.5586e-01],
        [-2.3730e-01,  4.4531e-01,  1.0449e-01, -2.9688e-01, -7.5781e-01],
        [ 7.4219e-02,  8.1641e-01, -5.9082e-02, -4.1406e-01, -4.2773e-01],
        [-2.1484e-01,  4.1602e-01,  1.9043e-02, -6.6797e-01, -7.3047e-01],
        [-3.5889e-02,  7.4609e-01, -2.8198e-02, -3.7695e-01, -1.4258e-01],
        [-1.5625e-01,  5.4297e-01,  6.9824e-02, -4.3945e-01, -8.6328e-01],
        [ 3.2715e-02,  6.3672e-01, -4.1016e-01, -3.9453e-01, -4.6484e-01],
        [-1.1328e-01,  4.9414e-01,  4.5898e-02, -3.6133e-01, -5.5469e-01],
        [ 9.9609e-02,  6.3672e-01,  1.9531e-01, -3.0469e-01, -3.8281e-01],
        [-4.8633e-01,  5.8594e-01,  2.6703e-04, -1.7578e-01, -6.1719e-01],
        [ 1.9238e-01,  8.1641e-01,  5.6641e-02, -3.1445e-01, -5.3125e-01],
        [ 4.8438e-01,  8.5449e-02, -8.2031e-01, -4.5703e-01,  3.1055e-01]],
       device='cuda:0', dtype=torch.bfloat16, grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)
out(train): SequenceClassifierOutput(loss=tensor(1.0801, device='cuda:0', grad_fn=<NllLossBackward0>), logits=tensor([[ 0.0996,  0.5859, -0.3418, -0.5469, -0.3398],
        [ 0.1484,  0.6445, -0.0898, -0.1475, -0.4102],
        [ 0.0214,  0.6367,  0.0369, -0.2432, -0.5117],
        [ 0.0430,  0.6992, -0.2852, -0.2969, -0.3320],
        [ 0.2988,  0.7109,  0.0405, -0.4316, -0.2812],
        [-0.0825,  0.4668, -0.1787, -0.5039, -0.5273],
        [-0.1895,  0.7852, -0.0791, -0.2734, -0.3203],
        [ 0.0302,  0.6836, -0.1206, -0.3555, -0.2451],
        [-0.3164,  0.6797, -0.2334, -0.3125, -0.4668],
        [-0.2031,  0.9336, -0.0178, -0.1025, -0.4590],
        [ 0.0618,  0.7188, -0.1279, -0.5859, -0.4590],
        [-0.4863,  0.5469, -0.0996, -0.3984, -0.2949],
        [-0.1060,  0.7539, -0.1729, -0.1816, -0.2412],
        [-0.0806,  0.4922, -0.0732, -0.5117, -0.4824],
        [ 0.2969,  0.7383,  0.0194, -0.4551, -0.4941],
        [-0.3828,  0.5938,  0.2061, -0.3262, -0.4980]], device='cuda:0',
       dtype=torch.bfloat16, grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)
out(train): SequenceClassifierOutput(loss=tensor(1.1003, device='cuda:0', grad_fn=<NllLossBackward0>), logits=tensor([[ 0.1143,  0.9102, -0.2422, -0.3008, -0.0835],
        [ 0.1973,  0.7344, -0.0884, -0.3164, -0.3008],
        [ 0.4004,  0.7734, -0.3730, -0.6406, -0.4688],
        [-0.1177,  0.9141, -0.1797, -0.1953, -0.4883],
        [-0.3262,  0.6914, -0.2949, -0.4512, -0.1426],
        [-0.3867,  0.5234, -0.1475,  0.1523, -0.5820],
        [-0.5586,  1.1406, -0.2266, -0.3984, -0.4629],
        [-0.2314,  1.0391, -0.0698, -0.4961, -0.4629],
        [-0.0569,  0.4746, -0.2002, -0.3125, -0.0801],
        [-0.1963,  0.3652, -0.2656, -0.3418, -0.6719],
        [ 0.2441,  0.6953,  0.1455, -0.2070, -0.3340],
        [ 0.1226,  0.8516, -0.1729, -0.3398, -0.3496],
        [ 0.2002,  0.7539,  0.1787, -0.3457, -0.4492],
        [ 0.2832,  0.8672, -0.0566, -0.2793, -0.4004],
        [ 0.0129,  0.6562,  0.0164, -0.2891, -0.4961],
        [-0.1680,  0.8320,  0.0957, -0.3984, -0.2715]], device='cuda:0',
       dtype=torch.bfloat16, grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)
out(train): SequenceClassifierOutput(loss=tensor(1.2295, device='cuda:0', grad_fn=<NllLossBackward0>), logits=tensor([[-0.0231,  0.4941, -0.1260, -0.6289, -0.2354],
        [ 0.1279,  0.7891, -0.4062, -0.6406, -0.2617],
        [ 0.0148,  0.8906, -0.1982, -0.3535, -0.5859],
        [-0.0659,  0.8398,  0.0952, -0.1543, -0.2061],
        [-0.0265,  0.7734, -0.3223, -0.2002, -0.1992],
        [-0.3496,  1.0156, -0.1885, -0.2988, -0.3887],
        [-0.2119,  0.6445, -0.3262, -0.2432, -0.4492],
        [-0.2520,  0.7227, -0.0376, -0.3379, -0.2480],
        [-0.0615,  0.6055, -0.1387, -0.3301, -0.3184],
        [-0.0288,  0.4961,  0.1289, -0.2773, -0.0154],
        [-0.0996,  0.7070, -0.2676, -0.7734, -0.3047],
        [-0.2080,  0.2871,  0.0684, -0.2871, -0.1279],
        [-0.0454,  0.7891,  0.0884, -0.1250, -0.2324],
        [-0.1680,  0.7500,  0.0728, -0.3066, -0.6406],
        [-0.1836,  0.5547,  0.1641, -0.3340, -0.3145],
        [-0.2832,  0.5859,  0.0593, -0.0620, -0.4727]], device='cuda:0',
       dtype=torch.bfloat16, grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)
        [-0.4395,  0.3691,  0.0315, -0.4531, -0.9297],2e-02, -3.1641e-01],, attentions=None)
        [-4.0283e-02,  5.1953e-01, -2.0508e-01, -5.1172e-01, -2.8906e-01],, attentions=None)
        [-1.3770e-01,  3.3398e-01, -1.9727e-01, -3.2227e-01, -3.9062e-01]],
       device='cuda:0', dtype=torch.bfloat16, grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)
out(train): SequenceClassifierOutput(loss=tensor(1.1526, device='cuda:0', grad_fn=<NllLossBackward0>), logits=tensor([[-0.0356,  0.8359,  0.0581, -0.3223, -0.3027],
        [-0.0640,  0.8047, -0.2520, -0.3516, -0.2539],
        [ 0.3105,  0.6719, -0.3770, -0.5508, -0.1895],
        [ 0.0254,  0.6289, -0.6094, -0.3418, -0.6055],
        [-0.2676,  0.5586,  0.0364, -0.2578, -0.4961],
        [-0.0806,  0.3887, -0.1177, -0.4902, -0.3535],
        [-0.1553,  0.8359,  0.3125, -0.5898, -0.3887],
        [-0.1748,  0.8867, -0.1758, -0.6523, -0.4590],
        [-0.0894,  0.8594, -0.0674, -0.1157, -0.3145],
        [-0.2354,  0.6523, -0.3008, -0.2930, -0.6641],
        [ 0.0374,  1.1016,  0.0515, -0.3613, -0.3320],
        [ 0.2461,  0.5781,  0.1167, -0.3301, -0.2441],
        [ 0.1914,  0.8672, -0.3652, -0.1289, -0.1748],
        [-0.0022,  0.7617,  0.2178, -0.2178, -0.3535],
        [-0.1104,  1.3516, -0.0300, -0.5430, -0.1924],
        [-0.2656,  0.5391, -0.0825, -0.3770, -0.6367]], device='cuda:0',
       dtype=torch.bfloat16, grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)
out(train): SequenceClassifierOutput(loss=tensor(0.8760, device='cuda:0', grad_fn=<NllLossBackward0>), logits=tensor([[-2.1387e-01,  3.8281e-01, -2.1973e-02, -7.6562e-01, -5.9766e-01],
        [ 5.4688e-02,  8.6719e-01, -9.7656e-02, -2.3633e-01, -3.3008e-01],
        [ 3.1128e-02,  9.0625e-01, -3.1982e-02, -6.3672e-01, -4.4141e-01],
        [ 3.5889e-02,  9.7266e-01, -2.6953e-01, -1.7676e-01, -2.3535e-01],
        [-1.8457e-01,  5.3906e-01, -2.6953e-01, -2.5586e-01, -1.0469e+00],
        [-2.2656e-01,  1.0938e+00, -2.2949e-01,  1.4355e-01, -1.4062e-01],
        [-4.5898e-01,  9.9609e-01, -2.4609e-01, -4.5312e-01, -4.0430e-01],
        [-5.2344e-01,  8.3594e-01, -5.5176e-02, -3.9062e-01, -3.4180e-01],
        [-2.2852e-01,  7.3438e-01, -1.5723e-01, -2.9102e-01, -2.2168e-01],
        [ 1.0254e-01,  9.1016e-01, -1.5625e-01, -4.9805e-01, -8.2031e-02],
        [-8.2031e-02,  9.9609e-01, -1.4258e-01, -4.0039e-01, -3.3984e-01],
        [-3.3398e-01,  1.0781e+00,  1.0498e-01, -3.3594e-01, -8.6328e-01],
        [-3.1445e-01,  9.1406e-01,  1.9238e-01, -6.1035e-02, -1.3281e-01],
        [ 2.7148e-01,  9.0234e-01, -3.6914e-01, -6.0156e-01, -5.5078e-01],
        [ 3.3760e-04,  1.0781e+00, -1.7480e-01, -3.0273e-01, -6.2891e-01],
        [-2.2168e-01,  8.9062e-01, -1.5723e-01, -3.7109e-01, -4.3359e-01]],
       device='cuda:0', dtype=torch.bfloat16, grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)
out(train): SequenceClassifierOutput(loss=tensor(0.9263, device='cuda:0', grad_fn=<NllLossBackward0>), logits=tensor([[-0.5625,  0.7969, -0.3164, -0.2070,  0.0583],
        [-0.1582,  0.9570, -0.0654, -0.4395, -0.2500],
        [-0.0215,  0.5703, -0.1982, -0.6094, -0.4395],
        [-0.1992,  1.1250, -0.0359, -0.6836, -0.1455],
        [-0.2266,  0.8359, -0.4844, -0.2471, -0.2891],
        [ 0.0664,  0.7461, -0.2559, -0.2412, -0.4941],
        [-0.1992,  0.7695, -0.4141, -0.1348, -0.3906],
        [ 0.1396,  0.9766, -0.2539, -0.3574, -0.5742],
        [-0.2305,  0.3984, -0.0143, -0.1426, -0.4043],
        [-0.0723,  1.1016,  0.0075, -0.4434, -0.1611],
        [ 0.2441,  0.8086, -0.1797, -0.2871, -0.1533],
        [-0.1245,  0.9922, -0.1787, -0.4844, -0.3125],
        [-0.2402,  0.8594, -0.0386, -0.3750, -0.4980],
        [-0.1367,  0.6406, -0.1582, -0.6016, -0.1904],
        [-0.1030,  0.6758, -0.1592, -0.2930, -0.5078],
        [-0.0938,  0.9922, -0.1191, -0.4766, -0.3535]], device='cuda:0',
       dtype=torch.bfloat16, grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)
        [-4.0283e-02,  5.1953e-01, -2.0508e-01, -5.1172e-01, -2.8906e-01],, attentions=None)
        [ 0.3359,  0.8047, -0.2773, -0.6055, -0.4043],2e-01, -2.8906e-01],, attentions=None)
        [-0.0146,  0.8906,  0.0654, -0.4316, -0.8750]], device='cuda:0',
       dtype=torch.bfloat16, grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)
out(train): SequenceClassifierOutput(loss=tensor(1.0293, device='cuda:0', grad_fn=<NllLossBackward0>), logits=tensor([[-0.1582,  0.6641,  0.1416, -0.0415, -0.4902],
        [-0.0243,  0.8281, -0.1934, -0.2295, -0.3203],
        [-0.0028,  0.6758, -0.3164, -0.5898, -0.4707],
        [-0.1279,  0.8516, -0.3027, -0.2324, -0.5977],
        [ 0.0559,  0.9023, -0.0120, -0.3574, -0.2266],
        [-0.0747,  0.5859, -0.4961, -0.5742, -0.4316],
        [-0.2334,  0.5742, -0.1660, -0.4238, -0.2148],
        [-0.1191,  0.8594, -0.1699, -0.4277, -0.6094],
        [-0.3730,  1.0781,  0.1328, -0.6445, -0.1865],
        [-0.1816,  0.8516, -0.3750, -0.4238, -0.1787],
        [-0.1318,  0.9023,  0.1553, -0.4824, -0.2432],
        [ 0.1758,  0.3418,  0.1299, -0.4355, -0.5352],
        [ 0.0476,  0.6953, -0.2910, -0.3633, -0.7734],
        [ 0.2178,  1.0859,  0.0361, -0.4785, -0.1826],
        [ 0.0165,  0.6523, -0.1787, -0.0728, -0.3145],
        [-0.2793,  0.7422, -0.2314, -0.2598, -0.4531]], device='cuda:0',
       dtype=torch.bfloat16, grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)
out(train): SequenceClassifierOutput(loss=tensor(1.1460, device='cuda:0', grad_fn=<NllLossBackward0>), logits=tensor([[ 1.2158e-01,  7.1484e-01, -2.7734e-01, -2.0020e-01, -2.6562e-01],
        [-4.4141e-01,  9.2285e-02, -7.2754e-02, -6.4062e-01, -5.3516e-01],
        [-7.6660e-02,  1.1406e+00, -1.9531e-03, -3.0859e-01, -2.7148e-01],
        [-2.9883e-01,  8.0859e-01, -4.2188e-01, -3.1250e-01, -5.3906e-01],
        [-3.5938e-01,  1.0156e+00, -1.6403e-03, -3.6523e-01, -5.8746e-04],
        [ 9.2773e-02,  7.1484e-01,  2.8125e-01, -4.2188e-01, -8.7109e-01],
        [ 1.1780e-02,  8.0859e-01, -8.8379e-02, -3.0273e-01, -3.2031e-01],
        [-2.1680e-01,  5.3516e-01, -1.9434e-01, -2.7930e-01, -3.5547e-01],
        [-1.9775e-02,  8.2031e-01,  4.9805e-02, -3.6328e-01, -1.4551e-01],
        [-1.7969e-01,  1.0938e+00, -6.0303e-02, -1.6699e-01, -1.4844e-01],
        [-5.5176e-02,  8.6719e-01, -4.7302e-03, -2.1484e-01, -5.1562e-01],
        [-4.5898e-01,  9.8438e-01, -4.0820e-01, -1.5625e-01, -3.3008e-01],
        [-3.4912e-02,  9.4141e-01, -3.8818e-02, -6.0547e-01, -7.6562e-01],
        [-1.5918e-01,  7.6562e-01, -5.7031e-01, -4.9609e-01, -5.4297e-01],
        [ 4.0234e-01,  2.1387e-01, -4.1602e-01, -6.3672e-01, -5.3125e-01],
        [-1.8945e-01,  5.1172e-01, -2.8125e-01, -6.5234e-01, -9.5312e-01]],
       device='cuda:0', dtype=torch.bfloat16, grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)
out(train): SequenceClassifierOutput(loss=tensor(0.9243, device='cuda:0', grad_fn=<NllLossBackward0>), logits=tensor([[-2.4121e-01,  1.2578e+00, -1.3770e-01, -4.2188e-01, -2.7344e-01],
        [ 1.2793e-01,  9.1797e-01,  1.8457e-01, -1.6211e-01, -4.4141e-01],
        [ 2.4902e-01,  9.8828e-01, -3.4766e-01, -4.3555e-01, -2.5195e-01],
        [-6.0059e-02,  1.0703e+00, -3.0469e-01, -4.3945e-01, -2.4512e-01],
        [ 4.1406e-01,  8.5156e-01, -4.6875e-01, -3.9844e-01, -1.5527e-01],
        [ 3.4766e-01,  9.7656e-01, -5.3516e-01, -5.0781e-01, -3.7109e-01],
        [-7.9102e-02,  1.0156e+00, -1.7480e-01, -3.8086e-01,  8.5938e-02],
        [ 5.4688e-02,  1.0547e+00,  1.9043e-01, -4.7266e-01, -1.0547e+00],
        [ 3.5095e-04,  1.1562e+00,  1.8555e-01, -2.0996e-01, -7.9688e-01],
        [-3.9453e-01,  7.5391e-01, -1.0742e-01, -6.4062e-01, -6.0938e-01],
        [ 2.3242e-01,  6.6016e-01, -3.3594e-01, -4.3750e-01, -3.5742e-01],
        [-2.2168e-01,  8.0859e-01,  1.0742e-01, -6.4453e-01, -6.4062e-01],
        [-5.5420e-02,  5.2344e-01, -1.4648e-01, -5.5469e-01, -8.7109e-01],
        [ 3.2227e-02,  1.1250e+00, -2.5781e-01, -2.6758e-01, -4.3164e-01],
        [ 3.5938e-01,  1.0625e+00, -2.4512e-01, -5.5078e-01, -3.0469e-01],
        [ 5.0781e-02,  8.5938e-01, -5.5859e-01, -3.3398e-01, -4.7070e-01]],
       device='cuda:0', dtype=torch.bfloat16, grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)
out(train): SequenceClassifierOutput(loss=tensor(1.0115, device='cuda:0', grad_fn=<NllLossBackward0>), logits=tensor([[-0.0615,  0.7734, -0.1885, -0.7422, -0.6484],
        [-0.0232,  1.1250, -0.1338, -0.8633, -0.5938],
        [ 0.1172,  0.4258, -0.2197, -0.2344, -0.3711],
        [-0.2324,  0.7734,  0.0635, -0.3867, -0.4004],
        [-0.1289,  0.8945, -0.1045, -0.1494, -0.2988],
        [-0.1226,  0.5547, -0.1191, -0.5000, -0.4082],
        [ 0.0728,  0.8516, -0.4727, -0.6797, -0.2373],
        [ 0.2578,  0.9297, -0.1030, -0.5586, -0.5703],
        [ 0.2354,  0.6016, -0.1729, -0.4199, -0.2021],
        [-0.2520,  0.7109, -0.1050, -0.2695, -0.6328],
        [-0.3184,  0.9961, -0.0791, -0.4707, -0.7148],
        [-0.1699,  0.6055, -0.1719, -0.2324, -0.4434],
        [ 0.3730,  0.0297, -0.7109, -0.6523,  0.5156],
        [-0.0981,  0.6094, -0.4082, -0.4629, -0.5664],
        [ 0.0220,  0.7734,  0.1016, -0.6211, -0.2393],
        [ 0.0535,  0.7852, -0.1016, -0.5273, -0.2061]], device='cuda:0',
       dtype=torch.bfloat16, grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)
out(train): SequenceClassifierOutput(loss=tensor(0.9990, device='cuda:0', grad_fn=<NllLossBackward0>), logits=tensor([[-0.0508,  0.8516, -0.5430, -0.3008, -0.0723],
        [-0.4668,  1.0938, -0.0850, -0.8398, -0.3398],
        [-0.2197,  1.1641, -0.2168, -0.4395, -0.3203],
        [-0.2637,  1.0703,  0.2070, -0.5039, -0.2695],
        [ 0.1611,  0.7930, -0.6055, -0.5195, -0.3145],
        [ 0.0879,  1.0156, -0.2500, -0.4863, -0.4180],
        [ 0.0124,  0.9023, -0.2988, -0.3340, -0.1572],
        [ 0.0669,  0.6914,  0.0287, -0.2910, -0.1953],
        [-0.0105,  1.0469,  0.4297, -0.6367, -0.4375],
        [ 0.2969,  0.4141,  0.0228, -0.4707, -0.1953],
        [ 0.0386,  0.9688, -0.1934, -0.6523, -0.3770],
        [-0.2656,  1.1953, -0.2256, -0.2637, -0.5391],
        [ 0.0977,  0.5391, -0.1279, -0.6445, -0.5703],
        [-0.2773,  0.6445, -0.2207, -0.4590, -0.3672],
        [-0.1260,  1.0156, -0.3848, -0.0859, -0.5586],
        [ 0.1523,  0.6289,  0.0723, -0.4492, -0.2676]], device='cuda:0',
       dtype=torch.bfloat16, grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)
        [ 0.3359,  0.8047, -0.2773, -0.6055, -0.4043],2e-01, -2.8906e-01],, attentions=None)
        [-0.0645,  1.1719, -0.0018, -0.5508, -0.4980],2e-01, -2.8906e-01],, attentions=None)
        [ 0.0432,  0.7227, -0.0214, -0.4492, -0.5742]], device='cuda:0',
       dtype=torch.bfloat16, grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)
out(train): SequenceClassifierOutput(loss=tensor(1.0508, device='cuda:0', grad_fn=<NllLossBackward0>), logits=tensor([[ 0.1177,  0.5352, -0.0315, -0.2578, -0.5391],
        [ 0.0898,  1.0547, -0.2168, -0.3750, -0.2793],
        [-0.2109,  0.8867, -0.2891, -0.0188, -0.3418],
        [ 0.0199,  0.8984,  0.0236, -0.4082, -0.4082],
        [ 0.0476,  1.0078, -0.1064, -0.8594, -0.3281],
        [ 0.1030,  0.9492,  0.0320, -0.3223, -0.1904],
        [ 0.0518,  1.0156, -0.2949, -0.4512, -0.2119],
        [-0.2891,  1.0391, -0.3359, -0.5820, -0.5352],
        [ 0.1436,  0.8906,  0.1660, -0.4219, -0.7344],
        [-0.0566,  0.5703,  0.1602, -0.2852, -0.4629],
        [ 0.2051,  1.0547, -0.4648, -0.2812, -0.2793],
        [ 0.0564,  0.9609, -0.5352, -0.5195, -0.3535],
        [ 0.0491,  0.6875,  0.2656, -0.2461, -0.3965],
        [-0.0542,  0.7461, -0.0635, -0.2051, -0.1387],
        [-0.2100,  0.8477, -0.4434, -0.1719, -0.5430],
        [ 0.2178,  0.6758, -0.0449, -0.4883, -0.4590]], device='cuda:0',
       dtype=torch.bfloat16, grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)
out(train): SequenceClassifierOutput(loss=tensor(1.0310, device='cuda:0', grad_fn=<NllLossBackward0>), logits=tensor([[ 0.2021,  0.8945, -0.4180, -0.3809, -0.1396],
        [-0.2383,  0.8906,  0.1738, -0.9727, -0.0378],
        [-0.0591,  0.9219, -0.2656, -0.4355, -0.1396],
        [-0.2139,  1.3047, -0.0157, -0.3301, -0.2637],
        [ 0.0820,  0.8867, -0.1963, -0.3672, -0.5195],
        [-0.1177,  0.9570, -0.4961, -0.0928, -0.3867],
        [-0.3809,  0.8125, -0.3027, -0.3398, -0.4668],
        [ 0.1338,  0.9648, -0.2734, -0.2432, -0.4141],
        [-0.3848,  0.7461, -0.3105, -0.2305, -0.2969],
        [-0.4883,  1.1094, -0.3066, -0.5586, -0.6562],
        [ 0.3027,  0.5234, -0.5391, -0.5742, -0.6484],
        [ 0.0113,  1.1562, -0.1709, -0.3555, -0.4688],
        [-0.0048,  1.1016, -0.4023, -0.4941, -0.3711],
        [-0.1216,  0.7188,  0.0742, -0.3867, -0.8164],
        [-0.3438,  0.4844, -0.4746, -0.3027, -0.5391],
        [-0.1621,  1.0312, -0.1602, -0.4805, -0.2773]], device='cuda:0',
       dtype=torch.bfloat16, grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)
out(train): SequenceClassifierOutput(loss=tensor(1.0831, device='cuda:0', grad_fn=<NllLossBackward0>), logits=tensor([[-0.0205,  1.0625, -0.0177, -0.3438, -0.5195],
        [-0.0410,  0.7344, -0.1924, -0.1494, -0.2832],
        [-0.4531,  0.5586,  0.1162, -0.7461, -0.6094],
        [ 0.2637,  0.6719, -0.3242, -0.5469, -0.3320],
        [-0.3262,  1.1094,  0.5352, -0.5625, -0.7578],
        [ 0.0830,  0.6797, -0.0288, -0.5234, -0.5547],
        [-0.1074,  0.6094, -0.4277, -0.3086, -0.4453]], device='cuda:0',
       dtype=torch.bfloat16, grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)
        [-0.0645,  1.1719, -0.0018, -0.5508, -0.4980],2e-01, -2.8906e-01],, attentions=None)
W0629 11:52:52.467000 139012379337344 torch/_dynamo/convert_frame.py:824]     result = inner_convert(
W0629 11:52:52.467000 139012379337344 torch/_dynamo/convert_frame.py:824]     result = inner_convert(
W0629 11:52:52.467000 139012379337344 torch/_dynamo/convert_frame.py:824]     result = inner_convert(
W0629 11:52:52.467000 139012379337344 torch/_dynamo/convert_frame.py:824]     result = inner_convert(
W0629 11:52:52.467000 139012379337344 torch/_dynamo/convert_frame.py:824]     result = inner_convert(
W0629 11:52:52.467000 139012379337344 torch/_dynamo/convert_frame.py:824]     result = inner_convert(
W0629 11:52:52.467000 139012379337344 torch/_dynamo/convert_frame.py:824]     result = inner_convert(
        [-0.0522,  0.8008,  0.1904, -0.3516, -0.5898],2e-01, -2.8906e-01],, attentions=None)
        [ 0.0217,  0.2871, -0.1680, -0.3613, -0.6523],
        [-0.4180,  0.6758, -0.0188, -0.5664, -0.7500],
        [ 0.1992,  0.4570, -0.2393, -0.4258, -0.7422]], device='cuda:0',
       dtype=torch.bfloat16, grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)
        [-0.0522,  0.8008,  0.1904, -0.3516, -0.5898],2e-01, -2.8906e-01],, attentions=None)
        [-0.2178,  0.9141,  0.0075, -0.7969, -0.6367],2e-01, -2.8906e-01],, attentions=None)
        [-0.2539,  0.8086, -0.1680, -0.4336, -0.8320]], device='cuda:0',
       dtype=torch.bfloat16, grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)
out(train): SequenceClassifierOutput(loss=tensor(1.0859, device='cuda:0', grad_fn=<NllLossBackward0>), logits=tensor([[-0.4355,  0.6094, -0.1084, -0.3145, -0.5117],
        [-0.0476,  0.6680,  0.3418, -0.6328, -0.6914],
        [-0.0630,  0.4375,  0.0503, -0.1934, -1.0000],
        [-0.1260,  0.9961, -0.0199, -0.4141, -0.5508],
        [-0.1260,  1.1406,  0.1670, -0.4648, -0.4141],
        [-0.0378,  0.4355,  0.1543, -0.6562, -0.6797],
        [ 0.1104,  0.9219,  0.0317, -0.5977, -1.0234],
        [-0.1504,  0.6016, -0.1895, -0.5273, -0.6836],
        [-0.0332,  0.8164,  0.3652, -0.3770, -0.5547],
        [ 0.0864,  0.5234, -0.1348, -0.7930, -0.6328],
        [-0.3008,  0.8867,  0.1475, -0.2793, -0.5977],
        [ 0.0311,  0.3398, -0.0060, -0.4277,  0.1177],
        [-0.3574,  0.4746,  0.2578, -0.6719, -0.5742],
        [ 0.1699,  0.6641, -0.2832, -0.5312, -0.6484],
        [-0.1445,  0.5742,  0.4355, -0.4570, -0.5664],
        [-0.4043,  0.8398,  0.4238, -0.5820, -0.5117]], device='cuda:0',
       dtype=torch.bfloat16, grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)
        [-0.2178,  0.9141,  0.0075, -0.7969, -0.6367],2e-01, -2.8906e-01],, attentions=None)
        [ 0.1650,  0.7461, -0.2910, -0.2715, -0.6367],2e-01, -2.8906e-01],, attentions=None)
        [ 0.0121,  0.6953, -0.3848, -0.5273, -0.5195]], device='cuda:0',
       dtype=torch.bfloat16, grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)
out(train): SequenceClassifierOutput(loss=tensor(0.8054, device='cuda:0', grad_fn=<NllLossBackward0>), logits=tensor([[ 0.0330,  1.1641,  0.1001, -0.5430, -0.8398],
        [-0.0688,  0.9336, -0.2812, -0.4004, -0.4199],
        [-0.2754,  0.8047, -0.1406, -0.5117, -0.8125],
        [-0.2363,  0.9414, -0.0679, -0.4023, -0.4512],
        [-0.0635,  1.0547, -0.1016, -0.4688, -0.6836],
        [ 0.0034,  1.0625, -0.4121, -0.3164, -0.2754],
        [ 0.1875,  0.9570, -0.1719, -0.3125, -0.2539],
        [ 0.2617,  0.4863, -0.1660, -0.2422, -0.6523],
        [ 0.1006,  0.3516, -0.6836, -0.6055, -0.7305],
        [ 0.0110,  1.1172, -0.0840, -0.5430, -0.5430],
        [ 0.0630,  0.5977, -0.3320, -0.5625, -0.6914],
        [-0.1484,  0.9336, -0.2197, -0.5977, -0.0303],
        [ 0.0884,  1.0781, -0.1221, -0.3730, -0.4941],
        [-0.1074,  1.0703, -0.3066, -0.5117, -0.5664],
        [-0.0233,  1.0078, -0.1650, -0.4473, -0.4141],
        [ 0.0162,  1.1484, -0.0601, -0.5195, -0.2676]], device='cuda:0',
       dtype=torch.bfloat16, grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)
out(train): SequenceClassifierOutput(loss=tensor(1.1389, device='cuda:0', grad_fn=<NllLossBackward0>), logits=tensor([[ 0.1943,  0.6133, -0.4863, -0.4844, -0.6367],
        [-0.0684,  0.8945,  0.2676, -0.4629, -0.8320],
        [-0.1113,  1.1328, -0.2812, -0.1523, -0.5703],
        [-0.1953,  0.6562, -0.1387, -0.6914, -0.6211],
        [-0.2734,  0.4180, -0.3672, -0.4473, -0.6484],
        [ 0.0337,  0.8984, -0.1455, -0.4648, -0.3691],
        [-0.1543,  0.4512, -0.1816, -0.5156, -0.8008],
        [-0.0383,  1.2500, -0.3262, -0.4688, -0.2441],
        [-0.0830,  1.0625, -0.2148, -0.6875, -0.6328],
        [ 0.0425,  1.2031, -0.2637, -0.6328, -0.4824],
        [-0.0938,  0.4141,  0.0240, -0.4980, -0.8438],
        [-0.1680,  0.7656,  0.0400, -0.0850, -0.6289],
        [-0.1855,  0.7305,  0.1738, -0.3535, -0.4688],
        [-0.3379,  0.8750, -0.0845, -0.2793, -0.1099],
        [-0.3340,  0.8711,  0.0157, -0.7812, -0.5938],
        [-0.0952,  0.9766,  0.0226, -0.8242, -0.1895]], device='cuda:0',
       dtype=torch.bfloat16, grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)
        [ 0.1650,  0.7461, -0.2910, -0.2715, -0.6367],2e-01, -2.8906e-01],, attentions=None)
        [ 0.0427,  0.9961,  0.3242, -0.6016, -0.6914],2e-01, -2.8906e-01],, attentions=None)
        [-0.2832,  0.6719, -0.0535, -0.6523, -0.7227]], device='cuda:0',
       dtype=torch.bfloat16, grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)
out(train): SequenceClassifierOutput(loss=tensor(0.9385, device='cuda:0', grad_fn=<NllLossBackward0>), logits=tensor([[-0.4395,  1.1250, -0.3828, -0.3535, -0.5586],
        [-0.2793,  0.6758, -0.4414, -0.4648, -0.3457],
        [-0.0591,  1.1641, -0.3301, -0.3652, -0.5938],
        [-0.0088,  0.6445, -0.4629, -0.4980, -0.3594],
        [ 0.1455,  0.9453, -0.2637, -0.6562, -0.3164],
        [ 0.0557,  0.9531, -0.1514, -0.5352, -0.4375],
        [ 0.1226,  1.3750, -0.1260, -0.4609, -0.2490],
        [ 0.1128,  0.7617, -0.0923, -0.4297, -0.3086],
        [ 0.0188,  0.9727, -0.3574, -0.4863, -0.2070],
        [-0.4082,  0.7070,  0.2070, -0.5625, -0.5469],
        [-0.1885,  0.7539, -0.0869, -0.3867, -1.0391],
        [-0.3926,  0.8125, -0.2891, -0.1582, -0.5469],
        [ 0.1006,  0.9375, -0.1455, -0.4492, -0.3262],
        [ 0.0129,  0.8906,  0.0305, -0.4980, -0.8203],
        [-0.3027,  0.7578, -0.4102, -0.1611, -0.6133],
        [-0.1738,  0.9414,  0.1309, -1.0312, -0.9336]], device='cuda:0',
       dtype=torch.bfloat16, grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)
out(train): SequenceClassifierOutput(loss=tensor(0.7944, device='cuda:0', grad_fn=<NllLossBackward0>), logits=tensor([[-0.0308,  1.4766, -0.0232, -0.5234, -0.2520],
        [-0.0361,  0.8594, -0.2559, -0.0518, -0.4668],
        [ 0.3086,  1.0703, -0.3203, -0.3809, -0.3906],
        [ 0.4121,  0.9961, -0.3770, -0.6562, -0.3594],
        [-0.2598,  1.2656, -0.0879, -0.2598, -0.3770],
        [-0.0315,  0.8359, -0.5430, -0.6367, -0.2598],
        [ 0.0781,  0.5938, -0.1338, -0.3848, -0.5898],
        [ 0.0625,  1.1094, -0.2422, -0.1943, -0.4902],
        [-0.1797,  0.9531, -0.3047, -0.1641, -0.4062],
        [ 0.2256,  1.0547, -0.1709, -0.3711, -0.5898],
        [-0.0630,  1.1641, -0.0036, -0.4453, -0.2930],
        [-0.0762,  1.0938, -0.2188, -0.5820, -0.3730],
        [ 0.2754,  1.3125, -0.2070, -0.7539, -0.1865],
        [ 0.1865,  0.9727, -0.3340, -0.7148, -0.3965],
        [-0.1797,  0.8672, -0.2363, -0.2695, -0.2520],
        [-0.2988,  0.8984, -0.6328, -0.5898, -0.7109]], device='cuda:0',
       dtype=torch.bfloat16, grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)
out(train): SequenceClassifierOutput(loss=tensor(1.0471, device='cuda:0', grad_fn=<NllLossBackward0>), logits=tensor([[-0.4043,  0.8945, -0.1641, -0.5977, -0.9453],
        [-0.0393,  0.7500, -0.4043, -0.2236, -0.5117],
        [ 0.1533,  1.0391,  0.0415, -0.5742, -0.3262],
        [-0.1445,  0.6836,  0.2451, -0.6641, -0.9688],
        [ 0.3379,  0.6016, -0.4961, -0.4277, -0.4434],
        [-0.2305,  0.8555,  0.0454, -0.4082, -0.6680],
        [-0.1865,  0.5742, -0.3320, -0.7188, -0.4570],
        [ 0.0918,  0.9922, -0.4277, -0.3320, -0.3398],
        [ 0.1104,  0.6875, -0.3750, -0.6289, -0.7383],
        [ 0.0383,  1.2031, -0.1328, -0.3730, -0.2891],
        [ 0.0698,  1.2578,  0.1157, -0.3125, -0.5625],
        [ 0.1147,  1.2188, -0.4121, -0.2451, -0.1846],
        [ 0.0947,  0.8906, -0.4609, -0.4941, -0.4316],
        [-0.2891,  0.8438, -0.3008, -0.4219, -0.5273],
        [-0.1934,  0.5977, -0.1494, -0.1270, -0.7031],
        [ 0.2227,  0.8984, -0.6406, -0.7148, -0.3027]], device='cuda:0',
       dtype=torch.bfloat16, grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)
out(train): SequenceClassifierOutput(loss=tensor(0.9224, device='cuda:0', grad_fn=<NllLossBackward0>), logits=tensor([[-0.2148,  1.1719, -0.5703, -0.1108, -0.1787],
        [-0.0564,  0.9492, -0.4531, -0.5664, -0.0669],
        [-0.1250,  1.0781, -0.3105, -0.5898, -0.5117],
        [-0.1079,  1.1094, -0.5352, -0.6094, -0.4355],
        [-0.1084,  0.8594, -0.5586, -0.2041, -0.4688],
        [-0.1934,  1.3438, -0.5820, -0.5781, -0.1943],
        [-0.0674,  0.9609, -0.2539, -0.5039, -0.3809],
        [ 0.0410,  0.7969, -0.4512, -0.6953, -0.2930],
        [-0.3945,  0.8242, -0.1367, -0.2969, -0.4238],
        [ 0.1138,  0.8477, -0.3223, -0.5781, -0.4102],
        [ 0.1475,  1.0391, -0.2773, -0.2949, -0.2139],
        [ 0.0703,  1.0391, -0.3789, -0.3672, -0.2871],
        [ 0.0977,  0.5820, -0.5742, -0.5273, -0.3223],
        [-0.1040,  0.8906, -0.1943, -0.5469, -0.5312],
        [ 0.3418,  1.1250, -0.3438, -0.6328, -0.4316],
        [-0.1367,  0.7773, -0.1729, -0.2949, -0.2412]], device='cuda:0',
       dtype=torch.bfloat16, grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)
        [ 0.0427,  0.9961,  0.3242, -0.6016, -0.6914],2e-01, -2.8906e-01],, attentions=None)
        [ 0.2139,  1.1016, -0.5078, -0.5703, -0.0933],2e-01, -2.8906e-01],, attentions=None)
        [-0.0613,  1.1484, -0.2676, -0.4805, -0.4023]], device='cuda:0',
       dtype=torch.bfloat16, grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)
out(train): SequenceClassifierOutput(loss=tensor(1.0198, device='cuda:0', grad_fn=<NllLossBackward0>), logits=tensor([[-0.1455,  0.8320, -0.3086, -0.3730, -0.6328],
        [ 0.0170,  1.0000, -0.1182, -0.4844, -0.5938],
        [ 0.0259,  0.7305, -0.2344, -0.4922, -0.7305],
        [-0.3086,  0.4805,  0.0947, -0.5781, -1.1250],
        [-0.0645,  1.2734, -0.2354, -0.6328, -0.4941],
        [-0.3828,  0.7227, -0.5039, -0.0255, -0.3555],
        [ 0.0796,  0.9258, -0.2656, -0.6055, -0.2637],
        [ 0.2500,  0.9336, -0.3262, -0.4473, -0.5703],
        [ 0.3652,  0.9883, -0.3184, -0.3359, -0.5859],
        [-0.1494,  0.7695,  0.0796, -0.7266, -0.6641],
        [ 0.0928,  1.1719, -0.3535, -0.6836, -0.4160],
        [-0.0356,  1.0000,  0.0530, -0.6289, -0.4043],
        [ 0.0850,  0.9805, -0.3496, -0.5312, -0.4922],
        [ 0.0311,  0.8359, -0.2334, -0.3965, -0.5859],
        [ 0.0825,  0.9766, -0.0786, -0.4648, -0.5195],
        [-0.2080,  1.0938, -0.1187, -0.7500, -0.3359]], device='cuda:0',
       dtype=torch.bfloat16, grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)
out(train): SequenceClassifierOutput(loss=tensor(1.2341, device='cuda:0', grad_fn=<NllLossBackward0>), logits=tensor([[ 2.9297e-01,  8.4375e-01, -3.4961e-01, -9.6191e-02, -4.4922e-01],
        [-3.7500e-01,  4.7852e-01, -1.1230e-01, -4.8242e-01, -8.8281e-01],
        [ 1.0303e-01,  9.5312e-01, -3.5938e-01, -3.0078e-01,  1.0157e-04],
        [ 2.3730e-01,  1.0156e+00,  6.6895e-02, -2.1484e-01, -6.4844e-01],
        [-1.2256e-01,  1.1484e+00, -6.9580e-03, -8.3984e-02, -3.5352e-01],
        [ 1.7285e-01,  7.0312e-01, -2.7148e-01, -4.5117e-01, -5.2344e-01],
        [-3.5547e-01,  8.9062e-01, -4.4336e-01, -2.0020e-01, -4.7852e-01],
        [-1.5039e-01,  1.0625e+00, -2.0801e-01, -7.3047e-01, -6.6016e-01],
        [ 4.3945e-02,  8.5156e-01,  2.8564e-02, -5.8984e-01, -3.8086e-01],
        [-4.3945e-01,  1.0312e+00, -2.5781e-01, -1.1768e-01, -4.8047e-01],
        [-1.7334e-02,  1.1562e+00, -2.6367e-01, -4.9609e-01, -3.7500e-01],
        [ 2.9492e-01,  1.0859e+00, -4.1992e-01, -6.2500e-01, -3.1445e-01],
        [-3.2959e-02,  1.0781e+00, -4.1211e-01, -6.1719e-01, -3.4766e-01],
        [-8.6914e-02,  5.0391e-01,  1.8164e-01, -5.5078e-01, -6.3672e-01],
        [-5.6396e-02,  1.0391e+00, -5.8838e-02, -4.5898e-01, -3.6523e-01],
        [-1.3867e-01,  9.9219e-01, -2.2852e-01, -5.0781e-01, -8.9062e-01]],
       device='cuda:0', dtype=torch.bfloat16, grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)
out(train): SequenceClassifierOutput(loss=tensor(1.0486, device='cuda:0', grad_fn=<NllLossBackward0>), logits=tensor([[ 0.0947,  0.8555, -0.6445, -0.4766,  0.0708],
        [ 0.0500,  0.8633, -0.2148, -0.2695, -0.3945],
        [-0.0075,  1.3828, -0.1523, -0.8633, -0.3086],
        [-0.0118,  1.2266, -0.4199, -0.8945, -0.5977],
        [ 0.2539,  1.0391, -0.3398, -0.6172, -0.6133],
        [-0.0405,  1.3203, -0.3438, -0.7969, -0.6875],
        [ 0.2246,  1.2812, -0.2891, -0.6445, -0.2520],
        [-0.1924,  0.8594,  0.0825, -0.6914, -0.7500],
        [-0.1553,  0.9766, -0.2578, -0.5586, -0.7539],
        [-0.0283,  0.7734, -0.3984, -0.8008, -0.3105],
        [ 0.3535,  0.9766, -0.6367, -0.7422, -0.4082],
        [ 0.0664,  0.8320,  0.2354, -0.1572, -1.1328],
        [ 0.0444,  1.0469, -0.0737, -0.0090, -0.4824],
        [ 0.1514,  0.9141, -0.0208, -0.3906, -0.4668],
        [-0.0025,  0.9688, -0.1895, -0.6602, -1.0234],
        [-0.3203,  1.0312, -0.3848, -0.4297, -0.5781]], device='cuda:0',
       dtype=torch.bfloat16, grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)
        [ 0.2139,  1.1016, -0.5078, -0.5703, -0.0933],2e-01, -2.8906e-01],, attentions=None)
        [-0.2393,  0.8438, -0.6016, -0.0108, -0.4277],2e-01, -2.8906e-01],, attentions=None)
        [-0.0630,  0.8984, -0.2637, -0.4629, -0.2285]], device='cuda:0',
       dtype=torch.bfloat16, grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)
out(train): SequenceClassifierOutput(loss=tensor(1.1421, device='cuda:0', grad_fn=<NllLossBackward0>), logits=tensor([[ 0.0635,  0.8750, -0.4434, -0.0493, -0.4414],
        [ 0.0547,  1.1172, -0.6094, -0.7930, -0.5156],
        [ 0.2236,  0.7891, -0.5898, -0.5430, -0.3223],
        [-0.1128,  0.9023, -0.1367, -0.5273, -0.6758],
        [ 0.1455,  1.2500, -0.2266, -0.6445, -0.4434],
        [-0.1846,  0.8359, -0.0266, -0.3457, -0.6953],
        [ 0.2773,  1.1797, -0.1357, -0.4629, -0.7266],
        [ 0.0496,  0.9609, -0.6680, -0.6133, -0.3652],
        [-0.0835,  0.9102,  0.0796, -0.1572, -0.4883],
        [-0.2793,  0.8203, -0.3516, -0.3867, -0.4336],
        [ 0.3379,  0.6133, -0.4492, -0.4453, -0.1992],
        [-0.0227,  1.0781, -0.3223, -0.0593, -0.7227],
        [-0.3398,  1.1172,  0.0356, -0.6680, -0.6836],
        [-0.0532,  1.2422, -0.0796, -0.4316, -0.4590],
        [ 0.1211,  0.7305, -0.1875, -0.2490, -0.5742],
        [ 0.1060,  1.0156, -0.6133, -0.6914, -0.1035]], device='cuda:0',
       dtype=torch.bfloat16, grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)
        [-0.2393,  0.8438, -0.6016, -0.0108, -0.4277],2e-01, -2.8906e-01],, attentions=None)
        [ 0.3711,  1.5156, -0.4082, -0.6562, -0.3672],2e-01, -2.8906e-01],, attentions=None)
        [-0.3184,  1.1172, -0.0742, -0.8203, -0.7773]], device='cuda:0',
       dtype=torch.bfloat16, grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)
out(train): SequenceClassifierOutput(loss=tensor(0.9087, device='cuda:0', grad_fn=<NllLossBackward0>), logits=tensor([[ 0.2969,  1.1484,  0.0461, -0.6328, -0.2393],
        [ 0.1914,  0.9844, -0.3145, -0.4727, -0.2109],
        [ 0.2715,  1.4531, -0.6562, -0.6992, -0.6016],
        [-0.0859,  1.2031, -0.1416, -0.8711, -0.4551],
        [-0.0488,  0.9492, -0.6367, -0.2031, -0.3086],
        [-0.5625,  0.9961, -0.4160, -0.1709, -0.4844],
        [-0.2930,  1.1172, -0.3027, -0.6602, -0.4160],
        [-0.1245,  1.3281, -0.1001, -0.6602, -0.4531],
        [ 0.1318,  0.7852, -0.7109, -0.7031,  0.3750],
        [ 0.0173,  0.9141, -0.2148, -0.2754, -0.5156],
        [ 0.0972,  1.0703, -0.0776, -0.4023, -0.4512],
        [ 0.0250,  1.0000, -0.2393, -0.4512, -0.4258],
        [ 0.1289,  0.9883, -0.3672, -0.5781, -0.3945],
        [-0.0898,  0.9414, -0.3164, -0.6875, -0.4023],
        [-0.2734,  1.2422, -0.1650, -0.3828, -0.5039],
        [-0.1060,  1.0234,  0.0684, -0.2969, -0.3555]], device='cuda:0',
       dtype=torch.bfloat16, grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)
out(train): SequenceClassifierOutput(loss=tensor(1.1228, device='cuda:0', grad_fn=<NllLossBackward0>), logits=tensor([[ 0.1885,  0.9961, -0.3789, -0.5195, -0.2227],
        [ 0.2656,  1.0547, -0.5156, -0.4746, -0.4512],
        [-0.3320,  1.3047, -0.2051, -0.3691, -0.8281],
        [ 0.0275,  1.1406, -0.1748, -0.5195, -0.6367],
        [ 0.0337,  1.0703, -0.3730, -0.4102, -0.5781],
        [-0.1367,  1.0469, -0.0732, -0.4961, -0.7539],
        [-0.1162,  1.0156, -0.3633, -0.5039, -0.8984],
        [ 0.1504,  1.0781, -0.2715, -0.8359, -0.4277],
        [-0.0349,  1.0391, -0.3340, -0.3340, -0.1982],
        [ 0.0503,  1.0703, -0.0569, -0.3203, -0.2871],
        [ 0.2617,  1.1016, -0.6406, -1.0312, -0.5156],
        [-0.1494,  0.9961, -0.1885, -0.3340, -0.5352],
        [-0.1270,  0.8281, -0.2129, -0.4004, -0.2871],
        [-0.1621,  0.8516, -0.2930, -0.7344, -0.7891],
        [ 0.0625,  1.1797, -0.2412, -0.4688, -0.3633],
        [-0.2490,  0.8281, -0.3340, -0.2598, -0.4980]], device='cuda:0',
       dtype=torch.bfloat16, grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)
out(train): SequenceClassifierOutput(loss=tensor(1.1060, device='cuda:0', grad_fn=<NllLossBackward0>), logits=tensor([[-6.2866e-03,  1.2109e+00, -1.9141e-01, -5.6250e-01, -5.0000e-01],
        [ 1.5430e-01,  1.1172e+00, -1.2891e-01, -1.8262e-01, -5.7617e-02],
        [ 8.3984e-02,  1.1094e+00, -4.1602e-01, -2.7148e-01, -1.9238e-01],
        [-2.4512e-01,  9.5703e-01, -3.0078e-01, -8.2422e-01, -7.3828e-01],
        [-2.0801e-01,  1.0547e+00, -1.5332e-01, -7.7734e-01, -1.9434e-01],
        [ 8.7891e-02,  1.1328e+00, -5.7129e-02, -2.6953e-01, -4.9414e-01],
        [ 7.8613e-02,  1.0547e+00, -2.0605e-01, -3.2617e-01, -6.2500e-01],
        [-6.4941e-02,  9.2578e-01, -6.3477e-02, -5.4688e-01, -3.4961e-01],
        [ 5.7602e-04,  1.0156e+00, -3.1641e-01, -2.8711e-01, -4.1016e-01],
        [-2.1094e-01,  4.7656e-01, -2.7710e-02, -4.1992e-01, -7.7344e-01],
        [-9.3384e-03,  1.2500e+00, -2.1191e-01, -6.1328e-01, -1.5625e-01],
        [ 3.7500e-01,  5.9375e-01, -5.5078e-01, -5.3516e-01, -5.7812e-01],
        [ 2.5000e-01,  9.1016e-01, -6.7188e-01, -5.5469e-01, -3.2422e-01],
        [-2.0605e-01,  1.2969e+00, -1.5918e-01, -5.6250e-01, -5.2344e-01],
        [ 1.6211e-01,  1.2188e+00, -3.1641e-01, -3.4961e-01, -3.7598e-02],
        [-2.0020e-01,  1.3906e+00, -7.2754e-02, -3.2422e-01, -4.8438e-01]],
       device='cuda:0', dtype=torch.bfloat16, grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)
out(train): SequenceClassifierOutput(loss=tensor(0.9148, device='cuda:0', grad_fn=<NllLossBackward0>), logits=tensor([[-0.2383,  1.4375, -0.1738, -1.0156, -0.6289],
        [-0.0107,  0.7422, -0.2354, -0.4355, -0.5625],
        [-0.3418,  1.1172, -0.0315, -0.4590, -0.4629],
        [-0.2012,  0.9805, -0.6523, -0.4395, -0.2119],
        [ 0.1396,  1.1953, -0.0081, -0.6367, -0.3594],
        [-0.0157,  0.8906, -0.3027, -0.5391, -0.3047],
        [ 0.0080,  1.0859, -0.2109, -0.5508, -0.5391],
        [ 0.2275,  0.8438, -0.3359, -0.5312, -0.3828],
        [-0.2969,  0.4785,  0.0513, -0.3438, -0.6289],
        [-0.0693,  1.0312, -0.2500, -0.6133, -0.4434],
        [-0.2891,  0.7852, -0.2949, -0.6406, -0.7656],
        [-0.0491,  0.9414, -0.1699, -0.5039, -0.5938],
        [-0.1611,  1.0547, -0.4648, -0.3613, -0.4941],
        [ 0.1611,  1.2031, -0.1099, -0.4316, -0.4121],
        [-0.1680,  1.2031, -0.2715, -0.5938, -0.5430],
        [ 0.0021,  0.8750,  0.0102, -0.6367, -0.6602]], device='cuda:0',
       dtype=torch.bfloat16, grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)
        [ 0.3711,  1.5156, -0.4082, -0.6562, -0.3672],2e-01, -2.8906e-01],, attentions=None)
        [-0.1367,  1.2578, -0.3438, -0.6211, -0.4062],2e-01, -2.8906e-01],, attentions=None)
        [-0.0806,  0.9336, -0.2852, -0.8164, -0.6172]], device='cuda:0',
       dtype=torch.bfloat16, grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)
out(train): SequenceClassifierOutput(loss=tensor(0.7739, device='cuda:0', grad_fn=<NllLossBackward0>), logits=tensor([[-0.4727,  1.1562, -0.3848, -0.3945, -0.4297],
        [ 0.0112,  0.9766, -0.3594, -0.7266, -0.3887],
        [-0.0625,  0.8984, -0.3906, -0.4863, -0.6367],
        [-0.0195,  1.0703, -0.3066, -0.3652, -0.4121],
        [ 0.0464,  0.9375, -0.2676, -0.4648, -0.3672],
        [ 0.1426,  1.1953, -0.7344, -0.0991, -0.4668],
        [-0.0166,  1.1172, -0.2988, -0.2236, -0.4648],
        [-0.0732,  1.2578, -0.5352, -0.4609, -0.5898],
        [-0.0532,  1.1016, -0.2227, -0.2812, -0.3418],
        [ 0.0442,  1.3516, -0.3418, -0.9766, -0.4082],
        [ 0.0093,  1.2109, -0.1719, -0.4551, -0.3828],
        [-0.3379,  1.0703, -0.1680, -0.6914, -0.5469],
        [-0.2314,  0.9844, -0.2539, -0.5664, -0.3711],
        [-0.0781,  0.9883, -0.4082, -0.5586, -0.3145],
        [ 0.2090,  1.1250, -0.4121, -0.5820, -0.5156],
        [ 0.0713,  0.9297, -0.3340, -0.6680, -0.3984]], device='cuda:0',
       dtype=torch.bfloat16, grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)
out(train): SequenceClassifierOutput(loss=tensor(0.8972, device='cuda:0', grad_fn=<NllLossBackward0>), logits=tensor([[-0.2695,  1.0469, -0.2197, -0.1553, -0.2734],
        [ 0.2246,  1.3672, -0.5117, -0.3223, -0.0845],
        [-0.1064,  0.8633, -0.2969, -0.1523, -0.4980],
        [ 0.0564,  0.7773, -0.2139, -0.5938, -0.6406],
        [ 0.0598,  0.9609, -0.3359, -0.6094, -0.3105],
        [ 0.0067,  0.9688, -0.3320, -0.4355, -0.3613],
        [-0.0732,  1.2500, -0.3496, -1.0625, -0.3359],
        [-0.3887,  1.2031, -0.3320, -0.8125, -0.4297],
        [-0.2500,  1.0938,  0.0874, -0.5703, -0.6523],
        [ 0.1670,  1.2500, -0.3594, -0.5391, -0.1436],
        [ 0.0908,  1.1328, -0.3203, -0.2334, -0.3789],
        [ 0.2344,  1.1250, -0.4863, -0.8047, -0.5430],
        [-0.3906,  0.7383, -0.3184, -0.4023, -0.5352],
        [-0.2402,  0.8789,  0.1338, -0.5234, -0.6523],
        [ 0.0488,  0.8477, -0.3691, -0.5742, -0.3789],
        [-0.2734,  1.0234, -0.2178, -0.2041, -0.3477]], device='cuda:0',
       dtype=torch.bfloat16, grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)
out(train): SequenceClassifierOutput(loss=tensor(0.7561, device='cuda:0', grad_fn=<NllLossBackward0>), logits=tensor([[-0.0200,  1.2500,  0.0303, -0.1865, -0.5781],
        [ 0.1348,  0.6953, -0.5000, -0.3730, -0.2852],
        [-0.0072,  1.1875, -0.4609, -0.4219, -0.0334],
        [ 0.0679,  1.1094, -0.1963, -0.3828, -0.3535],
        [-0.3418,  1.2500, -0.2246, -0.3945, -0.3691],
        [ 0.0332,  0.7539, -0.1816, -0.3105, -0.5156],
        [-0.0649,  1.0312, -0.0640, -0.7812, -1.2812],
        [-0.4102,  1.0469, -0.1768, -0.4219, -0.7422],
        [-0.1611,  0.9961, -0.1553, -0.6328, -0.4238],
        [ 0.0114,  1.1875, -0.2559, -0.5312, -0.4258],
        [ 0.1934,  1.0781, -0.3242, -0.8555, -0.7227],
        [-0.0972,  0.9258, -0.2734, -0.2617, -0.6797],
        [ 0.4082,  1.0625, -0.2178, -0.5391, -0.4531],
        [-0.1504,  0.9141, -0.6172, -0.3770, -0.6055],
        [ 0.0908,  1.0625, -0.6719, -0.5117, -0.6211],
        [ 0.2969,  1.2422, -0.6211, -0.4785, -0.5234]], device='cuda:0',
       dtype=torch.bfloat16, grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)
        [-0.1367,  1.2578, -0.3438, -0.6211, -0.4062],2e-01, -2.8906e-01],, attentions=None)
        [ 0.5195,  0.5430, -0.9180, -0.9258, -0.2988],2e-01, -2.8906e-01],, attentions=None)
        [ 0.0574,  0.6289, -0.3848, -0.6094, -0.7266]], device='cuda:0',
       dtype=torch.bfloat16, grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)
out(train): SequenceClassifierOutput(loss=tensor(0.7681, device='cuda:0', grad_fn=<NllLossBackward0>), logits=tensor([[-0.0483,  1.1797, -0.4492, -0.5820, -0.5430],
        [-0.0024,  0.9375,  0.1289, -0.5977, -0.4160],
        [ 0.0972,  1.2578, -0.3789, -0.8203, -0.4863],
        [ 0.1602,  1.3594, -0.5977, -0.4805, -0.2002],
        [ 0.0972,  1.4609, -0.3828, -1.1016, -0.3223],
        [ 0.0928,  0.9375, -0.6875, -0.6641, -0.3906],
        [-0.1309,  1.2422, -0.1299, -0.4375, -0.6445],
        [ 0.0603,  0.9297, -0.3711, -0.5352, -0.8164],
        [-0.0923,  1.3047, -0.0476, -0.4922, -0.7305],
        [-0.2559,  0.9258, -0.2812, -0.6602, -0.1689],
        [ 0.1060,  1.3984, -0.2656, -0.7734, -0.4004],
        [ 0.0835,  0.9727, -0.1982, -0.7344, -0.8086],
        [ 0.0981,  0.4551, -0.4688, -0.7148, -0.8672],
        [ 0.0713,  1.3438, -0.1006, -0.3594, -0.3105],
        [ 0.2432,  1.2500, -0.3789, -0.8594, -0.3105],
        [ 0.1328,  1.4766, -0.5078, -0.4395, -0.3555]], device='cuda:0',
       dtype=torch.bfloat16, grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)
        [ 0.5195,  0.5430, -0.9180, -0.9258, -0.2988],2e-01, -2.8906e-01],, attentions=None)
        [ 0.0938,  1.3281, -0.1357, -0.7500, -0.3320],2e-01, -2.8906e-01],, attentions=None)
        [-0.0210,  1.1406, -0.3164, -0.4414, -0.5977]], device='cuda:0',
       dtype=torch.bfloat16, grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)
out(train): SequenceClassifierOutput(loss=tensor(0.8557, device='cuda:0', grad_fn=<NllLossBackward0>), logits=tensor([[ 0.0874,  1.0312, -0.4219, -0.9492, -0.5742],
        [-0.0554,  0.7734, -0.2656, -0.7773, -0.6016],
        [ 0.0054,  1.2500, -0.3789, -0.7148, -0.6406],
        [-0.0145,  1.3906, -0.2754, -0.4824, -0.5898],
        [ 0.0137,  0.9844, -0.4727, -1.0000, -0.3398],
        [ 0.1475,  1.2344, -0.3281, -0.4180, -0.4082],
        [-0.2178,  0.9375, -0.4004, -0.5664, -0.3711],
        [ 0.2266,  0.9492, -0.6250, -0.4297, -0.4121],
        [ 0.2256,  1.1172, -0.2451, -0.4961, -0.6250],
        [ 0.0291,  1.0625, -0.4844, -0.5938, -0.2070],
        [-0.0771,  1.0000, -0.6328, -0.7969, -0.4297],
        [ 0.1445,  1.4062, -0.4004, -0.6172,  0.0898],
        [-0.1025,  0.7188, -0.7148, -0.9453, -0.5352],
        [-0.2197,  1.2500, -0.3047, -0.4824, -0.5430],
        [-0.3223,  1.4375, -0.3965, -0.6172, -0.4082],
        [-0.0898,  1.5078, -0.5547, -0.8164, -0.1504]], device='cuda:0',
       dtype=torch.bfloat16, grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)
out(train): SequenceClassifierOutput(loss=tensor(0.9237, device='cuda:0', grad_fn=<NllLossBackward0>), logits=tensor([[-0.0952,  1.1484, -0.6758, -0.5312, -0.6875],
        [ 0.2949,  0.9297, -0.5820, -0.9102, -0.5117],
        [-0.4043,  1.1406, -0.2617, -0.4648, -0.3789],
        [-0.2832,  1.1406, -0.2832, -0.3730, -0.4590],
        [-0.2715,  1.0703,  0.2031, -0.2715, -0.7383],
        [ 0.1187,  0.9766, -0.6875, -0.2520, -0.7461],
        [-0.0403,  0.8594, -0.1445, -0.8672, -0.3633],
        [-0.3438,  0.6875, -0.3223, -0.4180, -0.5977],
        [-0.1934,  1.1484, -0.4258, -0.7422, -0.8477],
        [ 0.1162,  1.1641, -0.5859, -0.7305, -0.7539],
        [-0.0894,  1.2188, -0.2354, -0.8516, -0.5117],
        [ 0.0483,  0.8359,  0.2061, -0.6211, -0.9648],
        [-0.2158,  0.8477,  0.1270, -0.7109, -0.8594],
        [-0.2344,  0.6211, -0.3184, -0.3691, -0.7148],
        [-0.1025,  1.4141, -0.2617, -1.3125, -0.5820],
        [-0.1445,  1.1641, -0.4609, -0.6250, -0.5820]], device='cuda:0',
       dtype=torch.bfloat16, grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)
out(train): SequenceClassifierOutput(loss=tensor(0.9312, device='cuda:0', grad_fn=<NllLossBackward0>), logits=tensor([[ 0.2188,  0.8125, -0.1846, -0.8242, -0.6875],
        [ 0.1064,  1.2734, -0.6602, -0.4219, -0.0603],
        [-0.2236,  1.2891, -0.6016, -0.1084, -0.6211],
        [-0.1602,  1.1328, -0.4668, -0.6211, -0.3496],
        [-0.1011,  1.2969, -0.2168, -1.0078, -0.6172],
        [ 0.4766,  1.1094, -0.1963, -0.3398, -0.2969],
        [-0.0317,  1.1719, -0.4297, -0.6914, -0.1973],
        [ 0.0337,  1.2969, -0.3535, -0.4395, -0.4375],
        [ 0.1299,  1.0469, -0.0227, -0.3945, -0.8711],
        [ 0.0330,  0.8164, -0.2070, -0.4785, -0.2148],
        [ 0.0630,  1.3203,  0.0135, -0.3945, -0.4824],
        [-0.0309,  1.2656, -0.1021, -0.9180, -0.2832],
        [ 0.1177,  1.1016, -0.3105, -0.8359, -0.2871],
        [-0.0275,  0.9609, -0.7227, -0.4316, -0.3574],
        [-0.4355,  0.9141, -0.4727, -0.0688, -0.4668],
        [ 0.2266,  0.9492, -0.5977, -0.7344, -0.1279]], device='cuda:0',
       dtype=torch.bfloat16, grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)
out(train): SequenceClassifierOutput(loss=tensor(0.9136, device='cuda:0', grad_fn=<NllLossBackward0>), logits=tensor([[-0.1406,  1.4141, -0.5391, -0.5430, -0.3340],
        [-0.2578,  1.2734, -0.1533, -0.9570, -0.5391],
        [-0.0364,  1.0625, -0.5234, -0.5312, -0.6445],
        [-0.2793,  1.3203, -0.4375, -0.4785, -0.3633],
        [ 0.3301,  0.4219, -0.5781, -0.3008,  0.5352],
        [ 0.0581,  1.2812, -0.3965, -0.7578, -0.5664],
        [-0.1309,  1.2969, -0.3164, -0.6836, -0.5312],
        [ 0.1201,  1.3047, -0.2930, -0.4414, -0.2178],
        [-0.5938,  1.0781, -0.3477, -0.6172, -0.3809],
        [-0.0981,  1.3047, -0.2480, -0.6953, -0.4375],
        [-0.0098,  0.9453, -0.3633, -0.7539, -0.7422],
        [ 0.0498,  1.4375, -0.3613, -0.5859, -0.2559],
        [-0.0063,  1.3047, -0.4941, -0.6992, -0.4023],
        [-0.1436,  1.0938, -0.1465, -0.7930, -0.6250],
        [-0.0957,  0.8555, -0.4414, -0.2295, -0.6523],
        [-0.0098,  1.2578, -0.4004, -0.6680, -0.3730]], device='cuda:0',
       dtype=torch.bfloat16, grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)
out(train): SequenceClassifierOutput(loss=tensor(0.9252, device='cuda:0', grad_fn=<NllLossBackward0>), logits=tensor([[ 0.2148,  1.3203, -0.4160, -0.5625, -0.3789],
        [-0.0598,  1.0078, -0.4668, -0.6094, -0.2969],
        [-0.2871,  0.7578, -0.0579, -0.9258, -0.4043],
        [ 0.2012,  1.1406, -0.3438, -0.6914, -0.4629],
        [-0.3438,  0.9727, -0.0088, -0.7031, -0.7539],
        [ 0.1973,  1.1641, -0.5547, -0.4961, -0.4258],
        [-0.1123,  1.1406, -0.0056, -0.5625, -0.5234]], device='cuda:0',
       dtype=torch.bfloat16, grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)
        [ 0.0938,  1.3281, -0.1357, -0.7500, -0.3320],2e-01, -2.8906e-01],, attentions=None)
W0629 11:52:52.467000 139012379337344 torch/_dynamo/convert_frame.py:824]     result = inner_convert(
W0629 11:52:52.467000 139012379337344 torch/_dynamo/convert_frame.py:824]     result = inner_convert(
W0629 11:52:52.467000 139012379337344 torch/_dynamo/convert_frame.py:824]     result = inner_convert(
W0629 11:52:52.467000 139012379337344 torch/_dynamo/convert_frame.py:824]     result = inner_convert(
W0629 11:52:52.467000 139012379337344 torch/_dynamo/convert_frame.py:824]     result = inner_convert(
W0629 11:52:52.467000 139012379337344 torch/_dynamo/convert_frame.py:824]     result = inner_convert(
W0629 11:52:52.467000 139012379337344 torch/_dynamo/convert_frame.py:824]     result = inner_convert(
W0629 11:52:52.467000 139012379337344 torch/_dynamo/convert_frame.py:824]     result = inner_convert(
        [ 0.0938,  1.3281, -0.1357, -0.7500, -0.3320],2e-01, -2.8906e-01],, attentions=None)
        [ 0.1592,  0.8398, -0.6133, -0.6875, -0.5391],2e-01, -2.8906e-01],, attentions=None)
        [-0.2129,  1.2656, -0.0168, -0.8828, -0.8516]], device='cuda:0',
       dtype=torch.bfloat16, grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)
out(train): SequenceClassifierOutput(loss=tensor(1.0432, device='cuda:0', grad_fn=<NllLossBackward0>), logits=tensor([[-2.0508e-01,  8.5156e-01, -1.8164e-01, -1.0234e+00, -7.1484e-01],
        [-6.0059e-02,  8.3594e-01, -8.0566e-02, -5.2344e-01, -9.2188e-01],
        [-1.1865e-01,  6.0938e-01, -4.0234e-01, -5.3906e-01, -1.0859e+00],
        [-1.9043e-02,  9.5312e-01, -1.6113e-01, -9.2969e-01, -8.3984e-01],
        [-2.8906e-01,  1.2422e+00,  1.8359e-01, -6.5625e-01, -6.4062e-01],
        [-1.5039e-01,  5.7031e-01,  9.7168e-02, -5.5859e-01, -9.8438e-01],
        [-1.4941e-01,  6.7969e-01, -1.7285e-01, -4.7070e-01, -1.1562e+00],
        [ 1.3867e-01,  1.0938e+00, -2.7148e-01, -9.7266e-01, -5.7031e-01],
        [-5.1270e-02,  7.3438e-01, -3.7109e-01, -9.8047e-01, -4.8438e-01],
        [-3.5352e-01,  1.0156e+00,  2.1362e-02, -7.1484e-01, -3.7109e-01],
        [ 2.7539e-01,  1.3125e+00,  1.7676e-01, -4.8047e-01, -3.3789e-01],
        [-5.3467e-02,  1.1875e+00, -1.7871e-01, -7.8906e-01, -5.6250e-01],
        [-1.2695e-01,  7.6562e-01, -1.3477e-01, -4.0820e-01, -7.3828e-01],
        [ 1.1139e-03,  1.2109e+00, -3.2812e-01, -9.0625e-01, -7.2656e-01],
        [-3.0273e-01,  1.0469e+00,  1.1816e-01, -9.6875e-01, -7.0312e-01],
        [-1.8799e-02,  5.0781e-01, -3.7891e-01, -8.3203e-01, -2.6953e-01]],
       device='cuda:0', dtype=torch.bfloat16, grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)
        [ 0.1592,  0.8398, -0.6133, -0.6875, -0.5391],2e-01, -2.8906e-01],, attentions=None)
        [-0.0069,  0.8750,  0.0923, -0.5859, -0.6680],2e-01, -2.8906e-01],, attentions=None)
        [ 0.1250,  0.8242, -0.5898, -0.5352, -0.3086]], device='cuda:0',
       dtype=torch.bfloat16, grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)
out(train): SequenceClassifierOutput(loss=tensor(0.7368, device='cuda:0', grad_fn=<NllLossBackward0>), logits=tensor([[ 0.1216,  0.7500, -0.2217, -0.5859, -0.6172],
        [ 0.0376,  1.0781, -0.1001, -0.6797, -0.9336],
        [-0.1250,  0.7500, -0.1523, -0.7891, -0.4453],
        [-0.0454,  1.0859, -0.4043, -0.6797, -0.9727],
        [ 0.0491,  0.9766, -0.2969, -0.6875, -0.4316],
        [ 0.1709,  1.0625, -0.2910, -0.9648, -0.7812],
        [ 0.1641,  1.3359, -0.1797, -0.4277, -0.3555],
        [-0.1729,  1.0234, -0.2871, -0.7383, -0.3848],
        [-0.2559,  1.2109, -0.2793, -0.9492, -0.8945],
        [-0.0135,  1.2969,  0.0583, -0.7617, -0.1157],
        [ 0.1787,  0.9922, -0.5352, -0.4473, -0.5234],
        [-0.3008,  1.2891, -0.3809, -0.8438, -0.4941],
        [ 0.2197,  1.2422, -0.3652, -0.7344, -0.4863],
        [-0.1138,  1.3125, -0.2598, -0.8164, -0.4941],
        [-0.0579,  0.7070, -0.4043, -0.4453, -0.8047],
        [ 0.0510,  1.0234, -0.3262, -0.5586, -0.3906]], device='cuda:0',
       dtype=torch.bfloat16, grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)
out(train): SequenceClassifierOutput(loss=tensor(0.8105, device='cuda:0', grad_fn=<NllLossBackward0>), logits=tensor([[ 0.1836,  1.1641, -0.6641, -0.6250, -0.5625],
        [ 0.2695,  0.8125, -0.5078, -0.9453, -0.3984],
        [-0.3301,  1.0078, -0.6172, -0.4199, -0.5664],
        [-0.0564,  1.0312, -0.3086, -0.0732, -0.4160],
        [-0.2363,  1.1484, -0.0645, -0.4238, -0.4922],
        [-0.1416,  1.0625, -0.6992, -0.4531, -0.3770],
        [ 0.0139,  1.0312, -0.0474, -0.6211, -0.7539],
        [ 0.1162,  1.2344, -0.1484, -0.6133, -0.3730],
        [-0.0747,  0.9375, -0.5664, -0.3848, -0.3281],
        [-0.0903,  0.9531, -0.4297, -0.5820, -0.3828],
        [-0.0125,  1.0156, -0.3496, -0.6367, -0.6875],
        [ 0.1846,  1.4766, -0.4785, -0.6523, -0.6680],
        [-0.3438,  1.2656, -0.5391, -0.6250, -0.5195],
        [ 0.0312,  0.9336, -0.5234, -0.5430, -0.3730],
        [ 0.3945,  0.9570, -0.7695, -0.8281, -0.7070],
        [ 0.1377,  1.1406, -0.4668, -0.6406, -0.5938]], device='cuda:0',
       dtype=torch.bfloat16, grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)
        [-0.0069,  0.8750,  0.0923, -0.5859, -0.6680],2e-01, -2.8906e-01],, attentions=None)
        [-0.0811,  1.2969, -0.7500, -0.6367, -0.3359],2e-01, -2.8906e-01],, attentions=None)
        [-0.0184,  0.8828, -0.3105, -0.7695, -0.4785]], device='cuda:0',
       dtype=torch.bfloat16, grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)
out(train): SequenceClassifierOutput(loss=tensor(0.8423, device='cuda:0', grad_fn=<NllLossBackward0>), logits=tensor([[-0.0679,  0.9766, -0.5547, -0.2432, -0.7969],
        [-0.1611,  1.2188, -0.1270, -0.7148, -0.5859],
        [ 0.0503,  1.2188, -0.2832, -0.6484, -0.6094],
        [ 0.2363,  1.1797, -0.4844, -0.5469, -0.3516],
        [ 0.0215,  1.2422, -0.1650, -0.7461, -0.4902],
        [ 0.2891,  1.0156, -0.4141, -0.5000, -0.4453],
        [-0.1992,  1.1406, -0.4805, -0.5625, -0.6641],
        [-0.0425,  1.0859, -0.7344, -0.8047, -0.6484],
        [-0.0142,  1.2188, -0.3203, -0.4160, -0.2891],
        [ 0.1357,  0.8281,  0.1504, -0.8359, -0.5430],
        [ 0.2139,  1.2422, -0.3770, -0.7266, -0.1064],
        [-0.0439,  0.8633, -0.1338, -0.4824, -0.2178],
        [ 0.0718,  1.4688, -0.1748, -0.1309, -0.4180],
        [ 0.0645,  1.0391, -0.4707, -0.6797, -0.2949],
        [-0.1040,  1.1484, -0.2559, -0.5547, -0.4883],
        [ 0.0349,  1.0625, -0.6055, -0.5703, -0.2598]], device='cuda:0',
       dtype=torch.bfloat16, grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)
out(train): SequenceClassifierOutput(loss=tensor(0.7534, device='cuda:0', grad_fn=<NllLossBackward0>), logits=tensor([[-0.2207,  1.1328, -0.4844, -0.6992, -0.2949],
        [ 0.0136,  1.1328, -0.1895, -0.6055, -1.3906],
        [ 0.4141,  1.5156, -0.4277, -0.8320, -0.2617],
        [ 0.1641,  0.9922,  0.1143, -0.7500, -0.7227],
        [ 0.0454,  0.9492, -0.5469, -0.3047, -0.3789],
        [-0.1455,  1.0312, -0.0557, -0.5664, -0.4512],
        [ 0.0532,  1.3125, -0.1504, -0.7812, -0.4062],
        [-0.3418,  0.9883,  0.0481, -0.9648, -0.8164],
        [ 0.1260,  1.0703, -0.3398, -0.8594, -0.3652],
        [ 0.1104,  1.2891, -0.3672, -0.5977, -0.3262],
        [-0.0659,  1.1406, -0.1250, -0.6914, -0.5117],
        [-0.0674,  1.4453, -0.3926, -0.3633, -0.7461],
        [-0.0255,  1.1094, -0.4121, -0.5664, -0.5078],
        [ 0.1875,  1.1406,  0.0752, -0.7773, -0.5430],
        [ 0.1455,  1.2344, -0.0781, -0.7734, -0.5859],
        [ 0.0850,  0.6992, -0.3867, -0.8398, -0.5625]], device='cuda:0',
       dtype=torch.bfloat16, grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)
out(train): SequenceClassifierOutput(loss=tensor(0.7787, device='cuda:0', grad_fn=<NllLossBackward0>), logits=tensor([[-0.1328,  1.1953, -0.5508, -0.3574, -0.3398],
        [-0.1064,  1.0625, -0.5039, -0.7969, -0.8359],
        [ 0.0149,  1.4297, -0.5312, -0.7891, -0.4551],
        [-0.4297,  0.5898, -0.5664, -0.3906, -0.5703],
        [ 0.0337,  1.3438, -0.3203, -0.9609, -0.7109],
        [-0.0884,  1.0234, -0.5195, -0.8477, -0.4277],
        [ 0.1133,  1.4141, -0.1582, -0.8359, -0.1455],
        [-0.0596,  1.0312, -0.1807, -0.6055, -0.4180],
        [-0.0192,  0.9219, -0.4727, -0.6367, -0.5703],
        [-0.1328,  0.9609,  0.0121, -0.6758, -0.7305],
        [-0.0173,  0.9883,  0.0172, -0.4297, -0.7812],
        [-0.3711,  0.9492, -0.4043, -0.5508, -0.4648],
        [ 0.5703,  0.4727, -1.1328, -0.7578,  0.2305],
        [ 0.0466,  1.0234, -0.0398, -0.3828, -0.8555],
        [-0.2061,  1.3047, -0.3574, -0.4004, -0.5352],
        [ 0.0103,  1.0547, -0.0864, -0.9141, -0.7305]], device='cuda:0',
       dtype=torch.bfloat16, grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)
out(train): SequenceClassifierOutput(loss=tensor(0.6917, device='cuda:0', grad_fn=<NllLossBackward0>), logits=tensor([[ 0.1074,  1.3828, -0.3789, -0.8945, -0.4219],
        [ 0.0060,  1.1172, -0.3008, -0.1357, -0.6719],
        [-0.0035,  1.1328, -0.5938, -0.8398, -0.1523],
        [ 0.1270,  1.2734, -0.5586, -0.9062, -0.4336],
        [-0.0023,  1.4141, -0.3242, -0.6016, -0.7109],
        [-0.0046,  1.0234, -0.2988, -0.7773, -0.5273],
        [ 0.1338,  0.8867, -0.4902, -0.7969, -0.5820],
        [-0.2773,  1.6562, -0.1797, -0.6328, -0.4023],
        [-0.2051,  0.7344, -0.5469, -0.4668, -0.3613],
        [ 0.1553,  1.0078, -0.5898, -0.6211, -0.4023],
        [-0.3086,  1.1797, -0.4473, -0.7969, -0.6055],
        [ 0.1045,  1.2734, -0.3750, -0.7383, -0.4238],
        [-0.0025,  1.8281, -0.3594, -0.8203, -0.0791],
        [ 0.0747,  1.0703, -0.3730, -0.8086, -0.5195],
        [-0.0123,  1.3984, -0.6875, -0.7656, -0.3926],
        [-0.0498,  0.8672, -0.6367, -0.5312, -0.3809]], device='cuda:0',
       dtype=torch.bfloat16, grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)
        [-0.0811,  1.2969, -0.7500, -0.6367, -0.3359],2e-01, -2.8906e-01],, attentions=None)
        [-0.1318,  1.5703, -0.7461, -0.8359, -0.1846],2e-01, -2.8906e-01],, attentions=None)
        [-0.0962,  0.9688, -0.2520, -0.6797, -0.7070]], device='cuda:0',
       dtype=torch.bfloat16, grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)
out(train): SequenceClassifierOutput(loss=tensor(0.9276, device='cuda:0', grad_fn=<NllLossBackward0>), logits=tensor([[-0.3125,  1.2812, -0.2324, -0.5039, -0.4375],
        [ 0.2539,  1.0234, -0.5586, -0.6641, -0.4922],
        [-0.0369,  1.2031, -0.0251, -0.9062, -0.5430],
        [ 0.0400,  1.0859, -0.3477, -0.7031, -0.5078],
        [-0.0981,  1.3281, -0.2598, -0.5391, -0.3652],
        [ 0.0347,  0.9609, -0.6641, -0.7578, -0.4668],
        [ 0.0679,  1.6172, -0.4355, -0.8828, -0.1543],
        [-0.1865,  1.2656, -0.3574, -0.4238, -0.4434],
        [-0.4414,  1.1562, -0.2344, -0.7930, -0.3535],
        [ 0.0537,  1.5156, -0.4023, -0.9492, -0.5664],
        [ 0.1133,  1.4609, -0.3125, -0.7695, -0.2988],
        [-0.1089,  1.0469, -0.6836, -0.8789, -0.6133],
        [-0.3516,  0.8789,  0.2148, -0.5234, -0.3965],
        [-0.1309,  0.7109, -0.3496, -0.8633, -0.6172],
        [ 0.3398,  0.9453, -0.7969, -0.6602, -0.0791],
        [-0.0063,  0.9414, -0.2793, -0.6562, -0.2695]], device='cuda:0',
       dtype=torch.bfloat16, grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)
out(train): SequenceClassifierOutput(loss=tensor(0.7335, device='cuda:0', grad_fn=<NllLossBackward0>), logits=tensor([[ 0.3457,  1.4609, -0.5273, -0.8320, -0.3457],
        [-0.4199,  1.0469, -0.5938, -0.7305, -0.5391],
        [ 0.0288,  0.8203, -0.0117, -0.5117, -0.7227],
        [-0.0359,  1.3438, -0.5781, -0.9609, -0.6211],
        [ 0.6719,  0.1270, -0.6992, -0.0393,  0.5703],
        [-0.6016,  1.0234, -0.3730, -0.6367, -0.4375],
        [ 0.2061,  1.3359, -0.5859, -0.7930, -0.3770],
        [-0.1826,  0.9023, -0.1138, -0.7109, -0.5312],
        [ 0.0830,  1.3516, -0.0354, -0.5156, -0.7305],
        [-0.0535,  1.3438, -0.3340, -0.5352, -0.3535],
        [-0.3984,  0.9023, -0.0947, -0.6562, -1.1484],
        [-0.0317,  1.0625, -0.6055, -0.5938, -0.5234],
        [ 0.0145,  1.0938, -0.3301, -0.5703, -0.3398],
        [-0.0132,  1.1719, -0.2217, -1.0547, -0.3906],
        [ 0.0491,  1.2031, -0.9258, -0.8086, -0.3418],
        [-0.1221,  1.2500, -0.4453, -0.7148, -0.4395]], device='cuda:0',
       dtype=torch.bfloat16, grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)
out(train): SequenceClassifierOutput(loss=tensor(0.8796, device='cuda:0', grad_fn=<NllLossBackward0>), logits=tensor([[ 0.0347,  1.2500, -0.7109, -0.8867, -0.7461],
        [-0.0045,  1.3047, -0.2891, -0.7773, -0.5664],
        [-0.0762,  0.9766, -0.0854, -0.6758, -0.6250],
        [-0.0347,  0.8164, -0.1875, -1.0234, -0.9375],
        [ 0.0239,  1.5000, -0.2314, -0.9219, -0.4941],
        [-0.0347,  1.2188, -0.4141, -0.2031, -0.6445],
        [ 0.0143,  1.2656, -0.1973, -0.6367, -0.1060],
        [ 0.1611,  0.9844, -0.6602, -0.6172, -0.5508],
        [ 0.2266,  1.1562, -0.5469, -0.4863, -0.5508],
        [ 0.1211,  0.7305, -0.3359, -0.9688, -0.6055],
        [ 0.0457,  1.1953, -0.3320, -0.8047, -0.3945],
        [ 0.0762,  1.2422, -0.1748, -0.9414, -0.4277],
        [ 0.2051,  1.5312, -0.2969, -0.6016, -0.2012],
        [ 0.0073,  1.2500, -0.5352, -0.4512, -0.2158],
        [ 0.2188,  1.3438, -0.1660, -0.2373, -0.6094],
        [-0.2559,  1.2969, -0.7422, -0.6328, -0.4531]], device='cuda:0',
       dtype=torch.bfloat16, grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)
        [-0.1318,  1.5703, -0.7461, -0.8359, -0.1846],2e-01, -2.8906e-01],, attentions=None)
        [-0.3945,  1.2500, -0.2305, -0.7031, -0.3340],2e-01, -2.8906e-01],, attentions=None)
        [-0.1523,  1.2734, -0.3418, -0.7578, -0.6250]], device='cuda:0',
       dtype=torch.bfloat16, grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)
out(train): SequenceClassifierOutput(loss=tensor(0.9268, device='cuda:0', grad_fn=<NllLossBackward0>), logits=tensor([[ 7.5684e-02,  6.6016e-01, -6.9922e-01, -6.2109e-01, -2.2852e-01],
        [-1.8652e-01,  1.0703e+00, -1.2451e-01, -7.1484e-01, -4.1992e-01],
        [ 3.0664e-01,  1.3359e+00, -6.1328e-01, -6.6797e-01, -7.8516e-01],
        [-1.7090e-01,  1.2109e+00, -3.3203e-01, -7.7734e-01, -6.2500e-01],
        [-4.2725e-02,  1.0938e+00, -3.4961e-01, -8.3984e-01, -6.5625e-01],
        [-1.1133e-01,  1.0234e+00, -4.2773e-01, -3.5938e-01, -5.5469e-01],
        [ 2.9883e-01,  1.2656e+00, -4.0039e-01, -5.4297e-01, -8.9062e-01],
        [-1.2085e-02,  1.1406e+00, -4.6484e-01, -1.0391e+00, -3.0469e-01],
        [ 2.6894e-04,  1.0469e+00, -4.5117e-01, -6.3281e-01, -4.5703e-01],
        [-1.0303e-01,  8.5938e-01, -1.2500e-01, -8.5547e-01, -7.2656e-01],
        [-2.3633e-01,  1.2656e+00, -2.8320e-01, -5.6250e-01, -5.1562e-01],
        [-1.3477e-01,  1.2656e+00, -7.1484e-01, -6.3672e-01, -6.2109e-01],
        [-3.5938e-01,  1.4297e+00, -4.6289e-01, -5.3516e-01, -7.3047e-01],
        [-2.6172e-01,  1.0703e+00, -1.7871e-01, -1.0156e+00, -6.7969e-01],
        [ 1.0889e-01,  1.2188e+00, -2.4023e-01, -7.7734e-01, -3.3789e-01],
        [ 1.4844e-01,  9.4141e-01, -3.0273e-01, -7.9688e-01, -7.5000e-01]],
       device='cuda:0', dtype=torch.bfloat16, grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)
        [-0.3945,  1.2500, -0.2305, -0.7031, -0.3340],2e-01, -2.8906e-01],, attentions=None)
        [ 0.0488,  1.7109, -0.2754, -0.6445, -0.5234],2e-01, -2.8906e-01],, attentions=None)
        [-0.2949,  0.9883, -0.4043, -0.4453, -0.7969]], device='cuda:0',
       dtype=torch.bfloat16, grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)
out(train): SequenceClassifierOutput(loss=tensor(0.7490, device='cuda:0', grad_fn=<NllLossBackward0>), logits=tensor([[ 0.2158,  1.1172, -0.4707, -0.7617, -0.4258],
        [-0.1982,  1.2891, -0.7227, -0.8047, -0.5898],
        [-0.0535,  1.2266, -0.3809, -0.9062, -0.6016],
        [ 0.1777,  1.1328, -0.5742, -0.7148, -0.6406],
        [-0.0115,  1.1406, -0.1787, -0.8594, -0.6445],
        [-0.3535,  0.9414, -0.2715, -0.3242, -0.4570],
        [ 0.5938,  1.5156, -0.2441, -0.7773, -0.3906],
        [-0.0938,  1.2266, -0.0459, -0.8164, -0.5898],
        [-0.0928,  1.0859, -0.4688, -0.5508, -0.3945],
        [ 0.1455,  1.4062, -0.5195, -0.7070, -0.7461],
        [ 0.0437,  1.0781, -0.4375, -0.8945, -0.6406],
        [ 0.3945,  0.9883, -0.0132, -0.6133, -0.4238],
        [ 0.1426,  1.3516, -0.1729, -0.5078, -0.6289],
        [-0.1641,  1.0391, -0.5234, -0.2988, -0.2432],
        [ 0.3262,  1.2734, -0.6406, -0.6875, -0.6680],
        [ 0.5156,  0.2119, -0.9414, -0.3398,  0.3887]], device='cuda:0',
       dtype=torch.bfloat16, grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)
out(train): SequenceClassifierOutput(loss=tensor(0.7830, device='cuda:0', grad_fn=<NllLossBackward0>), logits=tensor([[ 9.8633e-02,  1.4531e+00, -5.6641e-01, -1.0000e+00, -4.2188e-01],
        [ 7.5073e-03,  1.1328e+00, -2.6367e-01, -8.7109e-01, -2.7344e-01],
        [ 1.5527e-01,  1.3984e+00, -2.6758e-01, -8.3984e-01, -3.6523e-01],
        [ 1.2207e-01,  1.1406e+00, -5.8203e-01, -8.1250e-01, -5.6641e-01],
        [ 1.0107e-01,  9.6484e-01, -3.8086e-01, -3.9648e-01, -3.6719e-01],
        [ 1.1816e-01,  1.0156e+00, -7.6562e-01, -7.6953e-01, -6.0156e-01],
        [-7.9590e-02,  9.6094e-01, -3.7109e-01, -5.5078e-01, -2.6562e-01],
        [-2.1875e-01,  1.3750e+00, -5.1562e-01, -5.6641e-01, -2.4219e-01],
        [-2.5940e-03,  1.1562e+00, -4.4922e-01, -7.5781e-01, -4.7852e-01],
        [-5.3223e-02,  1.2344e+00, -7.4609e-01, -7.2266e-01, -4.5312e-01],
        [ 1.3657e-03,  1.1719e+00, -2.0312e-01, -8.3594e-01, -5.8984e-01],
        [ 7.7515e-03,  1.3047e+00, -4.9805e-01, -7.3438e-01, -3.9648e-01],
        [ 9.6680e-02,  1.1875e+00, -5.1953e-01, -6.1328e-01, -3.7305e-01],
        [-2.8711e-01,  8.5156e-01, -2.0703e-01, -6.8359e-01, -6.7969e-01],
        [ 7.9102e-02,  1.4219e+00, -5.7422e-01, -1.0234e+00, -8.0469e-01],
        [-2.1875e-01,  1.1719e+00, -4.3164e-01, -7.0703e-01, -3.9844e-01]],
       device='cuda:0', dtype=torch.bfloat16, grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)
out(train): SequenceClassifierOutput(loss=tensor(0.7859, device='cuda:0', grad_fn=<NllLossBackward0>), logits=tensor([[-0.0625,  1.2422, -0.1992, -0.5156, -0.3828],
        [ 0.3184,  1.2031, -0.2656, -0.5938, -0.5508],
        [ 0.1309,  1.5703, -0.4824, -0.7852, -0.4473],
        [-0.1680,  1.0938, -0.2832, -0.9492, -0.6719],
        [-0.0132,  1.3125, -0.5039, -0.6992, -0.4668],
        [-0.0908,  1.3438, -0.5156, -0.6250, -0.3379],
        [-0.1787,  1.6641, -0.4531, -0.9531, -0.4062],
        [ 0.0317,  1.4922, -0.6016, -0.6758, -0.4297],
        [-0.1035,  0.8828, -0.4531, -0.6133, -0.0098],
        [-0.1846,  1.2109, -0.3516, -0.7344, -0.5039],
        [-0.0664,  1.2969, -0.4453, -0.8242, -0.4473],
        [-0.2637,  0.9961, -0.4980, -0.5898, -0.4258],
        [-0.0503,  1.2891, -0.5977, -0.7344, -0.6250],
        [ 0.0869,  1.1250, -0.3672, -0.9531, -0.6016],
        [-0.3301,  1.0234, -0.3340, -1.1562, -0.6016],
        [ 0.3027,  1.3359, -0.5312, -0.8359, -0.2656]], device='cuda:0',
       dtype=torch.bfloat16, grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)
out(train): SequenceClassifierOutput(loss=tensor(1.0511, device='cuda:0', grad_fn=<NllLossBackward0>), logits=tensor([[-0.0432,  1.1172, -0.5508, -1.2344, -0.3223],
        [ 0.0840,  0.9922, -0.4531, -0.8125, -0.6172],
        [-0.1182,  1.2344, -0.0630, -0.9453, -0.6250],
        [ 0.0515,  1.0703, -0.1670, -0.6133, -0.3574],
        [ 0.2773,  1.2031, -0.6133, -0.8867, -0.3477],
        [-0.2402,  1.3594, -0.4473, -0.5469, -0.8477],
        [-0.1562,  1.4922, -0.4453, -0.9297, -0.6680],
        [-0.0327,  1.0547, -0.3770, -0.5547, -0.5234],
        [ 0.0762,  1.4453, -0.1699, -0.4707, -0.3301],
        [ 0.3281,  1.0312, -0.3945, -0.6484, -0.5938],
        [ 0.1953,  1.0781, -0.5859, -0.7852, -0.5039],
        [ 0.1504,  0.8320, -0.2168, -0.7266, -0.3047],
        [-0.2207,  1.1328, -0.2139, -0.5039, -0.3574],
        [-0.1245,  1.0469, -0.4902, -1.0000, -0.6094],
        [ 0.0228,  1.0234, -0.4219, -0.6289, -0.5078],
        [-0.3203,  1.3203, -0.5508, -0.2500, -0.4570]], device='cuda:0',
       dtype=torch.bfloat16, grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)
        [ 0.0488,  1.7109, -0.2754, -0.6445, -0.5234],2e-01, -2.8906e-01],, attentions=None)
        [ 0.2070,  0.9219, -0.6367, -0.5508, -0.5703],2e-01, -2.8906e-01],, attentions=None)
        [ 0.3379,  0.9102, -0.4746, -0.6094, -0.6367]], device='cuda:0',
       dtype=torch.bfloat16, grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)
out(train): SequenceClassifierOutput(loss=tensor(0.9847, device='cuda:0', grad_fn=<NllLossBackward0>), logits=tensor([[ 0.2793,  1.2109, -0.4863, -0.5859, -0.3828],
        [ 0.1074,  1.3281, -0.5898, -0.8789, -0.6055],
        [ 0.2676,  1.0078, -0.4727, -0.8281, -0.3242],
        [ 0.2334,  0.9883, -0.5156, -0.7656, -0.9609],
        [-0.5898,  1.4141, -0.3281, -0.2578, -0.3379],
        [ 0.1167,  0.6953, -0.4102, -0.4258, -0.3145],
        [-0.1777,  1.4766, -0.2461, -0.6953, -0.7305],
        [-0.1221,  1.3984, -0.4492, -0.8906, -0.4629],
        [-0.2295,  1.2500, -0.3926, -0.6055, -0.5000],
        [-0.4336,  0.8398, -0.3594, -0.5742, -0.8594],
        [ 0.0623,  1.2969, -0.4297, -0.8945, -0.4062],
        [ 0.2598,  1.3750, -0.3008, -0.7461, -0.4922],
        [ 0.0201,  1.2969, -0.5312, -0.7266, -0.5742],
        [-0.1533,  1.2422, -0.1299, -0.7969, -0.4805],
        [-0.0447,  1.5781, -0.3184, -0.8438, -0.3398],
        [-0.2012,  1.3359, -0.3066, -0.6289, -0.8086]], device='cuda:0',
       dtype=torch.bfloat16, grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)
out(train): SequenceClassifierOutput(loss=tensor(0.6118, device='cuda:0', grad_fn=<NllLossBackward0>), logits=tensor([[-0.0193,  0.7305, -0.3125, -0.8047, -0.7656],
        [-0.1406,  1.2812, -0.6562, -0.6836, -0.5742],
        [ 0.2256,  1.4297, -0.4883, -0.7500, -0.6953],
        [-0.2246,  1.3828, -0.2969, -0.5977, -0.5273],
        [-0.1211,  1.1953, -0.0291, -0.4531, -1.0391],
        [-0.2061,  1.4688, -0.7461, -0.9766, -0.6289],
        [-0.3379,  1.0938, -0.4746, -0.6172, -0.1729],
        [-0.0129,  1.0234, -0.5898, -0.7773, -0.5586],
        [-0.0928,  1.1953, -0.3301, -0.7266, -0.3340],
        [-0.1030,  1.2266, -0.5312, -0.8516, -0.3223],
        [ 0.1543,  1.5938, -0.6289, -0.7109, -0.3711],
        [-0.1943,  1.3594,  0.0027, -0.6484, -0.7539],
        [-0.1748,  1.2969, -0.5547, -0.6289, -0.4551],
        [ 0.2080,  1.3984, -0.6953, -0.8047, -0.6406],
        [-0.3203,  1.4062, -0.7031, -0.6602, -0.6406],
        [ 0.0303,  1.1875, -0.3906, -0.6641, -0.3066]], device='cuda:0',
       dtype=torch.bfloat16, grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)
out(train): SequenceClassifierOutput(loss=tensor(0.6415, device='cuda:0', grad_fn=<NllLossBackward0>), logits=tensor([[-0.1807,  1.1797, -0.5820, -0.5391, -0.0918],
        [ 0.0262,  1.3672, -0.1035, -0.8086, -0.5078],
        [-0.1523,  1.0703, -0.6133, -0.6680, -0.4492],
        [ 0.1260,  1.4375, -0.4219, -0.9648, -0.5156],
        [-0.0649,  1.4609, -0.3516, -0.8438, -0.7109],
        [ 0.0469,  1.5312, -0.3789, -0.6562, -0.3262],
        [ 0.1182,  1.1641, -0.4355, -0.4629, -0.6016],
        [ 0.0107,  1.0625, -0.4668, -1.0469, -0.9023],
        [ 0.0801,  1.0625, -0.4980, -0.6016, -0.4453],
        [-0.2520,  1.4219, -0.3027, -1.0938, -0.5430],
        [-0.1670,  1.4375, -0.2480, -0.5430, -0.5039],
        [-0.6641,  1.1797, -0.3105, -0.5430, -0.6758],
        [-0.1504,  1.1562, -0.3008, -0.6094, -0.4629],
        [ 0.2793,  1.4453, -0.4453, -0.4863, -0.6055],
        [ 0.0557,  1.2188, -0.4922, -0.5898, -0.6836],
        [ 0.0737,  0.9805, -0.3770, -0.9180, -0.7734]], device='cuda:0',
       dtype=torch.bfloat16, grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)
        [ 0.2070,  0.9219, -0.6367, -0.5508, -0.5703],2e-01, -2.8906e-01],, attentions=None)
        [ 0.3223,  1.3984, -0.6953, -0.8281, -0.4102],2e-01, -2.8906e-01],, attentions=None)
        [ 0.2275,  1.2969, -0.0386, -0.8984, -0.7383]], device='cuda:0',
       dtype=torch.bfloat16, grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)
out(train): SequenceClassifierOutput(loss=tensor(0.7623, device='cuda:0', grad_fn=<NllLossBackward0>), logits=tensor([[-0.2178,  1.1328, -0.3418, -0.6094, -0.3047],
        [ 0.0859,  1.4062, -0.4453, -0.8203, -0.5273],
        [-0.0396,  1.0938, -0.6172, -1.0938, -0.3867],
        [-0.0608,  1.1172, -0.5820, -0.6680, -0.7227],
        [ 0.1602,  1.1562, -0.4023, -0.5195, -0.4883],
        [ 0.1982,  1.2578, -0.5234, -0.8594, -0.5195],
        [-0.2295,  1.1562, -0.3945, -0.6016, -0.7227],
        [ 0.0986,  1.6172, -0.6875, -0.7461, -0.6016],
        [-0.3359,  1.4141,  0.0645, -1.1094, -0.5859],
        [-0.0889,  1.2891, -0.2422, -0.4180, -0.5391],
        [ 0.1040,  1.4219, -0.2061, -0.7930, -0.3730],
        [ 0.3164,  1.3047, -0.3750, -0.6445, -0.6133],
        [ 0.0265,  1.2422, -0.6250, -0.7383, -0.5234],
        [ 0.3848,  1.2422, -0.0947, -0.7617, -0.7969],
        [-0.1582,  1.6719, -0.5547, -0.8047, -0.4941],
        [-0.0771,  1.3672, -0.4570, -0.8828, -0.3652]], device='cuda:0',
       dtype=torch.bfloat16, grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)
out(train): SequenceClassifierOutput(loss=tensor(0.9772, device='cuda:0', grad_fn=<NllLossBackward0>), logits=tensor([[-0.1748,  1.1641, -0.4258, -0.6680, -0.4941],
        [-0.0306,  0.6953, -0.6680, -0.8672, -0.7891],
        [ 0.4141,  1.5703, -0.3965, -0.7695, -0.5000],
        [-0.0304,  1.2812, -0.5859, -0.6953, -0.3848],
        [-0.1826,  0.9883, -0.3711, -0.4551, -0.3047],
        [ 0.0908,  1.0547,  0.0562, -0.7891, -0.9180],
        [ 0.2676,  1.2031, -0.4141, -0.9609, -0.3984],
        [-0.0840,  1.2266, -0.3125, -0.6836, -0.4141],
        [-0.0708,  1.3359, -0.1553, -0.8164, -0.3594],
        [ 0.1416,  1.2891, -0.4355, -0.6875, -0.7188],
        [-0.0854,  1.2969, -0.3457, -0.5312, -0.3047],
        [-0.2891,  1.3594, -0.3047, -0.9609, -0.6562],
        [ 0.4219,  1.2422, -0.4727, -0.6680, -0.6133],
        [ 0.0540,  1.3125, -0.3066, -0.5469, -0.6680],
        [ 0.4453,  0.7422, -0.8711, -0.7461,  0.4746],
        [-0.2012,  0.8984, -0.6602, -0.9492, -0.6641]], device='cuda:0',
       dtype=torch.bfloat16, grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)
out(train): SequenceClassifierOutput(loss=tensor(0.6520, device='cuda:0', grad_fn=<NllLossBackward0>), logits=tensor([[ 0.0099,  1.3906, -0.4707, -0.8047, -0.5469],
        [ 0.1182,  1.3125, -0.0400, -0.8242, -0.6016],
        [ 0.1245,  1.2031, -0.5117, -1.1250, -0.4453],
        [ 0.1963,  1.5938, -0.5508, -0.8867, -0.3262],
        [ 0.2617,  1.4375, -0.6914, -1.2031, -0.6914],
        [ 0.2393,  1.1562, -0.5117, -0.8906, -0.5977],
        [-0.0811,  1.5156, -0.3906, -0.6992, -0.4707],
        [ 0.1807,  1.3516, -0.4512, -0.7188, -0.9414],
        [ 0.0569,  1.3281, -0.2041, -0.8477, -0.6367],
        [-0.0087,  1.3672, -0.6094, -0.8672, -0.3535],
        [ 0.2578,  1.5859, -0.5703, -0.7891, -0.4512],
        [ 0.0312,  1.2734, -0.3496, -1.0312, -0.6719],
        [ 0.2695,  0.8750, -0.8438, -0.7969, -0.9844],
        [ 0.0898,  1.2891, -0.2559, -0.8359, -0.7578],
        [ 0.1465,  1.6094, -0.5508, -0.9141, -0.5547],
        [ 0.0596,  1.4531, -0.4668, -0.6523, -0.3867]], device='cuda:0',
       dtype=torch.bfloat16, grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)
out(train): SequenceClassifierOutput(loss=tensor(0.8094, device='cuda:0', grad_fn=<NllLossBackward0>), logits=tensor([[ 0.1758,  1.5312, -0.4805, -0.7383, -0.5898],
        [-0.0713,  1.5312, -0.1953, -1.2578, -0.7070],
        [ 0.1279,  1.2266, -0.3477, -0.6875, -0.7734],
        [-0.1787,  1.4141, -0.3809, -0.6172, -0.4844],
        [ 0.0625,  1.1875, -0.2852, -0.7109, -0.2773],
        [-0.0786,  0.9141, -0.7188, -0.8945, -0.4727],
        [-0.0698,  1.3906, -0.5742, -0.7969, -0.3730],
        [ 0.2100,  1.1406, -0.1963, -0.7461, -0.9141],
        [-0.0027,  1.2344, -0.0679, -0.6992, -0.2969],
        [-0.0247,  1.3047, -0.1846, -0.3906, -0.5039],
        [ 0.3340,  1.3906, -0.3203, -0.9609, -0.8711],
        [-0.1138,  1.3125, -0.5508, -0.6172, -0.4336],
        [ 0.8008,  0.3379, -0.8984, -0.7734,  0.5938],
        [ 0.1621,  1.0625, -0.2637, -0.8555, -0.5469],
        [ 0.1357,  1.2656, -0.1338, -0.9023, -0.7930],
        [ 0.2373,  1.4453, -0.5469, -0.7070, -0.2539]], device='cuda:0',
       dtype=torch.bfloat16, grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)
out(train): SequenceClassifierOutput(loss=tensor(0.8595, device='cuda:0', grad_fn=<NllLossBackward0>), logits=tensor([[ 0.0061,  1.3359, -0.2695, -0.8047, -0.5703],
        [-0.3555,  1.4141, -0.2988, -0.8945, -0.4434],
        [-0.0820,  1.3984, -0.6211, -0.9570, -0.4883],
        [-0.0603,  1.3203, -0.2334, -1.1250, -0.3926],
        [ 0.0537,  1.2969, -0.7305, -0.7539, -0.3203],
        [ 0.2480,  1.4375, -0.4824, -0.6875, -0.3027],
        [ 0.0835,  0.9180, -0.4766, -0.6875, -0.1748],
        [ 0.1465,  1.3281, -0.7617, -0.7031, -0.6484],
        [-0.1436,  1.3281, -0.3574, -0.9570, -0.5586],
        [ 0.1641,  0.9648, -0.5117, -0.8086, -0.7461],
        [-0.0483,  1.4688, -0.5586, -0.9570, -0.3887],
        [-0.2832,  1.1484, -0.5234, -0.8750, -0.8477],
        [-0.0068,  1.2109, -0.2949, -0.8320, -0.4844],
        [-0.3262,  0.9023, -0.1309, -0.6914, -0.3730],
        [-0.3047,  1.2266, -0.6055, -0.6797, -0.5781],
        [ 0.0208,  1.3125, -0.3184, -0.6602, -0.4570]], device='cuda:0',
       dtype=torch.bfloat16, grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)
        [ 0.3223,  1.3984, -0.6953, -0.8281, -0.4102],2e-01, -2.8906e-01],, attentions=None)
        [ 0.1602,  1.2578, -0.4805, -0.9727, -0.3691],2e-01, -2.8906e-01],, attentions=None)
        [-0.0444,  1.3828, -0.3164, -0.8984, -0.6758]], device='cuda:0',
       dtype=torch.bfloat16, grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)
out(train): SequenceClassifierOutput(loss=tensor(0.8925, device='cuda:0', grad_fn=<NllLossBackward0>), logits=tensor([[-0.0430,  1.2031,  0.1494, -0.9414, -0.3984],
        [ 0.1455,  1.5547, -0.6875, -0.8633, -0.4727],
        [-0.2305,  1.4219, -0.4648, -0.7695, -0.5625],
        [-0.0820,  1.3594, -0.4785, -0.9961, -0.7930],
        [-0.0537,  1.5234, -0.3555, -1.1719, -0.5156],
        [ 0.3633,  1.0703, -0.1768, -0.3848, -0.4219],
        [ 0.0369,  1.5859, -0.2227, -0.8828, -0.6992],
        [ 0.1357,  1.0938, -0.6875, -0.6172, -0.2148],
        [-0.2090,  1.3047, -0.3887, -0.6797, -0.8438],
        [-0.0020,  1.1641, -0.3281, -0.7344, -0.5000],
        [ 0.0315,  1.3672, -0.3828, -0.7578, -0.5352],
        [-0.0593,  1.6562, -0.5273, -0.9492, -0.4824],
        [ 0.1367,  1.4531, -0.0061, -0.5469, -0.6133],
        [-0.1196,  1.2891, -0.4512, -0.8359, -0.6562],
        [-0.0771,  1.0312, -0.5703, -0.1426, -0.5977],
        [ 0.2949,  1.6094, -0.4316, -0.9922, -0.4082]], device='cuda:0',
       dtype=torch.bfloat16, grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)
out(train): SequenceClassifierOutput(loss=tensor(0.8652, device='cuda:0', grad_fn=<NllLossBackward0>), logits=tensor([[ 0.2715,  1.5078, -0.7383, -0.8438, -0.4531],
        [-0.1953,  1.3281, -0.2109, -0.8672, -0.5078],
        [ 0.2480,  1.5078, -0.4082, -0.6094, -0.5039],
        [-0.1211,  1.6094, -0.6016, -0.7461, -0.3711],
        [ 0.3379,  0.8281, -0.6680, -0.7422,  0.7852],
        [-0.1172,  1.5234, -0.7148, -0.9062, -0.6133],
        [ 0.0449,  1.2266, -0.5352, -0.6992, -0.5273],
        [ 0.0084,  1.3906, -0.3184, -0.9570, -0.5312],
        [-0.0635,  1.0000, -0.6680, -0.8438, -0.1914],
        [ 0.1689,  1.1016, -0.4258, -0.7734, -0.5977],
        [-0.0776,  1.4062, -0.4004, -0.9062, -0.6836],
        [ 0.2773,  1.4844, -0.6641, -1.0000, -0.3848],
        [-0.0415,  1.3281, -0.5391, -0.6875, -0.3066],
        [-0.1836,  1.0234, -0.7148, -0.8945, -0.6992],
        [-0.1387,  1.3203, -0.5078, -0.8203, -0.4219],
        [ 0.1377,  1.1641, -0.3027, -0.9844, -0.5430]], device='cuda:0',
       dtype=torch.bfloat16, grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)
out(train): SequenceClassifierOutput(loss=tensor(0.7874, device='cuda:0', grad_fn=<NllLossBackward0>), logits=tensor([[ 0.3262,  1.5000, -0.6172, -0.5391, -0.4219],
        [ 0.1660,  1.4766, -0.4141, -0.7461, -0.5234],
        [ 0.0039,  1.1953, -0.2275, -1.0391, -0.8047],
        [-0.0649,  1.3594, -0.2148, -0.8711, -0.6836],
        [-0.2676,  1.2578, -0.1006, -1.2031, -0.7461],
        [ 0.0265,  1.1875, -0.6016, -0.6289, -0.4941],
        [-0.2793,  1.2500, -0.3730, -0.5977, -0.5039]], device='cuda:0',
       dtype=torch.bfloat16, grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)
        [ 0.1602,  1.2578, -0.4805, -0.9727, -0.3691],2e-01, -2.8906e-01],, attentions=None)
W0629 11:52:52.467000 139012379337344 torch/_dynamo/convert_frame.py:824]     result = inner_convert(
W0629 11:52:52.467000 139012379337344 torch/_dynamo/convert_frame.py:824]     result = inner_convert(
W0629 11:52:52.467000 139012379337344 torch/_dynamo/convert_frame.py:824]     result = inner_convert(
W0629 11:52:52.467000 139012379337344 torch/_dynamo/convert_frame.py:824]     result = inner_convert(
W0629 11:52:52.467000 139012379337344 torch/_dynamo/convert_frame.py:824]     result = inner_convert(
W0629 11:52:52.467000 139012379337344 torch/_dynamo/convert_frame.py:824]     result = inner_convert(
W0629 11:52:52.467000 139012379337344 torch/_dynamo/convert_frame.py:824]     result = inner_convert(
        [ 0.0282,  1.1797, -0.4375, -0.7852, -0.3945],2e-01, -2.8906e-01],, attentions=None)
        [ 0.0165,  0.9141, -0.2773, -0.4375, -0.6641],
        [-0.0747,  0.9258, -0.3281, -0.7695, -0.7617],
        [ 0.1357,  1.1953, -0.4785, -0.6211, -0.9453]], device='cuda:0',
       dtype=torch.bfloat16, grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)
        [ 0.0282,  1.1797, -0.4375, -0.7852, -0.3945],2e-01, -2.8906e-01],, attentions=None)
        [ 0.0093,  1.2500, -0.5117, -1.1172, -0.7109],2e-01, -2.8906e-01],, attentions=None)
        [-0.0204,  1.0703, -0.3633, -0.8438, -0.6211]], device='cuda:0',
       dtype=torch.bfloat16, grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)
out(train): SequenceClassifierOutput(loss=tensor(0.8668, device='cuda:0', grad_fn=<NllLossBackward0>), logits=tensor([[-0.4961,  1.0312, -0.1650, -0.5391, -0.5312],
        [ 0.0620,  1.0156, -0.1494, -0.9297, -0.7656],
        [ 0.0320,  1.0938, -0.2139, -0.5703, -0.9180],
        [-0.1680,  1.0234, -0.4531, -0.5664, -0.7695],
        [ 0.0066,  1.3984, -0.2461, -0.7812, -0.6328],
        [-0.3789,  1.3203,  0.0186, -1.0156, -0.8086],
        [ 0.1777,  1.3047, -0.2246, -0.8086, -0.9062],
        [ 0.0167,  1.1719, -0.4453, -0.4727, -0.8906],
        [-0.1055,  1.3906, -0.3145, -0.8672, -0.5234],
        [-0.0630,  0.9336, -0.5977, -1.0703, -0.8711],
        [-0.0223,  1.1797, -0.3184, -0.6172, -0.3301],
        [-0.1875,  0.8945, -0.3574, -0.5664, -0.2178],
        [-0.3672,  1.0547, -0.0376, -0.7773, -0.8281],
        [-0.3340,  1.2188, -0.4980, -1.0234, -0.9336],
        [-0.1152,  0.9883, -0.4219, -0.8594, -0.4277],
        [ 0.0557,  1.5000, -0.3555, -0.7773, -0.8828]], device='cuda:0',
       dtype=torch.bfloat16, grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)
        [ 0.0093,  1.2500, -0.5117, -1.1172, -0.7109],2e-01, -2.8906e-01],, attentions=None)
        [ 0.3008,  1.3984, -0.9023, -0.8008, -0.6523],2e-01, -2.8906e-01],, attentions=None)
        [-0.0608,  1.1172, -0.3281, -0.9375, -0.6484]], device='cuda:0',
       dtype=torch.bfloat16, grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)
out(train): SequenceClassifierOutput(loss=tensor(0.5588, device='cuda:0', grad_fn=<NllLossBackward0>), logits=tensor([[-0.1729,  1.3203, -0.1523, -1.0781, -0.7891],
        [-0.3477,  1.2031, -0.0679, -0.6719, -0.4238],
        [-0.2061,  1.2422, -0.5820, -1.1484, -0.6289],
        [-0.2041,  1.5625, -0.2559, -0.9141, -0.2178],
        [-0.0272,  0.9453, -0.6797, -0.7500, -0.7500],
        [ 0.0361,  1.1719, -0.6680, -0.9297, -0.2451],
        [ 0.0596,  1.4531, -0.6758, -0.8320, -0.3438],
        [-0.1016,  1.1016, -0.3105, -0.7695, -0.6875],
        [ 0.1748,  0.7188, -0.8008, -1.0312, -0.2539],
        [-0.0688,  1.7109, -0.5039, -0.9023, -0.4941],
        [ 0.2275,  1.1875, -0.5547, -0.6680, -0.6992],
        [ 0.2148,  1.0625, -0.3906, -0.7500, -0.4805],
        [-0.1963,  1.2500, -0.6406, -0.6719, -0.5156],
        [-0.2305,  1.4375, -0.5898, -1.0234, -0.2793],
        [ 0.2393,  1.6562, -0.3457, -0.8164, -0.7695],
        [-0.0781,  1.5938, -0.4492, -0.4316, -0.3164]], device='cuda:0',
       dtype=torch.bfloat16, grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)
out(train): SequenceClassifierOutput(loss=tensor(0.9971, device='cuda:0', grad_fn=<NllLossBackward0>), logits=tensor([[ 0.0286,  1.1172, -0.3965, -0.4570, -1.0078],
        [-0.0549,  1.1094,  0.0240, -0.6562, -0.7070],
        [ 0.0889,  1.4688, -0.3379, -0.6758, -0.2090],
        [-0.0327,  1.0625, -0.1445, -0.8164, -0.8008],
        [-0.0344,  1.0859, -0.2812, -0.7227, -0.8398],
        [-0.0204,  1.1484, -0.1426, -0.7344, -0.5820],
        [ 0.0430,  1.0000, -0.4023, -0.9414, -0.7539],
        [-0.0369,  1.4688, -0.6836, -0.7773, -0.5859],
        [ 0.1147,  1.1641, -0.4336, -1.0547, -0.5039],
        [ 0.0693,  1.6016, -0.6094, -1.0234, -0.6055],
        [-0.0747,  0.9219, -0.4199, -0.7578, -0.7852],
        [-0.3145,  1.4453, -0.5000, -0.7773, -0.7461],
        [ 0.2734,  1.0156, -0.5938, -0.7656, -0.5430],
        [-0.1177,  1.3438, -0.4336, -0.6680, -0.5859],
        [-0.4336,  1.4062, -0.6523, -0.8359, -0.6211],
        [ 0.2695,  1.5859, -0.1250, -1.1328, -0.3086]], device='cuda:0',
       dtype=torch.bfloat16, grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)
        [ 0.3008,  1.3984, -0.9023, -0.8008, -0.6523],2e-01, -2.8906e-01],, attentions=None)
        [ 0.3008,  1.4531, -0.2734, -0.9297, -0.3887],2e-01, -2.8906e-01],, attentions=None)
        [ 0.1396,  1.0312, -0.2988, -1.1328, -0.6836]], device='cuda:0',
       dtype=torch.bfloat16, grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)
out(train): SequenceClassifierOutput(loss=tensor(0.7087, device='cuda:0', grad_fn=<NllLossBackward0>), logits=tensor([[-0.1475,  1.2344, -0.6836, -0.4004, -0.3945],
        [-0.1172,  1.3438, -0.5078, -0.4902, -0.5430],
        [-0.0688,  1.3516, -0.8359, -0.8750, -0.5820],
        [-0.1855,  1.0781, -0.3477, -0.8320, -0.5742],
        [ 0.1709,  1.3516, -0.8125, -0.7656, -0.4219],
        [ 0.2188,  1.4688, -0.6250, -0.9453, -0.4629],
        [-0.0610,  1.4219, -0.3555, -0.9375, -0.2031],
        [-0.2168,  1.1797, -0.2793, -0.8125, -0.6094],
        [-0.0374,  1.2656, -0.8672, -0.7500, -0.4395],
        [ 0.1670,  1.3984, -0.1367, -0.7422, -0.4609],
        [ 0.2041,  0.8828, -0.2451, -0.4941, -0.7070],
        [-0.1904,  1.1875, -0.7422, -0.6680, -0.4141],
        [-0.0021,  1.1641, -0.5078, -0.5000, -0.4043],
        [-0.0461,  1.2656, -0.2178, -0.4961, -0.6328],
        [-0.0942,  1.3984, -0.6641, -0.6914, -0.6484],
        [-0.0022,  1.2969, -0.2100, -1.0703, -0.8281]], device='cuda:0',
       dtype=torch.bfloat16, grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)
out(train): SequenceClassifierOutput(loss=tensor(0.5754, device='cuda:0', grad_fn=<NllLossBackward0>), logits=tensor([[ 0.1982,  1.6797, -0.4121, -0.8477, -0.7344],
        [-0.0493,  1.0078, -0.5273, -0.5312, -0.4512],
        [-0.3047,  1.0938, -0.6836, -1.0938, -0.4453],
        [ 0.2559,  1.2656, -0.8086, -1.0547, -0.6055],
        [-0.1924,  1.3984, -0.5586, -0.6836, -0.4707],
        [ 0.1582,  1.2500, -0.6406, -0.9141, -0.3027],
        [-0.0378,  1.1094, -0.3691, -0.8281, -0.9688],
        [-0.1001,  1.6562, -0.3965, -0.8164, -0.6328],
        [-0.0698,  1.3203, -0.3184, -0.5391, -0.7070],
        [ 0.2217,  1.3281, -0.2754, -0.4688, -0.4512],
        [-0.0757,  1.2500, -0.5273, -0.9727, -0.5078],
        [ 0.1182,  1.6719, -0.6445, -1.0391, -0.5586],
        [ 0.1660,  1.7422, -0.5430, -1.0625, -0.5312],
        [ 0.0135,  1.4766, -0.4902, -1.0234, -0.6367],
        [ 0.2598,  1.5000, -0.7656, -0.6133, -0.4023],
        [-0.0405,  1.5859, -0.2969, -0.8516, -0.4629]], device='cuda:0',
       dtype=torch.bfloat16, grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)
out(train): SequenceClassifierOutput(loss=tensor(0.8639, device='cuda:0', grad_fn=<NllLossBackward0>), logits=tensor([[ 0.0874,  1.2188, -0.3418, -1.0938, -1.1406],
        [-0.2275,  1.2500, -0.6641, -0.8711, -0.8047],
        [-0.2012,  1.3672, -0.2324, -1.2656, -0.6016],
        [ 0.0703,  1.2734, -0.2324, -0.7656, -0.9766],
        [-0.0047,  1.0156, -0.7031, -0.7969, -0.4297],
        [ 0.0496,  1.1562, -0.2559, -0.5703, -0.8516],
        [-0.2871,  1.2891, -0.8828, -1.0000, -0.3516],
        [-0.1855,  1.2422, -0.4102, -0.4707, -0.4336],
        [ 0.4883,  1.2500, -0.5781, -0.7070, -0.4727],
        [-0.0791,  1.1875, -0.3770, -0.5391, -0.3340],
        [ 0.0850,  1.5625, -0.5078, -0.9648, -0.4883],
        [-0.0952,  1.6719, -0.6133, -0.9141, -0.3965],
        [-0.0099,  1.3047, -0.5117, -0.6602, -0.6602],
        [-0.0214,  1.3750, -0.7188, -0.8789, -0.4824],
        [-0.2119,  1.0156, -0.5273, -0.2734, -0.7031],
        [ 0.0913,  1.4375, -0.6719, -0.8555, -0.7930]], device='cuda:0',
       dtype=torch.bfloat16, grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)
out(train): SequenceClassifierOutput(loss=tensor(0.7352, device='cuda:0', grad_fn=<NllLossBackward0>), logits=tensor([[-3.2617e-01,  1.3359e+00, -7.6172e-01, -6.4062e-01, -6.8750e-01],
        [ 2.8801e-04,  1.5469e+00, -7.1484e-01, -8.2422e-01, -2.0605e-01],
        [-1.5234e-01,  1.2422e+00, -5.5078e-01, -8.5938e-01, -7.6172e-01],
        [-2.5586e-01,  1.3047e+00, -4.3945e-01, -6.5234e-01, -5.9375e-01],
        [ 8.8867e-02,  1.0938e+00, -5.0781e-01, -3.6719e-01, -2.9688e-01],
        [ 1.5527e-01,  1.7500e+00, -6.0938e-01, -6.2109e-01, -6.1328e-01],
        [-1.6699e-01,  1.0312e+00, -1.6992e-01, -6.3281e-01, -4.1602e-01],
        [ 2.2705e-02,  1.1250e+00, -4.7656e-01, -9.6484e-01, -4.6484e-01],
        [-3.4375e-01,  1.1953e+00, -3.5742e-01, -7.3828e-01, -3.3984e-01],
        [ 9.4238e-02,  1.3125e+00, -7.9688e-01, -1.1094e+00, -3.5352e-01],
        [ 2.5781e-01,  1.1016e+00, -5.4297e-01, -8.2812e-01, -4.0039e-01],
        [ 1.3867e-01,  1.3828e+00, -5.4688e-01, -1.1172e+00, -4.2578e-01],
        [ 1.9336e-01,  1.4062e+00, -5.5469e-01, -5.4688e-01, -3.3203e-01],
        [-1.4355e-01,  1.1953e+00, -5.2734e-01, -6.4453e-01, -6.0547e-01],
        [ 6.2988e-02,  1.3906e+00, -6.3672e-01, -9.2578e-01, -4.8047e-01],
        [ 4.2480e-02,  1.5859e+00, -7.4219e-02, -6.8750e-01, -4.3945e-01]],
       device='cuda:0', dtype=torch.bfloat16, grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)
        [ 0.3008,  1.4531, -0.2734, -0.9297, -0.3887],2e-01, -2.8906e-01],, attentions=None)
        [ 0.4082,  1.2734, -0.8320, -0.6016, -0.4316],2e-01, -2.8906e-01],, attentions=None)
        [ 0.0649,  1.4844, -0.5703, -0.7188, -0.4590]], device='cuda:0',
       dtype=torch.bfloat16, grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)
out(train): SequenceClassifierOutput(loss=tensor(0.8694, device='cuda:0', grad_fn=<NllLossBackward0>), logits=tensor([[-0.2715,  1.3906, -0.3906, -0.7227, -0.6484],
        [ 0.0654,  1.0391, -0.3633, -0.7344, -0.4844],
        [ 0.0128,  1.2734, -0.4414, -0.6953, -0.2949],
        [-0.0461,  1.3828, -0.2676, -0.9219, -0.9258],
        [ 0.1240,  1.4844, -0.9375, -0.8828, -0.6289],
        [-0.2832,  1.2969, -0.5820, -0.4316, -0.8750],
        [ 0.0193,  1.1016, -0.6562, -0.8164, -0.2354],
        [ 0.1904,  1.3047, -0.7969, -0.6250, -1.0234],
        [ 0.1982,  1.0859, -0.4785, -0.7773, -0.5273],
        [ 0.1797,  1.3750, -0.3848, -1.1875, -0.5117],
        [ 0.0040,  1.3828, -0.6289, -0.4492, -0.3047],
        [ 0.0166,  1.3047, -0.4336, -1.0312, -0.3730],
        [ 0.2236,  1.6250, -0.3438, -0.6211, -0.5586],
        [-0.1582,  1.5000, -0.2832, -0.8477, -0.7500],
        [-0.0398,  1.5234, -0.4375, -0.6719, -0.4766],
        [-0.0728,  1.5938, -0.5547, -0.5938, -0.4824]], device='cuda:0',
       dtype=torch.bfloat16, grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)
out(train): SequenceClassifierOutput(loss=tensor(1.1519, device='cuda:0', grad_fn=<NllLossBackward0>), logits=tensor([[-0.1030,  1.2812, -0.3281, -0.7266, -0.5742],
        [-0.0957,  0.9453, -0.2695, -0.7148, -0.8984],
        [ 0.2188,  1.3984, -0.6289, -0.8555, -0.3047],
        [ 0.2754,  1.2422,  0.1196, -0.5156, -0.6641],
        [ 0.1318,  1.3281, -0.5078, -0.5703, -0.4258],
        [-0.1357,  1.3281, -0.7148, -0.6367, -0.7031],
        [-0.3691,  1.5078, -0.3965, -0.7266, -0.6758],
        [-0.2090,  1.2344, -0.4492, -0.8906, -0.7188],
        [ 0.2305,  1.2578, -0.2539, -0.4922, -0.6289],
        [ 0.0080,  1.0391, -0.4414, -0.5234, -0.3125],
        [ 0.0522,  1.3672, -0.5586, -0.7344, -0.5273],
        [-0.0674,  1.4141, -0.6211, -0.8398, -0.7305],
        [ 0.0190,  1.5781, -0.3027, -0.7891, -0.5586],
        [-0.1709,  1.3750, -0.4688, -0.6250, -0.5547],
        [ 0.0184,  1.4609, -0.6406, -0.8008, -0.4395],
        [-0.2021,  1.2500, -0.8125, -0.9414, -0.8555]], device='cuda:0',
       dtype=torch.bfloat16, grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)
out(train): SequenceClassifierOutput(loss=tensor(0.9910, device='cuda:0', grad_fn=<NllLossBackward0>), logits=tensor([[ 0.1602,  1.2812, -0.6719, -0.9336, -0.3477],
        [ 0.0413,  1.1484, -0.6641, -0.5469, -0.2871],
        [ 0.1689,  1.6406, -0.6016, -0.8633, -0.6953],
        [ 0.2432,  1.3125, -0.5625, -1.2578, -0.6133],
        [ 0.1011,  1.5781, -0.3633, -1.1328, -0.7188],
        [ 0.0659,  1.7031, -0.7148, -0.8398, -0.6484],
        [ 0.2812,  1.5547, -0.6484, -0.9023, -0.5898],
        [-0.0713,  1.4297, -0.2295, -1.0547, -0.7227],
        [ 0.1177,  1.1016, -0.2852, -0.5000, -0.4863],
        [ 0.0525,  1.3828, -0.5391, -0.9922, -0.3711],
        [ 0.3652,  1.2344, -0.3887, -0.8242, -0.7500],
        [-0.0432,  1.0938, -0.2012, -0.9375, -1.0547],
        [ 0.0219,  1.3281, -0.4609, -0.7383, -0.6133],
        [ 0.1592,  1.4141, -0.3164, -0.6719, -0.6094],
        [ 0.0260,  1.1797, -0.3770, -0.8359, -0.7148],
        [-0.1079,  1.2422, -0.6484, -0.6289, -0.3848]], device='cuda:0',
       dtype=torch.bfloat16, grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)
        [ 0.4082,  1.2734, -0.8320, -0.6016, -0.4316],2e-01, -2.8906e-01],, attentions=None)
        [-0.2871,  1.0156, -0.3496, -0.5430, -0.5312],2e-01, -2.8906e-01],, attentions=None)
        [ 0.4570,  1.2266, -0.4902, -0.6992, -0.3730]], device='cuda:0',
       dtype=torch.bfloat16, grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)
out(train): SequenceClassifierOutput(loss=tensor(0.9447, device='cuda:0', grad_fn=<NllLossBackward0>), logits=tensor([[-0.0991,  1.2188, -0.3711, -0.6328, -0.1982],
        [-0.0118,  1.4609, -0.4746, -0.9648, -0.5273],
        [ 0.1885,  1.0781, -0.6016, -0.8984, -0.7266],
        [ 0.1689,  1.1172, -0.6484, -0.7266, -0.7188],
        [ 0.1133,  1.4844, -0.5508, -0.8828, -0.6602],
        [ 0.0977,  1.3906, -0.3359, -0.6719, -0.8594],
        [ 0.1670,  1.3516, -0.5781, -1.0078, -0.6289],
        [ 0.0476,  1.5000, -0.5859, -0.8281, -0.6992],
        [-0.1572,  1.3203, -0.2676, -0.7891, -0.7188],
        [ 0.0381,  1.4844, -0.7109, -0.8438, -0.5977],
        [ 0.3496,  1.0703, -0.7656, -0.6719, -0.3047],
        [-0.0220,  1.1406, -0.6680, -0.5039, -0.4746],
        [-0.1279,  1.1406, -0.1157, -0.8359, -0.4082],
        [-0.3926,  1.6797, -0.5273, -0.7539, -0.5195],
        [ 0.1099,  1.5625, -0.6797, -0.7656, -0.4863],
        [ 0.0879,  1.3047, -0.5977, -0.9883, -0.3203]], device='cuda:0',
       dtype=torch.bfloat16, grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)
        [-0.2871,  1.0156, -0.3496, -0.5430, -0.5312],2e-01, -2.8906e-01],, attentions=None)
        [-0.3945,  1.8125, -0.6445, -0.9766, -0.6719],2e-01, -2.8906e-01],, attentions=None)
        [-0.2480,  1.3516, -0.4766, -0.7969, -0.6758]], device='cuda:0',
       dtype=torch.bfloat16, grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)
out(train): SequenceClassifierOutput(loss=tensor(0.7563, device='cuda:0', grad_fn=<NllLossBackward0>), logits=tensor([[ 0.2734,  1.4453, -0.2930, -1.0625, -0.2363],
        [ 0.2285,  1.1953, -0.3516, -0.7188, -0.6797],
        [ 0.0070,  1.7188, -0.7852, -0.9180, -0.5820],
        [-0.0050,  1.4453, -0.6367, -1.0156, -0.4238],
        [ 0.1631,  1.1641, -0.9531, -0.7188, -0.1138],
        [-0.0378,  1.7734, -0.3770, -0.8750, -0.6094],
        [-0.2637,  1.4141, -0.2363, -0.6172, -0.4629],
        [ 0.0048,  1.4141, -0.4727, -0.9531, -0.3633],
        [ 0.1885,  0.8633, -0.7656, -0.7812,  0.4668],
        [-0.0840,  1.2969, -0.4746, -0.9062, -0.8398],
        [ 0.2637,  1.1484, -0.5625, -0.9922, -0.6445],
        [-0.1846,  1.3047, -0.3652, -1.1250, -0.6719],
        [ 0.1396,  1.2188, -0.5430, -0.5742, -0.8164],
        [-0.0630,  1.4062, -0.4688, -0.6328, -0.6211],
        [-0.1699,  1.4141, -0.2490, -1.0781, -0.7812],
        [ 0.0208,  1.4453, -0.2773, -1.0156, -0.3223]], device='cuda:0',
       dtype=torch.bfloat16, grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)
out(train): SequenceClassifierOutput(loss=tensor(0.9436, device='cuda:0', grad_fn=<NllLossBackward0>), logits=tensor([[ 0.0439,  1.1953, -0.7188, -1.1875, -0.5625],
        [ 0.1904,  1.1094, -0.7344, -0.9219, -0.6172],
        [-0.2002,  1.5781, -0.4355, -0.7500, -0.6211],
        [ 0.1025,  1.2266, -0.4414, -0.5977, -0.5469],
        [-0.0420,  1.3906, -0.6953, -0.7695, -0.6133],
        [ 0.0840,  1.3438, -0.4121, -0.7969, -0.9297],
        [-0.1406,  1.1953, -0.5547, -0.7500, -0.7109],
        [ 0.1611,  1.3594, -0.5859, -0.8203, -0.7188],
        [-0.1074,  1.8203, -0.6172, -0.8828, -0.5273],
        [ 0.1582,  1.5938, -0.7578, -0.9102, -0.7070],
        [ 0.0420,  1.1953, -0.5625, -1.5000, -0.3711],
        [ 0.1748,  0.8828, -0.3301, -0.4688, -0.6797],
        [-0.1973,  1.3281, -0.5195, -0.3730, -0.5859],
        [-0.0757,  1.5312, -0.5742, -0.7500, -0.8672],
        [-0.1846,  1.0156, -0.7344, -0.4531, -0.1455],
        [-0.0684,  1.2422, -0.7578, -0.7148, -0.5156]], device='cuda:0',
       dtype=torch.bfloat16, grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)
        [-0.3945,  1.8125, -0.6445, -0.9766, -0.6719],2e-01, -2.8906e-01],, attentions=None)
        [ 0.0120,  0.9609, -1.0312, -0.5195, -0.5625],2e-01, -2.8906e-01],, attentions=None)
        [ 0.1963,  1.1094, -0.5664, -0.5469, -0.5078]], device='cuda:0',
       dtype=torch.bfloat16, grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)
out(train): SequenceClassifierOutput(loss=tensor(0.9382, device='cuda:0', grad_fn=<NllLossBackward0>), logits=tensor([[ 0.4492,  1.4219, -0.6055, -0.9102, -0.4863],
        [-0.2178,  1.4453, -0.5195, -0.9453, -0.3301],
        [ 0.1758,  1.2500, -0.4277, -1.0391, -0.6641],
        [ 0.0376,  1.6641, -0.5234, -0.7461, -0.8047],
        [-0.0339,  1.5547, -0.4961, -0.5039, -0.4316],
        [ 0.2021,  0.9844, -0.5352, -0.6719, -0.3145],
        [ 0.0276,  1.1719, -0.4297, -0.6406, -0.6641],
        [ 0.0332,  1.6484, -0.6875, -1.0938, -0.4629],
        [-0.3008,  1.1797, -0.3906, -0.8828, -0.6641],
        [-0.1914,  1.3828, -0.3379, -0.6055, -0.6016],
        [-0.1641,  1.7109, -0.5977, -0.9883, -0.6094],
        [ 0.2910,  1.0703, -0.2871, -0.9141, -0.4727],
        [ 0.1006,  1.3594, -0.6406, -1.0078, -0.4629],
        [ 0.0532,  1.3281, -0.2910, -0.7969, -0.5703],
        [ 0.1680,  1.5547, -0.8047, -0.7773, -0.5781],
        [-0.0028,  1.2891, -0.4180, -0.8281, -0.5000]], device='cuda:0',
       dtype=torch.bfloat16, grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)
out(train): SequenceClassifierOutput(loss=tensor(0.5566, device='cuda:0', grad_fn=<NllLossBackward0>), logits=tensor([[ 0.1128,  0.7812, -0.4648, -0.5508, -0.5820],
        [ 0.0491,  1.4297, -0.5352, -0.9570, -0.6836],
        [ 0.1387,  1.4688, -0.6445, -0.8984, -0.6641],
        [-0.1069,  1.2969, -0.2832, -0.8633, -0.5234],
        [ 0.1416,  1.2500, -0.3340, -0.5156, -1.2734],
        [-0.1260,  1.6719, -0.7891, -0.7969, -0.4043],
        [-0.0752,  1.1484, -0.5117, -0.8594, -0.6680],
        [ 0.2559,  1.4453, -0.2334, -0.5742, -0.5586],
        [-0.0503,  1.3984, -0.6328, -1.0000, -0.4668],
        [-0.0544,  1.4531, -0.6094, -0.7266, -0.6602],
        [ 0.4980,  1.2266, -0.7422, -0.7461, -0.6094],
        [-0.2793,  1.6016, -0.3809, -0.7812, -0.7383],
        [ 0.0869,  1.5938, -0.6484, -0.7930, -0.3301],
        [-0.0244,  1.5234, -0.5000, -0.9336, -0.5938],
        [-0.2832,  1.4375, -0.7266, -1.1406, -0.5859],
        [ 0.0025,  1.3047, -0.1924, -1.0625, -0.9023]], device='cuda:0',
       dtype=torch.bfloat16, grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)
out(train): SequenceClassifierOutput(loss=tensor(0.5858, device='cuda:0', grad_fn=<NllLossBackward0>), logits=tensor([[-0.5859,  1.4844, -0.4219, -0.6758, -0.1357],
        [-0.0889,  1.6484, -0.2910, -0.8750, -0.4277],
        [ 0.0859,  1.3281, -0.2578, -0.5586, -0.5234],
        [ 0.1895,  1.4609, -0.4043, -0.9258, -0.5664],
        [-0.2422,  1.6641, -0.5430, -0.9883, -0.4688],
        [ 0.0101,  1.5391, -0.6289, -0.8398, -0.3262],
        [ 0.0908,  1.5156, -0.5039, -0.6289, -0.6602],
        [ 0.0835,  1.3906, -0.8867, -0.7109, -0.5977],
        [-0.1660,  1.1406, -0.5664, -0.8867, -0.4922],
        [ 0.1377,  1.5312, -0.6289, -0.7695, -0.1729],
        [-0.1562,  1.1406, -0.5039, -0.4473, -0.2617],
        [-0.2412,  1.1406, -0.4727, -0.9375, -0.6875],
        [-0.0654,  1.5312, -0.6445, -0.8633, -0.4609],
        [ 0.0439,  1.4297, -0.8086, -0.8984, -0.4453],
        [-0.1099,  1.5469, -0.4551, -0.7070, -0.6055],
        [ 0.2217,  1.1016, -0.5742, -0.8359, -0.4023]], device='cuda:0',
       dtype=torch.bfloat16, grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)
        [ 0.0120,  0.9609, -1.0312, -0.5195, -0.5625],2e-01, -2.8906e-01],, attentions=None)
        [-3.6001e-05,  1.2656e+00, -7.1094e-01, -8.7109e-01, -7.8906e-01],, attentions=None)
        [ 6.2256e-02,  1.4766e+00, -5.1172e-01, -1.0234e+00, -7.4609e-01]],
       device='cuda:0', dtype=torch.bfloat16, grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)
out(train): SequenceClassifierOutput(loss=tensor(0.6647, device='cuda:0', grad_fn=<NllLossBackward0>), logits=tensor([[ 0.0332,  1.1875, -0.0525, -0.6719, -0.5273],
        [ 0.0605,  1.6719, -0.5977, -1.1953, -0.5742],
        [ 0.1377,  1.4766, -0.6250, -1.1719, -0.7617],
        [-0.0776,  1.2109, -0.6367, -0.8828, -0.5898],
        [-0.2490,  1.5703, -0.6133, -0.4512, -0.3438],
        [ 0.1074,  1.2188, -0.8242, -0.8320, -0.2656],
        [-0.0066,  1.1250, -0.5078, -0.8203, -0.4648],
        [-0.0289,  1.4453, -0.5781, -0.7852, -0.4688],
        [-0.1973,  1.4609, -0.2207, -0.8164, -0.5859],
        [-0.0084,  1.1953, -0.6328, -0.6758, -0.4805],
        [-0.1118,  1.7031, -0.1235, -1.0156, -0.4258],
        [-0.0791,  1.4766, -0.3984, -0.8125, -0.7969],
        [-0.2852,  1.3750, -0.8281, -0.7227, -0.4414],
        [ 0.3262,  1.5547, -0.4043, -0.9727, -0.8008],
        [ 0.0742,  1.6172, -0.7422, -0.5469, -0.2969],
        [ 0.1196,  1.2734, -0.6211, -1.2031, -0.6758]], device='cuda:0',
       dtype=torch.bfloat16, grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)
out(train): SequenceClassifierOutput(loss=tensor(0.9436, device='cuda:0', grad_fn=<NllLossBackward0>), logits=tensor([[-0.3574,  1.2109, -0.6328, -0.8086, -0.5586],
        [-0.0410,  0.7656, -0.7305, -0.9492, -0.7305],
        [ 0.2715,  1.5312, -0.6328, -1.1797, -0.4355],
        [-0.1187,  1.4375, -0.5898, -0.9336, -0.4785],
        [-0.1836,  1.5234, -0.3164, -0.7812, -0.2168],
        [ 0.1196,  1.3047, -0.0854, -0.4961, -0.9883],
        [-0.0037,  1.6328, -0.6523, -0.8945, -0.5000],
        [ 0.3652,  1.1797, -0.4609, -0.5430, -0.4395],
        [-0.1865,  1.6094, -0.3555, -0.5547, -0.1895],
        [-0.1396,  1.9297, -0.4883, -1.2266, -0.4082],
        [ 0.0776,  1.3516, -0.4863, -0.6328, -0.3613],
        [-0.2754,  1.2031, -0.5117, -0.8359, -0.4922],
        [ 0.0216,  1.1172, -0.5547, -1.0000, -0.8047],
        [ 0.0840,  1.2500, -0.4121, -0.8828, -0.5547],
        [ 0.3359,  0.8047, -0.9922, -1.0938,  0.0215],
        [-0.1426,  1.0312, -0.4746, -0.7500, -0.9258]], device='cuda:0',
       dtype=torch.bfloat16, grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)
out(train): SequenceClassifierOutput(loss=tensor(0.6620, device='cuda:0', grad_fn=<NllLossBackward0>), logits=tensor([[ 0.1885,  1.2969, -0.5586, -0.8203, -0.2695],
        [-0.0065,  1.3359, -0.5195, -0.7344, -0.4746],
        [ 0.2812,  1.1406, -0.8398, -0.8438, -0.3359],
        [-0.0596,  1.9453, -0.7383, -0.9961, -0.5820],
        [ 0.0381,  1.4688, -0.6445, -1.1719, -0.6797],
        [ 0.3730,  1.3203, -0.5703, -0.8711, -0.5898],
        [-0.1758,  1.7188, -0.7188, -0.8359, -0.5312],
        [ 0.0947,  1.5234, -0.2129, -0.6953, -0.8281],
        [-0.1484,  1.5000, -0.3926, -0.9453, -0.7109],
        [-0.1455,  1.4609, -0.4902, -0.5469, -0.3008],
        [ 0.3340,  1.4922, -0.7070, -0.7734, -0.3730],
        [ 0.0781,  1.1641, -0.4727, -0.6094, -0.3945],
        [-0.2480,  1.1016, -0.6289, -0.8164, -0.8516],
        [-0.0559,  1.2734, -0.4707, -0.8750, -0.4570],
        [ 0.0101,  1.6484, -0.5742, -0.9180, -0.7148],
        [ 0.1475,  1.7812, -0.6719, -0.5898, -0.5195]], device='cuda:0',
       dtype=torch.bfloat16, grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)
out(train): SequenceClassifierOutput(loss=tensor(0.7733, device='cuda:0', grad_fn=<NllLossBackward0>), logits=tensor([[-3.1445e-01,  1.6797e+00, -5.5859e-01, -8.7109e-01, -5.5469e-01],
        [ 1.3199e-03,  1.2422e+00, -4.4727e-01, -9.8438e-01, -7.4219e-01],
        [ 2.8320e-01,  1.5000e+00, -5.7422e-01, -8.9062e-01, -5.7812e-01],
        [-2.3145e-01,  1.4297e+00, -4.7070e-01, -6.6016e-01, -5.9375e-01],
        [-1.1865e-01,  1.4062e+00, -6.9922e-01, -9.8047e-01, -7.2656e-01],
        [ 1.7480e-01,  9.9219e-01, -1.0625e+00, -1.1406e+00,  5.4626e-03],
        [ 1.8262e-01,  1.4766e+00, -6.6797e-01, -9.4922e-01, -5.7422e-01],
        [ 1.3184e-01,  1.1250e+00, -4.2383e-01, -1.0859e+00, -6.4062e-01],
        [ 9.5703e-02,  1.4609e+00, -3.6133e-01, -7.5391e-01, -6.9141e-01],
        [ 1.2988e-01,  1.1797e+00, -2.9492e-01, -4.6094e-01, -5.3906e-01],
        [ 4.2480e-02,  1.5234e+00, -3.7695e-01, -5.3125e-01, -4.2383e-01],
        [ 6.0303e-02,  1.3594e+00, -5.5859e-01, -1.0859e+00, -3.2031e-01],
        [ 5.8984e-01,  5.7422e-01, -8.5547e-01, -9.9219e-01,  6.2988e-02],
        [-1.2158e-01,  1.1484e+00, -2.8711e-01, -5.9375e-01, -7.3828e-01],
        [-6.8848e-02,  1.4766e+00, -3.2227e-01, -1.0391e+00, -7.3438e-01],
        [-1.3867e-01,  1.4531e+00, -4.7461e-01, -8.9844e-01, -4.1602e-01]],
       device='cuda:0', dtype=torch.bfloat16, grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)
out(train): SequenceClassifierOutput(loss=tensor(0.8130, device='cuda:0', grad_fn=<NllLossBackward0>), logits=tensor([[ 0.1621,  1.5938, -0.7344, -0.6133, -0.4121],
        [-0.1138,  1.4375, -0.1885, -0.8516, -0.5547],
        [-0.1167,  1.6875, -0.4238, -0.9727, -0.7148],
        [ 0.0693,  1.0156, -0.5820, -0.9023, -0.2393],
        [ 0.0125,  1.1875, -0.8984, -0.8438, -0.2910],
        [ 0.1133,  1.4531, -0.4902, -1.0703, -0.4395],
        [ 0.0461,  1.1406, -0.5469, -1.0078, -0.6680],
        [-0.1025,  1.4766, -0.6484, -0.9805, -0.6445],
        [ 0.3789,  1.4609, -0.2334, -0.7031, -0.5664],
        [ 0.2432,  1.5000, -0.8750, -0.8750, -0.6719],
        [-0.0137,  1.4453, -0.7070, -0.9492, -0.2236],
        [ 0.1245,  1.2422, -0.2910, -0.9492, -0.4668],
        [ 0.0757,  1.1641, -0.6172, -0.6133, -0.6875],
        [ 0.3027,  1.4062, -0.3730, -0.5586, -0.3926],
        [-0.1748,  1.4688, -0.5938, -0.7578, -0.3750],
        [ 0.2656,  1.1875, -0.6367, -0.9453, -0.3867]], device='cuda:0',
       dtype=torch.bfloat16, grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)
        [-3.6001e-05,  1.2656e+00, -7.1094e-01, -8.7109e-01, -7.8906e-01],, attentions=None)
        [ 0.0229,  1.4688, -0.6641, -0.7969, -0.2676],9e-01, -7.8906e-01],, attentions=None)
        [-0.2334,  1.3750, -0.6133, -0.9570, -0.5039]], device='cuda:0',
       dtype=torch.bfloat16, grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)
out(train): SequenceClassifierOutput(loss=tensor(0.8322, device='cuda:0', grad_fn=<NllLossBackward0>), logits=tensor([[ 0.0884,  1.4375, -0.4102, -0.8711, -0.6836],
        [-0.0820,  1.3984, -0.9961, -0.9609, -0.5703],
        [-0.0752,  1.5078, -0.4199, -0.4570, -0.5586],
        [ 0.1084,  1.3281, -0.4844, -0.9258, -0.2285],
        [ 0.2148,  1.7812, -0.5625, -0.8516, -0.5742],
        [ 0.1245,  1.3125, -0.2891, -0.6562, -0.5469],
        [-0.1729,  1.6719, -0.7930, -1.0938, -0.5156],
        [ 0.2461,  1.3438, -0.8086, -1.1094, -0.3164],
        [ 0.1846,  1.3828, -0.3086, -0.6914, -1.0312],
        [ 0.2402,  1.2578, -0.5508, -0.7695, -0.4395],
        [-0.0352,  1.2812, -0.4180, -0.6992, -0.4805],
        [ 0.2148,  1.3359, -0.4316, -1.1875, -0.4512],
        [ 0.2773,  1.3906, -0.3750, -0.7383, -0.6602],
        [-0.1611,  1.2812, -0.5625, -0.9141, -0.4648],
        [-0.0728,  1.2891, -0.7383, -0.5156, -0.7812],
        [ 0.0391,  1.4062, -0.5547, -1.1719, -0.6836]], device='cuda:0',
       dtype=torch.bfloat16, grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)
out(train): SequenceClassifierOutput(loss=tensor(0.8108, device='cuda:0', grad_fn=<NllLossBackward0>), logits=tensor([[-0.1406,  1.6094, -0.5547, -0.7305, -0.2656],
        [-0.1416,  1.1875, -0.4375, -1.1328, -0.3027],
        [ 0.1226,  1.3359, -0.5547, -0.8672, -0.6641],
        [-0.0771,  1.6406, -0.6055, -0.5078, -0.6523],
        [-0.0581,  1.2188, -0.2949, -0.9492, -0.2773],
        [ 0.0505,  1.6250, -0.8125, -0.9531, -0.6953],
        [-0.0371,  1.4219, -0.8984, -1.0703, -0.3516],
        [ 0.1406,  1.5469, -0.4922, -1.0078, -0.4746],
        [-0.1719,  1.4297, -0.3066, -1.1484, -0.2285],
        [ 0.1270,  1.2344, -0.4629, -0.9141, -0.7188],
        [ 0.0033,  1.3438, -0.7930, -1.0234, -0.6562],
        [ 0.4473,  1.4375, -0.7891, -0.9492, -0.3887],
        [-0.0698,  1.6797, -0.4785, -0.7695, -0.5156],
        [-0.1328,  1.2969, -0.6367, -0.8477, -0.5859],
        [-0.1650,  1.0547, -0.2363, -0.4219, -0.8398],
        [-0.1318,  1.4609, -0.5508, -0.9141, -0.2539]], device='cuda:0',
       dtype=torch.bfloat16, grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)
out(train): SequenceClassifierOutput(loss=tensor(0.7966, device='cuda:0', grad_fn=<NllLossBackward0>), logits=tensor([[-0.0481,  1.4766, -0.5352, -0.6719, -0.5352],
        [-0.1777,  1.5078, -0.6211, -0.9961, -0.7734],
        [-0.0454,  1.2578, -0.3633, -1.1562, -0.8281],
        [ 0.1680,  1.0938, -0.7148, -0.8516, -0.5898],
        [ 0.0630,  1.3125, -0.3613, -0.8711, -0.7070],
        [ 0.3320,  1.3438, -0.8281, -0.9180, -0.5820],
        [-0.0522,  1.2266, -0.6523, -0.8359, -0.6484]], device='cuda:0',
       dtype=torch.bfloat16, grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)
        [ 0.0229,  1.4688, -0.6641, -0.7969, -0.2676],9e-01, -7.8906e-01],, attentions=None)
W0629 11:52:52.467000 139012379337344 torch/_dynamo/convert_frame.py:824]     result = inner_convert(
W0629 11:52:52.467000 139012379337344 torch/_dynamo/convert_frame.py:824]     result = inner_convert(
W0629 11:52:52.467000 139012379337344 torch/_dynamo/convert_frame.py:824]     result = inner_convert(
W0629 11:52:52.467000 139012379337344 torch/_dynamo/convert_frame.py:824]     result = inner_convert(
W0629 11:52:52.467000 139012379337344 torch/_dynamo/convert_frame.py:824]     result = inner_convert(
W0629 11:52:52.467000 139012379337344 torch/_dynamo/convert_frame.py:824]     result = inner_convert(
W0629 11:52:52.467000 139012379337344 torch/_dynamo/convert_frame.py:824]     result = inner_convert(
W0629 11:52:52.467000 139012379337344 torch/_dynamo/convert_frame.py:824]     result = inner_convert(
        [ 0.0229,  1.4688, -0.6641, -0.7969, -0.2676],9e-01, -7.8906e-01],, attentions=None)
        [ 7.5378e-03,  1.1172e+00, -1.6699e-01, -7.2656e-01, -6.3281e-01],, attentions=None)
        [ 7.2266e-02,  1.1875e+00, -8.1641e-01, -1.3828e+00, -5.2734e-01]],
       device='cuda:0', dtype=torch.bfloat16, grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)
out(train): SequenceClassifierOutput(loss=tensor(0.9449, device='cuda:0', grad_fn=<NllLossBackward0>), logits=tensor([[-0.0037,  1.2031, -0.8867, -1.0312, -0.6758],
        [ 0.0344,  1.4766, -0.4395, -1.0859, -0.8125],
        [ 0.1416,  1.2734, -0.5586, -1.1562, -0.5430],
        [-0.4102,  1.1250, -0.5859, -0.9648, -0.8633],
        [-0.2598,  1.1641, -0.6289, -1.0547, -0.3789],
        [ 0.1445,  1.3047, -0.1709, -0.8633, -0.9336],
        [ 0.0137,  0.9531, -0.5820, -1.1953, -0.4727],
        [-0.1216,  1.3438, -0.1270, -0.7070, -0.7109],
        [ 0.0708,  1.0938, -0.5195, -0.7227, -0.6875],
        [ 0.0214,  1.4531, -0.2637, -0.7773, -0.8281],
        [-0.2891,  1.3125, -0.0117, -0.8359, -0.8828],
        [ 0.0134,  1.0625, -0.3398, -0.6328, -0.5469],
        [ 0.2695,  1.2656, -0.6172, -0.9766, -0.5039],
        [-0.1089,  1.5000, -0.2236, -0.4941, -0.5430],
        [ 0.2275,  1.4453, -0.4629, -0.8516, -0.8047],
        [ 0.0664,  1.0703, -0.1924, -0.8320, -0.7188]], device='cuda:0',
       dtype=torch.bfloat16, grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)
        [ 7.5378e-03,  1.1172e+00, -1.6699e-01, -7.2656e-01, -6.3281e-01],, attentions=None)
        [-0.0024,  1.0156, -0.7188, -0.9336, -0.5664],6e-01, -6.3281e-01],, attentions=None)
        [-0.1196,  1.4531, -0.4746, -0.7773, -0.6680]], device='cuda:0',
       dtype=torch.bfloat16, grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)
out(train): SequenceClassifierOutput(loss=tensor(0.6995, device='cuda:0', grad_fn=<NllLossBackward0>), logits=tensor([[-0.0222,  1.2812, -0.6602, -0.8984, -0.5703],
        [ 0.2021,  1.1641, -0.8008, -0.9766, -0.5391],
        [-0.1172,  1.2891, -0.2441, -1.0156, -0.6055],
        [ 0.1602,  1.4219, -0.8867, -0.4453, -0.4766],
        [-0.2422,  1.4844, -0.5664, -0.8789, -0.4785],
        [ 0.1582,  1.2031, -0.8828, -0.6641, -0.3516],
        [ 0.1250,  1.0078, -0.2578, -0.7500, -0.6875],
        [-0.0659,  1.6250, -0.8242, -0.8477, -0.4141],
        [-0.0601,  1.2422, -0.4121, -0.7031, -0.5391],
        [ 0.0088,  1.1094, -0.6094, -0.9180, -0.5586],
        [ 0.1309,  1.0156, -0.2119, -0.6914, -0.4062],
        [ 0.1562,  1.3828, -0.6797, -0.8242, -0.3535],
        [-0.3574,  1.5234, -0.5703, -0.8945, -0.3887],
        [-0.0481,  1.3125, -0.6211, -0.8281, -0.6484],
        [ 0.2334,  1.5000, -1.0391, -0.7656, -0.8438],
        [-0.0825,  0.9844, -0.7344, -0.7891, -0.5938]], device='cuda:0',
       dtype=torch.bfloat16, grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)
out(train): SequenceClassifierOutput(loss=tensor(0.5192, device='cuda:0', grad_fn=<NllLossBackward0>), logits=tensor([[ 0.1084,  1.2031, -0.3691, -1.0000, -0.9492],
        [ 0.1045,  1.1641, -0.7227, -0.7070, -0.3672],
        [-0.0942,  1.1328, -0.6797, -0.8242, -0.6641],
        [ 0.1079,  1.4141, -0.3711, -1.0938, -0.6953],
        [-0.0791,  1.5859, -0.5391, -0.5859, -0.5352],
        [ 0.2656,  1.5703, -0.4941, -1.0938, -0.4961],
        [ 0.1650,  1.5781, -0.6602, -0.8828, -0.3125],
        [ 0.1113,  1.3359, -0.2451, -0.8242, -0.7148],
        [ 0.0223,  0.8789, -0.7891, -0.8516,  0.2598],
        [ 0.2119,  1.6875, -0.6797, -0.9062, -0.3340],
        [ 0.1943,  1.3828, -0.7422, -0.7461, -0.6836],
        [-0.2812,  1.3047, -0.1484, -0.8750, -0.6797],
        [-0.4277,  1.4609, -0.2539, -0.7969, -0.5078],
        [-0.0850,  1.5469, -0.4883, -1.0000, -0.5781],
        [-0.0160,  1.7344, -0.4238, -0.9297, -0.2354],
        [-0.3008,  1.2422, -0.4277, -0.5352, -0.6016]], device='cuda:0',
       dtype=torch.bfloat16, grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)
out(train): SequenceClassifierOutput(loss=tensor(0.9312, device='cuda:0', grad_fn=<NllLossBackward0>), logits=tensor([[ 0.1787,  1.3203, -0.3223, -0.5859, -0.6367],
        [-0.0391,  1.4922, -0.3730, -0.6758, -0.8125],
        [ 0.1934,  1.5859, -0.7852, -0.7305, -0.4395],
        [ 0.1924,  1.0625, -0.2617, -1.0078, -0.9414],
        [ 0.1562,  0.9297, -0.2305, -1.0547, -0.6367],
        [ 0.1030,  1.3438, -0.6016, -0.9766, -0.6016],
        [ 0.1553,  1.1172, -0.5078, -0.7969, -0.5781],
        [ 0.0613,  1.4609, -0.6523, -0.6250, -0.5469],
        [ 0.0601,  1.5234, -0.6328, -1.0234, -0.4043],
        [ 0.0303,  1.5312, -0.6172, -0.8984, -0.7031],
        [ 0.2520,  0.9375, -0.3496, -0.9414, -0.3984],
        [ 0.1748,  1.2734, -0.5391, -0.8672, -0.5391],
        [-0.0520,  1.0781, -0.4531, -0.9219, -0.3574],
        [-0.0378,  1.2266, -0.6211, -0.8242, -0.6211],
        [-0.1953,  1.3672, -0.7070, -0.9570, -0.9414],
        [ 0.0791,  1.5156, -0.5156, -0.8867, -0.3613]], device='cuda:0',
       dtype=torch.bfloat16, grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)
        [-0.0024,  1.0156, -0.7188, -0.9336, -0.5664],6e-01, -6.3281e-01],, attentions=None)
        [ 1.6016e-01,  1.3750e+00, -3.4766e-01, -6.2500e-01, -4.5312e-01],, attentions=None)
        [-1.8359e-01,  1.2734e+00, -2.8711e-01, -7.2266e-01, -9.3750e-01]],
       device='cuda:0', dtype=torch.bfloat16, grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)
out(train): SequenceClassifierOutput(loss=tensor(0.6323, device='cuda:0', grad_fn=<NllLossBackward0>), logits=tensor([[-0.3086,  0.9688, -0.5273, -0.3652, -0.3633],
        [ 0.1807,  1.6953, -0.6289, -1.0625, -0.6367],
        [-0.2812,  1.5234, -0.7461, -0.7070, -0.7852],
        [ 0.0109,  1.1016, -0.2520, -0.7227, -0.6172],
        [-0.0228,  1.4609, -0.5625, -0.9844, -0.7031],
        [ 0.0664,  1.5312, -0.5430, -1.0469, -0.5156],
        [ 0.0547,  2.0469, -0.4629, -1.2500, -0.2617],
        [-0.0703,  0.9883, -0.2129, -0.6406, -0.5273],
        [-0.3008,  1.4688, -0.5742, -0.7266, -0.6094],
        [-0.0251,  1.3984, -0.1572, -1.0469, -0.2949],
        [ 0.3535,  1.2734, -0.2637, -0.4961, -0.5469],
        [-0.4688,  1.5703, -0.5625, -0.6719, -0.6797],
        [ 0.5586,  0.7812, -1.0312, -0.8008, -0.1187],
        [ 0.1318,  1.6172, -0.4551, -0.8086, -0.6758],
        [-0.2598,  1.5078, -1.0078, -0.9141, -0.4023],
        [-0.2031,  1.3281, -0.4688, -1.3672, -0.9336]], device='cuda:0',
       dtype=torch.bfloat16, grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)
out(train): SequenceClassifierOutput(loss=tensor(0.5602, device='cuda:0', grad_fn=<NllLossBackward0>), logits=tensor([[ 0.2354,  1.4062, -0.4453, -0.9375, -0.4160],
        [ 0.0708,  1.1719, -0.5273, -0.5586, -0.2852],
        [ 0.2021,  1.3125, -0.7617, -0.8945, -0.4863],
        [ 0.1084,  1.4219, -0.6758, -0.9414, -0.5977],
        [-0.1289,  1.7266, -0.6289, -0.9219, -0.4707],
        [ 0.0459,  1.6562, -0.5664, -0.9258, -0.4238],
        [-0.1104,  1.5469, -0.4492, -1.0703, -0.4434],
        [ 0.3359,  1.4297, -0.6758, -0.9023, -0.3613],
        [-0.1089,  1.4375, -0.4434, -0.7500, -0.5117],
        [ 0.0376,  1.3359, -0.5391, -0.9531, -0.5508],
        [-0.0491,  1.1875, -0.5547, -1.1641, -0.6016],
        [-0.1094,  1.6172, -0.6875, -0.9844, -0.3770],
        [ 0.0332,  1.6562, -0.6289, -1.1797, -0.4219],
        [ 0.0211,  1.4141, -0.8008, -0.8828, -0.5898],
        [-0.0449,  1.5625, -0.7266, -0.9258, -0.6875],
        [ 0.0231,  1.4141, -0.7578, -1.1719, -0.4160]], device='cuda:0',
       dtype=torch.bfloat16, grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)
out(train): SequenceClassifierOutput(loss=tensor(0.8789, device='cuda:0', grad_fn=<NllLossBackward0>), logits=tensor([[ 0.3984,  1.1328, -0.3750, -0.8203, -0.8555],
        [-0.1602,  1.4531, -0.9102, -0.9336, -0.8477],
        [-0.0649,  1.3906, -0.8125, -0.9258, -0.5312],
        [-0.1719,  1.1562,  0.0957, -0.8398, -0.8242],
        [ 0.4727,  1.0312, -0.5977, -1.1016, -0.5039],
        [ 0.0045,  1.3359, -0.1328, -0.6562, -0.7461],
        [-0.0435,  0.9688, -0.7266, -0.9805, -0.4238],
        [-0.0320,  1.3828, -0.5039, -0.6250, -0.8047],
        [ 0.4668,  1.3828, -0.8047, -0.7383, -0.5117],
        [-0.1523,  1.5391, -0.7617, -0.8555, -0.5781],
        [ 0.0703,  1.3359, -0.8711, -0.8438, -0.5703],
        [-0.1030,  1.6719, -0.6914, -0.8711, -0.5898],
        [ 0.3438,  1.5547, -0.6953, -1.0078, -0.5938],
        [ 0.2314,  1.4297, -0.6875, -1.0078, -0.4004],
        [ 0.1050,  0.9922, -0.7344, -0.3496, -0.3965],
        [ 0.2578,  1.2891, -0.7305, -1.0000, -0.3496]], device='cuda:0',
       dtype=torch.bfloat16, grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)
out(train): SequenceClassifierOutput(loss=tensor(0.6750, device='cuda:0', grad_fn=<NllLossBackward0>), logits=tensor([[-0.0942,  1.4688, -1.0547, -0.7188, -0.4355],
        [ 0.0518,  1.5156, -0.6133, -1.0391, -0.4590],
        [-0.1631,  1.4297, -0.6484, -0.8008, -0.6484],
        [-0.0757,  1.3750, -0.9375, -0.8633, -0.5195],
        [ 0.1128,  1.6250, -0.6562, -0.6680, -0.5938],
        [ 0.0212,  1.8828, -0.7930, -0.7266, -0.6484],
        [ 0.0962,  1.2734, -0.3242, -0.7656, -0.4551],
        [-0.1523,  1.5156, -0.7031, -0.8906, -0.7344],
        [-0.0723,  1.6172, -0.7539, -0.6758, -0.0669],
        [ 0.0569,  1.0547, -0.8945, -1.1406, -0.2676],
        [ 0.2021,  1.1641, -0.8164, -0.7500, -0.3594],
        [ 0.0918,  1.6250, -0.6523, -0.8984, -0.3164],
        [ 0.0781,  1.6562, -0.7930, -0.7461, -0.5859],
        [-0.1235,  1.3594, -0.4043, -0.7852, -0.6289],
        [-0.1943,  1.4844, -0.5352, -0.9648, -0.6680],
        [ 0.0430,  1.4688, -0.2041, -0.8438, -0.5859]], device='cuda:0',
       dtype=torch.bfloat16, grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)
        [ 1.6016e-01,  1.3750e+00, -3.4766e-01, -6.2500e-01, -4.5312e-01],, attentions=None)
        [ 0.2715,  1.7109, -0.5469, -1.0312, -0.3789],0e-01, -4.5312e-01],, attentions=None)
        [ 0.1602,  1.6328, -0.9961, -0.5195, -0.7148],
        [ 0.1885,  1.4531, -0.8672, -0.6445, -0.2793]], device='cuda:0',
       dtype=torch.bfloat16, grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)
out(train): SequenceClassifierOutput(loss=tensor(0.8555, device='cuda:0', grad_fn=<NllLossBackward0>), logits=tensor([[-0.0688,  1.3516, -0.4531, -0.7578, -0.5234],
        [ 0.4004,  1.4297, -0.5859, -0.7344, -0.4766],
        [ 0.1523,  1.3906, -0.2422, -0.6992, -0.4336],
        [-0.1318,  0.9297, -0.3281, -0.7422, -0.8125],
        [ 0.0026,  1.4609, -0.6016, -0.7070, -0.4375],
        [ 0.0845,  1.1406, -0.8438, -0.3516, -0.6641],
        [ 0.3086,  1.2188, -0.6406, -1.0156, -0.5508],
        [ 0.3184,  1.3125, -0.7500, -0.8555, -0.8086],
        [ 0.4629,  1.3359, -0.7578, -0.9414, -0.5547],
        [ 0.0270,  1.3516, -0.7109, -0.9297, -0.2852],
        [ 0.2578,  1.3750, -0.8672, -0.5078, -0.2715],
        [ 0.2148,  1.7109, -0.5430, -1.0547, -0.8359],
        [ 0.2812,  1.5938, -0.6641, -0.8672, -0.5703],
        [ 0.3145,  1.4219, -0.5117, -0.7812, -0.3320],
        [ 0.1572,  1.5234, -0.4121, -0.7539, -0.4473],
        [-0.2598,  1.4453, -0.6797, -0.7812, -0.5820]], device='cuda:0',
       dtype=torch.bfloat16, grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)
out(train): SequenceClassifierOutput(loss=tensor(1.1301, device='cuda:0', grad_fn=<NllLossBackward0>), logits=tensor([[ 0.0356,  1.5312, -0.5312, -0.6289, -0.6836],
        [ 0.3730,  0.9805, -0.5156, -0.6289, -0.7070],
        [ 0.1250,  1.4375, -0.8047, -0.8047, -0.2871],
        [ 0.1104,  1.3672, -0.2061, -0.7891, -0.7344],
        [ 0.1201,  1.4219, -0.7148, -0.6719, -0.5430],
        [ 0.3477,  1.2812, -0.8047, -0.8750, -0.5195],
        [-0.3379,  1.3203, -0.8828, -0.6172, -0.4531],
        [-0.0588,  1.3047, -0.6211, -0.9805, -0.4648],
        [ 0.2080,  1.5000, -0.5352, -0.8789, -0.6328],
        [-0.0432,  1.5078, -0.6445, -0.6406, -0.3848],
        [ 0.0300,  1.6641, -0.8008, -1.0625, -0.2559],
        [ 0.1865,  1.6172, -0.6250, -1.0547, -0.3574],
        [ 0.0332,  1.5859, -0.9258, -0.8750, -0.6055],
        [-0.0513,  1.0469, -0.4570, -0.7617, -0.7188],
        [ 0.0486,  1.7109, -0.5352, -0.7305, -0.2910],
        [-0.1094,  1.2812, -0.5703, -0.7422, -0.7031]], device='cuda:0',
       dtype=torch.bfloat16, grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)
out(train): SequenceClassifierOutput(loss=tensor(0.9167, device='cuda:0', grad_fn=<NllLossBackward0>), logits=tensor([[ 0.0630,  1.3750, -0.9219, -0.8945,  0.0197],
        [ 0.1309,  1.2891, -0.6328, -0.6289, -0.3594],
        [-0.0127,  1.4844, -0.5312, -0.7266, -0.5391],
        [ 0.0791,  1.4766, -0.6484, -1.1250, -0.7852],
        [ 0.3730,  1.4688, -0.5938, -1.0078, -0.4688],
        [ 0.0767,  1.6797, -0.8672, -0.8320, -0.5430],
        [ 0.3105,  1.7422, -0.9414, -1.0781, -0.6172],
        [-0.2461,  1.4062, -0.3652, -0.8555, -0.8867],
        [-0.0400,  1.1953, -0.6289, -0.6445, -0.6719],
        [-0.1621,  1.2578, -0.2812, -1.2578, -0.3145],
        [-0.0986,  1.8438, -0.4453, -1.0938, -0.5430],
        [ 0.0845,  0.9805, -0.2090, -0.8633, -0.8086],
        [ 0.0222,  1.6641, -0.7773, -0.9961, -0.7305],
        [-0.0103,  1.4766, -0.1895, -0.7617, -0.5859],
        [ 0.1240,  1.3594, -0.6562, -1.0234, -0.7461],
        [-0.2109,  1.2812, -0.9414, -0.9023, -0.6328]], device='cuda:0',
       dtype=torch.bfloat16, grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)
        [ 0.2715,  1.7109, -0.5469, -1.0312, -0.3789],0e-01, -4.5312e-01],, attentions=None)
        [-0.1797,  1.3281, -0.4609, -0.4941, -0.6719],0e-01, -4.5312e-01],, attentions=None)
        [ 0.3203,  1.2812, -0.6797, -0.8164, -0.1152]], device='cuda:0',
       dtype=torch.bfloat16, grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)
out(train): SequenceClassifierOutput(loss=tensor(0.9404, device='cuda:0', grad_fn=<NllLossBackward0>), logits=tensor([[ 0.0952,  1.4141, -0.6641, -0.4219, -0.3301],
        [ 0.0400,  1.3672, -0.6562, -0.8398, -0.5938],
        [ 0.0107,  1.5469, -0.8125, -1.0547, -0.4629],
        [-0.0610,  1.5234, -0.4590, -0.9141, -0.7539],
        [ 0.1226,  1.3672, -0.5938, -0.6953, -0.5547],
        [-0.0679,  1.6953, -0.4238, -0.8477, -0.5352],
        [ 0.3184,  1.6172, -0.4023, -0.6367, -0.5938],
        [ 0.3887,  1.2500, -0.7578, -0.9688, -0.4844],
        [-0.1602,  1.2422, -0.2754, -0.7930, -0.7461],
        [-0.3633,  1.2578, -0.5508, -0.5430, -0.6914],
        [ 0.0188,  0.8086, -0.9219, -0.6992, -0.0381],
        [ 0.1699,  1.3516, -0.7539, -0.7422, -0.4434],
        [-0.3262,  1.4141, -0.3965, -0.8711, -0.3887],
        [ 0.0723,  1.5156, -0.5625, -0.9336, -0.7930],
        [-0.1270,  1.4766, -0.6172, -0.8750, -0.9102],
        [-0.0840,  1.4219, -0.7031, -0.8516, -0.7109]], device='cuda:0',
       dtype=torch.bfloat16, grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)
        [-0.1797,  1.3281, -0.4609, -0.4941, -0.6719],0e-01, -4.5312e-01],, attentions=None)
        [ 0.0513,  1.6719, -0.6328, -0.9492, -0.6562],0e-01, -4.5312e-01],, attentions=None)
        [ 0.3633,  0.8438, -0.7383, -0.9102, -0.4570]], device='cuda:0',
       dtype=torch.bfloat16, grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)
out(train): SequenceClassifierOutput(loss=tensor(0.7914, device='cuda:0', grad_fn=<NllLossBackward0>), logits=tensor([[ 0.1631,  1.6328, -0.7773, -0.7031, -0.7539],
        [ 0.2363,  1.2578, -0.5859, -0.5625, -0.2578],
        [ 0.2129,  2.0000, -0.8398, -0.7891, -0.5234],
        [ 0.0240,  1.2266, -0.6641, -0.6484, -0.7422],
        [-0.1143,  1.4766, -0.7578, -1.0859, -0.2715],
        [ 0.0869,  1.4609, -0.7617, -0.8789, -0.2637],
        [ 0.0347,  1.7969, -0.5352, -0.8828, -0.4531],
        [-0.0957,  1.6406, -0.6719, -0.8281, -0.3770],
        [ 0.1660,  1.1641, -0.7383, -0.6250, -0.1318],
        [ 0.3809,  1.1328, -0.6289, -0.6641, -0.6719],
        [ 0.0693,  1.2656, -0.8867, -1.0078, -0.3828],
        [ 0.1299,  1.7500, -0.4570, -0.6445, -0.7070],
        [ 0.2139,  1.2578, -0.6875, -0.6562, -0.6797],
        [-0.0195,  1.1719, -0.3203, -1.0469, -0.6719],
        [-0.1680,  1.0703, -0.5234, -0.7773, -0.6836],
        [-0.4004,  1.5703, -0.3789, -1.0078, -0.4492]], device='cuda:0',
       dtype=torch.bfloat16, grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)
out(train): SequenceClassifierOutput(loss=tensor(0.9064, device='cuda:0', grad_fn=<NllLossBackward0>), logits=tensor([[-0.0869,  1.4141, -0.6953, -0.9844, -0.7109],
        [-0.0801,  1.3594, -0.6445, -1.0234, -0.7188],
        [ 0.2656,  1.6172, -0.6445, -0.9141, -0.4609],
        [-0.0300,  1.1797, -0.7461, -0.7656, -0.4258],
        [ 0.0850,  1.6562, -0.6250, -0.5742, -0.3145],
        [-0.1973,  1.4922, -0.5703, -0.8320, -0.6875],
        [-0.0330,  1.5312, -0.7227, -0.7422, -0.5859],
        [ 0.2891,  1.3594, -0.4922, -0.6289, -0.6211],
        [ 0.3633,  1.6562, -0.8359, -0.6602, -0.3984],
        [ 0.1367,  1.6641, -0.7070, -0.9102, -0.6055],
        [ 0.0840,  1.4922, -0.5312, -0.8516, -0.5430],
        [ 0.3965,  0.9844, -0.7852, -0.7188, -0.4141],
        [-0.0141,  1.3125, -0.5781, -0.6055, -0.4727],
        [-0.0179,  1.4375, -0.7227, -0.9609, -0.5664],
        [ 0.0132,  1.1797, -0.4766, -0.3281, -0.4355],
        [ 0.0145,  1.4609, -0.6133, -0.6133, -0.3828]], device='cuda:0',
       dtype=torch.bfloat16, grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)
out(train): SequenceClassifierOutput(loss=tensor(0.9740, device='cuda:0', grad_fn=<NllLossBackward0>), logits=tensor([[ 1.4160e-01,  1.2109e+00, -5.5078e-01, -1.0391e+00, -9.3359e-01],
        [-5.0781e-02,  1.4297e+00, -6.0547e-01, -5.3125e-01, -6.7969e-01],
        [ 9.1797e-02,  1.3359e+00, -5.7031e-01, -8.5938e-01, -4.8242e-01],
        [-8.9844e-02,  1.2266e+00, -5.6641e-01, -1.1484e+00, -7.0703e-01],
        [-2.2070e-01,  1.4375e+00, -2.2559e-01, -8.2031e-01, -3.4570e-01],
        [-2.4512e-01,  1.4219e+00, -6.3672e-01, -9.1406e-01, -4.7461e-01],
        [-2.7466e-02,  1.4766e+00, -5.3906e-01, -1.0703e+00, -5.3906e-01],
        [ 1.0449e-01,  1.3906e+00, -4.0625e-01, -8.7109e-01, -3.3398e-01],
        [-6.0547e-02,  1.4844e+00, -8.5156e-01, -8.1250e-01, -6.1328e-01],
        [ 1.3046e-03,  1.1953e+00, -1.3965e-01, -6.6406e-01, -7.7734e-01],
        [-1.1572e-01,  1.6250e+00, -4.1992e-01, -7.7344e-01, -5.3516e-01],
        [-8.4473e-02,  1.1016e+00, -6.8750e-01, -9.8438e-01, -7.1875e-01],
        [-1.1182e-01,  1.7266e+00, -6.3672e-01, -1.1328e+00, -4.9609e-01],
        [-9.8145e-02,  1.3750e+00, -4.8438e-01, -8.5938e-01, -2.2852e-01],
        [ 7.2266e-02,  1.6953e+00, -7.7734e-01, -1.0156e+00, -6.1719e-01],
        [-4.3164e-01,  1.4375e+00, -5.8594e-01, -1.0156e+00, -7.3047e-01]],
       device='cuda:0', dtype=torch.bfloat16, grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)
out(train): SequenceClassifierOutput(loss=tensor(0.7008, device='cuda:0', grad_fn=<NllLossBackward0>), logits=tensor([[ 0.0505,  1.4688, -0.7773, -1.0391, -0.3789],
        [-0.2656,  1.3359, -0.6719, -0.7266, -0.5938],
        [-0.0659,  1.6641, -0.3203, -1.0312, -0.7188],
        [ 0.1680,  1.4375, -0.6328, -0.6914, -0.2334],
        [-0.0125,  1.7500, -0.6641, -0.9961, -0.5430],
        [ 0.0942,  1.1094, -0.3750, -0.7188, -0.0603],
        [ 0.0130,  1.5312, -0.7891, -1.1094, -0.3750],
        [ 0.1011,  1.5000, -0.8711, -1.1172, -0.2295],
        [-0.2246,  1.0234, -0.4883, -1.0469, -0.7383],
        [ 0.1318,  1.5625, -0.6562, -1.1719, -0.6758],
        [-0.1167,  1.4219, -0.3496, -0.6016, -0.5195],
        [ 0.3516,  1.4375, -0.5820, -1.0000, -0.5117],
        [ 0.1963,  1.5078, -0.7930, -0.5234, -0.5430],
        [ 0.0884,  1.6484, -0.7305, -0.9844, -0.2002],
        [-0.0100,  1.6328, -0.7148, -0.8867, -0.5664],
        [-0.0476,  1.7656, -0.4453, -0.9297, -0.7344]], device='cuda:0',
       dtype=torch.bfloat16, grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)
        [ 0.0513,  1.6719, -0.6328, -0.9492, -0.6562],0e-01, -4.5312e-01],, attentions=None)
        [-0.0923,  1.5625, -0.5469, -0.8242, -0.7031],0e-01, -4.5312e-01],, attentions=None)
        [-0.2520,  1.5312, -0.5234, -0.7695, -0.6758],
        [ 0.0334,  1.2422, -0.3164, -0.6562, -0.3516]], device='cuda:0',
       dtype=torch.bfloat16, grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)
out(train): SequenceClassifierOutput(loss=tensor(0.5568, device='cuda:0', grad_fn=<NllLossBackward0>), logits=tensor([[-0.1865,  1.6484, -0.4961, -0.9102, -0.5703],
        [-0.2070,  1.3828, -0.3516, -1.0625, -0.6250],
        [ 0.3281,  1.1016, -0.4238, -0.6523, -0.4570],
        [ 0.0649,  1.6875, -0.8711, -1.0156, -0.5195],
        [ 0.1260,  1.7734, -0.3320, -0.5859, -0.7227],
        [ 0.0571,  1.4531, -0.7070, -0.8750, -0.5430],
        [ 0.3945,  1.6250, -0.5391, -0.5078, -0.3828],
        [-0.2021,  1.4922, -0.8906, -0.8086, -0.5586],
        [ 0.2227,  1.3828, -0.9570, -0.5117, -0.7852],
        [ 0.1206,  1.5156, -0.9414, -1.0938, -0.5156],
        [-0.2773,  1.5625, -0.5234, -0.9375, -0.4141],
        [-0.0977,  1.4453, -0.6406, -0.9492, -0.6211],
        [-0.1777,  1.5312, -0.5312, -0.9023, -0.3730],
        [ 0.1240,  1.7344, -0.3789, -0.7773, -0.6094],
        [ 0.0039,  1.5703, -0.6797, -0.8398, -0.7070],
        [-0.1309,  1.4609, -0.5508, -0.9062, -0.7578]], device='cuda:0',
       dtype=torch.bfloat16, grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)
out(train): SequenceClassifierOutput(loss=tensor(0.6865, device='cuda:0', grad_fn=<NllLossBackward0>), logits=tensor([[ 0.1816,  1.4453, -0.5547, -1.0234, -0.2500],
        [ 0.0605,  1.7656, -0.7188, -0.9570, -0.6445],
        [ 0.3867,  0.8086, -0.8984, -0.5117, -0.3828],
        [-0.1738,  1.1484, -0.4238, -0.9062, -0.6797],
        [ 0.3301,  1.2969, -0.7266, -0.9531, -0.4062],
        [ 0.2197,  0.9375, -0.4668, -0.7500, -0.6055],
        [ 0.0452,  1.8125, -0.5391, -0.8320, -0.4863],
        [ 0.0256,  1.3047, -0.5977, -1.0391, -0.3164],
        [-0.2559,  1.4766, -0.6133, -0.9258, -0.5195],
        [ 0.3008,  1.7031, -0.7461, -0.8633, -0.6289],
        [-0.1245,  1.5078, -0.8594, -0.8008, -0.3477],
        [-0.1426,  1.3750, -0.8242, -0.8281, -0.3770],
        [ 0.1406,  1.1016, -0.4297, -0.8359, -0.6250],
        [ 0.2266,  1.6016, -0.1611, -0.8047, -0.6992],
        [ 0.1270,  1.3984, -0.6953, -0.6445, -0.6523],
        [-0.0908,  1.2734, -0.3066, -0.4492, -0.8398]], device='cuda:0',
       dtype=torch.bfloat16, grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)
out(train): SequenceClassifierOutput(loss=tensor(0.5649, device='cuda:0', grad_fn=<NllLossBackward0>), logits=tensor([[ 0.3008,  1.7109, -0.5977, -0.5938, -0.5664],
        [ 0.2354,  1.1328, -0.8203, -1.0078, -0.3164],
        [-0.1885,  1.4219, -0.5508, -0.7070, -0.6016],
        [ 0.2812,  1.2500, -0.5977, -0.8164, -0.3828],
        [-0.1895,  1.2188, -0.6523, -0.8164, -0.6523],
        [ 0.1206,  0.8984, -0.5820, -0.3320, -0.5703],
        [-0.1050,  1.6641, -0.4590, -0.8594, -0.6055],
        [ 0.0277,  1.6797, -0.5391, -0.8398, -0.6094],
        [ 0.1953,  1.3594, -0.4512, -0.9180, -0.5938],
        [ 0.1855,  1.2422, -0.5781, -0.9141, -0.5234],
        [ 0.2812,  1.5625, -0.5547, -0.9766, -0.6055],
        [-0.2236,  1.3594, -0.5234, -0.9492, -0.7266],
        [-0.2773,  1.3516, -0.3418, -0.7734, -0.6680],
        [-0.1826,  1.3516, -0.8867, -0.6250, -0.4316],
        [-0.0649,  1.4375, -0.7305, -0.9062, -0.3145],
        [ 0.1162,  1.7578, -0.6719, -0.7227, -0.5938]], device='cuda:0',
       dtype=torch.bfloat16, grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)
        [-0.0923,  1.5625, -0.5469, -0.8242, -0.7031],0e-01, -4.5312e-01],, attentions=None)
        [ 1.5723e-01,  1.2109e+00, -9.7656e-01, -1.1641e+00, -5.8838e-02],, attentions=None)
        [ 1.0547e-01,  1.2031e+00, -7.5000e-01, -8.0469e-01, -7.5781e-01]],
       device='cuda:0', dtype=torch.bfloat16, grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)
out(train): SequenceClassifierOutput(loss=tensor(0.6039, device='cuda:0', grad_fn=<NllLossBackward0>), logits=tensor([[-0.1523,  1.4922, -0.7422, -1.2969, -0.5273],
        [ 0.1660,  1.3438, -0.7500, -0.8359, -0.3906],
        [-0.0067,  1.6328, -0.7734, -1.1797, -0.4746],
        [-0.0425,  1.9453, -0.8320, -1.1719, -0.5117],
        [ 0.4141,  1.5859, -0.8789, -1.0547, -0.4863],
        [-0.0630,  1.5391, -0.4883, -1.0234, -0.6367],
        [-0.0356,  1.7109, -0.7422, -0.9102, -0.6055],
        [-0.1719,  1.5781, -0.4180, -0.7656, -0.7344],
        [-0.1504,  1.5234, -0.2295, -0.8828, -0.6875],
        [-0.0859,  1.5938, -0.5664, -0.7891, -0.6367],
        [ 0.2109,  1.2266, -0.8164, -0.9102, -0.5195],
        [-0.0908,  1.2891, -0.4512, -0.7344, -0.8984],
        [-0.0223,  0.9688, -0.5156, -0.8672, -0.9961],
        [ 0.1914,  1.7734, -0.6445, -0.9883, -0.6328],
        [ 0.5352,  1.6797, -0.7656, -0.7461, -0.5625],
        [-0.1279,  1.5781, -0.9141, -0.9727, -0.4062]], device='cuda:0',
       dtype=torch.bfloat16, grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)
out(train): SequenceClassifierOutput(loss=tensor(0.7213, device='cuda:0', grad_fn=<NllLossBackward0>), logits=tensor([[-0.1069,  1.2969, -0.5391, -1.1016, -0.7344],
        [-0.1562,  1.4922, -0.4902, -0.8398, -0.5352],
        [-0.0206,  1.2031, -0.3926, -0.8438, -0.6250],
        [ 0.0134,  1.6484, -0.3223, -0.6211, -0.4766],
        [ 0.1377,  1.3594, -0.6406, -0.7930, -0.5898],
        [ 0.3750,  0.8867, -0.7773, -0.9492, -0.2256],
        [ 0.0986,  1.6641, -0.7734, -1.2500, -0.5195],
        [ 0.2256,  1.5078, -0.4121, -0.9180, -0.6758],
        [ 0.0957,  1.1875, -0.5781, -0.9844, -0.4629],
        [ 0.0884,  0.8945, -0.2305, -0.4727, -0.5586],
        [-0.0496,  1.4375, -0.3125, -1.0000, -0.3066],
        [-0.2148,  1.4922, -0.8633, -1.0469, -0.5195],
        [ 0.6562,  0.3555, -1.1484, -0.6445,  0.4199],
        [-0.1309,  1.2266, -0.5820, -0.7344, -0.5977],
        [ 0.1138,  1.6641, -0.3281, -0.7969, -0.6836],
        [-0.0103,  1.4844, -0.5273, -1.0781, -0.7969]], device='cuda:0',
       dtype=torch.bfloat16, grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)
out(train): SequenceClassifierOutput(loss=tensor(0.7565, device='cuda:0', grad_fn=<NllLossBackward0>), logits=tensor([[ 0.1641,  1.1406, -0.9492, -0.5859, -0.1455],
        [ 0.0139,  1.6406, -0.2363, -1.2891, -0.4238],
        [ 0.0238,  1.6250, -0.5820, -0.9688, -0.6953],
        [-0.0923,  1.2812, -0.3242, -0.9805, -0.4043],
        [ 0.0613,  1.3672, -0.8047, -0.9023, -0.4648],
        [-0.0369,  1.5547, -0.5859, -0.9609, -0.6875],
        [ 0.1562,  1.3594, -0.6758, -1.1719, -0.5586],
        [-0.0767,  1.4688, -0.6562, -1.2109, -0.5078],
        [ 0.0972,  1.7422, -0.2158, -1.1406, -0.4551],
        [ 0.1357,  1.1719, -0.3984, -0.8789, -0.7539],
        [ 0.0442,  1.5000, -0.6289, -0.9766, -0.5195],
        [ 0.3066,  1.2812, -0.6250, -0.9883, -0.4375],
        [-0.1768,  1.1406, -0.7148, -0.7695, -0.9883],
        [ 0.1504,  1.4922, -0.3262, -0.8984, -0.5703],
        [-0.2734,  1.6641, -0.4727, -1.2109, -0.4570],
        [ 0.0054,  1.2578, -0.5938, -0.9453, -0.3750]], device='cuda:0',
       dtype=torch.bfloat16, grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)
out(train): SequenceClassifierOutput(loss=tensor(1.1124, device='cuda:0', grad_fn=<NllLossBackward0>), logits=tensor([[-0.0197,  1.3984, -0.4824, -0.8398, -0.8945],
        [ 0.0176,  1.4531, -0.9375, -0.9648, -0.5039],
        [ 0.0226,  1.6016, -0.7109, -0.7305, -0.6094],
        [-0.1523,  1.4453, -0.5859, -0.8438, -0.3340],
        [-0.2041,  1.1562, -0.4609, -0.9727, -0.4707],
        [-0.1758,  1.5781, -0.5469, -0.6719, -0.3301],
        [-0.2275,  1.6172, -0.6406, -0.8945, -0.5547],
        [ 0.1562,  1.2422, -0.1924, -0.7930, -0.7188],
        [ 0.0613,  1.6328, -0.5078, -0.8086, -0.8789],
        [ 0.3145,  1.7188, -0.5938, -1.1953, -0.4277],
        [ 0.1250,  1.7344, -0.7734, -0.8711, -0.5547],
        [ 0.0525,  1.3203, -0.7656, -0.8125, -0.6484],
        [ 0.1270,  1.6094, -0.8672, -0.9805, -0.4883],
        [ 0.0120,  1.4453, -0.4355, -0.3438, -0.4629],
        [ 0.3281,  1.2891, -0.2178, -0.3398, -0.4688],
        [-0.2236,  1.4609, -0.5820, -0.7812, -0.2480]], device='cuda:0',
       dtype=torch.bfloat16, grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)
        [ 1.5723e-01,  1.2109e+00, -9.7656e-01, -1.1641e+00, -5.8838e-02],, attentions=None)
        [-0.0130,  1.4922, -0.7734, -0.8281, -0.4980],1e+00, -5.8838e-02],, attentions=None)
        [ 0.0143,  1.7109, -0.7656, -1.0391, -0.5781]], device='cuda:0',
       dtype=torch.bfloat16, grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)
out(train): SequenceClassifierOutput(loss=tensor(0.7469, device='cuda:0', grad_fn=<NllLossBackward0>), logits=tensor([[-0.0913,  1.6406, -0.7305, -0.8945, -0.3691],
        [-0.1484,  1.5859, -0.5078, -0.8906, -0.5742],
        [-0.1357,  1.7266, -0.6484, -1.0469, -0.6445],
        [-0.0728,  1.6641, -0.7617, -0.9102, -0.3594],
        [ 0.1191,  1.0156, -0.7109, -0.9961, -0.0043],
        [-0.1475,  1.8125, -0.7227, -0.6562, -0.6172],
        [ 0.0781,  1.5391, -0.4844, -1.0781, -0.6406],
        [ 0.1963,  1.5859, -0.6211, -0.7188, -0.4688],
        [-0.2178,  1.4688, -0.5820, -0.8086, -0.3711],
        [-0.2891,  1.5703, -0.3379, -1.0156, -0.5898],
        [-0.0879,  1.3281, -0.5508, -0.8320, -0.6250],
        [ 0.0162,  1.5312, -0.5625, -0.9023, -0.5352],
        [-0.0269,  1.4922, -0.5664, -0.9883, -0.5742],
        [-0.1787,  1.6406, -0.6406, -0.9023, -0.8008],
        [-0.4531,  1.2578, -0.4258, -0.7852, -0.6680],
        [-0.1660,  1.5469, -0.8203, -1.0781, -0.6289]], device='cuda:0',
       dtype=torch.bfloat16, grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)
out(train): SequenceClassifierOutput(loss=tensor(0.7238, device='cuda:0', grad_fn=<NllLossBackward0>), logits=tensor([[ 0.1230,  1.7422, -0.5898, -0.8945, -0.5469],
        [-0.0317,  1.5391, -0.4980, -0.9766, -0.4395],
        [-0.0432,  1.3594, -0.5273, -0.9844, -0.5898],
        [ 0.3008,  1.8438, -0.3086, -0.7227, -0.6211],
        [-0.2637,  1.7109, -0.4941, -1.1250, -0.8828],
        [ 0.0884,  1.4141, -0.8164, -0.8203, -0.7070],
        [-0.1523,  1.4688, -0.7383, -0.7695, -0.6484]], device='cuda:0',
       dtype=torch.bfloat16, grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)
        [-0.0130,  1.4922, -0.7734, -0.8281, -0.4980],1e+00, -5.8838e-02],, attentions=None)
W0629 11:52:52.467000 139012379337344 torch/_dynamo/convert_frame.py:824]     result = inner_convert(
W0629 11:52:52.467000 139012379337344 torch/_dynamo/convert_frame.py:824]     result = inner_convert(
W0629 11:52:52.467000 139012379337344 torch/_dynamo/convert_frame.py:824]     result = inner_convert(
W0629 11:52:52.467000 139012379337344 torch/_dynamo/convert_frame.py:824]     result = inner_convert(
W0629 11:52:52.467000 139012379337344 torch/_dynamo/convert_frame.py:824]     result = inner_convert(
W0629 11:52:52.467000 139012379337344 torch/_dynamo/convert_frame.py:824]     result = inner_convert(
W0629 11:52:52.467000 139012379337344 torch/_dynamo/convert_frame.py:824]     result = inner_convert(
        [ 0.1523,  1.5078, -0.3711, -0.8867, -0.3477],1e+00, -5.8838e-02],, attentions=None)
        [ 0.1699,  1.1797, -0.6562, -1.0625, -0.8828],
        [ 0.1855,  1.3672, -0.4473, -1.0391, -0.5664],
        [-0.0679,  1.3125, -0.5820, -1.1016, -0.5352]], device='cuda:0',
       dtype=torch.bfloat16, grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)
out(train): SequenceClassifierOutput(loss=tensor(1.0846, device='cuda:0', grad_fn=<NllLossBackward0>), logits=tensor([[ 0.3281,  1.4922, -1.1172, -0.7617, -0.1895],
        [-0.3105,  1.4609, -0.3301, -1.0000, -0.9297],
        [-0.0311,  1.1562, -0.5781, -1.0625, -0.4141],
        [ 0.0302,  1.5312, -0.5078, -1.1250, -0.9062],
        [ 0.4473,  1.1328, -0.6406, -0.9336, -0.7344],
        [-0.2363,  1.4688, -0.5039, -0.7344, -0.7539],
        [-0.1748,  1.6328, -0.4863, -0.6328, -0.5742],
        [-0.1445,  1.4609, -0.3086, -0.6016, -0.8633],
        [ 0.0669,  1.0938, -0.4688, -0.5195, -0.7266],
        [-0.0322,  1.5078, -0.3535, -0.9375, -0.5234],
        [ 0.1069,  1.3281, -0.5820, -0.7109, -0.7344],
        [-0.2754,  0.9375, -0.7539, -1.0078, -0.9258],
        [ 0.1299,  1.2422, -0.5547, -1.1016, -0.8164],
        [-0.0850,  1.5781, -0.3320, -0.7812, -0.4141],
        [-0.0623,  1.0938, -0.7109, -1.2031, -0.4551],
        [-0.1680,  1.3672, -0.2637, -0.6680, -0.5898]], device='cuda:0',
       dtype=torch.bfloat16, grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)
        [ 0.1523,  1.5078, -0.3711, -0.8867, -0.3477],1e+00, -5.8838e-02],, attentions=None)
        [-0.2363,  1.4062, -0.3828, -1.1484, -0.8555],1e+00, -5.8838e-02],, attentions=None)
        [-0.3125,  1.9297, -0.3926, -1.1172, -0.7461]], device='cuda:0',
       dtype=torch.bfloat16, grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)
out(train): SequenceClassifierOutput(loss=tensor(0.7126, device='cuda:0', grad_fn=<NllLossBackward0>), logits=tensor([[-5.1758e-02,  1.6797e+00, -6.3281e-01, -8.6328e-01, -6.4453e-01],
        [-3.1494e-02,  1.0859e+00, -2.8516e-01, -1.0078e+00, -8.1250e-01],
        [ 1.7480e-01,  1.4453e+00, -4.6875e-01, -6.6016e-01, -7.6172e-01],
        [ 2.7100e-02,  1.5312e+00, -6.0938e-01, -1.2109e+00, -4.8828e-01],
        [-1.1063e-03,  1.5000e+00, -7.0703e-01, -7.1875e-01, -5.8984e-01],
        [ 2.0117e-01,  1.3203e+00, -5.7422e-01, -1.0000e+00, -4.4141e-01],
        [-5.4199e-02,  1.2188e+00,  2.3560e-02, -1.1016e+00, -6.7969e-01],
        [-8.9844e-02,  1.3594e+00, -5.8594e-01, -9.0625e-01, -4.6289e-01],
        [-2.6953e-01,  1.2266e+00, -5.3516e-01, -8.9844e-01, -6.6797e-01],
        [-1.4526e-02,  1.2500e+00, -5.6641e-01, -9.8438e-01, -5.7031e-01],
        [ 6.9824e-02,  1.6250e+00, -4.1016e-01, -9.8438e-01, -5.3125e-01],
        [-3.1738e-02,  1.6094e+00, -8.3594e-01, -1.3203e+00, -6.6406e-01],
        [-1.2207e-01,  1.4688e+00, -5.8594e-01, -8.2812e-01, -2.8516e-01],
        [ 5.5078e-01,  7.8125e-01, -8.6719e-01, -7.6953e-01,  2.1680e-01],
        [ 3.7109e-01,  1.4609e+00, -6.4844e-01, -8.3594e-01, -6.0938e-01],
        [-1.4062e-01,  1.2812e+00, -8.1543e-02, -6.8359e-01, -6.3281e-01]],
       device='cuda:0', dtype=torch.bfloat16, grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)
        [-0.2363,  1.4062, -0.3828, -1.1484, -0.8555],1e+00, -5.8838e-02],, attentions=None)
        [ 0.0466,  1.5781, -0.6484, -0.7422, -0.3984],1e+00, -5.8838e-02],, attentions=None)
        [ 0.1118,  1.2969, -0.5742, -0.5938, -0.5664]], device='cuda:0',
       dtype=torch.bfloat16, grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)
out(train): SequenceClassifierOutput(loss=tensor(0.8766, device='cuda:0', grad_fn=<NllLossBackward0>), logits=tensor([[ 0.2539,  1.3516, -0.4414, -0.5938, -1.0156],
        [-0.0359,  1.1641, -0.1162, -0.9023, -0.9219],
        [ 0.2461,  1.5469, -0.8008, -0.7383, -0.3613],
        [ 0.2734,  1.4297, -0.3926, -0.8984, -0.9727],
        [-0.0486,  1.6250, -0.5039, -0.7773, -0.7812],
        [ 0.0918,  1.0391, -0.3066, -0.8359, -0.7188],
        [ 0.1855,  1.3125, -0.4980, -0.6875, -0.6367],
        [ 0.1094,  1.4922, -0.8203, -0.7578, -0.2637],
        [-0.0405,  1.5000, -0.6406, -0.8945, -0.4434],
        [ 0.2637,  1.7891, -0.6523, -0.6562, -0.5195],
        [ 0.1357,  1.3438, -0.5430, -0.9766, -0.9453],
        [-0.2402,  1.2188, -0.7422, -0.6289, -0.6641],
        [ 0.1680,  1.1875, -0.5195, -0.9414, -0.5781],
        [-0.0325,  1.6328, -0.4492, -0.8438, -0.5664],
        [ 0.0287,  1.6328, -0.7227, -0.6953, -0.3867],
        [ 0.2598,  2.0156, -0.7578, -1.1250, -0.3320]], device='cuda:0',
       dtype=torch.bfloat16, grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)
out(train): SequenceClassifierOutput(loss=tensor(0.7887, device='cuda:0', grad_fn=<NllLossBackward0>), logits=tensor([[ 0.1157,  1.7734, -1.0625, -1.0156, -0.7227],
        [ 0.3086,  1.6172, -0.6641, -0.4668, -0.6875],
        [-0.1299,  1.1016, -0.9062, -1.0156, -0.7383],
        [-0.2715,  1.4844, -0.4453, -0.7461, -0.5039],
        [-0.0267,  1.7266, -0.1621, -0.7773, -0.8086],
        [ 0.2812,  1.2578, -0.2305, -0.7891, -0.3691],
        [ 0.3320,  1.5781, -0.9375, -0.6953, -0.4043],
        [ 0.5820,  1.4453, -0.6289, -0.7109, -0.8906],
        [-0.0854,  1.6016, -0.8672, -0.9023, -0.6875],
        [ 0.1904,  1.5703, -0.3672, -0.7773, -0.8867],
        [ 0.1226,  1.6250, -0.8086, -0.9375, -0.5156],
        [-0.0859,  1.3750, -0.5703, -0.8242, -0.5625],
        [ 0.2715,  0.7578, -0.9805, -0.2988,  0.1992],
        [-0.2500,  1.2891, -0.6914, -0.3672, -0.5430],
        [ 0.0747,  1.4609, -0.4453, -1.0156, -0.7773],
        [ 0.1055,  1.5312, -0.8984, -1.0781, -0.4629]], device='cuda:0',
       dtype=torch.bfloat16, grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)
        [ 0.0466,  1.5781, -0.6484, -0.7422, -0.3984],1e+00, -5.8838e-02],, attentions=None)
        [-0.3652,  1.3125, -0.7227, -0.7539, -0.6445],1e+00, -5.8838e-02],, attentions=None)
        [-0.4121,  1.3750, -0.1875, -1.2031, -0.9102]], device='cuda:0',
       dtype=torch.bfloat16, grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)
out(train): SequenceClassifierOutput(loss=tensor(0.5126, device='cuda:0', grad_fn=<NllLossBackward0>), logits=tensor([[-0.0322,  1.9219, -0.6641, -0.6484, -0.5742],
        [-0.2930,  1.2422, -0.6172, -0.4883, -0.4277],
        [-0.0610,  1.7109, -0.5625, -1.1016, -0.4141],
        [ 0.2520,  1.5625, -0.5547, -0.8672, -0.5117],
        [-0.1562,  2.1562, -0.7656, -0.7109, -0.2109],
        [-0.2988,  1.3125, -0.6133, -0.6562, -0.6211],
        [ 0.1855,  1.3672, -0.6406, -1.0156, -0.6914],
        [ 0.0229,  1.5938, -0.5547, -0.8438, -0.5352],
        [-0.2432,  1.5469, -0.5234, -0.6094, -0.5352],
        [ 0.0679,  1.3750, -0.8164, -0.9023, -0.5859],
        [-0.0364,  1.4062, -0.5742, -0.9062, -0.6562],
        [ 0.2461,  1.6016, -0.9297, -0.9766, -0.2891],
        [ 0.1367,  1.8828, -0.5000, -1.1562, -0.3926],
        [ 0.0835,  1.7266, -0.7539, -1.1484, -0.5820],
        [ 0.0544,  1.4141, -1.0469, -0.7031, -0.6289],
        [ 0.1475,  1.3828, -0.9297, -1.1953, -0.4629]], device='cuda:0',
       dtype=torch.bfloat16, grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)
out(train): SequenceClassifierOutput(loss=tensor(0.7721, device='cuda:0', grad_fn=<NllLossBackward0>), logits=tensor([[ 0.2891,  1.5000, -0.6367, -1.2891, -0.6641],
        [-0.0874,  1.3672, -0.6289, -0.9883, -0.4355],
        [-0.0625,  1.3750, -0.5156, -0.8633, -0.7578],
        [-0.1748,  1.3516, -0.3516, -0.7969, -1.1875],
        [ 0.3848,  1.0703, -0.9336, -0.9258,  0.5039],
        [-0.1885,  1.5391, -0.1533, -0.6445, -0.7539],
        [-0.0036,  1.3359, -0.6172, -0.9062, -0.7383],
        [ 0.0083,  1.3828, -0.8047, -0.5742, -0.2637],
        [ 0.2109,  1.3750, -0.7578, -0.9414, -0.6406],
        [ 0.1797,  1.4844, -0.3398, -0.7930, -0.5742],
        [ 0.0239,  1.7188, -0.9609, -0.7461, -0.3984],
        [ 0.2373,  1.8359, -0.9258, -0.9336, -0.3613],
        [-0.0549,  1.4453, -0.5859, -1.1250, -0.5977],
        [ 0.1904,  1.3047, -0.9883, -0.9180, -0.5156],
        [ 0.0398,  1.0859, -0.5508, -0.5000, -0.4375],
        [ 0.1523,  1.6328, -0.6211, -0.9141, -0.6250]], device='cuda:0',
       dtype=torch.bfloat16, grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)
out(train): SequenceClassifierOutput(loss=tensor(0.6522, device='cuda:0', grad_fn=<NllLossBackward0>), logits=tensor([[-0.0791,  1.4375, -1.1250, -0.9141, -0.3711],
        [ 0.0742,  1.5625, -0.7812, -0.9258, -0.5781],
        [-0.0403,  1.6250, -0.8516, -0.6133, -0.4902],
        [-0.0576,  1.7422, -0.7734, -0.7695, -0.5547],
        [-0.0991,  1.5234, -0.5742, -0.6133, -0.4863],
        [ 0.1006,  1.6562, -0.8281, -0.9805, -0.5664],
        [ 0.4570,  1.1328, -0.5703, -0.9062, -0.4902],
        [-0.1318,  1.5625, -0.7500, -0.7930, -0.4551],
        [-0.1963,  1.4844, -0.5469, -0.6992, -0.2832],
        [ 0.3066,  1.5859, -0.7422, -0.8398, -0.3223],
        [ 0.2852,  1.5625, -0.9023, -0.9258, -0.5625],
        [-0.0084,  1.8828, -0.3965, -0.9805, -0.2598],
        [-0.0918,  1.4219, -0.9805, -0.8242, -0.3047],
        [-0.1040,  1.5312, -0.5742, -0.7148, -0.9062],
        [ 0.1631,  1.5312, -0.7148, -0.8320, -0.7461],
        [ 0.0089,  1.5781, -0.6016, -0.9453, -0.5195]], device='cuda:0',
       dtype=torch.bfloat16, grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)
out(train): SequenceClassifierOutput(loss=tensor(0.8853, device='cuda:0', grad_fn=<NllLossBackward0>), logits=tensor([[ 0.1025,  1.3125, -0.5586, -0.9688, -0.9648],
        [ 0.1992,  1.6641, -0.6484, -1.1641, -0.6914],
        [ 0.0464,  1.4609, -0.5391, -0.7188, -0.6875],
        [-0.1836,  1.6172, -0.5508, -0.9453, -0.3867],
        [ 0.0383,  1.4609, -0.5938, -0.8555, -0.4668],
        [-0.4141,  1.2500, -0.5156, -0.6602, -0.7148],
        [ 0.2695,  1.5391, -0.2354, -0.8984, -0.5430],
        [-0.1445,  1.1406, -0.6250, -0.6875, -0.7930],
        [ 0.0284,  1.6797, -0.7227, -0.9180, -0.7539],
        [ 0.0136,  1.3438, -0.5547, -0.6719, -0.7266],
        [-0.0461,  1.3047, -0.7773, -0.7461, -0.6523],
        [ 0.0830,  1.0312, -0.5234, -0.7539, -0.2109],
        [ 0.1055,  1.5078, -0.7227, -1.0000, -0.3340],
        [ 0.3066,  1.1094, -0.5547, -0.6523, -0.2354],
        [ 0.1318,  1.7109, -0.5898, -0.8164, -0.6797],
        [-0.1245,  1.3203, -0.8594, -0.7852, -0.5039]], device='cuda:0',
       dtype=torch.bfloat16, grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)
        [-0.3652,  1.3125, -0.7227, -0.7539, -0.6445],1e+00, -5.8838e-02],, attentions=None)
        [ 2.0312e-01,  1.3672e+00, -4.4727e-01, -5.4688e-01, -5.5469e-01],, attentions=None)
        [-2.3340e-01,  1.9688e+00, -6.6797e-01, -1.0234e+00, -6.4844e-01]],
       device='cuda:0', dtype=torch.bfloat16, grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)
out(train): SequenceClassifierOutput(loss=tensor(1.1155, device='cuda:0', grad_fn=<NllLossBackward0>), logits=tensor([[ 0.0913,  1.2812, -0.4980, -0.7344, -0.8008],
        [ 0.2266,  1.4219, -0.4844, -0.7539, -0.3965],
        [ 0.2871,  1.6484, -0.6133, -0.7812, -0.4609],
        [ 0.0640,  1.6797, -0.2793, -0.7852, -0.8164],
        [ 0.0913,  1.3672, -1.1250, -0.8164, -0.0259],
        [ 0.2061,  1.3672, -1.1328, -1.1484, -0.6406],
        [-0.1982,  1.5469, -0.7969, -0.7461, -0.6094],
        [-0.0586,  1.5000, -0.4258, -1.0859, -0.8242],
        [ 0.4434,  1.4453, -0.6094, -1.0625, -0.5039],
        [-0.2812,  1.2422, -0.4707, -0.7969, -0.6562],
        [-0.0378,  1.4609, -0.5977, -0.8672, -0.5977],
        [ 0.0474,  1.3516, -0.6914, -0.8438, -0.3379],
        [-0.1045,  1.6797, -0.6289, -0.7617, -0.5508],
        [-0.2012,  1.0938, -0.5469, -0.5547, -0.7617],
        [-0.2520,  1.6016, -0.6172, -0.8672, -0.4766],
        [ 0.0698,  1.1094, -0.5312, -0.8516, -0.7109]], device='cuda:0',
       dtype=torch.bfloat16, grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)
        [ 2.0312e-01,  1.3672e+00, -4.4727e-01, -5.4688e-01, -5.5469e-01],, attentions=None)
        [-0.0266,  1.4453, -0.9492, -0.8828, -0.7852],8e-01, -5.5469e-01],, attentions=None)
        [ 0.0403,  1.4297, -0.8516, -0.8359, -0.7852]], device='cuda:0',
       dtype=torch.bfloat16, grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)
out(train): SequenceClassifierOutput(loss=tensor(0.7964, device='cuda:0', grad_fn=<NllLossBackward0>), logits=tensor([[ 0.4922,  0.8203, -0.6250, -0.0581,  0.3906],
        [ 0.2275,  1.2891, -0.8906, -1.0625, -0.5469],
        [ 0.3926,  1.6797, -0.8320, -0.9102, -0.4434],
        [ 0.0439,  1.3750, -0.5703, -0.9023, -0.8594],
        [ 0.2217,  1.6172, -0.7227, -0.8086, -0.8008],
        [ 0.0050,  1.0469, -0.3926, -0.7148, -0.7266],
        [ 0.0708,  1.7578, -0.6758, -0.8320, -0.2988],
        [ 0.4316,  1.6250, -0.4082, -0.9414, -1.0156],
        [ 0.2812,  1.4297, -0.6523, -1.0000, -0.7422],
        [ 0.3027,  1.0000, -1.2422, -0.9727,  0.2910],
        [ 0.0762,  1.4219, -0.8672, -0.7031, -0.6133],
        [-0.0732,  1.5156, -0.7930, -0.9102, -0.6680],
        [ 0.0330,  1.7656, -0.4863, -0.8008, -0.3730],
        [-0.1807,  1.0156, -0.8477, -0.9805, -0.2598],
        [-0.1084,  1.4844, -0.4180, -0.7422, -0.6016],
        [ 0.3574,  1.5000, -0.5391, -0.6250, -0.3066]], device='cuda:0',
       dtype=torch.bfloat16, grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)
        [-0.0266,  1.4453, -0.9492, -0.8828, -0.7852],8e-01, -5.5469e-01],, attentions=None)
        [ 0.1455,  1.5469, -0.6719, -1.0859, -0.7383],8e-01, -5.5469e-01],, attentions=None)
        [ 0.2852,  0.7188, -1.0547, -0.3438,  0.2871]], device='cuda:0',
       dtype=torch.bfloat16, grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)
out(train): SequenceClassifierOutput(loss=tensor(0.6472, device='cuda:0', grad_fn=<NllLossBackward0>), logits=tensor([[-0.0085,  1.6875, -0.8281, -1.1719, -0.5273],
        [-0.4570,  1.4297, -0.5898, -0.7422, -0.5352],
        [-0.0791,  1.6016, -0.9883, -0.9375, -0.5664],
        [ 0.0728,  1.6797, -0.7070, -0.6484, -0.2285],
        [ 0.2109,  1.9375, -0.7695, -1.0156, -0.7461],
        [ 0.1030,  1.1875, -0.9023, -0.8398, -0.8711],
        [ 0.0269,  1.3047, -0.8750, -0.8203, -0.4688],
        [ 0.2490,  1.2266, -0.6328, -0.8867, -0.4121],
        [-0.2754,  1.4922, -0.5508, -0.7852, -0.4219],
        [ 0.2949,  1.5391, -0.8789, -0.9570, -0.3770],
        [-0.0052,  1.5078, -0.8164, -0.8906, -0.3652],
        [ 0.1621,  1.4688, -0.6523, -0.7500, -0.6016],
        [ 0.2080,  1.5469, -0.5312, -0.8086, -0.3281],
        [-0.2178,  1.5703, -0.6445, -1.0078, -0.8594],
        [ 0.0879,  1.5938, -0.5781, -1.1484, -0.5000],
        [ 0.1396,  1.0938, -0.4512, -1.0469, -0.4668]], device='cuda:0',
       dtype=torch.bfloat16, grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)
out(train): SequenceClassifierOutput(loss=tensor(0.6681, device='cuda:0', grad_fn=<NllLossBackward0>), logits=tensor([[ 0.0435,  1.5625, -0.6133, -0.9688, -0.7539],
        [ 0.3555,  1.6016, -0.4844, -0.5039, -0.5273],
        [ 0.0962,  1.9453, -0.8711, -1.1406, -0.7461],
        [-0.1133,  1.6406, -0.6094, -0.8516, -0.5469],
        [-0.0060,  1.2812, -0.8789, -0.9258, -0.4609],
        [ 0.1602,  1.3672, -0.9102, -0.9219, -0.5273],
        [-0.2500,  1.7500, -0.7422, -1.0312, -0.7852],
        [ 0.1768,  1.9062, -0.8750, -1.2812, -0.3691],
        [ 0.0040,  1.1016, -0.3789, -0.6992, -0.2969],
        [-0.0415,  1.2109, -0.7344, -0.8086, -0.8633],
        [-0.0535,  1.7031, -0.6914, -1.1953, -0.8047],
        [-0.0093,  1.9297, -0.5898, -0.6523, -0.5625],
        [ 0.1118,  1.7344, -1.0078, -0.9062, -0.8164],
        [-0.0078,  1.6406, -0.5469, -1.0859, -0.6406],
        [-0.5117,  1.5391, -0.5938, -0.4512, -0.7891],
        [ 0.1270,  1.6406, -0.5469, -0.8555, -0.5938]], device='cuda:0',
       dtype=torch.bfloat16, grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)
out(train): SequenceClassifierOutput(loss=tensor(0.8804, device='cuda:0', grad_fn=<NllLossBackward0>), logits=tensor([[ 3.1250e-01,  1.2188e+00, -7.0703e-01, -1.2109e+00, -4.9609e-01],
        [-2.0117e-01,  1.4766e+00, -7.8516e-01, -8.0078e-01, -8.2422e-01],
        [ 2.0215e-01,  1.3828e+00, -9.3359e-01, -1.1094e+00, -7.3438e-01],
        [ 1.0938e-01,  1.2891e+00, -7.5000e-01, -6.0938e-01, -3.1836e-01],
        [-1.1169e-02,  1.8828e+00, -4.5703e-01, -7.1875e-01, -7.8906e-01],
        [-4.0625e-01,  1.7734e+00, -7.3047e-01, -8.5547e-01, -8.0469e-01],
        [ 2.5586e-01,  1.4609e+00, -6.9922e-01, -7.1094e-01, -7.1094e-01],
        [ 1.6406e-01,  1.3281e+00, -7.5781e-01, -7.6953e-01, -6.5234e-01],
        [-1.2207e-01,  1.7188e+00, -5.1953e-01, -7.0703e-01, -4.7461e-01],
        [-1.9455e-03,  1.8438e+00, -9.7656e-01, -7.9688e-01, -5.8203e-01],
        [-1.0376e-03,  1.5938e+00, -7.1094e-01, -1.1172e+00, -6.8359e-01],
        [ 2.2168e-01,  1.2344e+00, -3.6133e-01, -8.7500e-01, -5.6641e-01],
        [-2.1484e-01,  1.3125e+00, -5.7031e-01, -7.6562e-01, -5.9375e-01],
        [-2.0898e-01,  1.4844e+00, -4.5117e-01, -8.6719e-01, -6.1328e-01],
        [ 1.8945e-01,  1.3984e+00, -4.7070e-01, -6.6406e-01, -3.8086e-01],
        [ 1.2451e-01,  1.5781e+00, -8.0859e-01, -7.3828e-01, -2.4414e-01]],
       device='cuda:0', dtype=torch.bfloat16, grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)
out(train): SequenceClassifierOutput(loss=tensor(0.9746, device='cuda:0', grad_fn=<NllLossBackward0>), logits=tensor([[ 2.7344e-01,  1.6484e+00, -3.3789e-01, -8.6719e-01, -7.3828e-01],
        [ 1.4160e-01,  1.2969e+00, -1.0703e+00, -7.5781e-01, -4.9023e-01],
        [-2.3145e-01,  1.6172e+00, -6.6406e-01, -8.7891e-01, -3.0273e-01],
        [-1.5918e-01,  1.3750e+00, -5.7422e-01, -9.3750e-01, -5.1172e-01],
        [ 7.9346e-03,  1.2734e+00, -7.1484e-01, -6.9531e-01, -7.5000e-01],
        [ 5.8838e-02,  1.2188e+00, -4.3555e-01, -7.2656e-01, -6.0938e-01],
        [ 6.0059e-02,  1.7109e+00, -8.5547e-01, -8.9062e-01, -4.5117e-01],
        [ 6.2500e-02,  1.4375e+00, -8.1250e-01, -1.0625e+00, -5.8984e-01],
        [-9.3750e-02,  1.7422e+00, -5.5859e-01, -6.6406e-01, -5.9766e-01],
        [ 2.4023e-01,  1.0859e+00, -5.8203e-01, -7.4219e-01, -1.0000e+00],
        [-6.9046e-04,  1.1953e+00, -6.5625e-01, -6.0156e-01, -3.3789e-01],
        [-2.6733e-02,  1.5000e+00, -5.6641e-01, -8.3984e-01, -4.3750e-01],
        [ 1.6602e-01,  1.6172e+00, -7.5781e-01, -9.2969e-01, -5.9375e-01],
        [-1.9824e-01,  1.4141e+00, -3.7500e-01, -6.5625e-01, -3.1641e-01],
        [ 1.3574e-01,  1.5703e+00, -6.9922e-01, -5.8594e-01, -1.8848e-01],
        [-3.6865e-02,  1.2891e+00, -6.5625e-01, -1.0078e+00, -3.9258e-01]],
       device='cuda:0', dtype=torch.bfloat16, grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)
        [ 0.1455,  1.5469, -0.6719, -1.0859, -0.7383],8e-01, -5.5469e-01],, attentions=None)
        [-0.0190,  1.7656, -0.5703, -1.0391, -0.7227],8e-01, -5.5469e-01],, attentions=None)
        [-0.1465,  1.6094, -0.3008, -0.5547, -0.7383]], device='cuda:0',
       dtype=torch.bfloat16, grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)
out(train): SequenceClassifierOutput(loss=tensor(0.5123, device='cuda:0', grad_fn=<NllLossBackward0>), logits=tensor([[ 0.4512,  0.9570, -0.7344, -0.9961, -0.6680],
        [-0.1934,  1.4453, -0.6055, -0.8906, -0.8984],
        [-0.1211,  1.3359, -0.5000, -0.9883, -0.6094],
        [ 0.0327,  1.4531, -0.7461, -0.8242, -0.6289],
        [ 0.0488,  1.3438, -0.6172, -0.6758, -0.8750],
        [ 0.0496,  1.5938, -0.7891, -0.9453, -0.3398],
        [-0.0972,  1.0938, -0.7500, -0.9414, -0.4746],
        [ 0.3555,  1.4141, -0.4355, -0.6367, -0.4492],
        [-0.0084,  1.7266, -0.8047, -0.8867, -0.4785],
        [ 0.0781,  1.3906, -0.8750, -1.0312, -0.6797],
        [ 0.0106,  1.4688, -0.6133, -1.1250, -0.4785],
        [-0.1689,  1.5391, -0.5625, -0.6641, -0.8516],
        [ 0.1504,  1.3828, -0.7109, -0.8594, -0.5977],
        [ 0.1216,  1.7812, -0.7383, -0.9414, -0.4922],
        [-0.0698,  1.6875, -0.5859, -0.9492, -0.7109],
        [-0.0255,  1.2188, -0.7227, -0.7578, -0.5820]], device='cuda:0',
       dtype=torch.bfloat16, grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)
out(train): SequenceClassifierOutput(loss=tensor(0.4972, device='cuda:0', grad_fn=<NllLossBackward0>), logits=tensor([[-0.3730,  1.2734, -0.8047, -0.6719, -0.5234],
        [ 0.1582,  1.9219, -0.3203, -0.9102, -0.6562],
        [ 0.1328,  1.3594, -0.7734, -0.5703, -0.5938],
        [-0.1147,  1.5859, -0.6758, -0.9492, -0.5391],
        [-0.2324,  1.6484, -0.5430, -0.5234, -0.5312],
        [ 0.0713,  1.5078, -0.8203, -0.3340, -0.3652],
        [ 0.4883,  1.6172, -0.7109, -0.7930, -0.3652],
        [ 0.2324,  1.3125, -0.9258, -1.0078, -0.7070],
        [ 0.2305,  1.5547, -0.7305, -0.7070, -0.4785],
        [-0.1807,  1.6641, -0.7539, -0.7930, -0.7422],
        [-0.2520,  1.5469, -0.5781, -0.9609, -0.5234],
        [ 0.0315,  1.3594, -0.7031, -0.6914, -0.3477],
        [-0.0811,  1.8750, -0.8320, -0.8984, -0.5156],
        [-0.0356,  1.8047, -0.7852, -0.8477, -0.4609],
        [ 0.1631,  1.6406, -0.5781, -0.8242, -0.5078],
        [-0.2139,  1.6641, -0.5742, -1.3203, -0.4199]], device='cuda:0',
       dtype=torch.bfloat16, grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)
out(train): SequenceClassifierOutput(loss=tensor(0.6538, device='cuda:0', grad_fn=<NllLossBackward0>), logits=tensor([[-0.2295,  1.5078, -0.6562, -0.7773, -0.4199],
        [ 0.0126,  1.5625, -0.7734, -1.1016, -0.6953],
        [ 0.1187,  1.3438, -0.7852, -0.7695, -0.8008],
        [ 0.2988,  1.1250, -0.6055, -0.9609, -0.7188],
        [-0.0287,  1.4219, -0.6016, -0.8320, -0.3926],
        [ 0.3223,  1.2969, -0.9258, -0.6602, -0.4531],
        [-0.2266,  1.7578, -0.2412, -0.6289, -0.7305],
        [ 0.1279,  1.7656, -0.8047, -0.7812, -0.5977],
        [-0.2832,  1.5078, -0.6875, -0.6484, -0.5859],
        [ 0.1816,  1.8672, -0.6523, -0.9570, -0.5469],
        [ 0.1426,  1.4922, -0.4219, -0.9531, -0.4199],
        [-0.1201,  1.7109, -0.7344, -1.0859, -0.4004],
        [ 0.0299,  1.2891, -0.5117, -0.8477, -0.4727],
        [-0.1011,  1.3359, -0.4707, -0.5781, -0.6836],
        [-0.0309,  1.5469, -0.6758, -1.1406, -0.5273],
        [-0.0403,  1.2188, -0.6602, -0.6406, -0.7695]], device='cuda:0',
       dtype=torch.bfloat16, grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)
out(train): SequenceClassifierOutput(loss=tensor(0.4987, device='cuda:0', grad_fn=<NllLossBackward0>), logits=tensor([[-1.3672e-01,  1.6484e+00, -3.1445e-01, -8.7891e-01, -4.7461e-01],
        [ 2.3340e-01,  1.0625e+00, -1.1172e+00, -7.2266e-01, -6.2500e-02],
        [ 5.5176e-02,  1.6484e+00, -6.6406e-01, -8.7109e-01, -5.8984e-01],
        [ 1.6895e-01,  1.3984e+00, -3.5352e-01, -9.1406e-01, -6.4062e-01],
        [-1.1328e-01,  1.4766e+00, -5.4688e-01, -9.1016e-01, -7.8125e-01],
        [ 5.0049e-03,  1.2109e+00, -7.2656e-01, -7.6562e-01, -6.4844e-01],
        [-1.6211e-01,  1.5000e+00, -4.0039e-01, -8.1250e-01, -8.1250e-01],
        [-3.9648e-01,  1.4844e+00, -8.3984e-01, -8.6719e-01, -7.6172e-01],
        [ 3.1738e-02,  1.5312e+00, -8.9062e-01, -7.5781e-01, -9.2188e-01],
        [ 1.5030e-03,  1.8906e+00, -7.3047e-01, -4.5312e-01, -4.0820e-01],
        [-3.6377e-02,  1.6094e+00, -3.6719e-01, -7.2656e-01, -7.7344e-01],
        [-1.5015e-02,  1.4062e+00, -4.5508e-01, -7.8125e-01, -5.0781e-01],
        [ 2.1484e-01,  1.5547e+00, -6.0938e-01, -9.0625e-01, -4.8438e-01],
        [-1.5918e-01,  1.3438e+00, -6.7969e-01, -7.2656e-01, -5.8594e-01],
        [ 4.9561e-02,  1.9688e+00, -8.1250e-01, -8.4766e-01, -3.5352e-01],
        [ 2.6172e-01,  1.8281e+00, -7.6953e-01, -7.8906e-01, -5.3125e-01]],
       device='cuda:0', dtype=torch.bfloat16, grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)
        [-0.0190,  1.7656, -0.5703, -1.0391, -0.7227],8e-01, -5.5469e-01],, attentions=None)
        [ 0.1836,  1.5391, -0.5898, -1.2031, -0.7617],8e-01, -5.5469e-01],, attentions=None)
        [ 0.8867,  0.9375, -1.1484, -1.0234, -0.0649],
        [ 0.1592,  1.2266, -0.4570, -0.9766, -0.8906]], device='cuda:0',
       dtype=torch.bfloat16, grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)
out(train): SequenceClassifierOutput(loss=tensor(0.5510, device='cuda:0', grad_fn=<NllLossBackward0>), logits=tensor([[ 0.2891,  1.6250, -0.6250, -1.0469, -0.5625],
        [-0.0217,  1.4297, -0.4648, -0.6992, -0.8906],
        [ 0.3828,  1.8047, -0.6914, -0.8633, -0.7461],
        [ 0.0684,  1.6562, -0.8867, -0.8555, -0.4961],
        [ 0.1367,  1.5391, -0.7891, -1.0078, -0.6602],
        [-0.0371,  1.6484, -0.7070, -1.4531, -0.9258],
        [ 0.1660,  1.7188, -0.7734, -1.0391, -0.4707],
        [ 0.1040,  1.5781, -0.6719, -0.8438, -0.7031],
        [-0.0908,  1.3906, -0.2891, -1.0156, -0.6016],
        [-0.0294,  1.7578, -0.7734, -1.0078, -0.5156],
        [ 0.3242,  1.8203, -0.6523, -1.0938, -0.7539],
        [ 0.0249,  1.4844, -0.5547, -0.7070, -0.7461],
        [ 0.1709,  1.1719, -1.1953, -1.1797, -0.6562],
        [ 0.5039,  1.4688, -0.5000, -1.1406, -0.6328],
        [ 0.2295,  1.4453, -1.1172, -1.0547, -0.4512],
        [ 0.1143,  1.6797, -0.9375, -1.0391, -0.5547]], device='cuda:0',
       dtype=torch.bfloat16, grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)
out(train): SequenceClassifierOutput(loss=tensor(0.7562, device='cuda:0', grad_fn=<NllLossBackward0>), logits=tensor([[ 3.7695e-01,  1.6250e+00, -6.0938e-01, -8.9062e-01, -4.6484e-01],
        [ 1.0645e-01,  1.6719e+00, -6.5234e-01, -1.3281e+00, -6.3281e-01],
        [ 2.9883e-01,  1.5781e+00, -5.1172e-01, -1.0156e+00, -6.2500e-01],
        [-3.0664e-01,  1.8281e+00, -3.5156e-01, -6.9141e-01, -6.3672e-01],
        [-5.2734e-02,  1.5312e+00, -7.6172e-01, -7.6172e-01, -3.2031e-01],
        [ 5.2002e-02,  1.5391e+00, -5.4688e-01, -8.4375e-01, -5.2734e-01],
        [ 2.9373e-04,  1.4062e+00, -5.5469e-01, -7.3828e-01, -2.9883e-01],
        [ 6.2500e-01,  1.3203e+00, -5.7422e-01, -9.0234e-01, -5.5859e-01],
        [ 7.0801e-02,  1.4688e+00, -4.3750e-01, -8.7109e-01, -4.1406e-01],
        [ 3.2616e-04,  1.3438e+00, -5.7422e-01, -5.3516e-01, -4.7266e-01],
        [ 9.5215e-02,  1.5391e+00, -1.8945e-01, -1.0547e+00, -6.5234e-01],
        [-1.1523e-01,  1.5391e+00, -8.6719e-01, -9.6094e-01, -3.5352e-01],
        [ 4.2383e-01,  5.7031e-01, -8.0469e-01, -3.8086e-01,  3.4180e-01],
        [ 2.0410e-01,  1.2734e+00, -2.5781e-01, -6.2891e-01, -6.2109e-01],
        [-2.7344e-01,  1.6719e+00, -5.1172e-01, -1.0859e+00, -7.8516e-01],
        [-1.7700e-02,  1.6719e+00, -4.1992e-01, -8.1250e-01, -9.2188e-01]],
       device='cuda:0', dtype=torch.bfloat16, grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)
out(train): SequenceClassifierOutput(loss=tensor(0.7759, device='cuda:0', grad_fn=<NllLossBackward0>), logits=tensor([[ 0.1504,  1.5938, -0.7031, -1.0547, -0.5117],
        [-0.3398,  1.3750, -0.3711, -0.8594, -0.2100],
        [-0.1416,  1.6641, -0.6914, -0.9414, -0.3828],
        [ 0.1602,  1.4062, -0.6602, -1.2656, -0.4121],
        [ 0.3047,  1.2656, -0.8906, -0.8711, -0.7344],
        [-0.0588,  1.7344, -0.4023, -1.0234, -0.6445],
        [ 0.2578,  1.1562, -0.7109, -0.9336, -0.4082],
        [ 0.1699,  1.6328, -0.9609, -0.6406, -0.5430],
        [ 0.1226,  1.6562, -0.1680, -0.7031, -0.6133],
        [ 0.2617,  1.4453, -0.6680, -1.0234, -0.7344],
        [ 0.0405,  1.8516, -0.5000, -1.1641, -0.2891],
        [ 0.1982,  1.4375, -0.5117, -1.1797, -0.6328],
        [ 0.0811,  1.2969, -0.6094, -1.0391, -0.6914],
        [ 0.1514,  1.3984, -0.4219, -0.8633, -0.6758],
        [-0.0593,  1.3359, -0.6328, -0.9453, -0.8086],
        [ 0.2188,  1.4219, -0.5586, -1.0781, -0.7930]], device='cuda:0',
       dtype=torch.bfloat16, grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)
out(train): SequenceClassifierOutput(loss=tensor(1.0898, device='cuda:0', grad_fn=<NllLossBackward0>), logits=tensor([[ 0.3750,  1.3125, -0.7383, -0.9219, -0.8672],
        [ 0.0815,  2.0625, -0.9570, -0.9727, -0.3945],
        [-0.0864,  1.4922, -0.7539, -0.9414, -0.4648],
        [-0.1094,  1.5547, -0.9922, -0.8164, -0.5859],
        [ 0.0801,  1.6562, -0.4395, -0.9648, -0.5664],
        [ 0.1387,  1.6328, -0.7812, -0.8125, -0.3086],
        [-0.0996,  1.8594, -0.6914, -0.8320, -0.4453],
        [ 0.2002,  1.5234, -0.6328, -0.7656, -0.4785],
        [ 0.0830,  1.5781, -0.2520, -0.9414, -0.7305],
        [-0.0177,  1.5000, -0.7422, -1.0000, -0.6836],
        [ 0.3691,  1.7031, -0.7305, -1.0312, -0.6523],
        [ 0.3984,  1.5391, -0.7578, -0.6562, -0.6836],
        [-0.2334,  1.7500, -0.5586, -0.9414, -0.3398],
        [ 0.2344,  1.1484, -0.4531, -0.5664, -0.6211],
        [-0.1689,  1.4453, -0.7500, -0.3789, -0.5352],
        [ 0.3691,  1.3750, -0.9375, -0.5938, -0.4004]], device='cuda:0',
       dtype=torch.bfloat16, grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)
        [ 0.1836,  1.5391, -0.5898, -1.2031, -0.7617],8e-01, -5.5469e-01],, attentions=None)
        [-0.1191,  1.5547, -0.7109, -0.5664, -0.4570],8e-01, -5.5469e-01],, attentions=None)
        [ 0.1035,  2.0312, -0.7109, -1.0391, -0.6250]], device='cuda:0',
       dtype=torch.bfloat16, grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)
out(train): SequenceClassifierOutput(loss=tensor(0.7916, device='cuda:0', grad_fn=<NllLossBackward0>), logits=tensor([[ 0.0371,  1.9062, -0.3281, -1.1016, -0.5977],
        [-0.1270,  1.4531, -0.5586, -1.1875, -0.5664],
        [ 0.1016,  1.6953, -0.7422, -0.8672, -0.3789],
        [ 0.0630,  1.6016, -0.8906, -1.0156, -0.6289],
        [ 0.1592,  0.6016, -0.9961, -0.5391,  0.7891],
        [-0.2178,  1.6641, -0.8633, -0.8711, -0.5703],
        [-0.0156,  1.3906, -0.6602, -0.9023, -0.7656],
        [ 0.0063,  1.3672, -0.7734, -0.8125, -0.1973],
        [-0.1016,  1.4375, -0.7812, -0.9297, -0.3516],
        [-0.0913,  1.6562, -0.6055, -1.2656, -0.4531],
        [ 0.1406,  1.6016, -0.6250, -0.7500, -0.6953],
        [ 0.0063,  1.7969, -0.6914, -0.9609, -0.5508],
        [ 0.0718,  1.7812, -0.6094, -1.0547, -0.8281],
        [ 0.0194,  1.5391, -0.9023, -0.8789, -0.8711],
        [-0.2500,  1.4062, -0.7031, -0.6328, -0.7148],
        [ 0.1021,  1.7734, -0.8438, -0.9961, -0.6133]], device='cuda:0',
       dtype=torch.bfloat16, grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)
out(train): SequenceClassifierOutput(loss=tensor(0.7263, device='cuda:0', grad_fn=<NllLossBackward0>), logits=tensor([[-0.1904,  1.5859, -0.7930, -0.7305, -0.5898],
        [-0.0986,  1.6094, -0.7500, -1.0391, -0.4844],
        [-0.0162,  1.6094, -0.3965, -1.0156, -0.9492],
        [ 0.1738,  1.5391, -0.6992, -0.8047, -0.5938],
        [-0.0811,  1.8359, -0.4941, -1.0312, -0.9492],
        [ 0.0311,  1.5547, -0.8242, -0.6055, -0.7070],
        [ 0.1709,  1.5469, -0.6836, -0.8984, -0.3887]], device='cuda:0',
       dtype=torch.bfloat16, grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)
        [-0.1191,  1.5547, -0.7109, -0.5664, -0.4570],8e-01, -5.5469e-01],, attentions=None)
W0629 11:52:52.467000 139012379337344 torch/_dynamo/convert_frame.py:824]     result = inner_convert(
W0629 11:52:52.467000 139012379337344 torch/_dynamo/convert_frame.py:824]     result = inner_convert(
W0629 11:52:52.467000 139012379337344 torch/_dynamo/convert_frame.py:824]     result = inner_convert(
W0629 11:52:52.467000 139012379337344 torch/_dynamo/convert_frame.py:824]     result = inner_convert(
W0629 11:52:52.467000 139012379337344 torch/_dynamo/convert_frame.py:824]     result = inner_convert(
W0629 11:52:52.467000 139012379337344 torch/_dynamo/convert_frame.py:824]     result = inner_convert(
W0629 11:52:52.467000 139012379337344 torch/_dynamo/convert_frame.py:824]     result = inner_convert(
        [ 0.0042,  1.7500, -0.2334, -0.8125, -0.5312],8e-01, -5.5469e-01],, attentions=None)
        [ 0.2471,  1.2578, -0.6875, -1.1094, -0.9648],
        [-0.2148,  1.3750, -0.6406, -0.7695, -0.8398],
        [ 0.1934,  1.4688, -0.5664, -0.8359, -0.8555]], device='cuda:0',
       dtype=torch.bfloat16, grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)
        [ 0.0042,  1.7500, -0.2334, -0.8125, -0.5312],8e-01, -5.5469e-01],, attentions=None)
        [-0.1729,  1.3750, -0.6992, -1.2734, -0.6328],8e-01, -5.5469e-01],, attentions=None)
        [-0.1504,  1.5703, -0.3887, -1.0547, -0.8164]], device='cuda:0',
       dtype=torch.bfloat16, grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)
out(train): SequenceClassifierOutput(loss=tensor(0.7252, device='cuda:0', grad_fn=<NllLossBackward0>), logits=tensor([[-0.0330,  1.3672, -0.4414, -0.7500, -0.3359],
        [ 0.2275,  1.4453, -0.5000, -0.8359, -0.6445],
        [ 0.1118,  1.6328, -0.4297, -1.0156, -0.9453],
        [-0.0569,  1.4453, -0.7031, -0.6367, -0.5312],
        [-0.2559,  1.6250, -0.3945, -0.9336, -0.7812],
        [ 0.1807,  1.5781, -0.4648, -0.9219, -0.8203],
        [ 0.2188,  1.5391, -0.4160, -1.0625, -1.0625],
        [ 0.0374,  1.3125, -0.6680, -1.0703, -0.7852],
        [ 0.2559,  1.4844, -0.1768, -0.8672, -0.8789],
        [-0.0374,  1.4141, -1.0547, -1.2031, -0.6328],
        [-0.2168,  1.1172, -0.3008, -0.8281, -0.7266],
        [-0.0840,  1.1016, -0.4609, -0.6992, -0.1553],
        [-0.0806,  1.4609, -0.4199, -1.0625, -0.7422],
        [ 0.0732,  1.5078, -0.5000, -0.7422, -0.5859],
        [-0.1650,  1.7422, -0.4121, -0.9844, -1.0000],
        [ 0.0952,  1.5234, -0.2402, -1.0078, -0.6055]], device='cuda:0',
       dtype=torch.bfloat16, grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)
        [-0.1729,  1.3750, -0.6992, -1.2734, -0.6328],8e-01, -5.5469e-01],, attentions=None)
        [-0.1914,  1.6328, -0.9922, -0.9414, -0.4414],8e-01, -5.5469e-01],, attentions=None)
        [ 0.3730,  1.5000, -0.6211, -0.7852, -0.5703]], device='cuda:0',
       dtype=torch.bfloat16, grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)
out(train): SequenceClassifierOutput(loss=tensor(0.4354, device='cuda:0', grad_fn=<NllLossBackward0>), logits=tensor([[ 1.4844e-01,  1.4844e+00, -4.6289e-01, -9.2578e-01, -7.7344e-01],
        [ 1.9043e-01,  1.4062e+00, -4.8047e-01, -9.9219e-01, -5.5078e-01],
        [-1.1749e-03,  1.4844e+00, -5.7031e-01, -9.0234e-01, -7.0703e-01],
        [ 1.7822e-02,  1.9766e+00, -5.5469e-01, -1.0312e+00, -5.5078e-01],
        [-2.4658e-02,  1.3984e+00, -9.1797e-01, -9.5312e-01, -7.4219e-01],
        [ 1.0352e-01,  1.6953e+00, -8.6328e-01, -1.1562e+00, -4.4727e-01],
        [ 3.4180e-01,  1.8438e+00, -1.0938e+00, -8.1250e-01, -3.7305e-01],
        [ 1.6895e-01,  1.6016e+00, -4.0430e-01, -9.9609e-01, -8.9453e-01],
        [ 2.4512e-01,  1.0312e+00, -9.0625e-01, -1.1406e+00, -1.6992e-01],
        [ 2.8442e-02,  1.9062e+00, -6.9922e-01, -7.3438e-01, -8.6328e-01],
        [-2.2827e-02,  1.3984e+00, -5.7812e-01, -8.6328e-01, -7.8516e-01],
        [ 1.6211e-01,  1.4219e+00, -4.2578e-01, -7.8906e-01, -6.0938e-01],
        [-1.2598e-01,  1.7578e+00, -8.4375e-01, -5.9375e-01, -1.8359e-01],
        [ 6.1768e-02,  1.6641e+00, -7.0703e-01, -8.3594e-01, -5.4688e-01],
        [ 3.1128e-02,  1.8750e+00, -7.4609e-01, -1.2266e+00, -2.6758e-01],
        [-8.6426e-02,  1.3750e+00, -5.7812e-01, -8.2422e-01, -3.3398e-01]],
       device='cuda:0', dtype=torch.bfloat16, grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)
out(train): SequenceClassifierOutput(loss=tensor(0.8998, device='cuda:0', grad_fn=<NllLossBackward0>), logits=tensor([[ 0.4473,  1.5078, -0.5820, -0.8359, -0.9180],
        [-0.0981,  1.4688, -0.2930, -0.6680, -0.9414],
        [ 0.3828,  1.4844, -0.9609, -0.8555, -0.4180],
        [ 0.1001,  1.4297, -0.3848, -0.8398, -0.9258],
        [ 0.0977,  1.6250, -0.4141, -0.9453, -0.3359],
        [ 0.2734,  1.3203, -0.5078, -1.0625, -0.8242],
        [ 0.3066,  1.0156, -0.9688, -1.2031, -0.5859],
        [-0.0737,  1.6406, -0.8828, -0.6875, -0.3965],
        [ 0.0383,  1.3828, -0.9336, -0.9219, -0.4883],
        [ 0.2598,  1.7812, -0.5938, -0.6523, -0.6133],
        [-0.2051,  1.1406, -0.5117, -0.9961, -0.5859],
        [ 0.1187,  1.5469, -0.6250, -0.7148, -0.7109],
        [ 0.3223,  1.1328, -0.5273, -0.8242, -0.5625],
        [ 0.0106,  1.3516, -0.6797, -0.7148, -0.5664],
        [-0.1206,  1.6484, -0.5781, -0.6406, -0.6758],
        [ 0.2158,  1.6562, -0.2520, -0.9570, -0.1357]], device='cuda:0',
       dtype=torch.bfloat16, grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)
out(train): SequenceClassifierOutput(loss=tensor(0.8008, device='cuda:0', grad_fn=<NllLossBackward0>), logits=tensor([[ 0.1035,  1.8750, -0.4219, -0.8242, -0.8438],
        [-0.1328,  1.7031, -0.7969, -0.7930, -0.4648],
        [ 0.3262,  1.5234, -0.7695, -0.9258, -0.6758],
        [ 0.1279,  1.4766, -0.4082, -0.5703, -0.4043],
        [ 0.1416,  1.5859, -0.4414, -0.9688, -0.5312],
        [-0.0325,  1.3984, -0.5859, -0.7578, -0.5898],
        [ 0.0806,  1.4922, -0.8555, -0.8555, -0.4199],
        [ 0.2354,  1.7031, -0.5234, -0.7539, -0.6562],
        [ 0.1738,  1.8281, -0.8242, -1.0469, -0.4824],
        [ 0.0474,  1.4688, -0.4805, -1.3125, -0.7930],
        [ 0.1709,  1.4922, -0.8359, -0.8359, -0.6172],
        [ 0.1582,  0.9375, -1.0000, -0.4336, -0.5039],
        [ 0.2910,  0.8516, -0.6641, -1.0312, -0.0854],
        [-0.0193,  1.7734, -0.7266, -0.6680, -0.4648],
        [-0.1455,  1.4531, -0.8672, -0.8594, -0.1377],
        [ 0.1250,  1.7734, -0.6367, -0.9258, -0.5625]], device='cuda:0',
       dtype=torch.bfloat16, grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)
        [-0.1914,  1.6328, -0.9922, -0.9414, -0.4414],8e-01, -5.5469e-01],, attentions=None)
        [-0.2891,  1.4766, -0.6914, -0.5508, -0.9727],8e-01, -5.5469e-01],, attentions=None)
        [ 0.1016,  1.2656, -0.4180, -1.2266, -0.8945]], device='cuda:0',
       dtype=torch.bfloat16, grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)
out(train): SequenceClassifierOutput(loss=tensor(0.4670, device='cuda:0', grad_fn=<NllLossBackward0>), logits=tensor([[ 1.4355e-01,  1.7578e+00, -8.0859e-01, -9.0625e-01, -5.2344e-01],
        [-5.6763e-03,  1.4922e+00, -5.5859e-01, -5.2344e-01, -3.2812e-01],
        [ 8.4473e-02,  1.5234e+00, -9.1406e-01, -1.2812e+00, -3.7500e-01],
        [ 2.6562e-01,  1.1406e+00, -6.0156e-01, -9.1016e-01, -5.8594e-01],
        [-4.7119e-02,  1.9609e+00, -7.0703e-01, -9.7656e-01, -5.8203e-01],
        [ 6.2180e-04,  1.5156e+00, -6.6797e-01, -8.0469e-01, -5.7031e-01],
        [ 2.6172e-01,  1.3594e+00, -5.9375e-01, -1.0938e+00, -5.3125e-01],
        [-2.6001e-02,  1.7734e+00, -5.9766e-01, -1.0078e+00, -5.6250e-01],
        [-6.7139e-03,  1.3516e+00, -6.0938e-01, -8.0469e-01, -5.9766e-01],
        [-5.6641e-02,  1.7266e+00, -7.6172e-01, -7.2266e-01, -5.3906e-01],
        [-2.5195e-01,  1.6797e+00, -5.0781e-01, -8.2812e-01, -8.2031e-01],
        [ 2.2583e-02,  1.5391e+00, -7.7344e-01, -9.4141e-01, -4.4727e-01],
        [-4.7852e-02,  1.8359e+00, -7.1875e-01, -1.1172e+00, -3.7305e-01],
        [ 1.2268e-02,  1.4531e+00, -8.2422e-01, -1.1406e+00, -5.8594e-01],
        [-2.2095e-02,  2.0312e+00, -1.0312e+00, -7.1484e-01, -6.8750e-01],
        [-1.8433e-02,  1.6406e+00, -9.2188e-01, -9.7266e-01, -7.3828e-01]],
       device='cuda:0', dtype=torch.bfloat16, grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)
out(train): SequenceClassifierOutput(loss=tensor(0.7802, device='cuda:0', grad_fn=<NllLossBackward0>), logits=tensor([[ 0.0510,  1.6172, -0.2227, -0.9844, -0.9023],
        [-0.2793,  1.5156, -0.8672, -0.5039, -0.9727],
        [ 0.1104,  1.3750, -0.5391, -0.9453, -0.6914],
        [ 0.1465,  1.4375, -0.4648, -1.3906, -0.7578],
        [ 0.4004,  0.9883, -1.0391, -0.5703,  0.5039],
        [ 0.1177,  1.5000, -0.3789, -0.8594, -0.8047],
        [-0.0732,  1.5547, -0.8477, -0.8047, -0.5898],
        [ 0.1865,  1.3750, -0.8164, -0.5078, -0.3320],
        [ 0.4766,  1.3594, -0.7539, -0.9219, -0.6641],
        [-0.2129,  1.4141, -0.4785, -0.9961, -0.6289],
        [ 0.1187,  1.6797, -0.6992, -0.8789, -0.3359],
        [ 0.1289,  1.8125, -1.1250, -1.1172, -0.1357],
        [-0.0527,  1.7656, -0.7305, -0.8555, -0.5898],
        [-0.0830,  1.5000, -1.1250, -1.0625, -0.6016],
        [-0.0679,  1.3906, -0.4434, -0.2490, -0.4785],
        [ 0.1914,  1.4766, -0.3887, -0.8359, -0.5039]], device='cuda:0',
       dtype=torch.bfloat16, grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)
out(train): SequenceClassifierOutput(loss=tensor(0.6226, device='cuda:0', grad_fn=<NllLossBackward0>), logits=tensor([[ 0.2559,  1.8672, -1.0000, -0.9531, -0.3008],
        [-0.1875,  1.6953, -0.8398, -0.9062, -0.4805],
        [-0.2109,  1.6016, -0.6797, -0.7812, -0.4473],
        [-0.1455,  1.6562, -0.8984, -0.6797, -0.4844],
        [ 0.2471,  1.7812, -0.7344, -0.5977, -0.4824],
        [ 0.0135,  2.0469, -0.9258, -0.6836, -0.5625],
        [-0.0032,  1.6562, -0.4062, -0.6758, -0.3828],
        [-0.2305,  1.7500, -0.6133, -0.8242, -0.5234],
        [-0.1826,  1.3047, -0.7109, -0.7695, -0.1699],
        [ 0.0776,  1.5781, -0.7773, -1.1250, -0.4883],
        [ 0.4648,  1.5234, -1.0000, -0.7148, -0.4062],
        [ 0.2070,  1.8438, -0.4707, -1.0156, -0.8047],
        [ 0.0923,  1.3828, -0.7109, -0.8555, -0.3984],
        [-0.0613,  1.3672, -0.9922, -1.0625, -0.5703],
        [ 0.1235,  1.5156, -0.5547, -0.7812, -0.6055],
        [ 0.2539,  1.6016, -0.4883, -0.8047, -0.4727]], device='cuda:0',
       dtype=torch.bfloat16, grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)
        [-0.2891,  1.4766, -0.6914, -0.5508, -0.9727],8e-01, -5.5469e-01],, attentions=None)
        [ 0.1055,  1.6797, -0.9492, -0.5352, -0.6367],8e-01, -5.5469e-01],, attentions=None)
        [ 0.2373,  1.6719, -0.7461, -0.7773, -0.3574]], device='cuda:0',
       dtype=torch.bfloat16, grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)
out(train): SequenceClassifierOutput(loss=tensor(0.7803, device='cuda:0', grad_fn=<NllLossBackward0>), logits=tensor([[ 0.0113,  1.6953, -0.5938, -0.9414, -0.3574],
        [ 0.2109,  1.5781, -0.6602, -0.9531, -0.5547],
        [ 0.1768,  1.4375, -0.8047, -0.8125, -0.8359],
        [-0.1729,  1.2812, -0.6758, -0.8945, -0.8164],
        [ 0.2891,  1.7500, -0.8203, -0.8164, -0.4941],
        [-0.1167,  1.4922, -0.7969, -0.7383, -0.6758],
        [ 0.1826,  1.5703, -0.6445, -0.9570, -0.4980],
        [ 0.0498,  1.5391, -0.7383, -0.7891, -0.6328],
        [ 0.1543,  1.6484, -1.0391, -1.1719, -0.3672],
        [ 0.1533,  1.5938, -0.9688, -1.1250, -0.4004],
        [ 0.1777,  1.4375, -0.6875, -0.6484, -0.3457],
        [ 0.1514,  1.4766, -0.5547, -1.1328, -0.3750],
        [-0.0757,  1.6953, -0.5625, -0.8594, -0.5117],
        [ 0.1348,  1.3984, -0.7070, -0.6250, -0.8711],
        [ 0.2148,  1.8359, -0.6250, -1.3125, -0.5859],
        [-0.0952,  1.6953, -0.7773, -0.8281, -0.5508]], device='cuda:0',
       dtype=torch.bfloat16, grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)
out(train): SequenceClassifierOutput(loss=tensor(1.0450, device='cuda:0', grad_fn=<NllLossBackward0>), logits=tensor([[ 0.3281,  1.3906, -0.9102, -1.0078, -0.6289],
        [ 0.1523,  1.3359, -0.7734, -1.0234, -0.3398],
        [ 0.1768,  1.6250, -0.9258, -0.6328, -0.1152],
        [ 0.3008,  1.5234, -0.2969, -0.8398, -0.7148],
        [ 0.2256,  1.5859, -0.9609, -1.0312, -0.1367],
        [ 0.1816,  1.4453, -1.0391, -1.1641, -0.8398],
        [ 0.1152,  1.4062, -0.5391, -0.6953, -0.6758],
        [-0.3262,  1.3906, -0.6289, -0.5352, -0.7969],
        [-0.0299,  1.4141, -0.5625, -0.9414, -0.4512],
        [ 0.2012,  1.5078, -0.4961, -0.7773, -0.6602],
        [-0.0498,  1.5391, -0.7227, -0.8125, -0.6953],
        [ 0.1289,  1.7109, -1.0547, -1.0547, -0.4746],
        [-0.4434,  1.7344, -0.8789, -1.0312, -0.5078],
        [-0.1260,  1.5234, -0.6367, -0.9414, -0.8828],
        [ 0.0796,  1.6562, -0.6836, -0.7812, -0.4688],
        [ 0.0728,  1.3828, -0.4707, -0.7148, -0.5508]], device='cuda:0',
       dtype=torch.bfloat16, grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)
out(train): SequenceClassifierOutput(loss=tensor(0.9209, device='cuda:0', grad_fn=<NllLossBackward0>), logits=tensor([[ 0.1553,  1.5625, -0.9844, -1.2812, -0.3613],
        [-0.0613,  0.9180, -0.7500, -0.5664, -0.2012],
        [ 0.1318,  1.9531, -0.6875, -1.0078, -0.5781],
        [ 0.1436,  1.6719, -0.7930, -1.1094, -0.7227],
        [ 0.0889,  1.5938, -0.7344, -0.9883, -0.7188],
        [ 0.1426,  1.6875, -0.8516, -1.1016, -0.5156],
        [ 0.1953,  1.6406, -0.7969, -0.9727, -0.6289],
        [ 0.1030,  1.6328, -0.6289, -1.0234, -0.5781],
        [-0.0850,  1.5000, -0.5664, -0.9414, -0.6562],
        [-0.0294,  1.7422, -0.6016, -0.9961, -0.6055],
        [ 0.0830,  1.9219, -0.5977, -1.1406, -0.6523],
        [ 0.2520,  1.2188, -0.7070, -1.1719, -0.6250],
        [-0.0728,  1.5469, -0.7227, -0.9961, -0.8789],
        [ 0.0879,  1.2891, -0.2100, -0.6953, -0.5078],
        [-0.0708,  1.4062, -0.5156, -0.7891, -0.5859],
        [-0.1279,  1.7422, -0.7578, -0.8594, -0.4531]], device='cuda:0',
       dtype=torch.bfloat16, grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)
        [ 0.1055,  1.6797, -0.9492, -0.5352, -0.6367],8e-01, -5.5469e-01],, attentions=None)
        [-0.1250,  1.9062, -0.2598, -0.7617, -0.5273],8e-01, -5.5469e-01],, attentions=None)
        [ 0.3027,  1.5625, -0.4355, -0.8320, -0.6836]], device='cuda:0',
       dtype=torch.bfloat16, grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)
out(train): SequenceClassifierOutput(loss=tensor(0.9027, device='cuda:0', grad_fn=<NllLossBackward0>), logits=tensor([[ 0.1357,  1.4297, -0.5898, -0.7422, -0.5234],
        [ 0.3828,  1.2734, -0.8242, -0.8867, -0.2109],
        [ 0.2559,  1.4609, -0.8594, -1.1484, -0.6367],
        [-0.0266,  1.4766, -0.5508, -0.9805, -0.5586],
        [ 0.1406,  1.5469, -0.6016, -1.0391, -0.4766],
        [ 0.0131,  1.3984, -0.1895, -0.8438, -0.8828],
        [ 0.1245,  1.6172, -0.5625, -0.6758, -0.6445],
        [ 0.0894,  1.4766, -0.6953, -1.0000, -0.6484],
        [ 0.1152,  1.4375, -0.5898, -0.7305, -0.5195],
        [-0.6406,  1.6797, -0.6680, -0.9258, -0.9805],
        [ 0.0825,  1.3750, -0.9492, -0.5469, -0.6445],
        [ 0.0669,  1.6250, -0.7617, -0.6406, -0.4023],
        [-0.4102,  1.5859, -0.3633, -1.0625, -0.7070],
        [-0.2266,  1.7031, -0.5898, -0.9023, -0.6953],
        [-0.1309,  1.5000, -0.6406, -0.9180, -0.5000],
        [-0.1455,  1.5781, -0.7344, -0.9648, -0.7891]], device='cuda:0',
       dtype=torch.bfloat16, grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)
out(train): SequenceClassifierOutput(loss=tensor(0.6407, device='cuda:0', grad_fn=<NllLossBackward0>), logits=tensor([[-0.0317,  1.4062, -0.7227, -0.6445, -0.4238],
        [-0.0918,  1.6328, -0.6914, -0.8164, -0.5859],
        [ 0.1206,  1.6562, -0.9492, -1.1250, -0.3105],
        [-0.4121,  1.4922, -0.8320, -0.9336, -0.6836],
        [-0.1113,  1.5938, -0.6758, -1.0469, -0.8633],
        [ 0.1226,  1.2266, -0.6914, -0.9219, -0.3750],
        [ 0.1357,  1.8281, -0.6328, -0.4375, -0.5781],
        [-0.0859,  1.1484, -0.2168, -0.6445, -0.4141],
        [-0.1172,  1.4219, -0.5273, -0.5938, -0.4082],
        [ 0.0393,  1.5156, -0.5195, -0.8320, -0.4062],
        [-0.2383,  1.6562, -0.7891, -1.0000, -0.5586],
        [-0.3027,  1.7422, -0.4531, -0.9141, -0.4004],
        [ 0.3887,  1.3750, -0.4570, -0.8398, -0.6484],
        [ 0.1396,  1.5547, -0.8789, -1.1797, -0.8359],
        [ 0.0444,  1.7656, -0.4316, -1.0156, -0.8984],
        [-0.0684,  1.4062, -0.7227, -0.7891, -0.5156]], device='cuda:0',
       dtype=torch.bfloat16, grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)
        [-0.1250,  1.9062, -0.2598, -0.7617, -0.5273],8e-01, -5.5469e-01],, attentions=None)
        [-0.3926,  1.3438, -0.3594, -1.0703, -0.7188],8e-01, -5.5469e-01],, attentions=None)
        [ 0.0776,  1.5000, -0.8281, -0.9961, -0.6445]], device='cuda:0',
       dtype=torch.bfloat16, grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)
out(train): SequenceClassifierOutput(loss=tensor(0.8558, device='cuda:0', grad_fn=<NllLossBackward0>), logits=tensor([[ 0.0771,  1.3750, -0.9688, -1.1484, -0.7656],
        [-0.0413,  1.4297, -0.5898, -1.1094, -0.7734],
        [-0.0649,  1.5000, -0.7695, -0.8242, -0.5898],
        [ 0.4473,  1.3281, -0.7656, -0.8438, -0.4102],
        [ 0.0265,  1.6172, -0.6445, -0.8203, -0.6562],
        [ 0.1025,  1.6484, -0.5547, -0.7227, -0.4648],
        [ 0.0481,  1.3125, -0.9766, -0.8789, -0.6719],
        [-0.0498,  1.5078, -0.6602, -0.8828, -0.8594],
        [ 0.2197,  1.8750, -0.8047, -0.8594, -0.6250],
        [-0.0023,  1.8594, -0.9180, -0.8945, -0.7617],
        [ 0.0464,  1.3828, -0.7461, -1.2344, -0.5664],
        [ 0.0515,  0.9180, -0.5078, -1.0469, -0.5195],
        [ 0.2168,  1.2656, -0.5352, -0.6992, -0.4336],
        [-0.0559,  1.5859, -0.6094, -0.9922, -0.4785],
        [ 0.1641,  0.9766, -0.7773, -0.4668,  0.0140],
        [ 0.3730,  1.1406, -1.2734, -0.9062, -0.0815]], device='cuda:0',
       dtype=torch.bfloat16, grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)
out(train): SequenceClassifierOutput(loss=tensor(0.9324, device='cuda:0', grad_fn=<NllLossBackward0>), logits=tensor([[ 0.3320,  1.5391, -0.7812, -0.9297, -0.5781],
        [ 0.0742,  1.4766, -0.8281, -0.9961, -0.5508],
        [-0.0923,  1.3516, -0.5352, -0.7188, -0.4375],
        [ 0.0532,  1.0469, -0.9180, -0.8672, -0.5195],
        [ 0.0167,  1.3750, -0.7227, -0.7695, -0.3711],
        [ 0.1050,  1.7031, -0.6797, -0.9219, -0.8359],
        [-0.0190,  1.6875, -0.6328, -0.6992, -0.5898],
        [-0.1680,  1.4766, -0.7305, -0.9961, -0.8164],
        [-0.0198,  1.7109, -0.8633, -0.5312, -0.7500],
        [-0.2119,  1.8203, -0.1494, -0.8789, -0.6680],
        [ 0.2363,  1.0312, -0.9492, -0.8398, -0.1406],
        [-0.1504,  1.6562, -0.6055, -0.6367, -0.6562],
        [ 0.1064,  1.4766, -0.6602, -1.0000, -0.1670],
        [-0.2393,  1.5469, -0.6328, -0.7695, -0.4180],
        [-0.0845,  1.7500, -0.7383, -0.9375, -0.7461],
        [-0.2334,  1.7422, -0.6133, -0.7227, -0.7930]], device='cuda:0',
       dtype=torch.bfloat16, grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)
out(train): SequenceClassifierOutput(loss=tensor(0.6974, device='cuda:0', grad_fn=<NllLossBackward0>), logits=tensor([[-0.0615,  1.8984, -0.7852, -0.8203, -0.5898],
        [-0.1533,  1.5391, -0.5234, -0.9297, -0.6172],
        [-0.0569,  1.6250, -0.6523, -0.8633, -0.7812],
        [-0.1963,  1.4375, -0.5391, -0.5547, -0.4512],
        [ 0.0776,  1.6562, -0.7031, -0.9727, -0.5547],
        [ 0.0569,  1.0625, -0.6172, -0.6484, -0.4316],
        [ 0.1138,  1.7500, -0.6953, -0.9844, -0.4688],
        [-0.0737,  1.5234, -0.5703, -1.1484, -0.4746],
        [-0.1143,  1.1094, -0.4043, -1.2031, -0.6523],
        [-0.0869,  1.5703, -0.4648, -0.9297, -0.8398],
        [ 0.2539,  1.3594, -0.4375, -0.8906, -0.9609],
        [-0.1670,  1.4844, -0.1196, -0.7656, -0.6445],
        [ 0.0205,  1.6250, -0.7930, -0.6094, -0.5977],
        [ 0.3320,  1.8594, -0.6992, -0.9922, -0.5508],
        [-0.1387,  1.6484, -0.8438, -1.0156, -0.7031],
        [-0.0549,  1.5547, -0.8633, -1.0703, -0.8438]], device='cuda:0',
       dtype=torch.bfloat16, grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)
out(train): SequenceClassifierOutput(loss=tensor(0.6504, device='cuda:0', grad_fn=<NllLossBackward0>), logits=tensor([[-0.2871,  1.6406, -0.9531, -0.5469, -0.5820],
        [-0.0991,  1.1250, -0.6172, -0.7305, -0.7031],
        [-0.1748,  1.9062, -0.7344, -1.0469, -0.5586],
        [ 0.3516,  1.5234, -0.9414, -1.0234, -0.5742],
        [ 0.0688,  1.5703, -0.4531, -0.8945, -0.5352],
        [-0.0483,  1.5391, -0.6875, -0.8047, -0.7852],
        [ 0.4023,  1.4219, -1.0078, -1.0078, -0.5352],
        [-0.3262,  1.5156, -0.7461, -1.0000, -0.4590],
        [ 0.0413,  1.7031, -0.9805, -1.0781, -0.7188],
        [-0.3086,  1.0547, -0.6406, -0.9922, -0.9883],
        [ 0.1191,  1.3672, -0.5547, -0.7852, -0.9531],
        [ 0.2910,  1.7734, -1.0391, -0.9297, -0.3008],
        [-0.0240,  1.8047, -0.4668, -0.6094, -0.5430],
        [-0.0156,  1.3906, -0.6836, -0.6719, -0.5430],
        [ 0.1670,  1.7266, -0.9570, -0.8242, -0.5977],
        [-0.1426,  1.4922, -0.5508, -0.7578, -0.6562]], device='cuda:0',
       dtype=torch.bfloat16, grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)
        [-0.3926,  1.3438, -0.3594, -1.0703, -0.7188],8e-01, -5.5469e-01],, attentions=None)
        [-0.0466,  1.9688, -0.4727, -0.7617, -0.4414],8e-01, -5.5469e-01],, attentions=None)
        [-0.1221,  1.4844, -0.6875, -0.9883, -0.5586]], device='cuda:0',
       dtype=torch.bfloat16, grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)
out(train): SequenceClassifierOutput(loss=tensor(0.6721, device='cuda:0', grad_fn=<NllLossBackward0>), logits=tensor([[-0.0474,  1.7109, -0.7461, -0.8281, -0.5195],
        [ 0.0347,  1.6016, -0.8242, -1.0312, -0.5156],
        [ 0.1699,  1.3125, -0.7695, -0.7656, -0.6992],
        [ 0.0255,  1.3984, -0.6875, -0.7969, -0.8398],
        [-0.0579,  1.4531, -0.7461, -1.3125, -0.7500],
        [ 0.2734,  1.4297, -0.6094, -0.7656, -0.4512],
        [-0.0071,  1.5391, -0.6562, -1.1250, -0.7617],
        [-0.2061,  1.7344, -0.6914, -1.0469, -0.4043],
        [-0.0457,  1.4297, -0.3672, -0.9062, -0.4531],
        [-0.1406,  1.8281, -0.8633, -1.0469, -0.7227],
        [-0.0669,  1.6328, -0.6055, -0.9531, -0.7578],
        [ 0.2754,  1.7812, -1.0156, -1.0781, -0.4883],
        [-0.0781,  1.3125, -0.3027, -0.6289, -0.5547],
        [-0.0166,  1.5938, -0.3906, -0.7969, -0.6055],
        [ 0.3379,  1.2891, -1.0234, -0.9531, -0.6484],
        [-0.2178,  1.1797, -0.5742, -0.6367, -0.7734]], device='cuda:0',
       dtype=torch.bfloat16, grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)
out(train): SequenceClassifierOutput(loss=tensor(0.5144, device='cuda:0', grad_fn=<NllLossBackward0>), logits=tensor([[-0.0245,  1.7656, -0.8125, -1.1719, -0.5547],
        [ 0.5430,  1.5781, -0.6445, -0.7500, -0.3516],
        [-0.1699,  1.3281, -0.5078, -1.0781, -0.3691],
        [ 0.3008,  1.4688, -0.9062, -0.8906, -0.7734],
        [-0.0820,  1.2891, -0.4141, -1.0078, -0.5977],
        [ 0.0991,  1.1328, -0.5117, -0.6367, -0.4336],
        [-0.0503,  1.6172, -0.5312, -1.0312, -0.7539],
        [-0.3184,  1.5859, -0.6211, -0.5859, -0.4980],
        [-0.2314,  1.5156, -0.2812, -0.6680, -0.7344],
        [ 0.1270,  1.6484, -0.7617, -0.7266, -0.5664],
        [-0.1162,  1.6016, -0.6523, -0.8828, -0.6406],
        [-0.2188,  1.5312, -0.4141, -1.0312, -0.6172],
        [ 0.1270,  1.2578, -0.7617, -0.8516, -0.4824],
        [ 0.0518,  1.3438, -0.6758, -0.8633, -0.7500],
        [ 0.2305,  1.8516, -0.9961, -0.8867, -0.7852],
        [ 0.0140,  1.8203, -0.9336, -0.8711, -0.4336]], device='cuda:0',
       dtype=torch.bfloat16, grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)
out(train): SequenceClassifierOutput(loss=tensor(1.1083, device='cuda:0', grad_fn=<NllLossBackward0>), logits=tensor([[-0.0194,  1.4531, -0.7188, -1.1719, -0.7773],
        [ 0.1855,  1.1562, -0.7148, -0.7461, -0.4414],
        [ 0.1963,  1.4766, -1.0000, -1.3594, -0.4941],
        [-0.5742,  1.7812, -0.8047, -0.6836, -0.5117],
        [-0.2539,  1.9609, -0.5938, -0.8281, -0.4297],
        [-0.0498,  1.2734, -0.3594, -0.6836, -0.5898],
        [ 0.3281,  1.2812, -0.7422, -1.0469, -0.3477],
        [-0.2148,  1.4375, -0.6758, -0.8477, -0.5703],
        [-0.1484,  1.3203, -0.8164, -0.9648, -0.4062],
        [-0.1709,  1.2109, -0.4902, -0.6953, -0.5586],
        [ 0.2061,  1.7500, -0.8438, -0.8047, -0.7227],
        [ 0.3164,  1.6562, -0.7461, -0.7500, -0.4844],
        [-0.2393,  1.1641, -0.7266, -0.6797, -0.9297],
        [ 0.1045,  1.6328, -0.7148, -0.6055, -0.6797],
        [-0.0544,  1.4609, -0.2988, -0.9297, -0.3809],
        [-0.0732,  1.4531, -0.6328, -0.5859, -0.9180]], device='cuda:0',
       dtype=torch.bfloat16, grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)
        [-0.0466,  1.9688, -0.4727, -0.7617, -0.4414],8e-01, -5.5469e-01],, attentions=None)
        [ 0.2754,  1.5391, -0.7148, -0.9492, -0.5898],8e-01, -5.5469e-01],, attentions=None)
        [ 0.1030,  1.9922, -0.7227, -0.9453, -0.4082]], device='cuda:0',
       dtype=torch.bfloat16, grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)
out(train): SequenceClassifierOutput(loss=tensor(0.7180, device='cuda:0', grad_fn=<NllLossBackward0>), logits=tensor([[ 0.0557,  1.7812, -0.4434, -0.8867, -0.5312],
        [-0.0835,  1.5781, -0.8516, -0.7617, -0.7461],
        [ 0.0153,  1.4297, -0.7656, -0.6602, -0.6133],
        [-0.0134,  1.4688, -0.6758, -0.8281, -0.6836],
        [ 0.1592,  1.4922, -0.7500, -1.0781, -0.3418],
        [ 0.0381,  1.4922, -0.8203, -1.2500, -0.4570],
        [ 0.3711,  1.7031, -0.5586, -1.0234, -0.4512],
        [ 0.1572,  1.3281, -0.5156, -0.9883, -0.7227],
        [-0.0825,  1.6094, -0.6016, -0.8555, -0.6055],
        [ 0.1641,  1.5078, -0.2715, -0.6328, -0.4414],
        [ 0.1514,  1.6406, -0.2988, -1.4453, -0.5156],
        [-0.0894,  1.4531, -0.4336, -0.9219, -0.8633],
        [ 0.8281,  1.1016, -0.9258, -0.6367,  0.5625],
        [ 0.0332,  1.5156, -0.6445, -0.9922, -0.7070],
        [-0.1650,  1.6328, -0.7656, -1.1016, -0.4180],
        [ 0.0260,  1.8047, -0.8203, -0.8438, -0.5469]], device='cuda:0',
       dtype=torch.bfloat16, grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)
out(train): SequenceClassifierOutput(loss=tensor(0.6903, device='cuda:0', grad_fn=<NllLossBackward0>), logits=tensor([[ 0.2070,  1.1797, -0.7930, -0.9375, -0.4453],
        [ 0.0038,  1.6875, -0.7148, -0.9492, -0.7617],
        [ 0.2373,  1.7109, -0.8984, -1.0859, -0.5625],
        [ 0.0586,  1.4375, -0.7383, -0.9570, -0.4922],
        [ 0.1982,  1.4609, -0.7891, -1.0312, -0.9883],
        [ 0.1299,  1.3203, -0.6328, -1.0938, -0.5469],
        [ 0.4707,  1.3438, -0.8398, -0.8203, -0.5195],
        [ 0.0762,  1.5000, -0.9961, -0.4785, -0.2207],
        [ 0.3105,  1.9609, -0.1758, -1.0312, -0.6172],
        [ 0.2344,  1.6406, -0.4609, -0.9609, -0.6797],
        [-0.0684,  1.9141, -0.6211, -0.8047, -0.4551],
        [-0.0280,  1.5234, -0.5781, -0.7656, -0.6641],
        [-0.3633,  1.3906, -0.8203, -1.0234, -0.7109],
        [ 0.0815,  1.4453, -0.4062, -0.8086, -0.5234],
        [-0.2090,  1.6406, -0.7930, -1.1953, -0.4531],
        [ 0.1904,  1.3125, -0.7266, -0.8008, -0.6133]], device='cuda:0',
       dtype=torch.bfloat16, grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)
out(train): SequenceClassifierOutput(loss=tensor(1.1169, device='cuda:0', grad_fn=<NllLossBackward0>), logits=tensor([[-0.2061,  1.4141, -0.6719, -0.7305, -0.5508],
        [ 0.0181,  1.7969, -0.7812, -1.0234, -0.4375],
        [ 0.1924,  1.5312, -0.9414, -0.6250, -0.2832],
        [-0.1494,  1.5156, -0.9727, -0.6602, -0.4082],
        [ 0.3770,  1.3281, -0.8047, -1.0547, -0.4277],
        [ 0.1348,  1.5234, -1.1016, -0.8125, -0.8125],
        [-0.3848,  1.8047, -0.8594, -0.7539, -0.4102],
        [ 0.2256,  1.3828, -0.6914, -0.8672, -0.1387],
        [-0.1572,  1.7422, -0.4746, -0.8242, -0.7344],
        [-0.1777,  1.7812, -0.3516, -1.2109, -0.4766],
        [ 0.1143,  1.7891, -0.7695, -1.0078, -0.2324],
        [ 0.1118,  1.3828, -0.6133, -0.9766, -0.6914],
        [-0.0742,  1.4844, -0.7188, -1.0156, -0.4336],
        [ 0.4512,  1.4688, -0.9883, -0.8086, -0.6680],
        [ 0.4551,  1.1875, -0.6836, -0.7109, -0.8594],
        [ 0.0742,  1.6172, -0.8086, -0.5703, -0.4863]], device='cuda:0',
       dtype=torch.bfloat16, grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)
out(train): SequenceClassifierOutput(loss=tensor(0.6221, device='cuda:0', grad_fn=<NllLossBackward0>), logits=tensor([[ 0.3301,  1.1016, -0.1992, -0.8633, -0.8516],
        [ 0.2441,  1.7188, -0.8086, -1.0625, -0.6445],
        [-0.1201,  1.8047, -0.4766, -0.7773, -0.4375],
        [-0.0674,  1.6094, -0.8828, -0.9102, -0.5352],
        [-0.0615,  1.5234, -0.5195, -0.9492, -0.7617],
        [ 0.5000,  1.2500, -0.7266, -0.7500, -0.6484],
        [-0.1670,  1.4609, -0.6875, -0.7422, -0.5547],
        [ 0.0972,  1.5391, -0.2988, -1.1172, -0.8086],
        [ 0.0747,  1.7734, -0.7266, -0.6250, -0.4785],
        [-0.1157,  1.5781, -0.4980, -1.0625, -0.4043],
        [ 0.3379,  1.5703, -0.7578, -0.9180, -0.7188],
        [-0.0086,  1.4922, -0.9727, -1.2656, -0.3047],
        [ 0.0427,  1.2656, -0.6797, -1.0469, -0.6602],
        [ 0.3242,  1.6641, -0.7383, -0.7812, -0.7891],
        [ 0.1729,  1.5469, -0.9609, -0.8203, -0.4297],
        [ 0.0481,  0.4277, -0.8984, -0.5195, -0.0801]], device='cuda:0',
       dtype=torch.bfloat16, grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)
out(train): SequenceClassifierOutput(loss=tensor(0.8710, device='cuda:0', grad_fn=<NllLossBackward0>), logits=tensor([[ 0.1465,  1.6484, -0.8008, -0.8555, -0.6367],
        [ 0.1235,  1.8828, -0.7773, -0.7617, -0.4805],
        [-0.2676,  1.5781, -0.5273, -0.9023, -0.6250],
        [ 0.2090,  1.5391, -0.8203, -0.9961, -0.6875],
        [ 0.2236,  1.7188, -0.6328, -0.6953, -0.7695],
        [ 0.0723,  1.5781, -0.4336, -0.8594, -0.5859],
        [ 0.2812,  2.0938, -0.6836, -0.9492, -0.2852],
        [-0.1934,  1.6641, -0.5352, -0.5742, -0.4160],
        [ 0.3008,  1.5391, -0.4629, -0.9414, -0.5820],
        [ 0.2324,  0.8164, -1.0938, -0.5977, -0.4121],
        [ 0.3320,  1.4844, -0.6328, -0.8320, -0.5898],
        [-0.0425,  1.4531, -0.6328, -0.9492, -0.5859],
        [ 0.1699,  1.4766, -0.5977, -0.9453, -0.7188],
        [-0.0364,  1.7344, -1.0469, -0.8477, -0.5312],
        [-0.2988,  1.4141, -0.6055, -0.8477, -0.6719],
        [-0.1240,  1.6250, -0.6797, -0.9844, -0.6250]], device='cuda:0',
       dtype=torch.bfloat16, grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)
        [ 0.2754,  1.5391, -0.7148, -0.9492, -0.5898],8e-01, -5.5469e-01],, attentions=None)
W0629 11:52:52.467000 139012379337344 torch/_dynamo/convert_frame.py:824]     result = inner_convert(
        [ 0.2754,  1.5391, -0.7148, -0.9492, -0.5898],8e-01, -5.5469e-01],, attentions=None)
W0629 11:52:52.467000 139012379337344 torch/_dynamo/convert_frame.py:824]     result = inner_convert(
W0629 11:52:52.467000 139012379337344 torch/_dynamo/convert_frame.py:824]     result = inner_convert(
W0629 11:52:52.467000 139012379337344 torch/_dynamo/convert_frame.py:824]     result = inner_convert(
W0629 11:52:52.467000 139012379337344 torch/_dynamo/convert_frame.py:824]     result = inner_convert(
W0629 11:52:52.467000 139012379337344 torch/_dynamo/convert_frame.py:824]     result = inner_convert(
W0629 11:52:52.467000 139012379337344 torch/_dynamo/convert_frame.py:824]     result = inner_convert(
W0629 11:52:52.467000 139012379337344 torch/_dynamo/convert_frame.py:824]     result = inner_convert(
        [ 0.2754,  1.5391, -0.7148, -0.9492, -0.5898],8e-01, -5.5469e-01],, attentions=None)
        [-0.2100,  1.4062, -0.7305, -0.9648, -0.7070],8e-01, -5.5469e-01],, attentions=None)
        [ 0.2500,  1.3906, -0.8203, -0.8516, -0.8906]], device='cuda:0',
       dtype=torch.bfloat16, grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)
out(train): SequenceClassifierOutput(loss=tensor(1.0288, device='cuda:0', grad_fn=<NllLossBackward0>), logits=tensor([[ 0.4316,  1.6172, -0.9141, -1.1172, -0.3301],
        [-0.0083,  1.3203, -0.4551, -1.1250, -0.8242],
        [-0.0811,  1.1172, -0.6055, -0.6133, -0.6523],
        [ 0.2812,  1.6484, -0.4551, -0.7422, -1.1484],
        [ 0.4746,  1.4844, -0.8594, -1.2031, -0.5664],
        [-0.2402,  1.7656, -0.1602, -0.5781, -0.9648],
        [ 0.2021,  1.5391, -0.6484, -0.9375, -0.7734],
        [-0.1279,  1.4531, -0.1914, -0.7578, -0.9141],
        [ 0.3691,  1.4844, -0.5352, -0.9219, -0.8320],
        [-0.0762,  1.4375, -0.5312, -0.9297, -0.4570],
        [ 0.1465,  1.5703, -0.3750, -1.0234, -0.7344],
        [-0.2090,  0.9688, -0.7109, -0.5781, -0.8555],
        [ 0.2451,  1.1953, -0.8281, -1.1250, -0.9766],
        [ 0.2324,  1.5312, -0.4395, -1.0000, -0.4805],
        [ 0.1406,  1.1016, -0.7539, -1.2578, -0.5898],
        [-0.1709,  1.3828, -0.5898, -0.7578, -0.3691]], device='cuda:0',
       dtype=torch.bfloat16, grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)
        [-0.2100,  1.4062, -0.7305, -0.9648, -0.7070],8e-01, -5.5469e-01],, attentions=None)
        [-0.2988,  1.5391, -0.2812, -1.3281, -0.7344],8e-01, -5.5469e-01],, attentions=None)
        [ 0.0148,  1.7188, -0.4375, -0.7461, -0.7539]], device='cuda:0',
       dtype=torch.bfloat16, grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)
out(train): SequenceClassifierOutput(loss=tensor(0.7085, device='cuda:0', grad_fn=<NllLossBackward0>), logits=tensor([[-0.2012,  1.5625, -0.7617, -0.8281, -0.5898],
        [ 0.3027,  1.2656, -0.6836, -1.0469, -0.5977],
        [ 0.2334,  1.4375, -0.8281, -1.0078, -0.8438],
        [ 0.0105,  1.6797, -0.4688, -0.8281, -0.8086],
        [ 0.3555,  1.4062, -0.9102, -0.6602, -0.4922],
        [-0.0405,  1.2734, -0.6602, -1.0234, -0.6484],
        [-0.2793,  1.5312, -0.5312, -1.0078, -0.8047],
        [ 0.2188,  1.5234, -0.7188, -1.0625, -0.7305],
        [ 0.2695,  1.3594, -0.6211, -1.1484, -0.7969],
        [ 0.1846,  1.1719, -0.7695, -0.9609, -0.4102],
        [-0.0981,  1.8125, -0.6172, -1.0859, -0.7422],
        [-0.2832,  1.8125, -1.0859, -0.7305, -0.5195],
        [-0.4219,  1.6172, -0.4199, -0.9023, -0.4688],
        [ 0.3223,  1.0703, -1.0547, -0.7070,  0.0776],
        [-0.0076,  1.7109, -0.8398, -0.9570, -0.5938],
        [ 0.3730,  1.1875, -0.4141, -0.9297, -0.7227]], device='cuda:0',
       dtype=torch.bfloat16, grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)
out(train): SequenceClassifierOutput(loss=tensor(0.6149, device='cuda:0', grad_fn=<NllLossBackward0>), logits=tensor([[ 0.0476,  1.4453, -0.7773, -0.8477, -0.9219],
        [-0.0874,  1.5938, -0.5859, -0.8008, -0.9219],
        [-0.2871,  1.6641, -0.4941, -0.4336, -0.7461],
        [ 0.1396,  1.6250, -0.4473, -0.8086, -0.7305],
        [-0.2324,  1.6719, -0.2578, -0.8828, -0.3770],
        [-0.0669,  1.6328, -0.3438, -0.9219, -0.8242],
        [ 0.2129,  1.6797, -0.5273, -0.5859, -0.7539],
        [-0.2871,  1.4062, -0.5117, -1.2031, -0.8906],
        [ 0.0227,  1.3594, -0.6250, -1.1719, -0.6484],
        [-0.3789,  1.2891, -0.3633, -0.7773, -0.9258],
        [ 0.1934,  1.5625, -0.3555, -0.7461, -0.7383],
        [ 0.2148,  1.7812, -0.3887, -1.0938, -0.6055],
        [-0.0835,  1.6484, -0.4688, -1.2344, -0.8750],
        [-0.0850,  1.5156, -0.4336, -1.0156, -0.5117],
        [-0.1543,  1.4219, -0.6914, -0.7070, -0.9844],
        [ 0.3398,  1.2969, -0.7773, -1.0391, -0.7305]], device='cuda:0',
       dtype=torch.bfloat16, grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)
        [-0.2988,  1.5391, -0.2812, -1.3281, -0.7344],8e-01, -5.5469e-01],, attentions=None)
        [-2.0508e-01,  1.6016e+00, -7.3828e-01, -1.2734e+00, -4.6289e-01],, attentions=None)
        [ 6.7444e-03,  1.6562e+00, -5.8594e-01, -1.0391e+00, -6.3672e-01]],
       device='cuda:0', dtype=torch.bfloat16, grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)
        [-2.0508e-01,  1.6016e+00, -7.3828e-01, -1.2734e+00, -4.6289e-01],, attentions=None)
        [ 0.0693,  1.7344, -0.3418, -1.0547, -0.7422],4e+00, -4.6289e-01],, attentions=None)
        [ 0.2656,  1.4688, -0.6875, -1.3594, -0.8398]], device='cuda:0',
       dtype=torch.bfloat16, grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)
out(train): SequenceClassifierOutput(loss=tensor(0.5470, device='cuda:0', grad_fn=<NllLossBackward0>), logits=tensor([[-0.0693,  1.4297, -0.7383, -0.6133, -0.6367],
        [-0.0537,  1.7266, -0.9492, -0.8086, -0.6758],
        [ 0.0082,  1.7422, -0.7227, -0.9258, -0.7617],
        [ 0.0197,  1.3281, -0.4902, -0.7500, -0.7227],
        [-0.0859,  1.7734, -0.8516, -1.1797, -0.6562],
        [-0.1953,  1.8906, -0.7188, -1.2266, -0.4160],
        [-0.0042,  1.9609, -0.6953, -1.1484, -0.5547],
        [ 0.1621,  1.2031, -0.5781, -1.2031, -0.4414],
        [-0.0493,  1.3516, -0.6445, -0.6289, -0.3086],
        [ 0.1924,  1.6562, -0.4824, -1.1484, -0.7773],
        [ 0.2070,  1.1641, -0.4570, -0.8477, -0.9883],
        [-0.2070,  1.6172, -0.5820, -1.0938, -0.1934],
        [ 0.5703,  0.8633, -1.0078, -1.0156,  0.3789],
        [-0.0923,  1.5781, -0.6094, -0.7617, -0.7578],
        [-0.4004,  1.7578, -0.6719, -0.8945, -0.3535],
        [-0.0742,  1.3750, -0.5508, -0.8633, -0.7109]], device='cuda:0',
       dtype=torch.bfloat16, grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)
out(train): SequenceClassifierOutput(loss=tensor(0.4867, device='cuda:0', grad_fn=<NllLossBackward0>), logits=tensor([[-0.0708,  1.8438, -1.0703, -0.8281, -0.5156],
        [-0.2070,  1.3359, -0.4082, -0.8555, -0.3867],
        [ 0.1963,  1.5938, -0.8984, -0.8711, -0.3965],
        [ 0.5352,  1.6016, -1.1641, -0.9961, -0.4180],
        [-0.1572,  2.1719, -0.4219, -0.7930, -0.5898],
        [ 0.2559,  1.4141, -0.8086, -0.9570, -0.4844],
        [ 0.2656,  1.6797, -0.6406, -0.9570, -0.7773],
        [ 0.3145,  1.6953, -0.6562, -1.0625, -0.4961],
        [-0.0767,  1.3906, -0.7305, -0.9766, -0.6992],
        [-0.1016,  1.5859, -0.9805, -0.8242, -0.7930],
        [-0.3262,  1.4531, -0.7812, -0.7109, -0.4414],
        [ 0.1367,  1.5547, -0.9023, -0.9258, -0.0620],
        [-0.1426,  1.7812, -0.6016, -1.0234, -0.4043],
        [ 0.1660,  1.5156, -0.8672, -0.9648, -0.4766],
        [ 0.1650,  1.4375, -0.7734, -1.0156, -0.4355],
        [-0.1494,  1.7188, -0.7539, -0.7109, -0.3613]], device='cuda:0',
       dtype=torch.bfloat16, grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)
out(train): SequenceClassifierOutput(loss=tensor(0.7820, device='cuda:0', grad_fn=<NllLossBackward0>), logits=tensor([[ 0.0737,  1.7422, -0.5430, -0.8867, -0.6680],
        [ 0.2617,  1.6094, -0.8086, -0.5312, -0.6602],
        [ 0.1318,  1.5859, -0.3730, -1.2891, -0.4824],
        [-0.0991,  1.5703, -0.2246, -0.9102, -1.1562],
        [ 0.4629,  0.8438, -0.8750, -0.5430,  0.1445],
        [ 0.3301,  1.3750, -0.7578, -0.8320, -0.6445],
        [ 0.1582,  1.2891, -1.1484, -0.8867, -0.4629],
        [ 0.1807,  1.5469, -0.6484, -0.7109, -0.5508],
        [ 0.4434,  1.7344, -0.8398, -0.8633, -0.4531],
        [ 0.1050,  1.4922, -0.7422, -1.0625, -0.4883],
        [-0.0723,  1.6562, -0.7461, -0.9961, -0.6523],
        [ 0.2949,  1.7734, -0.5586, -0.7383, -0.5039],
        [-0.0427,  1.3906, -0.9648, -0.8750, -0.5859],
        [ 0.4961,  1.8906, -0.7422, -1.0312, -0.7070],
        [ 0.0177,  1.1562, -0.6055, -0.3027, -0.8203],
        [ 0.0098,  1.3359, -0.6172, -0.9492, -0.4922]], device='cuda:0',
       dtype=torch.bfloat16, grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)
out(train): SequenceClassifierOutput(loss=tensor(0.6003, device='cuda:0', grad_fn=<NllLossBackward0>), logits=tensor([[ 0.2129,  1.8828, -0.9766, -1.0547, -0.5938],
        [-0.2090,  1.7656, -0.8008, -0.9648, -0.6289],
        [ 0.3184,  1.7422, -0.8477, -0.4785, -0.7070],
        [-0.0562,  1.4531, -0.9375, -0.8242, -0.7344],
        [ 0.1157,  1.5391, -0.9492, -0.6406, -0.3867],
        [ 0.0674,  1.7109, -0.8711, -0.6641, -0.5430],
        [ 0.1157,  1.6094, -0.4648, -0.9180, -0.5430],
        [ 0.1689,  1.4922, -0.9609, -0.9922, -0.5742],
        [ 0.0287,  1.1797, -0.8906, -0.7109, -0.3008],
        [ 0.3066,  1.5156, -0.7891, -1.1016, -0.2402],
        [ 0.3398,  1.4688, -0.9219, -0.9102, -0.3789],
        [ 0.1533,  1.8047, -0.6758, -0.8945, -0.5117],
        [ 0.0306,  1.2500, -0.6914, -0.7578, -0.2793],
        [-0.1348,  1.5000, -0.8008, -0.6484, -0.4121],
        [ 0.0208,  1.4297, -0.6367, -0.9961, -0.7422],
        [ 0.4277,  1.9453, -0.4062, -0.9297, -0.5117]], device='cuda:0',
       dtype=torch.bfloat16, grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)
        [ 0.0693,  1.7344, -0.3418, -1.0547, -0.7422],4e+00, -4.6289e-01],, attentions=None)
        [ 2.1387e-01,  1.5625e+00, -1.0156e+00, -9.0234e-01, -3.8672e-01],, attentions=None)
        [-4.3640e-03,  1.6250e+00, -7.5000e-01, -9.7656e-01, -4.0820e-01]],
       device='cuda:0', dtype=torch.bfloat16, grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)
out(train): SequenceClassifierOutput(loss=tensor(0.7577, device='cuda:0', grad_fn=<NllLossBackward0>), logits=tensor([[-0.1465,  1.7812, -0.7812, -0.8945, -0.5430],
        [ 0.1846,  1.7578, -0.7930, -1.1406, -0.7070],
        [ 0.1924,  1.6406, -0.8164, -0.9727, -0.7617],
        [-0.0576,  1.3281, -0.5742, -0.9375, -0.8906],
        [ 0.2393,  1.7734, -1.1250, -0.6602, -0.6289],
        [ 0.0452,  1.6641, -0.6133, -0.3262, -0.5508],
        [ 0.3320,  1.4609, -0.7539, -1.0000, -0.5781],
        [-0.0062,  1.5234, -0.6367, -0.7344, -1.0312],
        [ 0.3027,  1.5547, -0.9648, -0.7148, -0.5156],
        [ 0.1211,  1.3516, -0.8086, -1.1250, -0.4512],
        [ 0.3359,  1.6094, -0.8242, -0.5430, -0.4844],
        [ 0.2715,  1.6172, -0.2812, -1.0703, -0.4492],
        [ 0.3770,  1.7422, -0.5430, -0.9844, -0.6094],
        [ 0.2637,  2.0469, -0.7422, -0.8594, -0.8594],
        [ 0.2275,  1.7031, -0.7695, -0.6719, -0.5625],
        [ 0.1211,  1.7188, -1.0547, -0.8164, -0.4688]], device='cuda:0',
       dtype=torch.bfloat16, grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)
out(train): SequenceClassifierOutput(loss=tensor(1.0374, device='cuda:0', grad_fn=<NllLossBackward0>), logits=tensor([[ 0.2871,  1.2344, -0.9062, -1.0469, -0.5039],
        [-0.1187,  1.1641, -0.7266, -0.9609, -0.7852],
        [ 0.4844,  1.4609, -1.0391, -0.8984, -0.1279],
        [ 0.1025,  1.6094, -0.2256, -0.5000, -0.6445],
        [ 0.2119,  1.8828, -0.8633, -0.5273, -0.5977],
        [ 0.3223,  1.3750, -0.6875, -1.3203, -0.8203],
        [-0.2168,  1.4531, -0.4746, -0.6875, -0.6680],
        [ 0.1680,  1.4219, -0.8164, -1.1328, -0.5703],
        [ 0.3105,  1.3594, -0.7578, -0.8281, -0.4473],
        [ 0.2461,  1.4766, -0.6289, -0.5625, -0.5781],
        [ 0.0559,  1.6719, -0.8008, -1.0234, -0.5469],
        [ 0.1953,  1.4922, -0.9727, -0.9297, -0.7070],
        [ 0.0413,  1.8516, -0.7344, -0.8555, -0.3438],
        [-0.0320,  1.3984, -0.7461, -0.8945, -0.6367],
        [ 0.1582,  1.8672, -0.7461, -1.0000, -0.2539],
        [-0.2197,  1.5078, -0.8242, -0.9180, -0.6602]], device='cuda:0',
       dtype=torch.bfloat16, grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)
out(train): SequenceClassifierOutput(loss=tensor(0.9205, device='cuda:0', grad_fn=<NllLossBackward0>), logits=tensor([[ 6.5918e-02,  1.9609e+00, -8.6328e-01, -8.9453e-01, -3.9258e-01],
        [ 2.1777e-01,  1.2969e+00, -9.8828e-01, -6.0156e-01, -2.6172e-01],
        [ 1.9824e-01,  1.7109e+00, -9.3750e-01, -1.0391e+00, -8.4766e-01],
        [ 5.4932e-04,  1.7344e+00, -8.4766e-01, -9.1406e-01, -5.5078e-01],
        [-5.3223e-02,  1.5391e+00, -7.4219e-01, -8.6719e-01, -3.0859e-01],
        [-1.0693e-01,  1.7656e+00, -4.5703e-01, -1.1328e+00, -6.4062e-01],
        [ 1.7480e-01,  1.6016e+00, -8.6328e-01, -8.9062e-01, -6.7188e-01],
        [ 1.3489e-02,  1.1875e+00, -7.5391e-01, -9.6484e-01, -7.6172e-01],
        [-2.8320e-01,  1.1172e+00, -3.2422e-01, -7.5000e-01, -5.7031e-01],
        [ 2.3535e-01,  1.7266e+00, -7.4219e-01, -1.3750e+00, -5.4688e-01],
        [ 9.6680e-02,  1.9609e+00, -6.9141e-01, -1.1797e+00, -7.8516e-01],
        [ 1.5234e-01,  1.6953e+00, -6.9824e-02, -6.6797e-01, -8.7891e-01],
        [ 3.7109e-01,  1.6406e+00, -6.6797e-01, -1.0000e+00, -7.8516e-01],
        [ 6.7871e-02,  1.4141e+00, -5.8984e-01, -1.1250e+00, -7.5684e-02],
        [ 2.0508e-01,  1.5781e+00, -8.4766e-01, -9.1406e-01, -6.9531e-01],
        [-8.6426e-02,  1.6172e+00, -7.6172e-01, -6.9531e-01, -5.8203e-01]],
       device='cuda:0', dtype=torch.bfloat16, grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)
        [ 2.1387e-01,  1.5625e+00, -1.0156e+00, -9.0234e-01, -3.8672e-01],, attentions=None)
        [-0.1699,  1.6250, -0.5977, -0.8281, -0.4922],4e-01, -3.8672e-01],, attentions=None)
        [ 0.3672,  1.3047, -0.7305, -0.6367, -0.3105]], device='cuda:0',
       dtype=torch.bfloat16, grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)
out(train): SequenceClassifierOutput(loss=tensor(0.8965, device='cuda:0', grad_fn=<NllLossBackward0>), logits=tensor([[ 0.0923,  1.4844, -0.6484, -0.8867, -0.2412],
        [ 0.2441,  1.2266, -1.1719, -0.8047, -0.2002],
        [ 0.2656,  1.6719, -0.9492, -1.2109, -0.8828],
        [-0.2080,  1.3516, -0.6094, -0.6289, -0.7812],
        [ 0.2070,  1.5469, -0.6797, -0.9688, -0.4512],
        [ 0.1768,  1.5000, -0.7266, -0.7891, -0.6211],
        [ 0.2490,  1.4922, -0.5234, -0.7539, -0.7930],
        [ 0.1021,  1.6719, -0.9258, -0.8633, -0.6094],
        [-0.0981,  1.4219, -0.5625, -0.8555, -0.5117],
        [-0.4102,  1.6484, -0.6992, -0.6953, -0.8867],
        [-0.1084,  1.7891, -0.4551, -0.9570, -0.6562],
        [ 0.1196,  1.2734, -0.7148, -0.7539, -0.5195],
        [-0.2500,  1.4453, -0.6680, -0.8164, -0.4609],
        [-0.0854,  1.9141, -0.3984, -0.9023, -0.3555],
        [ 0.2451,  1.7578, -0.8477, -0.7617, -0.7461],
        [ 0.0762,  1.3672, -0.8984, -1.0547, -0.3555]], device='cuda:0',
       dtype=torch.bfloat16, grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)
out(train): SequenceClassifierOutput(loss=tensor(0.6296, device='cuda:0', grad_fn=<NllLossBackward0>), logits=tensor([[-0.1670,  1.6016, -0.7383, -1.0625, -0.5391],
        [ 0.4297,  1.1719, -1.1172, -0.7500, -0.5859],
        [ 0.0186,  1.6328, -0.8867, -0.9453, -0.4590],
        [ 0.1099,  1.6719, -0.6289, -1.1250, -0.5664],
        [-0.1318,  1.8828, -0.7227, -1.1016, -0.4941],
        [ 0.3809,  1.3438, -0.5977, -0.8203, -0.5508],
        [ 0.3945,  1.8906, -0.8672, -1.0391, -0.2500],
        [ 0.1465,  1.2109, -0.0156, -0.8477, -0.0996],
        [ 0.2910,  1.7031, -0.5625, -0.7383, -0.4531],
        [ 0.2656,  1.4844, -0.7500, -1.0938, -0.4980],
        [ 0.1357,  1.9141, -0.9141, -0.9648, -0.7266],
        [ 0.1592,  1.6875, -0.5156, -0.6797, -0.4492],
        [-0.2080,  1.2109, -0.4824, -1.0078, -0.7891],
        [ 0.0080,  1.6797, -0.5938, -0.8633, -0.7812],
        [ 0.1484,  1.5234, -0.3848, -1.1328, -0.5117],
        [-0.2412,  1.3516, -0.6680, -0.7969, -0.6836]], device='cuda:0',
       dtype=torch.bfloat16, grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)
        [-0.1699,  1.6250, -0.5977, -0.8281, -0.4922],4e-01, -3.8672e-01],, attentions=None)
        [-0.1475,  1.5156, -0.5820, -0.8438, -0.6562],4e-01, -3.8672e-01],, attentions=None)
        [-0.0898,  1.6172, -0.8086, -0.8320, -0.4824]], device='cuda:0',
       dtype=torch.bfloat16, grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)
out(train): SequenceClassifierOutput(loss=tensor(0.8391, device='cuda:0', grad_fn=<NllLossBackward0>), logits=tensor([[ 0.1299,  1.3281, -0.6680, -0.9766, -0.6094],
        [ 0.3301,  1.5391, -0.8008, -1.0156, -0.6055],
        [ 0.0527,  1.5234, -0.9453, -1.0391, -0.6523],
        [ 0.2432,  1.7812, -0.6250, -0.8359, -0.5859],
        [ 0.1514,  1.7266, -0.9336, -0.7734, -0.4473],
        [-0.1699,  1.9297, -0.6680, -1.0312, -0.5859],
        [ 0.1074,  1.3906, -0.9805, -0.9023, -0.6484],
        [ 0.1309,  1.6406, -0.6484, -0.6211, -0.8594],
        [-0.2070,  1.8203, -0.6523, -1.0156, -0.6016],
        [-0.1650,  1.9141, -0.8125, -1.0234, -0.4941],
        [ 0.2988,  1.6328, -1.0234, -1.2891, -0.6836],
        [ 0.1934,  1.4297, -0.2988, -0.8203, -0.6055],
        [-0.0498,  1.4219, -0.6211, -1.0156, -0.5234],
        [ 0.2021,  1.4141, -0.8438, -0.7656, -0.6719],
        [ 0.0146,  0.8906, -0.4902, -0.7500, -0.1196],
        [ 0.0544,  1.8203, -0.8398, -0.8789, -0.6992]], device='cuda:0',
       dtype=torch.bfloat16, grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)
out(train): SequenceClassifierOutput(loss=tensor(0.9419, device='cuda:0', grad_fn=<NllLossBackward0>), logits=tensor([[-0.1177,  1.6328, -0.5859, -0.9180, -0.8477],
        [-0.1050,  1.5234, -0.5586, -0.9297, -0.7773],
        [-0.0283,  1.5000, -0.5273, -0.7773, -0.4941],
        [-0.2080,  1.5000, -0.8633, -0.9766, -0.2910],
        [-0.0703,  1.5469, -0.6641, -0.8945, -0.6328],
        [ 0.2207,  1.4453, -0.7461, -0.5938, -0.6484],
        [ 0.0096,  1.5625, -0.8242, -0.8789, -0.7031],
        [-0.2090,  1.4141, -0.4238, -1.2656, -0.5977],
        [-0.0025,  1.3516, -0.7500, -0.3867, -0.7891],
        [ 0.0153,  1.3828, -0.6250, -1.1719, -0.5312],
        [ 0.1187,  1.3984, -1.0781, -1.0156, -0.2266],
        [ 0.0172,  1.1797, -0.8750, -0.8555, -0.5430],
        [ 0.0039,  1.9062, -0.6250, -1.2266, -0.5820],
        [-0.0520,  1.6094, -0.6758, -0.6406, -0.5703],
        [ 0.2695,  1.5391, -0.9023, -0.9648, -0.3750],
        [-0.3262,  1.3359, -0.6680, -0.9609, -0.4023]], device='cuda:0',
       dtype=torch.bfloat16, grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)
out(train): SequenceClassifierOutput(loss=tensor(0.6703, device='cuda:0', grad_fn=<NllLossBackward0>), logits=tensor([[ 0.0732,  1.7656, -0.8672, -1.0156, -0.8750],
        [-0.4102,  1.7344, -0.5391, -0.5820, -0.6758],
        [-0.1611,  1.8281, -0.4805, -0.7070, -0.8594],
        [-0.0078,  1.6797, -0.8945, -0.9531, -0.5664],
        [ 0.0491,  1.7812, -0.5625, -0.9336, -0.3184],
        [ 0.2129,  1.2969, -0.4121, -0.8516, -0.1895],
        [-0.1396,  1.6406, -0.7344, -0.8203, -0.5859],
        [ 0.1338,  1.5078, -0.7344, -1.0469, -0.7148],
        [-0.1357,  1.3359, -0.3887, -1.2422, -0.7656],
        [ 0.2393,  1.6484, -0.5977, -0.9336, -0.8320],
        [ 0.1904,  1.2656, -0.5078, -0.8984, -0.8242],
        [ 0.3203,  1.6250, -0.5469, -0.9141, -0.8164],
        [ 0.1680,  1.6719, -1.1094, -0.5117, -0.3379],
        [ 0.1250,  1.8047, -1.2656, -0.7305, -0.5664],
        [-0.1982,  1.7031, -0.9023, -0.9961, -0.8516],
        [ 0.4043,  1.6562, -0.5430, -1.0156, -0.6445]], device='cuda:0',
       dtype=torch.bfloat16, grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)
        [-0.1475,  1.5156, -0.5820, -0.8438, -0.6562],4e-01, -3.8672e-01],, attentions=None)
        [-0.4062,  1.7812, -0.6914, -1.1953, -0.5391],4e-01, -3.8672e-01],, attentions=None)
        [-0.1934,  1.5156, -0.8281, -1.0625, -0.5039]], device='cuda:0',
       dtype=torch.bfloat16, grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)
out(train): SequenceClassifierOutput(loss=tensor(0.5016, device='cuda:0', grad_fn=<NllLossBackward0>), logits=tensor([[-0.0437,  1.1562, -0.9453, -0.7109, -0.6406],
        [-0.0742,  1.6875, -0.8047, -1.1172, -0.5938],
        [ 0.0640,  1.4844, -0.8594, -0.8750, -0.7500],
        [ 0.0175,  1.5859, -0.8203, -0.9141, -0.5312],
        [ 0.1943,  1.8438, -0.6562, -0.8633, -0.5859],
        [-0.0532,  1.6250, -0.7148, -1.1172, -0.7422],
        [ 0.2109,  1.7656, -0.6406, -0.7109, -0.5820],
        [ 0.1758,  1.5312, -0.5977, -1.1172, -0.4004],
        [ 0.2461,  1.3594, -0.6562, -0.7695, -0.6094],
        [ 0.0742,  1.5078, -0.9570, -1.0625, -0.6797],
        [-0.1611,  1.4141, -0.5898, -0.7188, -0.4980],
        [-0.3477,  1.6016, -0.7617, -0.6797, -0.4668],
        [-0.1865,  1.5938, -0.8438, -1.0391, -0.3145],
        [ 0.2578,  1.6328, -0.7461, -0.8828, -0.6992],
        [ 0.1069,  1.9844, -0.6484, -0.9102, -0.9141],
        [-0.2480,  1.4219, -0.5000, -1.0469, -0.6641]], device='cuda:0',
       dtype=torch.bfloat16, grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)
out(train): SequenceClassifierOutput(loss=tensor(0.6318, device='cuda:0', grad_fn=<NllLossBackward0>), logits=tensor([[-0.2676,  1.7422, -0.7109, -1.2188, -0.6094],
        [ 0.1069,  1.5000, -0.7695, -0.8828, -0.5469],
        [ 0.2383,  1.1562, -0.7109, -0.5508, -0.3281],
        [ 0.1533,  1.6094, -0.8750, -0.8789, -0.6367],
        [-0.0200,  1.4922, -0.8828, -0.5117, -0.4941],
        [ 0.3516,  1.2578, -0.6484, -0.7812, -0.5234],
        [-0.1895,  1.5781, -0.7930, -1.0312, -0.2578],
        [-0.1064,  1.6797, -0.6953, -0.7539, -0.5547],
        [-0.3066,  1.7266, -0.4805, -0.8984, -0.7617],
        [ 0.0806,  2.0938, -0.8750, -0.9609, -0.3164],
        [ 0.0134,  1.8438, -0.6836, -0.9727, -0.3105],
        [-0.0427,  1.7500, -0.8438, -1.1562, -0.4707],
        [-0.1865,  1.3828, -0.3457, -1.0547, -0.6875],
        [-0.0605,  1.7031, -0.3594, -0.8320, -0.7656],
        [ 0.2041,  1.5859, -1.0078, -1.0078, -0.5938],
        [ 0.1895,  1.3594, -0.6289, -0.5391, -0.5586]], device='cuda:0',
       dtype=torch.bfloat16, grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)
out(train): SequenceClassifierOutput(loss=tensor(0.4711, device='cuda:0', grad_fn=<NllLossBackward0>), logits=tensor([[-0.0728,  1.5859, -0.7539, -0.9375, -0.6172],
        [ 0.2832,  1.0312, -1.1562, -0.9570, -0.1973],
        [ 0.0325,  1.6719, -0.6836, -0.8555, -0.7109],
        [ 0.1475,  1.6250, -0.4961, -0.9336, -0.6328],
        [-0.2832,  1.2109, -0.6641, -0.8086, -0.8398],
        [-0.0101,  1.3203, -0.5742, -0.7539, -0.5117],
        [-0.0962,  1.5859, -0.6328, -0.8477, -0.8633],
        [-0.1777,  1.6953, -1.0469, -0.7344, -0.4062],
        [-0.0064,  1.3906, -0.4707, -0.6445, -0.6055],
        [-0.0845,  1.6016, -0.8008, -1.0156, -0.9062],
        [ 0.2373,  1.9062, -0.8789, -0.9688, -0.4648],
        [-0.1865,  1.4062, -0.7773, -0.7891, -0.5352],
        [ 0.0145,  1.7656, -0.5273, -0.7930, -0.4609],
        [-0.0884,  1.6250, -0.7266, -0.7500, -0.4707],
        [-0.1445,  1.9297, -0.5312, -0.9883, -0.5195],
        [ 0.0444,  1.6719, -0.7812, -0.9023, -0.4395]], device='cuda:0',
       dtype=torch.bfloat16, grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)
out(train): SequenceClassifierOutput(loss=tensor(1.0397, device='cuda:0', grad_fn=<NllLossBackward0>), logits=tensor([[ 0.1348,  1.3594, -0.9453, -1.2188, -1.0703],
        [ 0.3535,  1.3672, -0.8047, -0.6133, -0.6484],
        [ 0.1299,  1.7500, -0.7344, -0.8320, -0.6797],
        [-0.1621,  1.9609, -0.6484, -0.6875, -0.5625],
        [ 0.1758,  2.0312, -0.5117, -0.7891, -0.2617],
        [-0.2910,  1.2422, -0.2715, -0.9023, -0.4102],
        [ 0.5508,  0.7734, -0.9180, -0.5039,  0.2656],
        [ 0.1709,  1.3984, -0.5117, -1.0156, -0.2988],
        [ 0.2637,  1.5312, -0.7539, -1.0000, -0.6094],
        [-0.0151,  1.4375, -0.8086, -1.0234, -0.2676],
        [ 0.0491,  2.0156, -0.6523, -0.9453, -0.9727],
        [ 0.2520,  1.4844, -0.8008, -0.7305, -0.6016],
        [-0.1157,  1.5234, -0.4883, -0.6289, -0.6211],
        [ 0.1523,  1.7969, -0.7070, -0.8125, -0.7617],
        [ 0.0435,  1.5234, -0.5625, -0.8789, -0.4590],
        [ 0.3555,  1.4141, -0.6289, -0.6562, -0.6094]], device='cuda:0',
       dtype=torch.bfloat16, grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)
        [-0.4062,  1.7812, -0.6914, -1.1953, -0.5391],4e-01, -3.8672e-01],, attentions=None)
        [ 0.0049,  1.9297, -0.8555, -1.0781, -0.7617],4e-01, -3.8672e-01],, attentions=None)
        [ 0.1807,  1.7891, -0.8281, -1.0625, -0.4863]], device='cuda:0',
       dtype=torch.bfloat16, grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)
out(train): SequenceClassifierOutput(loss=tensor(0.6777, device='cuda:0', grad_fn=<NllLossBackward0>), logits=tensor([[ 0.1064,  1.6250, -0.9297, -0.8320, -0.7500],
        [-0.0100,  1.8828, -0.7578, -0.8594, -0.8945],
        [ 0.1133,  1.7344, -0.7734, -0.9922, -0.4473],
        [-0.0054,  1.5469, -0.6211, -0.7188, -0.6914],
        [-0.1504,  1.3984, -0.6914, -0.9375, -0.6484],
        [ 0.3809,  1.4688, -0.7031, -1.3906, -0.2637],
        [ 0.0796,  1.6953, -0.7461, -1.2344, -0.4941],
        [ 0.0752,  1.6016, -0.5039, -1.1875, -0.5508],
        [-0.2246,  1.7266, -0.7148, -1.0312, -0.4961],
        [-0.0295,  1.4844, -0.7500, -0.5352, -0.2432],
        [ 0.2422,  1.9062, -0.7500, -0.8203, -0.7070],
        [-0.2246,  1.5078, -0.6797, -0.8516, -0.7617],
        [ 0.8789,  0.8906, -1.2734, -0.9336,  0.6914],
        [ 0.2188,  1.3047, -0.2910, -0.8633, -0.6719],
        [ 0.0688,  1.6172, -0.6562, -1.1250, -0.6602],
        [ 0.3594,  1.6250, -1.0078, -0.7656, -0.5156]], device='cuda:0',
       dtype=torch.bfloat16, grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)
out(train): SequenceClassifierOutput(loss=tensor(0.7219, device='cuda:0', grad_fn=<NllLossBackward0>), logits=tensor([[ 0.2002,  1.4844, -0.9062, -1.1875, -0.2891],
        [-0.0588,  1.5156, -0.5273, -1.0469, -0.4434],
        [-0.2559,  1.8438, -0.7422, -0.8008, -0.5508],
        [ 0.2949,  1.5000, -0.4512, -0.9883, -0.4062],
        [ 0.3438,  1.1641, -1.0156, -1.0469, -0.4492],
        [-0.1318,  1.7500, -0.7383, -1.0312, -0.5820],
        [ 0.3574,  1.0547, -0.9297, -0.8906, -0.4707],
        [ 0.1147,  1.5625, -1.0781, -0.9297, -0.3809],
        [ 0.1631,  1.5078, -0.6211, -1.0234, -0.5898],
        [ 0.0291,  1.6172, -0.9492, -0.9414, -0.5586],
        [-0.1128,  1.8594, -0.7227, -1.0703, -0.7422],
        [-0.1406,  1.7188, -1.0312, -1.0859, -0.6797],
        [-0.0413,  1.0938, -0.6562, -1.0625, -0.5781],
        [ 0.1484,  1.3906, -0.4863, -1.0391, -0.4336],
        [-0.0938,  1.5000, -0.5703, -1.0469, -0.6094],
        [ 0.3184,  1.2109, -0.9062, -1.0547, -0.2676]], device='cuda:0',
       dtype=torch.bfloat16, grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)
out(train): SequenceClassifierOutput(loss=tensor(1.1044, device='cuda:0', grad_fn=<NllLossBackward0>), logits=tensor([[-0.3047,  1.4609, -0.6445, -0.9141, -1.1172],
        [ 0.2559,  1.7109, -0.9375, -0.6328, -0.5469],
        [ 0.4297,  1.8750, -0.7422, -0.8789, -0.5312],
        [-0.0549,  1.6484, -0.8242, -0.6562, -0.5430],
        [ 0.2773,  1.4766, -0.7539, -1.1562, -0.4062],
        [ 0.3145,  1.6797, -0.9727, -0.7305, -0.4102],
        [ 0.2168,  1.8672, -0.9141, -1.1094, -0.4844],
        [ 0.2275,  1.5781, -0.8633, -0.7969, -0.6445],
        [ 0.0767,  1.6172, -0.6094, -0.9062, -0.7656],
        [ 0.1787,  1.4141, -0.7617, -1.2812, -0.5781],
        [ 0.4453,  1.7344, -0.6953, -0.8867, -0.3848],
        [ 0.1475,  1.6172, -0.4941, -0.7305, -0.8008],
        [ 0.0723,  1.5000, -0.7227, -1.0312, -0.4980],
        [ 0.4316,  1.2969, -0.9219, -0.8125, -0.3496],
        [ 0.0918,  1.5703, -0.5781, -0.8516, -0.7930],
        [ 0.0913,  1.4688, -0.7891, -0.8047, -0.4570]], device='cuda:0',
       dtype=torch.bfloat16, grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)
out(train): SequenceClassifierOutput(loss=tensor(0.6356, device='cuda:0', grad_fn=<NllLossBackward0>), logits=tensor([[ 0.2207,  1.2109, -0.8320, -0.7852, -0.5859],
        [ 0.1299,  1.5078, -0.8555, -1.1797, -0.5742],
        [-0.1387,  1.7500, -0.7148, -1.0234, -0.4297],
        [ 0.4199,  1.6641, -0.8867, -1.2188, -0.3613],
        [ 0.5273,  1.3672, -0.5664, -0.6133, -0.5977],
        [ 0.3145,  1.3438, -0.8633, -0.7383, -0.3066],
        [-0.6055,  1.7109, -0.3281, -0.4473, -0.6992],
        [-0.1064,  1.6250, -0.3906, -0.8164, -0.8086],
        [ 0.0226,  1.9688, -0.8945, -0.4746, -0.6875],
        [ 0.1729,  1.3906, -0.9570, -0.8672, -0.5859],
        [-0.0815,  1.5156, -0.7070, -0.8984, -0.4824],
        [ 0.0786,  1.6250, -0.8516, -0.9180, -0.5234],
        [ 0.2139,  1.5938, -0.8867, -1.0938, -0.6641],
        [ 0.4238,  1.9453, -0.7148, -1.0703, -0.7109],
        [-0.1367,  1.6250, -0.7266, -1.0391, -0.5508],
        [ 0.2520,  0.6914, -0.7422, -0.6992,  0.0201]], device='cuda:0',
       dtype=torch.bfloat16, grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)
        [ 0.0049,  1.9297, -0.8555, -1.0781, -0.7617],4e-01, -3.8672e-01],, attentions=None)
        [-0.3594,  1.4453, -0.5195, -0.8516, -0.7500],4e-01, -3.8672e-01],, attentions=None)
        [-0.1338,  1.3906, -0.7734, -0.9102, -0.6836]], device='cuda:0',
       dtype=torch.bfloat16, grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)
out(train): SequenceClassifierOutput(loss=tensor(0.7137, device='cuda:0', grad_fn=<NllLossBackward0>), logits=tensor([[ 0.0143,  1.6719, -0.5586, -0.8438, -0.4727],
        [ 0.1992,  1.6250, -0.8750, -0.9844, -0.4277],
        [ 0.3125,  1.4609, -0.6562, -0.9531, -0.6680],
        [ 0.1182,  1.5469, -0.5664, -0.8750, -0.6328],
        [-0.1245,  1.7266, -0.2656, -0.9375, -0.9727],
        [ 0.3906,  1.1875, -0.8555, -0.7773, -0.6406],
        [ 0.0282,  1.5469, -0.8750, -0.8203, -0.5898]], device='cuda:0',
       dtype=torch.bfloat16, grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)
        [-0.3594,  1.4453, -0.5195, -0.8516, -0.7500],4e-01, -3.8672e-01],, attentions=None)
W0629 11:52:52.467000 139012379337344 torch/_dynamo/convert_frame.py:824]     result = inner_convert(
W0629 11:52:52.467000 139012379337344 torch/_dynamo/convert_frame.py:824]     result = inner_convert(
W0629 11:52:52.467000 139012379337344 torch/_dynamo/convert_frame.py:824]     result = inner_convert(
W0629 11:52:52.467000 139012379337344 torch/_dynamo/convert_frame.py:824]     result = inner_convert(
W0629 11:52:52.467000 139012379337344 torch/_dynamo/convert_frame.py:824]     result = inner_convert(
W0629 11:52:52.467000 139012379337344 torch/_dynamo/convert_frame.py:824]     result = inner_convert(
W0629 11:52:52.467000 139012379337344 torch/_dynamo/convert_frame.py:824]     result = inner_convert(
        [ 0.0035,  1.6016, -0.6133, -0.8398, -0.4668],4e-01, -3.8672e-01],, attentions=None)
        [ 0.3418,  1.0469, -0.7812, -1.0234, -0.8516],
        [-0.0408,  1.4062, -0.6914, -0.9219, -0.8516],
        [-0.0188,  1.4922, -0.6406, -0.7891, -0.7422]], device='cuda:0',
       dtype=torch.bfloat16, grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)
out(train): SequenceClassifierOutput(loss=tensor(1.0703, device='cuda:0', grad_fn=<NllLossBackward0>), logits=tensor([[ 0.1279,  1.4531, -0.9219, -1.1406, -0.4277],
        [-0.0522,  1.4531, -0.3184, -1.0547, -0.9492],
        [ 0.0140,  1.1484, -0.6914, -1.0703, -0.3730],
        [ 0.3438,  1.4766, -0.5234, -1.1875, -0.8594],
        [ 0.3184,  1.2422, -0.8203, -1.1641, -0.6562],
        [-0.2969,  1.5781, -0.5000, -0.6641, -0.9102],
        [-0.0659,  1.3828, -0.5195, -0.9336, -0.6016],
        [-0.4082,  1.5000, -0.4180, -0.7383, -0.7734],
        [ 0.3086,  1.4062, -0.6055, -0.7461, -0.6211],
        [-0.1553,  1.4844, -0.5859, -1.2422, -0.7031],
        [-0.0469,  1.2344, -0.3613, -1.0156, -0.6992],
        [-0.0908,  1.1953, -0.9023, -0.7461, -0.5039],
        [-0.0840,  1.7109, -0.5234, -0.7656, -0.9414],
        [ 0.1748,  1.6172, -0.7500, -0.8086, -0.7109],
        [-0.1494,  1.2656, -0.9375, -1.2578, -0.5234],
        [ 0.0889,  1.5156, -0.6328, -0.7969, -0.5742]], device='cuda:0',
       dtype=torch.bfloat16, grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)
        [ 0.0035,  1.6016, -0.6133, -0.8398, -0.4668],4e-01, -3.8672e-01],, attentions=None)
        [-0.1021,  1.5859, -0.3848, -1.0547, -0.7617],4e-01, -3.8672e-01],, attentions=None)
        [-0.2158,  1.4922, -0.3418, -0.8438, -0.6875]], device='cuda:0',
       dtype=torch.bfloat16, grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)
out(train): SequenceClassifierOutput(loss=tensor(0.6676, device='cuda:0', grad_fn=<NllLossBackward0>), logits=tensor([[-0.0476,  1.8672, -0.6484, -0.8945, -0.4004],
        [ 0.1436,  1.3828, -0.7500, -0.9258, -0.5664],
        [ 0.0374,  1.3438, -0.6055, -0.9336, -0.8828],
        [-0.1631,  1.6016, -0.5508, -1.1094, -0.7852],
        [ 0.4199,  1.3672, -0.7578, -0.7891, -0.5586],
        [ 0.0112,  1.5625, -0.5508, -0.8867, -0.6992],
        [-0.1406,  1.4609, -0.5078, -0.9297, -0.6758],
        [-0.0879,  1.3438, -0.5938, -0.8242, -0.5117],
        [-0.0162,  1.5078, -0.3750, -1.2891, -0.8125],
        [ 0.2949,  1.1953, -0.8203, -1.0078, -0.5977],
        [-0.2930,  1.7500, -0.5195, -1.0859, -0.7188],
        [-0.3027,  1.6094, -1.1484, -0.9844, -0.5742],
        [-0.1011,  1.7266, -0.4668, -0.9570, -0.4336],
        [ 0.9805,  1.0234, -1.1016, -0.4629,  0.0845],
        [-0.0903,  1.3828, -0.4434, -0.9375, -0.7812],
        [-0.0060,  1.5156, -0.4121, -0.9961, -0.8125]], device='cuda:0',
       dtype=torch.bfloat16, grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)
        [-0.1021,  1.5859, -0.3848, -1.0547, -0.7617],4e-01, -3.8672e-01],, attentions=None)
        [ 0.1846,  1.7422, -0.4473, -0.7578, -0.4746],4e-01, -3.8672e-01],, attentions=None)
        [-0.5078,  1.2656, -0.6328, -1.0078, -0.4336]], device='cuda:0',
       dtype=torch.bfloat16, grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)
out(train): SequenceClassifierOutput(loss=tensor(0.8777, device='cuda:0', grad_fn=<NllLossBackward0>), logits=tensor([[ 0.0457,  1.2891, -0.6445, -1.0781, -0.8906],
        [-0.0554,  1.5703, -0.2734, -0.6133, -0.9531],
        [-0.0903,  1.6875, -0.9336, -0.8477, -0.5977],
        [ 0.2344,  1.2578, -0.3887, -1.0391, -0.7734],
        [-0.0640,  1.2109, -0.6016, -1.1406, -0.6523],
        [ 0.2812,  1.5391, -0.5508, -0.8750, -0.2852],
        [ 0.0569,  1.0625, -0.4883, -1.0781, -0.6641],
        [ 0.0031,  1.5547, -0.8242, -1.3750, -0.5820],
        [ 0.2314,  1.6562, -0.7578, -1.0469, -0.3438],
        [ 0.5820,  1.5391, -0.6133, -0.6445, -0.6914],
        [-0.0042,  1.2422, -0.5547, -0.7578, -0.9805],
        [-0.0128,  1.3906, -0.8633, -0.7656, -0.5195],
        [ 0.3730,  1.4297, -0.8477, -1.0859, -0.3613],
        [ 0.2852,  1.6016, -0.7812, -0.9023, -0.5312],
        [ 0.1426,  1.5781, -0.8711, -0.7148, -0.6211],
        [-0.0820,  1.7891, -0.7891, -1.2969, -0.6250]], device='cuda:0',
       dtype=torch.bfloat16, grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)
out(train): SequenceClassifierOutput(loss=tensor(0.7711, device='cuda:0', grad_fn=<NllLossBackward0>), logits=tensor([[ 0.0104,  1.6328, -0.8945, -0.7891, -0.8594],
        [ 0.2041,  1.6797, -0.6406, -1.0312, -0.4883],
        [ 0.3418,  1.3047, -0.5938, -0.9062, -0.7734],
        [-0.0796,  1.5625, -0.4688, -0.8359, -0.5977],
        [-0.1338,  1.6562, -0.5234, -0.6836, -0.5312],
        [ 0.1650,  1.1562, -0.7148, -0.7539, -0.5664],
        [ 0.2246,  1.5469, -0.9805, -1.0234, -0.6406],
        [ 0.3652,  1.8828, -0.4004, -1.0000, -0.8047],
        [-0.0894,  1.9844, -0.5039, -0.7656, -0.5820],
        [ 0.1348,  1.6016, -0.2812, -1.1641, -0.5273],
        [ 0.2256,  1.9062, -0.7305, -0.9219, -0.7656],
        [-0.0033,  1.1406, -0.8281, -0.8359, -0.3340],
        [ 0.5664,  0.8633, -1.1562, -0.5781,  0.1182],
        [ 0.1846,  1.8203, -0.7734, -0.5156, -0.3086],
        [-0.0505,  1.7734, -0.8359, -0.9609, -0.4121],
        [-0.0593,  1.8203, -0.9180, -1.0859, -0.3730]], device='cuda:0',
       dtype=torch.bfloat16, grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)
out(train): SequenceClassifierOutput(loss=tensor(0.5889, device='cuda:0', grad_fn=<NllLossBackward0>), logits=tensor([[-0.0574,  1.8125, -0.7891, -0.5977, -0.4023],
        [ 0.2539,  1.5000, -0.4062, -0.7852, -0.5312],
        [ 0.1367,  1.5391, -0.8555, -1.1875, -0.4219],
        [ 0.1846,  1.1172, -1.3828, -1.1172, -0.0664],
        [ 0.2676,  1.6484, -0.7188, -1.0469, -0.6523],
        [-0.1045,  1.8750, -0.9062, -1.0469, -0.5820],
        [-0.0317,  1.3047, -0.9648, -1.0469, -0.4238],
        [-0.3340,  1.7109, -0.9688, -0.7656, -0.7422],
        [ 0.1699,  1.7188, -0.2393, -0.8008, -0.8125],
        [ 0.0300,  1.7812, -0.9688, -1.1484, -0.7969],
        [-0.1787,  1.8594, -0.8906, -0.8281, -0.4922],
        [-0.0258,  1.6328, -0.5195, -1.0859, -0.6484],
        [ 0.0344,  1.1719, -0.4375, -0.3848, -0.5156],
        [ 0.0540,  1.4922, -0.7031, -0.6836, -0.4414],
        [ 0.3008,  1.6875, -0.6680, -0.6992, -0.4297],
        [-0.0496,  1.7812, -0.2637, -0.8359, -0.9453]], device='cuda:0',
       dtype=torch.bfloat16, grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)
        [ 0.1846,  1.7422, -0.4473, -0.7578, -0.4746],4e-01, -3.8672e-01],, attentions=None)
        [ 0.5273,  1.6328, -0.9102, -0.6758, -0.6055],4e-01, -3.8672e-01],, attentions=None)
        [ 0.1318,  1.2109, -1.0781, -1.1797, -0.3008]], device='cuda:0',
       dtype=torch.bfloat16, grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)
out(train): SequenceClassifierOutput(loss=tensor(0.7574, device='cuda:0', grad_fn=<NllLossBackward0>), logits=tensor([[ 0.2734,  1.5234, -0.6289, -0.8516, -0.9570],
        [-0.0552,  1.6328, -0.8242, -0.9180, -0.4980],
        [-0.0070,  1.5000, -0.7734, -0.9961, -0.6523],
        [ 0.0243,  1.6562, -0.2256, -0.9727, -0.7031],
        [ 0.4141,  1.0156, -1.0781, -1.0391,  0.3867],
        [ 0.4082,  1.2109, -0.4648, -0.7578, -0.3750],
        [-0.0159,  1.5156, -1.0547, -1.0156, -0.4805],
        [-0.2080,  1.5547, -0.8242, -0.6758, -0.5312],
        [ 0.2812,  1.3906, -0.9766, -0.9531, -0.7734],
        [-0.2871,  1.5156, -0.6758, -0.6133, -0.7344],
        [ 0.1445,  1.6172, -1.1094, -0.8438, -0.3496],
        [ 0.0066,  1.7500, -1.0234, -1.0469, -0.6172],
        [-0.1318,  1.7500, -0.7266, -0.9688, -0.8281],
        [ 0.1865,  1.4922, -0.8047, -1.0156, -0.6211],
        [-0.1865,  1.4766, -0.2754, -0.4492, -0.7344],
        [ 0.1768,  1.4688, -0.8281, -0.9375, -0.6406]], device='cuda:0',
       dtype=torch.bfloat16, grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)
out(train): SequenceClassifierOutput(loss=tensor(0.6106, device='cuda:0', grad_fn=<NllLossBackward0>), logits=tensor([[-0.0879,  1.6875, -0.8828, -0.4570, -0.5977],
        [-0.0762,  1.8984, -0.9258, -1.0625, -0.2891],
        [-0.0898,  1.6562, -0.7695, -0.9102, -0.5898],
        [-0.1016,  1.9297, -0.7500, -0.9180, -0.6602],
        [-0.2656,  1.9766, -0.6484, -0.7656, -0.6836],
        [ 0.0889,  1.6875, -0.8984, -0.9375, -0.4082],
        [-0.1582,  1.7344, -0.3711, -0.8984, -0.3633],
        [ 0.1660,  1.4766, -0.8320, -0.8555, -0.5898],
        [ 0.0811,  1.2188, -0.8086, -0.4375, -0.3242],
        [ 0.3496,  1.2891, -0.6836, -1.1328, -0.6445],
        [ 0.2930,  1.3438, -0.8164, -0.9492, -0.6758],
        [-0.0554,  1.9141, -0.6523, -0.9180, -0.6484],
        [ 0.2715,  1.1094, -0.6797, -0.7852,  0.0781],
        [-0.0679,  1.4688, -0.6406, -0.9297, -0.5312],
        [-0.0157,  1.5312, -0.5312, -0.8438, -0.7188],
        [ 0.2637,  1.7578, -0.6641, -1.0078, -0.4590]], device='cuda:0',
       dtype=torch.bfloat16, grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)
out(train): SequenceClassifierOutput(loss=tensor(0.8368, device='cuda:0', grad_fn=<NllLossBackward0>), logits=tensor([[ 0.2754,  1.6562, -0.5195, -1.1250, -1.0703],
        [ 0.3730,  1.8359, -1.1250, -1.0781, -0.6719],
        [ 0.4453,  1.5312, -0.7031, -0.8438, -0.4902],
        [ 0.2695,  1.5859, -0.7852, -0.9688, -0.5508],
        [ 0.1445,  1.5859, -1.0469, -0.8281, -0.5430],
        [-0.0623,  1.4453, -0.5898, -0.8242, -0.6836],
        [ 0.1660,  1.6172, -0.8750, -1.2891, -0.5547],
        [ 0.0776,  1.4062, -0.8594, -1.0547, -0.8047],
        [-0.2441,  1.4375, -0.6641, -1.2031, -1.0312],
        [-0.1758,  1.5000, -0.4199, -0.7109, -0.6992],
        [ 0.2852,  1.3359, -1.0547, -0.9531, -0.0742],
        [ 0.2637,  1.7031, -0.4785, -0.6797, -0.5039],
        [ 0.1807,  1.4141, -0.9570, -0.9102, -0.4453],
        [ 0.0540,  0.9805, -0.5625, -0.6992, -0.4375],
        [ 0.2871,  1.7031, -0.8047, -0.9102, -0.5547],
        [-0.1201,  1.9062, -0.7031, -0.6992, -0.5781]], device='cuda:0',
       dtype=torch.bfloat16, grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)
        [ 0.5273,  1.6328, -0.9102, -0.6758, -0.6055],4e-01, -3.8672e-01],, attentions=None)
        [ 0.1328,  1.6641, -0.5312, -0.8438, -0.5898],4e-01, -3.8672e-01],, attentions=None)
        [ 0.0374,  1.6719, -0.9922, -0.8906, -0.4668]], device='cuda:0',
       dtype=torch.bfloat16, grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)
out(train): SequenceClassifierOutput(loss=tensor(1.0531, device='cuda:0', grad_fn=<NllLossBackward0>), logits=tensor([[ 0.2334,  1.4844, -0.5156, -1.0859, -0.8281],
        [-0.3203,  1.4062, -0.7031, -1.1641, -0.6602],
        [ 0.3633,  1.9219, -0.7891, -0.6719, -0.2285],
        [ 0.1777,  1.2734, -0.2314, -0.8594, -0.6172],
        [ 0.2080,  1.9297, -0.9336, -0.9102, -0.6445],
        [-0.0054,  1.4922, -1.0859, -0.9648, -0.8008],
        [ 0.1045,  1.3438, -0.6680, -0.6445, -0.6758],
        [-0.4160,  1.7891, -0.6914, -1.0156, -0.5195],
        [ 0.4922,  1.6641, -0.6875, -0.5742, -0.2617],
        [ 0.4668,  1.4219, -0.7305, -0.6133, -0.4395],
        [-0.1553,  2.0156, -0.7617, -1.0625, -0.5195],
        [ 0.0972,  1.5391, -0.9336, -1.2031, -0.6211],
        [ 0.1924,  1.5078, -0.5430, -0.7500, -0.7109],
        [ 0.3066,  1.5234, -0.5820, -0.8203, -0.5898],
        [-0.0303,  1.7031, -0.4805, -0.9023, -0.4980],
        [-0.0310,  1.6797, -0.7695, -1.0781, -0.6641]], device='cuda:0',
       dtype=torch.bfloat16, grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)
out(train): SequenceClassifierOutput(loss=tensor(0.8700, device='cuda:0', grad_fn=<NllLossBackward0>), logits=tensor([[ 0.0957,  1.5391, -1.0781, -1.3750,  0.1143],
        [ 0.2236,  1.2656, -0.9961, -0.6367, -0.4883],
        [ 0.1055,  1.8828, -0.6094, -0.8711, -0.5938],
        [ 0.1128,  1.8516, -0.7930, -1.0781, -0.6992],
        [ 0.0947,  1.3984, -0.7734, -1.0469, -0.4199],
        [ 0.2656,  1.5391, -0.9375, -0.8984, -0.5273],
        [ 0.4805,  1.5000, -1.1328, -0.9492, -0.5078],
        [ 0.0640,  1.5469, -0.4648, -0.8711, -0.9492],
        [ 0.0115,  1.2969, -0.6094, -0.9023, -0.7188],
        [-0.0767,  1.6094, -0.7500, -1.0000, -0.2832],
        [ 0.2812,  1.9922, -0.6797, -0.8516, -0.4199],
        [ 0.2158,  1.4297, -0.5547, -0.7734, -0.9492],
        [ 0.1377,  1.6406, -0.7617, -0.8828, -0.6289],
        [ 0.0718,  1.3750, -0.5352, -0.5234, -0.4395],
        [ 0.0452,  1.7500, -0.5625, -0.6758, -0.4414],
        [-0.2324,  1.6016, -1.0234, -0.7188, -0.4668]], device='cuda:0',
       dtype=torch.bfloat16, grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)
out(train): SequenceClassifierOutput(loss=tensor(0.5889, device='cuda:0', grad_fn=<NllLossBackward0>), logits=tensor([[-9.2285e-02,  1.7578e+00, -5.8594e-01, -1.1094e+00, -6.4844e-01],
        [-2.7847e-04,  1.4531e+00, -6.2891e-01, -9.0625e-01, -9.0234e-01],
        [-2.7148e-01,  1.5547e+00, -4.9219e-01, -5.6641e-01, -5.3516e-01],
        [ 2.0898e-01,  1.3906e+00, -7.9297e-01, -1.1016e+00, -4.9219e-01],
        [-3.6523e-01,  1.4375e+00, -4.3945e-01, -8.8281e-01, -4.5508e-01],
        [ 4.5312e-01,  1.1875e+00, -6.6797e-01, -9.6875e-01, -6.6406e-01],
        [ 4.1602e-01,  1.5703e+00, -7.3828e-01, -1.2422e+00, -4.1211e-01],
        [ 1.3867e-01,  1.6094e+00, -9.6875e-01, -1.0234e+00, -3.9258e-01],
        [ 9.6680e-02,  1.7734e+00, -4.5117e-01, -1.0156e+00, -5.7031e-01],
        [ 1.9043e-01,  1.7656e+00, -8.0469e-01, -1.1016e+00, -7.8125e-01],
        [ 1.9922e-01,  1.4453e+00, -1.2422e+00, -1.1094e+00, -2.7148e-01],
        [-2.1387e-01,  1.7734e+00, -5.7812e-01, -8.3594e-01, -2.0605e-01],
        [ 3.8086e-01,  1.5938e+00, -7.3438e-01, -1.1953e+00, -3.3398e-01],
        [-1.3574e-01,  1.4766e+00, -5.8594e-01, -6.6406e-01, -6.9922e-01],
        [-5.3711e-02,  1.2031e+00, -6.7578e-01, -9.0234e-01, -5.4297e-01],
        [ 3.3203e-01,  1.1250e+00, -1.0547e+00, -8.5938e-01, -4.2969e-01]],
       device='cuda:0', dtype=torch.bfloat16, grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)
        [ 0.1328,  1.6641, -0.5312, -0.8438, -0.5898],4e-01, -3.8672e-01],, attentions=None)
        [-8.3496e-02,  1.6406e+00, -9.6484e-01, -7.4609e-01, -6.2500e-01],, attentions=None)
        [-1.1084e-01,  1.7578e+00, -6.2109e-01, -1.0312e+00, -2.7734e-01]],
       device='cuda:0', dtype=torch.bfloat16, grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)
out(train): SequenceClassifierOutput(loss=tensor(0.6219, device='cuda:0', grad_fn=<NllLossBackward0>), logits=tensor([[-0.1079,  1.5938, -1.0234, -0.9336, -0.2676],
        [ 0.1250,  1.4844, -0.7148, -0.8867, -0.5703],
        [ 0.0698,  1.5234, -0.8633, -0.9648, -0.2832],
        [-0.1699,  1.4688, -0.7852, -0.9570, -0.5586],
        [ 0.2285,  1.7188, -0.9961, -1.0156, -0.4355],
        [ 0.1060,  1.5391, -0.7539, -0.7617, -0.5859],
        [ 0.2715,  1.5391, -0.8242, -0.8750, -0.3730],
        [-0.1113,  1.2969, -0.3086, -0.7852, -0.3340],
        [ 0.1270,  1.4766, -0.6641, -0.7812, -0.6484],
        [ 0.1738,  1.1953, -0.8555, -0.8594, -0.2051],
        [ 0.3984,  1.7500, -0.8867, -0.8008, -0.6055],
        [-0.1729,  1.7344, -0.5234, -0.7734, -0.5312],
        [-0.0261,  1.4062, -0.3945, -1.1172, -0.8945],
        [ 0.1104,  1.5391, -0.7578, -0.9336, -0.3965],
        [-0.2373,  1.7891, -0.4043, -1.2422, -0.9375],
        [-0.1611,  1.3359, -0.3457, -0.9648, -0.6094]], device='cuda:0',
       dtype=torch.bfloat16, grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)
out(train): SequenceClassifierOutput(loss=tensor(0.6688, device='cuda:0', grad_fn=<NllLossBackward0>), logits=tensor([[ 0.0732,  1.2578, -0.8398, -1.1875, -0.5820],
        [-0.0294,  1.4062, -0.4160, -0.9023, -0.4805],
        [ 0.0928,  1.3125, -0.4805, -0.9258, -0.7188],
        [ 0.2031,  1.6562, -0.6602, -0.8984, -0.5781],
        [-0.0062,  1.6016, -0.8125, -0.9180, -0.5664],
        [-0.1631,  1.7734, -0.3047, -1.5000, -0.9688],
        [ 0.0583,  1.1797, -0.8398, -0.9336, -0.6289],
        [-0.1650,  1.5859, -0.8594, -0.5547, -0.5703],
        [ 0.1484,  1.3750, -0.6289, -1.4453, -0.5352],
        [ 0.1318,  1.4375, -0.5430, -0.9297, -0.6328],
        [ 0.1060,  1.7422, -0.7617, -0.7031, -0.6250],
        [ 0.1934,  1.7891, -0.9414, -0.6992, -0.4844],
        [ 0.1699,  1.4141, -0.6562, -1.1875, -0.6445],
        [-0.1865,  1.6328, -1.0469, -1.1484, -1.0312],
        [ 0.1387,  1.4531, -0.3398, -1.0000, -0.6055],
        [ 0.3457,  1.2500, -0.8516, -0.9258, -0.3008]], device='cuda:0',
       dtype=torch.bfloat16, grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)
out(train): SequenceClassifierOutput(loss=tensor(0.5764, device='cuda:0', grad_fn=<NllLossBackward0>), logits=tensor([[ 1.6602e-01,  1.4141e+00, -3.9648e-01, -9.3359e-01, -4.7852e-01],
        [-3.6621e-02,  1.7188e+00, -5.2734e-01, -1.0234e+00, -7.5391e-01],
        [-1.6699e-01,  1.6797e+00, -4.6289e-01, -5.5469e-01, -6.7969e-01],
        [ 1.2402e-01,  1.5625e+00, -7.4609e-01, -7.8516e-01, -4.7266e-01],
        [ 1.6992e-01,  1.6406e+00, -6.7969e-01, -1.0938e+00, -6.5625e-01],
        [-1.5411e-03,  1.5312e+00, -9.6094e-01, -1.1094e+00, -2.3438e-01],
        [ 2.5977e-01,  1.5391e+00, -7.5781e-01, -1.0625e+00, -5.4688e-01],
        [ 3.5400e-02,  1.6641e+00, -7.0312e-01, -7.5000e-01, -5.2344e-01],
        [-5.0781e-02,  1.7109e+00, -7.1484e-01, -5.3906e-01, -7.0312e-01],
        [ 2.0215e-01,  1.7812e+00, -9.5703e-01, -6.0547e-01, -5.2734e-01],
        [-3.4766e-01,  1.5234e+00, -6.9531e-01, -9.2188e-01, -6.0156e-01],
        [ 5.0781e-01,  1.3203e+00, -5.7812e-01, -4.9219e-01, -5.9766e-01],
        [-1.2109e-01,  1.4219e+00, -7.5391e-01, -9.2969e-01, -4.7656e-01],
        [-2.5195e-01,  1.7031e+00, -5.2344e-01, -8.5938e-01, -3.7109e-01],
        [ 2.2461e-01,  1.2734e+00, -7.6172e-01, -1.1094e+00, -7.5391e-01],
        [ 2.5586e-01,  1.2578e+00, -3.7109e-01, -5.5859e-01, -8.8672e-01]],
       device='cuda:0', dtype=torch.bfloat16, grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)
        [-8.3496e-02,  1.6406e+00, -9.6484e-01, -7.4609e-01, -6.2500e-01],, attentions=None)
        [ 0.1187,  1.5234, -0.8438, -1.0469, -0.3691],9e-01, -6.2500e-01],, attentions=None)
        [ 0.0469,  1.6484, -0.4199, -0.8125, -0.3008]], device='cuda:0',
       dtype=torch.bfloat16, grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)
out(train): SequenceClassifierOutput(loss=tensor(0.6599, device='cuda:0', grad_fn=<NllLossBackward0>), logits=tensor([[-0.2910,  1.9531, -0.8984, -1.1250, -0.4961],
        [-0.0452,  1.5156, -0.6719, -0.6992, -0.7539],
        [ 0.0366,  1.5859, -0.6719, -1.0625, -0.5234],
        [ 0.0737,  1.5312, -1.0625, -0.9922, -0.4512],
        [-0.1436,  2.1250, -0.3496, -0.7578, -0.4141],
        [ 0.3359,  1.2891, -0.7539, -0.9609, -0.3965],
        [-0.0820,  1.8047, -0.5742, -1.2500, -0.3379],
        [-0.0417,  1.6328, -0.6055, -0.8555, -0.4180],
        [ 0.0199,  1.2578, -0.3594, -1.3750, -0.6602],
        [ 0.1050,  1.6172, -0.6484, -0.8242, -0.8047],
        [ 0.4102,  1.3281, -0.9375, -0.9883, -0.7109],
        [-0.0089,  1.6953, -0.3262, -1.0078, -0.7695],
        [-0.1787,  1.9219, -0.6094, -0.6641, -0.5898],
        [ 0.1533,  1.5703, -0.8945, -0.7617, -0.6484],
        [ 0.0835,  1.7031, -0.5625, -1.3359, -0.4414],
        [-0.0258,  1.8125, -0.5859, -1.1016, -0.5156]], device='cuda:0',
       dtype=torch.bfloat16, grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)
out(train): SequenceClassifierOutput(loss=tensor(0.6365, device='cuda:0', grad_fn=<NllLossBackward0>), logits=tensor([[-0.3242,  1.7188, -0.7344, -0.6641, -0.9219],
        [ 0.0674,  1.4141, -0.8594, -1.1250, -0.3535],
        [ 0.1338,  1.4375, -1.2969, -1.0938, -0.5781],
        [ 0.3145,  1.7422, -0.7891, -1.2344, -0.5781],
        [-0.1064,  1.5469, -0.6602, -1.1484, -0.7344],
        [ 0.1680,  1.6172, -0.5547, -0.5039, -0.1235],
        [ 0.4316,  1.1328, -1.5156, -0.8359, -0.2988],
        [-0.3301,  1.5469, -0.4238, -0.8477, -0.5273],
        [ 0.0137,  1.6172, -0.7812, -1.2266, -0.5781],
        [ 0.0210,  1.5312, -0.7891, -0.9570, -0.6758],
        [ 0.2383,  1.5547, -0.7539, -0.6914, -0.8711],
        [ 0.3574,  1.7734, -0.6719, -0.8477, -0.6211],
        [-0.0510,  1.6797, -0.5430, -0.6680, -0.7070],
        [ 0.0825,  1.3438, -0.8750, -0.5547, -0.6641],
        [ 0.1021,  1.6562, -1.2891, -0.9492, -0.8594],
        [-0.1357,  1.8281, -0.5312, -0.8867, -0.4980]], device='cuda:0',
       dtype=torch.bfloat16, grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)
out(train): SequenceClassifierOutput(loss=tensor(0.7113, device='cuda:0', grad_fn=<NllLossBackward0>), logits=tensor([[ 0.0894,  1.8438, -1.0312, -0.9414, -0.3281],
        [-0.1021,  1.6484, -0.5508, -1.1875, -0.5352],
        [-0.0253,  1.6797, -0.7812, -0.6641, -0.5547],
        [-0.0598,  1.5859, -0.5430, -1.1562, -0.7734],
        [-0.1279,  1.4375, -0.5938, -0.9922, -0.4766],
        [ 0.1689,  1.7969, -0.8516, -0.9453, -0.6992],
        [-0.1318,  1.9297, -0.6094, -0.9961, -0.5625],
        [ 0.1572,  1.2578, -0.7227, -0.9648, -0.5156],
        [ 0.1060,  1.8906, -0.5703, -0.9609, -0.6562],
        [ 0.2207,  1.5469, -0.8477, -0.8125, -0.5352],
        [-0.1079,  1.4844, -0.5977, -0.9844, -0.3848],
        [-0.0645,  1.5781, -0.6289, -0.8750, -0.5820],
        [-0.0371,  1.5469, -0.8164, -0.6172, -0.6328],
        [ 0.5078,  1.4922, -0.6602, -1.1484, -0.8008],
        [ 0.2227,  1.7188, -0.7305, -0.7266, -0.2832],
        [ 0.0155,  2.0312, -1.0234, -1.0391, -0.5703]], device='cuda:0',
       dtype=torch.bfloat16, grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)
        [ 0.1187,  1.5234, -0.8438, -1.0469, -0.3691],9e-01, -6.2500e-01],, attentions=None)
        [-0.0510,  1.6641, -0.8945, -0.8008, -0.4512],9e-01, -6.2500e-01],, attentions=None)
        [-0.1758,  1.3125, -0.4453, -0.4023, -0.3848]], device='cuda:0',
       dtype=torch.bfloat16, grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)
out(train): SequenceClassifierOutput(loss=tensor(0.5052, device='cuda:0', grad_fn=<NllLossBackward0>), logits=tensor([[-0.1367,  1.7812, -1.1250, -0.9609, -0.5195],
        [ 0.3672,  1.2344, -0.8242, -0.8711, -0.2734],
        [ 0.2793,  1.5625, -0.4766, -1.2109, -0.4395],
        [ 0.1973,  1.6250, -0.7969, -0.9531, -0.5234],
        [-0.4219,  1.4453, -0.6680, -0.6680, -0.6992],
        [-0.1416,  1.2812, -0.4746, -0.8633, -0.3945],
        [ 0.2295,  1.6406, -0.6406, -0.7422, -0.8828],
        [-0.1963,  1.5547, -0.6758, -0.7344, -0.6289],
        [ 0.1211,  1.3438, -0.2373, -0.8203, -0.6836],
        [-0.0498,  1.6094, -0.6758, -1.0625, -0.4668],
        [ 0.1133,  1.6641, -0.6094, -0.8984, -0.6445],
        [-0.0947,  1.6172, -0.8750, -1.0156, -0.5430],
        [ 0.3770,  1.4375, -0.5430, -0.6016, -0.5430],
        [ 0.2002,  1.2812, -0.9023, -0.7266, -0.4883],
        [-0.1973,  1.7578, -0.7656, -1.0312, -0.3105],
        [ 0.1338,  1.7344, -0.7344, -0.7539, -0.3828]], device='cuda:0',
       dtype=torch.bfloat16, grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)
out(train): SequenceClassifierOutput(loss=tensor(1.0227, device='cuda:0', grad_fn=<NllLossBackward0>), logits=tensor([[ 0.1069,  1.7578, -0.3711, -1.1562, -0.9102],
        [ 0.3477,  1.4297, -0.7734, -0.6523, -0.5234],
        [ 0.0344,  1.7578, -1.2266, -0.9688, -0.1670],
        [-0.3477,  1.5469, -0.8828, -0.8516, -0.4805],
        [-0.0195,  1.8125, -0.4688, -0.9219, -0.4980],
        [ 0.0287,  1.5859, -0.4316, -0.6211, -0.5938],
        [ 0.4141,  0.7969, -0.9570, -0.5195,  0.2168],
        [-0.3613,  1.6484, -0.3516, -0.6406, -0.6836],
        [ 0.2812,  1.5938, -0.8359, -0.7344, -0.4707],
        [-0.1064,  1.3984, -0.5195, -0.9258, -0.4316],
        [ 0.0708,  1.5078, -0.7305, -1.1641, -0.9180],
        [-0.0977,  1.8984, -0.6758, -0.7461, -0.4980],
        [ 0.1572,  1.5625, -0.9375, -1.0078, -0.4238],
        [ 0.3770,  1.5625, -1.3047, -1.0078, -0.5469],
        [-0.1826,  1.3281, -0.4219, -1.0859, -0.7812],
        [ 0.1914,  1.7031, -0.8203, -0.5977, -0.8594]], device='cuda:0',
       dtype=torch.bfloat16, grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)
out(train): SequenceClassifierOutput(loss=tensor(0.8691, device='cuda:0', grad_fn=<NllLossBackward0>), logits=tensor([[-0.1855,  1.6172, -0.6758, -1.0312, -0.3164],
        [ 0.0420,  1.3750, -0.4336, -0.9219, -0.7070],
        [ 0.1147,  1.5234, -1.0469, -1.3281, -0.4824],
        [ 0.2520,  1.2891, -0.4980, -0.7617, -0.8789],
        [ 0.3008,  1.3516, -0.7617, -1.1016, -0.9531],
        [ 0.3184,  0.9492, -1.0469, -0.6562, -0.6289],
        [ 0.0698,  1.2656, -0.8008, -0.9375, -0.4531],
        [ 0.1992,  1.2500, -0.5664, -0.9531, -0.6562],
        [ 0.0160,  1.6875, -0.7383, -1.1094, -0.9219],
        [ 0.2539,  1.4531, -0.8555, -1.1484, -0.4512],
        [-0.1523,  1.6172, -0.8945, -0.7773, -0.3066],
        [-0.0154,  0.8359, -1.2344, -0.8789, -0.4219],
        [-0.0542,  1.8125, -0.6641, -0.8281, -0.5430],
        [ 0.0099,  2.0000, -0.7305, -0.8477, -0.5391],
        [-0.1377,  1.4375, -0.5195, -0.8086, -0.4375],
        [ 0.1348,  1.6406, -0.4668, -0.8242, -0.8125]], device='cuda:0',
       dtype=torch.bfloat16, grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)
        [-0.0510,  1.6641, -0.8945, -0.8008, -0.4512],9e-01, -6.2500e-01],, attentions=None)
        [ 7.2754e-02,  1.7500e+00, -6.1719e-01, -1.2188e+00, -6.9141e-01],, attentions=None)
        [-7.2266e-02,  1.7891e+00, -5.6250e-01, -8.3203e-01, -6.6797e-01]],
       device='cuda:0', dtype=torch.bfloat16, grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)
out(train): SequenceClassifierOutput(loss=tensor(0.7177, device='cuda:0', grad_fn=<NllLossBackward0>), logits=tensor([[ 0.3340,  1.6094, -0.8984, -1.0703, -0.3730],
        [-0.1494,  1.9453, -0.5898, -1.0312, -0.6680],
        [-0.0261,  1.5781, -0.8633, -1.2500, -0.7695],
        [ 0.0052,  1.6250, -0.7617, -1.0469, -0.6445],
        [-0.0491,  1.2656, -0.9102, -0.8516, -0.6289],
        [-0.1416,  1.4922, -0.7031, -1.1094, -0.4902],
        [ 0.2754,  1.2500, -0.7305, -1.0156, -0.6055],
        [-0.0019,  1.2344, -1.1562, -1.1719, -0.5078],
        [ 0.2852,  1.6250, -0.5039, -1.0156, -0.4902],
        [ 0.2969,  1.5547, -0.6484, -0.8867, -0.7109],
        [ 0.0342,  1.6953, -0.8594, -0.7344, -0.4727],
        [ 0.2734,  1.4141, -0.6719, -1.0156, -0.5703],
        [-0.0188,  1.4219, -0.7617, -1.1484, -0.5000],
        [ 0.1279,  1.1875, -0.5430, -0.7656, -0.4961],
        [-0.3984,  1.5703, -0.6406, -0.9961, -0.6133],
        [ 0.0410,  1.2734, -1.0625, -0.9102, -0.4316]], device='cuda:0',
       dtype=torch.bfloat16, grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)
out(train): SequenceClassifierOutput(loss=tensor(1.0872, device='cuda:0', grad_fn=<NllLossBackward0>), logits=tensor([[-0.1553,  1.4688, -0.4082, -0.9531, -0.9609],
        [-0.0105,  1.9375, -1.0469, -0.6055, -0.6055],
        [-0.0500,  1.5547, -0.6211, -1.0781, -0.5234],
        [-0.2041,  1.4844, -0.7422, -0.7617, -0.4570],
        [ 0.1885,  1.6094, -0.7109, -0.9258, -0.6523],
        [-0.2061,  1.5391, -1.0859, -0.7031, -0.6758],
        [-0.0845,  1.5391, -0.7578, -0.7070, -0.5352],
        [ 0.1445,  1.4609, -0.6641, -0.8750, -0.4570],
        [ 0.1104,  1.4922, -0.6289, -0.6914, -0.8359],
        [-0.1021,  1.7188, -0.4316, -0.9453, -0.5000],
        [ 0.1934,  1.5000, -0.7539, -1.0938, -0.4551],
        [ 0.0618,  1.2422, -0.8203, -0.6836, -0.3027],
        [-0.0532,  1.6562, -0.7227, -0.8164, -0.7695],
        [ 0.4023,  1.2109, -0.9922, -0.6836, -0.1797],
        [ 0.0052,  1.5234, -0.6523, -0.5820, -0.5820],
        [-0.1494,  1.5703, -0.8906, -0.4805, -0.3301]], device='cuda:0',
       dtype=torch.bfloat16, grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)
out(train): SequenceClassifierOutput(loss=tensor(0.6354, device='cuda:0', grad_fn=<NllLossBackward0>), logits=tensor([[ 0.3711,  1.1797, -0.4902, -0.8789, -0.4941],
        [-0.2402,  1.5156, -0.4668, -0.9570, -0.8164],
        [-0.0425,  1.6953, -0.7148, -1.0859, -0.4961],
        [ 0.3594,  1.4219, -0.7383, -0.7734, -0.5625],
        [-0.0845,  1.4922, -0.3379, -0.5156, -0.5586],
        [ 0.0630,  1.4922, -0.5117, -0.8867, -0.5039],
        [ 0.2070,  1.7109, -0.7227, -0.6914, -0.5625],
        [ 0.2168,  1.3594, -0.1260, -0.7773, -0.8320],
        [ 0.2275,  1.7109, -0.8789, -0.8789, -0.4062],
        [ 0.0042,  1.3906, -0.7500, -1.0391, -0.4844],
        [ 0.2324,  1.8750, -0.8516, -0.8945, -0.5117],
        [ 0.0698,  1.6797, -1.1562, -1.1016, -0.5547],
        [ 0.0576,  1.4297, -0.8086, -0.9727, -0.7969],
        [-0.0037,  1.8047, -0.8203, -0.7109, -0.8438],
        [-0.0322,  1.6172, -0.6484, -0.9961, -0.5625],
        [ 0.3398,  0.3047, -0.9727, -0.5352,  0.3711]], device='cuda:0',
       dtype=torch.bfloat16, grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)
out(train): SequenceClassifierOutput(loss=tensor(0.8552, device='cuda:0', grad_fn=<NllLossBackward0>), logits=tensor([[ 0.1299,  1.8047, -0.8555, -0.6914, -0.7227],
        [ 0.0498,  1.9688, -0.8633, -1.0156, -0.2852],
        [ 0.2910,  1.4453, -0.8906, -0.9648, -0.1689],
        [ 0.2520,  1.5703, -1.0312, -0.9297, -0.2363],
        [ 0.3691,  1.3281, -0.5859, -0.7305, -0.7109],
        [ 0.0532,  1.8359, -0.6094, -1.0391, -0.7656],
        [ 0.3770,  1.6562, -0.5312, -0.8633, -0.3672],
        [ 0.4707,  1.3281, -0.6484, -0.8789, -0.9531],
        [-0.1494,  1.6172, -0.4785, -1.0234, -0.7891],
        [-0.1113,  0.9531, -0.9609, -0.5078, -0.4414],
        [ 0.0214,  1.9219, -0.7773, -0.6406, -0.4375],
        [ 0.2295,  1.5781, -0.5391, -0.9688, -0.5117],
        [ 0.1436,  1.6641, -0.5977, -1.0547, -0.8125],
        [-0.2578,  1.7734, -0.9922, -0.8906, -0.5547],
        [ 0.1426,  1.4375, -0.9258, -0.6406, -0.4727],
        [-0.0156,  1.9297, -0.6797, -1.1406, -0.3887]], device='cuda:0',
       dtype=torch.bfloat16, grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)
        [ 7.2754e-02,  1.7500e+00, -6.1719e-01, -1.2188e+00, -6.9141e-01],, attentions=None)
W0629 11:52:52.467000 139012379337344 torch/_dynamo/convert_frame.py:824]     result = inner_convert(
        [ 7.2754e-02,  1.7500e+00, -6.1719e-01, -1.2188e+00, -6.9141e-01],, attentions=None)
W0629 11:52:52.467000 139012379337344 torch/_dynamo/convert_frame.py:824]     result = inner_convert(
W0629 11:52:52.467000 139012379337344 torch/_dynamo/convert_frame.py:824]     result = inner_convert(
W0629 11:52:52.467000 139012379337344 torch/_dynamo/convert_frame.py:824]     result = inner_convert(
W0629 11:52:52.467000 139012379337344 torch/_dynamo/convert_frame.py:824]     result = inner_convert(
W0629 11:52:52.467000 139012379337344 torch/_dynamo/convert_frame.py:824]     result = inner_convert(
W0629 11:52:52.467000 139012379337344 torch/_dynamo/convert_frame.py:824]     result = inner_convert(
W0629 11:52:52.467000 139012379337344 torch/_dynamo/convert_frame.py:824]     result = inner_convert(
W0629 11:52:52.467000 139012379337344 torch/_dynamo/convert_frame.py:824]     result = inner_convert(
        [ 7.2754e-02,  1.7500e+00, -6.1719e-01, -1.2188e+00, -6.9141e-01],, attentions=None)
        [-0.1602,  0.1094,  0.1572, -0.3730, -0.9062],8e+00, -6.9141e-01],, attentions=None)
        [-0.1982, -0.1621,  0.4102, -0.2422, -0.3613],
        [-0.1787,  0.3145,  0.5820,  0.2188, -0.9062],
        [ 0.0259, -0.1650,  0.7812,  0.0544, -0.9141],
        [-0.5625,  0.0583,  0.3457,  0.1094, -0.7695],
        [-0.2129, -0.2178,  0.3418, -0.0986, -0.6445],
        [-0.3047,  0.1709,  0.6094, -0.1201, -0.8789],
        [-0.1943, -0.0618,  0.2490, -0.2041, -0.5430],
        [-0.4512,  0.1553,  0.2949, -0.4551, -0.7227],
        [-0.4199, -0.0276,  0.2695, -0.0447, -0.6758],
        [-0.2188, -0.0132,  0.4707, -0.4570, -0.6055],
        [-0.6875, -0.2402, -0.0952, -0.4023, -0.6602],
        [-0.4688,  0.0854,  0.5039, -0.0688, -0.2715]], device='cuda:0',
       dtype=torch.bfloat16, grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None), SequenceClassifierOutput(loss=tensor(1.5708, device='cuda:0', grad_fn=<NllLossBackward0>), logits=tensor([[-0.1172, -0.2441,  0.3945,  0.0291, -0.5859],
        [-0.5078,  0.2363,  0.4473, -0.3652, -0.6289],
        [-0.3008, -0.3672,  0.5156, -0.0996, -0.7852],
        [-0.3672, -0.1270,  0.2871,  0.2715, -0.9570],
        [-0.0986, -0.1416,  0.4473,  0.0082, -0.4844],
        [-0.2148,  0.4512, -0.4082, -0.2832, -0.3926],
        [-0.1523,  0.3770,  0.6133, -0.5938, -0.4316],
        [-0.5039,  0.2793,  0.2080, -0.3379, -0.8320],
        [-0.2109,  0.1631,  0.5664, -0.1836, -0.5781],
        [-0.0898,  0.2275,  0.2090, -0.0679, -0.8945],
        [ 0.1270, -0.1240,  0.6094, -0.3555, -0.5078],
        [-0.3301, -0.3438,  0.3047, -0.2393, -0.2598],
        [-0.3418, -0.2891,  0.3125, -0.2471, -1.0078],
        [-0.3516, -0.0981,  0.5859, -0.1709, -0.7930],
        [-0.4180,  0.0928,  0.4023, -0.2305, -0.7305],
        [-0.1030, -0.1328,  0.2773, -0.1396, -1.1719]], device='cuda:0',
       dtype=torch.bfloat16, grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None), SequenceClassifierOutput(loss=tensor(1.6460, device='cuda:0', grad_fn=<NllLossBackward0>), logits=tensor([[-0.1357,  0.0084,  0.4023, -0.3145, -0.8125],
        [ 0.0015,  0.4551,  0.6836, -0.1982, -0.5820],
        [-0.2520, -0.2041,  0.9336, -0.1758, -0.4590],
        [-0.3691, -0.1016,  0.3652, -0.2090, -0.7031],
        [-0.4219, -0.5508,  0.3477, -0.0845, -0.6211],
        [ 0.0981, -0.1445,  0.8594, -0.4434, -0.9844],
        [-0.3574, -0.0408,  0.5508, -0.3340, -0.7773],
        [-0.1235,  0.0767,  0.1309, -0.4082, -0.6328],
        [-0.2461,  0.2354,  0.5742, -0.1611, -0.3691],
        [-0.1021, -0.2148, -0.0044, -0.3145, -0.6680],
        [-0.3164, -0.1211,  0.5273, -0.2891, -0.3711],
        [-0.1484, -0.0147,  0.3750, -0.3340, -0.6836],
        [-0.2207,  0.1904,  0.4922, -0.2344, -0.1143],
        [-0.1289, -0.1289,  0.4570, -0.4863, -0.4043],
        [-0.5312,  0.0430,  0.3828, -0.1797, -0.4648],
        [-0.1436, -0.4551,  0.0304, -0.1836, -0.4961]], device='cuda:0',
       dtype=torch.bfloat16, grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None), SequenceClassifierOutput(loss=tensor(1.5200, device='cuda:0', grad_fn=<NllLossBackward0>), logits=tensor([[-0.6914,  0.1816,  0.3086, -0.1240, -0.6562],
        [-0.3320, -0.0781,  0.3867, -0.3613, -0.7891],
        [-0.1196,  0.0981, -0.0238, -0.1406, -0.3164],
        [-0.2402,  0.0864,  0.4395, -0.2422, -0.5078],
        [-0.5000, -0.0308,  0.3770, -0.3262, -0.5781],
        [-0.4453,  0.4023,  0.1768, -0.0737, -0.8164],
        [ 0.2432, -0.1445,  0.1904, -0.3145, -0.2148],
        [-0.6523, -0.1074,  0.1235, -0.2422, -0.5859],
        [-0.6523, -0.0938,  0.2871, -0.1494, -0.7695],
        [-0.5547,  0.1309,  0.2891, -0.0581, -0.4902],
        [-0.1436, -0.2715,  0.3340, -0.4219, -1.0312],
        [-0.3887, -0.0535,  0.2734, -0.1768, -0.3477],
        [-0.2578,  0.1172,  0.1807, -0.4238, -0.8203],
        [-0.2520,  0.2344, -0.0454,  0.0605, -0.2539],
        [-0.3926,  0.0210, -0.0791, -0.3008, -0.5312],
        [-0.2617, -0.0889,  0.5312, -0.6172, -0.5977]], device='cuda:0',
       dtype=torch.bfloat16, grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None), SequenceClassifierOutput(loss=tensor(1.6182, device='cuda:0', grad_fn=<NllLossBackward0>), logits=tensor([[-0.5156,  0.0103,  0.1367, -0.4238, -0.9688],
        [-0.2490, -0.1582,  0.5391, -0.1621, -0.8164],
        [-0.3906, -0.5078,  0.2266, -0.0820, -0.9531],
        [-0.1133,  0.2871,  0.5156, -0.3281, -0.7734],
        [-0.3555,  0.4473,  0.3711, -0.1309, -0.8086],
        [-0.4062,  0.1338,  0.4531, -0.3457, -0.6758],
        [-0.1924, -0.2432,  0.3906,  0.0591, -0.8672],
        [-0.3086, -0.1299,  0.1748, -0.3750, -0.7812],
        [-0.3145, -0.0388,  0.6953, -0.1416, -0.9180],
        [-0.1562,  0.2539,  0.5508, -0.2793, -0.3145],
        [-0.0271, -0.0238,  0.7461, -0.0130, -0.7773],
        [ 0.0334, -0.0417,  0.4258, -0.2266, -0.6641],
        [-0.2734,  0.0289,  0.5352,  0.0286, -0.3418],
        [-0.0864, -0.0439,  0.1729,  0.1396, -0.3008],
        [-0.1562,  0.2051,  0.3242, -0.3926, -0.8281],
        [-0.3789, -0.5352,  0.0015, -0.1592, -0.4668]], device='cuda:0',
       dtype=torch.bfloat16, grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None), SequenceClassifierOutput(loss=tensor(1.6255, device='cuda:0', grad_fn=<NllLossBackward0>), logits=tensor([[-0.2539, -0.3281,  0.0131, -0.0977, -0.6797],
        [-0.1826,  0.0118,  0.3281, -0.0012, -0.8906],
        [-0.3066, -0.1943,  0.4023, -0.6797, -0.7422],
        [-0.4121, -0.1172,  0.0334, -0.3770, -0.3262],
        [-0.2832,  0.0515,  0.1689, -0.3047, -0.3848],
        [-0.3652, -0.1455,  0.4746, -0.2471, -0.5078],
        [-0.2793, -0.2949,  0.3086, -0.3008, -0.4395],
        [-0.4043,  0.0854,  0.1001, -0.2393, -0.5938],
        [-0.3047,  0.1738,  0.4551, -0.0811, -0.4727],
        [-0.2832, -0.1221,  0.3477, -0.1660, -0.8750],
        [-0.5391, -0.0981,  0.5195, -0.1748, -0.7578],
        [-0.2559, -0.1992,  0.4492, -0.1143, -0.4102],
        [ 0.0869, -0.6836, -0.3418, -0.4961, -0.1113],
        [ 0.0603,  0.2363,  0.6367, -0.3867, -0.5117],
        [-0.0894,  0.0605,  0.2305, -0.5234, -0.3477],
        [-0.0669, -0.2988,  0.4062, -0.2891, -0.7695]], device='cuda:0',
       dtype=torch.bfloat16, grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None), SequenceClassifierOutput(loss=tensor(1.6279, device='cuda:0', grad_fn=<NllLossBackward0>), logits=tensor([[-0.3535,  0.2930,  0.3184, -0.1885, -0.2949],
        [-0.0227, -0.1084,  0.7969, -0.2275, -0.5938],
        [-0.4219, -0.1650,  0.1279, -0.1504, -0.5078],
        [ 0.0312,  0.0112,  0.4590, -0.3066, -0.8281],
        [-0.2812, -0.1426,  0.6953, -0.1592, -0.8047],
        [-0.2969,  0.0952,  0.5430, -0.2754, -0.6562],
        [-0.4180,  0.0459,  0.5078, -0.1357, -0.7578],
        [-0.2871, -0.0217,  0.3496, -0.1152, -0.7266],
        [-0.0608,  0.2148,  0.3848,  0.2119, -0.3203],
        [-0.4023, -0.1846,  0.2520, -0.6211, -0.9414],
        [-0.4141,  0.0261,  0.8047, -0.2617, -0.5547],
        [-0.4727, -0.3164,  0.1797, -0.2773, -0.0496],
        [-0.2949, -0.1338,  0.2988, -0.3555, -0.6836],
        [-0.1738,  0.2197, -0.0315, -0.4375, -0.5430],
        [-0.5547,  0.2422,  0.4297, -0.5078, -0.5039],
        [-0.1069, -0.1250,  0.9141, -0.2090, -0.4805]], device='cuda:0',
       dtype=torch.bfloat16, grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None), SequenceClassifierOutput(loss=tensor(1.6025, device='cuda:0', grad_fn=<NllLossBackward0>), logits=tensor([[ 0.1245, -0.2656, -0.0630, -0.2158, -0.4102],
        [-0.1748,  0.0447, -0.0352, -0.2617, -0.2832],
        [-0.3613, -0.3477,  0.5234,  0.2598, -0.8242],
        [-0.2930, -0.0918,  0.3359, -0.0972, -0.3730],
        [-0.1768, -0.0236,  0.3730, -0.1973, -0.5430],
        [-0.2100, -0.1328,  0.4824,  0.1094, -0.4531],
        [-0.0879, -0.1846,  0.3867, -0.2812, -0.5391],
        [-0.4629,  0.1367,  0.5000, -0.4961, -0.5117],
        [-0.2266, -0.3613,  0.1270, -0.4062, -0.5195],
        [-0.2402, -0.0479,  0.2080, -0.0928, -0.3184],
        [-0.0806, -0.0703,  0.1855, -0.4121, -0.3516],
        [-0.2832,  0.0898, -0.0693, -0.3457, -0.6484],
        [-0.6836,  0.2080,  0.4141, -0.1523, -0.5938],
        [ 0.2266,  0.0781,  0.1475, -0.4473, -0.5195],
        [-0.0903,  0.3711, -0.1592, -0.2969, -0.2598],
        [-0.1357,  0.0747,  0.2754, -0.3828, -0.6484]], device='cuda:0',
       dtype=torch.bfloat16, grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None), SequenceClassifierOutput(loss=tensor(1.5513, device='cuda:0', grad_fn=<NllLossBackward0>), logits=tensor([[-0.3867,  0.0437,  0.4102, -0.2930, -0.5391],
        [-0.1436, -0.1611,  0.5586, -0.2637, -0.7500],
        [-0.3418,  0.0374,  0.5742,  0.0110, -0.6914],
        [-0.1494,  0.2695,  0.3613,  0.0540, -0.8086],
        [-0.6016, -0.0256,  0.8242, -0.3164, -0.3945],
        [-0.4199,  0.0967,  0.2871, -0.3008, -0.5781],
        [-0.0493, -0.3809,  0.3438, -0.0076, -0.6250],
        [-0.1309, -0.2852,  0.2988, -0.2734, -0.2070],
        [-0.4590,  0.2676,  0.1523, -0.5312, -1.0781],
        [-0.2500,  0.1562,  0.4902, -0.0679, -0.9336],
        [-0.0544,  0.1348,  0.3027, -0.4980, -0.4707],
        [-0.2520,  0.0361,  0.5234, -0.1226, -0.2080],
        [-0.2773, -0.4199,  0.0688, -0.2559, -0.4961],
        [-0.4023,  0.5195,  0.3828, -0.3613, -0.7031],
        [-0.3984, -0.0908,  0.1865, -0.1562, -0.7148],
        [-0.1660,  0.2236,  0.0981, -0.1729, -0.8164]], device='cuda:0',
       dtype=torch.bfloat16, grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None), SequenceClassifierOutput(loss=tensor(1.5024, device='cuda:0', grad_fn=<NllLossBackward0>), logits=tensor([[-0.1826,  0.2891,  0.1943, -0.3086, -0.3672],
        [-0.2344,  0.0898,  0.7773, -0.4082, -0.6875],
        [-0.5742, -0.0278,  0.4551, -0.4824, -0.7070],
        [-0.3281,  0.1738,  0.0752, -0.0806, -0.8320],
        [-0.2754,  0.0330,  0.2969, -0.1719, -0.6758],
        [-0.2129, -0.0300,  0.4395, -0.1523, -0.5117],
        [-0.2812, -0.2773,  0.1592, -0.5117, -0.7383],
        [-0.3418,  0.1660,  0.3457, -0.1011, -0.5156],
        [-0.2676,  0.3496,  0.5898, -0.6680, -0.8672],
        [-0.2246, -0.0366,  0.1895, -0.3223, -0.5547],
        [-0.3027, -0.1426,  0.2363, -0.3223, -0.8516],
        [-0.1318, -0.2559,  0.2217, -0.3379, -0.4434],
        [-0.2051, -0.1128,  0.4785,  0.0623, -0.2383],
        [-0.2930,  0.0527,  0.2832, -0.4668, -0.8320],
        [-0.7891,  0.2344,  0.4512, -0.2852, -0.6406],
        [-0.4043,  0.3066,  0.4414, -0.1016, -1.0469]], device='cuda:0',
       dtype=torch.bfloat16, grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None), SequenceClassifierOutput(loss=tensor(1.6323, device='cuda:0', grad_fn=<NllLossBackward0>), logits=tensor([[-0.3750,  0.0459,  0.1006, -0.4824, -0.4746],
        [-0.1807,  0.0510,  0.2432, -0.0211, -0.5938],
        [ 0.0437,  0.1758,  0.5195, -0.5938, -0.2617],
        [-0.1953, -0.3047,  0.4590, -0.2441, -0.1895],
        [ 0.2832, -0.1196, -0.5703, -0.2422,  0.4434],
        [-0.1973,  0.0967,  0.4141, -0.3965, -0.8398],
        [-0.2832, -0.1836,  0.1738,  0.0962, -0.6875],
        [-0.3320, -0.2949,  0.0315, -0.3008, -0.5938],
        [-0.4043,  0.5000,  0.2949, -0.1436, -0.1201],
        [-0.4570,  0.1768,  0.5625, -0.3047, -0.6484],
        [-0.4199, -0.0011,  0.7461, -0.3047, -0.7852],
        [-0.2812, -0.0173,  0.2402, -0.1387, -0.4043],
        [-0.5039,  0.0400,  0.1826,  0.0645, -0.9062],
        [-0.1855,  0.0674,  0.1260, -0.4004, -0.1797],
        [-0.4160,  0.0052,  0.3164, -0.1436, -0.5078],
        [-0.0400, -0.1807,  0.3496, -0.2422, -0.5625]], device='cuda:0',
       dtype=torch.bfloat16, grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None), SequenceClassifierOutput(loss=tensor(1.4619, device='cuda:0', grad_fn=<NllLossBackward0>), logits=tensor([[-0.2441, -0.2305,  0.0479, -0.1709, -0.4082],
        [-0.2871,  0.2393,  0.4707, -0.2637, -1.0000],
        [-0.2969,  0.1729,  0.6875, -0.0503, -0.7148],
        [-0.1650, -0.0054,  0.2002,  0.0415, -0.6797],
        [ 0.0923, -0.0393, -0.2969, -0.4258, -0.1816],
        [ 0.0437, -0.0278,  0.3496, -0.2715, -0.2314],
        [-0.0623, -0.0171,  0.1206, -0.3086,  0.2227],
        [-0.4551,  0.2432,  0.1758, -0.5430, -0.3652],
        [ 0.0708,  0.2451,  0.1777, -0.5195, -0.4609],
        [ 0.1260,  0.0505,  0.1709, -0.3086, -0.0908],
        [ 0.1006,  0.3398, -0.2363, -0.3223, -0.3691],
        [-0.1396,  0.1494,  0.0591, -0.4180, -0.3320],
        [ 0.2354,  0.3008,  0.3887, -0.2500, -0.6094],
        [-0.3379, -0.1865, -0.0417, -0.3672, -0.3672],
        [-0.1602,  0.1094,  0.1572, -0.3730, -0.9062],8e+00, -6.9141e-01],, attentions=None)
        [-0.2773,  0.1045, -0.0266,  0.0391, -0.2334]], device='cuda:0',
       dtype=torch.bfloat16, grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None), SequenceClassifierOutput(loss=tensor(1.4526, device='cuda:0', grad_fn=<NllLossBackward0>), logits=tensor([[-0.2002,  0.3281, -0.1455, -0.5508, -0.3535],
        [-0.0369,  0.0186,  0.2793, -0.5156, -0.4629],
        [-0.5469,  0.2344, -0.0410, -0.5508, -0.5664],
        [-0.2109, -0.0265, -0.0845, -0.0483, -0.6445],
        [ 0.0659, -0.3184,  0.7227, -0.4062, -0.6172],
        [-0.0581,  0.1270,  0.0342, -0.4160, -0.2109],
        [ 0.1235, -0.1216,  0.4668,  0.0126, -0.8516],
        [ 0.0947,  0.1758,  0.0356,  0.0649, -0.2275],
        [-0.3750, -0.1289,  0.0762, -0.1187, -0.2539],
        [-0.2441,  0.4102,  0.2275, -0.5742, -0.0767],
        [ 0.0119,  0.1719,  0.0216,  0.1406, -0.8125],
        [-0.3984,  0.3633, -0.3047, -0.2598, -0.1523],
        [-0.3555,  0.4570, -0.0112, -0.3535, -0.2412],
        [-0.3613,  0.2305, -0.2080, -0.0503, -0.6445],
        [-0.0762, -0.4648,  0.0796, -0.0014, -0.5547],
        [-0.2422,  0.1025,  0.0192, -0.4141, -0.4961]], device='cuda:0',
       dtype=torch.bfloat16, grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None), SequenceClassifierOutput(loss=tensor(1.4482, device='cuda:0', grad_fn=<NllLossBackward0>), logits=tensor([[-0.2617, -0.0879,  0.4434, -0.4316, -0.8594],
        [-0.1982,  0.3770,  0.3398, -0.1719, -0.3867],
        [-0.3789,  0.0918,  0.1768, -0.4023, -0.5234],
        [-0.2500,  0.1426,  0.3672, -0.4355, -0.0054],
        [-0.1006, -0.1699,  0.0713, -0.2578, -0.5234],
        [-0.0806, -0.2227, -0.0337, -0.3027, -0.0566],
        [ 0.4102, -0.0728,  0.2949, -0.3359, -0.0513],
        [-0.3613,  0.1748,  0.2734, -0.1016, -0.6953],
        [-0.0084, -0.0488,  0.1660, -0.4414, -0.6758],
        [-0.0835,  0.1816,  0.1621, -0.5898, -0.3984],
        [-0.1201, -0.1787,  0.0640, -0.2051, -0.8477],
        [ 0.1270,  0.0056, -0.0737, -0.1621, -0.2354],
        [ 0.0048,  0.5859,  0.2812, -0.1523, -0.4629],
        [-0.2256,  0.3945,  0.3203, -0.3281, -0.2715],
        [ 0.0825,  0.2109,  0.2100, -0.3516,  0.0081],
        [-0.3984,  0.1670,  0.2334, -0.3262, -0.4023]], device='cuda:0',
       dtype=torch.bfloat16, grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None), SequenceClassifierOutput(loss=tensor(1.5220, device='cuda:0', grad_fn=<NllLossBackward0>), logits=tensor([[-0.0952, -0.1040,  0.3438, -0.1328, -0.8672],
        [-0.1484,  0.1299,  0.4707, -0.4746, -0.8047],
        [-0.0640, -0.1104,  0.3477, -0.4492, -0.1982],
        [-0.2373,  0.2539,  0.4492, -0.3711, -0.7227],
        [-0.1699, -0.0869,  0.2539, -0.4609, -1.0391],
        [ 0.0130,  0.2676, -0.0669, -0.2236, -0.4238],
        [-0.3965,  0.0466,  0.1699, -0.1807, -0.8672],
        [ 0.1079,  0.1465,  0.0435,  0.0525, -0.2695],
        [ 0.1855,  0.2246,  0.3926, -0.2520, -0.4531],
        [-0.1846,  0.1846,  0.1504, -0.3730, -0.6523],
        [-0.2314, -0.1270,  0.3398, -0.3828, -0.9766],
        [-0.1250,  0.2598,  0.5312, -0.0442, -0.4023],
        [-0.5898,  0.2637,  0.4434, -0.0123, -0.2617],
        [-0.3965,  0.1001,  0.2891, -0.1807, -0.2051],
        [-0.4004, -0.1953,  0.3574, -0.4199, -0.4492],
        [-0.2236,  0.4883,  0.5352, -0.5312,  0.0198]], device='cuda:0',
       dtype=torch.bfloat16, grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None), SequenceClassifierOutput(loss=tensor(1.5024, device='cuda:0', grad_fn=<NllLossBackward0>), logits=tensor([[-0.0693,  0.2246,  0.0317, -0.2021, -0.2236],
        [-0.1230,  0.2617,  0.0840, -0.1465, -0.4082],
        [-0.4766, -0.0508, -0.0315, -0.1177, -1.0547],
        [-0.3066,  0.2148, -0.0522, -0.3438, -0.4102],
        [ 0.1943,  0.2461,  0.5586, -0.2598, -1.0469],
        [-0.3516,  0.2100, -0.0430, -0.3594, -0.1895],
        [-0.2949, -0.0630, -0.0552, -0.1523, -0.2969],
        [-0.1465, -0.1250,  0.5156, -0.0403, -0.7500],
        [ 0.0542, -0.0801,  0.1016, -0.1348, -0.2520],
        [-0.0262,  0.1855,  0.7148, -0.2090, -0.6953],
        [ 0.0383,  0.0879,  0.2197, -0.3047, -0.2461],
        [-0.2178,  0.1758,  0.2266, -0.5273, -0.6719],
        [ 0.1953, -0.1836, -0.3516, -0.2070,  0.3848],
        [-0.3340,  0.3828,  0.2207, -0.2314, -0.3242],
        [-0.0249, -0.1504,  0.1963,  0.3398, -0.2852],
        [-0.0684,  0.1191,  0.3359, -0.2041, -0.0603]], device='cuda:0',
       dtype=torch.bfloat16, grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None), SequenceClassifierOutput(loss=tensor(1.4463, device='cuda:0', grad_fn=<NllLossBackward0>), logits=tensor([[-0.2002,  0.5586,  0.6406, -0.5273, -0.3477],
        [-0.0155,  0.2070,  0.3711, -0.1523, -0.5469],
        [-0.2988,  0.3262,  0.1152, -0.2422, -0.2930],
        [-0.4180,  0.2314, -0.1660, -0.2256, -0.1060],
        [-0.2090,  0.2373,  0.3848, -0.4160, -0.7461],
        [ 0.1582, -0.0250,  0.1206, -0.1816,  0.0142],
        [-0.0737,  0.1904, -0.2520, -0.2559, -0.0757],
        [-0.1436,  0.3398, -0.0908, -0.2422, -0.3496],
        [-0.1631,  0.1030,  0.4355, -0.1514, -0.3906],
        [-0.3066, -0.0132,  0.1167, -0.1504, -0.2490],
        [-0.5273, -0.0430, -0.1816, -0.0471, -0.2773],
        [-0.2070,  0.3730,  0.0598, -0.4258, -0.4648],
        [-0.1768,  0.3203,  0.1826, -0.1523, -0.1021],
        [ 0.1602,  0.2402,  0.0762, -0.2168, -0.1108],
        [-0.0359,  0.1660, -0.2012, -0.4355, -0.0889],
        [ 0.1045, -0.0410,  0.4336,  0.2080, -0.7148]], device='cuda:0',
       dtype=torch.bfloat16, grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None), SequenceClassifierOutput(loss=tensor(1.3809, device='cuda:0', grad_fn=<NllLossBackward0>), logits=tensor([[-0.0278,  0.1104,  0.0991, -0.2275, -0.3203],
        [-0.1338, -0.1875,  0.4551, -0.0623, -0.4785],
        [ 0.0889,  0.3027,  0.4414, -0.1904, -0.3613],
        [-0.1973,  0.2207,  0.5039, -0.2988, -0.7227],
        [-0.0845,  0.3516,  0.2988, -0.1719, -0.3867],
        [-0.1230,  0.1787,  0.4023, -0.1245, -1.0391],
        [-0.3320,  0.1758,  0.1553, -0.2734, -0.1924],
        [-0.4961,  0.3770,  0.0923,  0.0077, -0.4121],
        [ 0.2158,  0.3535,  0.3359, -0.4395, -0.2656],
        [-0.5391,  0.2012,  0.0033, -0.4219, -0.1318],
        [ 0.0674, -0.0493,  0.0356,  0.0781, -0.5234],
        [-0.3359,  0.3223, -0.0190, -0.1836, -0.2422],
        [-0.2715, -0.3672,  0.0815, -0.5586, -0.2471],
        [-0.1787,  0.6836,  0.2852, -0.1426, -0.4609],
        [-0.3730,  0.1650,  0.3008, -0.1191, -0.4316],
        [-0.1582,  0.3535,  0.1299, -0.1221, -0.3379]], device='cuda:0',
       dtype=torch.bfloat16, grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None), SequenceClassifierOutput(loss=tensor(1.6113, device='cuda:0', grad_fn=<NllLossBackward0>), logits=tensor([[-0.3809,  0.0118,  0.5234, -0.0481, -1.0938],
        [-0.2988, -0.4414,  0.2393, -0.2012, -0.5703],
        [-0.0547, -0.0288,  0.3457, -0.2949, -1.0312],
        [-0.1084,  0.1416,  0.0237, -0.3516, -0.4492],
        [ 0.0027, -0.1289,  0.5977,  0.0109, -0.6328],
        [ 0.0422,  0.1699,  0.1973,  0.0129, -0.3438],
        [ 0.1641,  0.0148, -0.1221,  0.0084, -0.1367],
        [ 0.5586, -0.2520, -0.6367,  0.2148,  0.4961],
        [-0.1855,  0.3164,  0.1104, -0.1279, -0.3320],
        [-0.3691,  0.1680,  0.6992, -0.1807, -0.7188],
        [ 0.3301, -0.0227, -0.1494, -0.2461,  0.0576],
        [-0.3340,  0.3965,  0.1855, -0.0786, -0.3965],
        [-0.1855,  0.2539, -0.3828, -0.3496, -0.3574],
        [-0.0947,  0.0530,  0.1934, -0.0270, -0.4609],
        [-0.0850,  0.1660,  0.2676, -0.0708,  0.0287],
        [-0.1113,  0.4727, -0.0732, -0.0698, -0.3770]], device='cuda:0',
       dtype=torch.bfloat16, grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None), SequenceClassifierOutput(loss=tensor(1.4492, device='cuda:0', grad_fn=<NllLossBackward0>), logits=tensor([[-0.3984,  0.3262,  0.3418,  0.3242, -0.7930],
        [-0.0474,  0.3984,  0.1582, -0.1299, -0.6016],
        [-0.5078,  0.1875,  0.4434, -0.1758, -0.8828],
        [ 0.0698,  0.3008,  0.1113, -0.0806, -0.2910],
        [-0.2637,  0.3672,  0.3906, -0.2148, -0.1270],
        [ 0.2461, -0.0977, -0.0962, -0.3008, -0.3301],
        [-0.5781,  0.2852, -0.0581,  0.0535, -0.2891],
        [-0.3887,  0.2539,  0.0698, -0.2070, -0.5312],
        [ 0.2002, -0.0175, -0.2930, -0.5430,  0.0542],
        [-0.0449, -0.0325,  0.6797, -0.2363, -0.7148],
        [-0.4355,  0.2949,  0.0728, -0.2852, -0.0430],
        [-0.1167,  0.2793,  0.3516, -0.4199, -0.3555],
        [-0.0518,  0.3340,  0.4727, -0.3965, -0.5234],
        [-0.1152, -0.0022,  0.2217, -0.4883, -0.7148],
        [ 0.2119,  0.1260, -0.0576, -0.0830, -0.3516],
        [-0.2480,  0.1680,  0.1138, -0.0884, -0.2471]], device='cuda:0',
       dtype=torch.bfloat16, grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None), SequenceClassifierOutput(loss=tensor(1.4707, device='cuda:0', grad_fn=<NllLossBackward0>), logits=tensor([[-0.1113,  0.3789,  0.3066, -0.2734, -0.4492],
        [-0.4121,  0.1387,  0.3535, -0.3887, -1.4609],
        [-0.2656, -0.1157,  0.7070, -0.4062, -0.2061],
        [-0.0435,  0.2539,  0.3730, -0.0654, -0.4473],
        [ 0.1021,  0.1572, -0.2148, -0.1621, -0.1011],
        [-0.1133, -0.0693,  0.6172,  0.0986, -0.5273],
        [ 0.0674,  0.1387,  0.3809, -0.1211, -0.3086],
        [-0.1226,  0.3066,  0.2393,  0.1118, -0.7422],
        [-0.3652,  0.1279,  0.3320, -0.4062, -0.1992],
        [-0.0525,  0.2402,  0.4062, -0.3242, -0.1689],
        [-0.2207,  0.2793,  0.3145, -0.1855, -0.3027],
        [ 0.1494, -0.0688,  0.2578, -0.1367, -0.1250],
        [-0.3379,  0.3145,  0.3320, -0.4004, -0.3652],
        [ 0.0347,  0.5469,  0.3457, -0.5469, -0.1992],
        [-0.1309,  0.0820,  0.6211, -0.1006, -0.5625],
        [-0.3184, -0.0364,  0.2617, -0.4570, -0.5859]], device='cuda:0',
       dtype=torch.bfloat16, grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None), SequenceClassifierOutput(loss=tensor(1.4468, device='cuda:0', grad_fn=<NllLossBackward0>), logits=tensor([[-0.2930,  0.2490, -0.0120, -0.0811, -0.2480],
        [-0.0967,  0.3672,  0.3281, -0.0742, -0.3145],
        [-0.4082,  0.3242,  0.0864, -0.0437, -0.2734],
        [-0.4336,  0.2656,  0.4043, -0.4766, -0.7695],
        [ 0.3418,  0.1553, -0.3945, -0.4414, -0.1816],
        [-0.3125,  0.1670,  0.2305, -0.4160,  0.0089],
        [-0.2070,  0.3496,  0.4883, -0.2227, -0.1338],
        [-0.1147,  0.2188,  0.2461, -0.0889, -0.7344],
        [-0.2148,  0.2021,  0.0830, -0.2393, -0.2422],
        [-0.3730, -0.4121,  0.1216, -0.1455, -0.3730],
        [-0.5312,  0.2188,  0.2090, -0.2324, -0.5352],
        [-0.3789,  0.1982, -0.0334,  0.1621, -0.5000],
        [ 0.1641,  0.0422,  0.0708, -0.2617, -0.4297],
        [-0.4551,  0.0452,  0.4766,  0.0781, -0.8438],
        [-0.4824,  0.2188, -0.0513, -0.0564, -1.0547],
        [-0.4805,  0.1123,  0.3242, -0.6328, -0.9297]], device='cuda:0',
       dtype=torch.bfloat16, grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None), SequenceClassifierOutput(loss=tensor(1.3496, device='cuda:0', grad_fn=<NllLossBackward0>), logits=tensor([[-0.2217,  0.2070,  0.0598, -0.3848, -0.4785],
        [-0.1206,  0.2520, -0.0010,  0.3555, -0.6641],
        [ 0.0869, -0.3066,  0.0742, -0.3398, -0.3633],
        [ 0.0835,  0.0981,  0.0405, -0.3613, -0.2051],
        [-0.2715,  0.2715,  0.2041, -0.3574, -0.3809],
        [-0.3086,  0.2930, -0.0064, -0.5820, -0.2715],
        [-0.4453,  0.0459,  0.3320, -0.2617, -0.3691],
        [-0.2988,  0.4707,  0.4980, -0.0500, -0.2715],
        [-0.2617,  0.1465,  0.1895, -0.0139, -0.3066],
        [-0.0933,  0.0474,  0.2393, -0.3438, -0.3496],
        [-0.2275,  0.5312,  0.0967, -0.5859, -0.3535],
        [-0.1123,  0.4805, -0.1260, -0.2715, -0.1157],
        [ 0.3477,  0.2539,  0.3945, -0.5430,  0.3457],
        [-0.0271,  0.2334, -0.1328, -0.3203, -0.4336],
        [-0.0304,  0.2969,  0.0737, -0.1689, -0.0664],
        [-0.2891,  0.3105,  0.0513, -0.4082, -0.3613]], device='cuda:0',
       dtype=torch.bfloat16, grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None), SequenceClassifierOutput(loss=tensor(1.4619, device='cuda:0', grad_fn=<NllLossBackward0>), logits=tensor([[-0.0859,  0.1494,  0.5000, -0.3105, -0.6641],
        [ 0.0369,  0.1963, -0.1445, -0.1611, -0.1660],
        [ 0.1206,  0.1309,  0.2217, -0.3379, -0.1406],
        [-0.5078,  0.3105,  0.2139, -0.5156, -0.8398],
        [ 0.0286, -0.3262, -0.2305, -0.4395, -0.1299],
        [-0.0378, -0.1357,  0.6680, -0.1992, -0.6328],
        [-0.1943, -0.0840,  0.7070, -0.1660, -0.8320],
        [ 0.1357,  0.0928,  0.1621, -0.3730, -0.6367],
        [-0.1768, -0.2910,  0.0320, -0.2031, -0.6562],
        [-0.1816,  0.5859,  0.2910, -0.3164, -0.3770],
        [-0.1523,  0.2393,  0.0908, -0.0986, -0.2637],
        [ 0.1797,  0.5156, -0.0082, -0.3438,  0.3574],
        [-0.0618,  0.3418,  0.1406, -0.1299, -0.5312],
        [-0.3711,  0.4453,  0.2734, -0.2539, -0.2969],
        [-0.1377,  0.3535,  0.1533, -0.0603, -0.5508],
        [-0.1074, -0.0481,  0.1230, -0.3672, -0.2520]], device='cuda:0',
       dtype=torch.bfloat16, grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None), SequenceClassifierOutput(loss=tensor(1.5073, device='cuda:0', grad_fn=<NllLossBackward0>), logits=tensor([[-0.0046, -0.2031,  0.4355, -0.1147, -0.0923],
        [-0.2480, -0.0444,  0.2217, -0.4160, -0.2559],
        [-0.0640,  0.4062,  0.2734, -0.2676, -0.2988],
        [-0.0820,  0.1504,  0.3555, -0.3672, -0.5273],
        [ 0.2109,  0.4062,  0.0732,  0.2480, -0.3516],
        [-0.3516, -0.0967,  0.1904, -0.1543, -0.3555],
        [-0.0376,  0.3789,  0.1641, -0.2295, -0.1206],
        [ 0.1118,  0.0576,  0.0728, -0.4219, -0.4160],
        [-0.5430,  0.2754,  0.2578, -0.1045, -0.5703],
        [-0.0038,  0.1689, -0.0222, -0.6016, -0.3027],
        [-0.1035,  0.2578,  0.2295, -0.2285, -0.2949],
        [-0.0596,  0.5156,  0.5781,  0.1445, -0.5391],
        [-0.2734, -0.1221,  0.0708,  0.0947, -0.1226],
        [-0.1865,  0.0732,  0.4141, -0.0124, -0.2354],
        [-0.0576,  0.2451, -0.0767, -0.1719, -0.2217],
        [-0.3535,  0.3613,  0.6406, -0.2471, -0.6641]], device='cuda:0',
       dtype=torch.bfloat16, grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None), SequenceClassifierOutput(loss=tensor(1.5684, device='cuda:0', grad_fn=<NllLossBackward0>), logits=tensor([[-0.2363,  0.1904,  0.0025, -0.5742, -1.1016],
        [ 0.1260,  0.2451,  0.3066, -0.3203, -0.6406],
        [ 0.0359,  0.3594,  0.3652, -0.1680, -0.6172],
        [-0.2656,  0.0405,  0.1504, -0.1641, -0.1914],
        [-0.1318, -0.0928,  0.0287,  0.0208, -0.3320],
        [-0.1494,  0.2891,  0.1250, -0.2871, -0.4707],
        [ 0.0825,  0.1641,  0.3477, -0.3809, -0.6094],
        [-0.3223, -0.0679, -0.0474, -0.0967, -0.6133],
        [-0.2852, -0.3281,  0.3027, -0.4648, -0.8789],
        [-0.4219,  0.5000,  0.0703, -0.2373, -0.2432],
        [-0.2852,  0.1128,  0.6094, -0.1709, -0.2754],
        [ 0.2354,  0.3672, -0.2695,  0.0123, -0.3691],
        [-0.1211, -0.0457,  0.1050, -0.0325, -0.1611],
        [-0.1289,  0.1973,  0.4004, -0.0649, -0.0591],
        [ 0.0598, -0.0366, -0.2217, -0.3262, -0.1260],
        [ 0.0356, -0.0420,  0.7031,  0.1235, -0.6484]], device='cuda:0',
       dtype=torch.bfloat16, grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None), SequenceClassifierOutput(loss=tensor(1.6001, device='cuda:0', grad_fn=<NllLossBackward0>), logits=tensor([[ 0.0908,  0.0659,  0.3125, -0.1758, -0.0171],
        [-0.2148, -0.1216,  0.1689, -0.3574, -0.4277],
        [ 0.0869,  0.1309,  0.1660, -0.0674, -0.1582],
        [-0.1318,  0.0415, -0.3223, -0.1416, -0.2334],
        [ 0.1533,  0.3203,  0.2080, -0.2129, -0.3262],
        [-0.2090,  0.5039,  0.0859, -0.0544, -0.1924],
        [ 0.1025,  0.1572,  0.4570, -0.3711, -0.1914],
        [-0.1162,  0.0053, -0.0593, -0.0583, -0.5508],
        [-0.0315,  0.0200,  0.3906, -0.4102, -0.2227],
        [-0.3281,  0.2520, -0.1689,  0.0513, -0.2695],
        [ 0.2500,  0.0757, -0.0198, -0.0479, -0.1875],
        [ 0.2188, -0.1963,  0.3633,  0.1045, -0.1924],
        [-0.4473,  0.4160,  0.4746, -0.0107, -0.4180],
        [-0.2471,  0.1729,  0.2969, -0.4141, -0.4102],
        [-0.0491,  0.3242,  0.1758, -0.1885,  0.0294],
        [-0.1104, -0.1299,  0.1055, -0.1328, -0.3496]], device='cuda:0',
       dtype=torch.bfloat16, grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None), SequenceClassifierOutput(loss=tensor(1.4175, device='cuda:0', grad_fn=<NllLossBackward0>), logits=tensor([[-0.5508,  0.5273,  0.1279, -0.1963, -0.3945],
        [-0.1846,  0.1826,  0.1455, -0.0659, -0.2432],
        [-0.2256,  0.4688,  0.2832, -0.3125, -0.2891],
        [-0.3613,  0.3008,  0.1475, -0.3594, -0.6211],
        [-0.1147,  0.1245,  0.3203, -0.1650, -0.5625],
        [-0.4746,  0.0115,  0.2148, -0.1001, -0.3496],
        [-0.2178, -0.0496,  0.5664, -0.0562, -0.2559],
        [ 0.2100,  0.3164,  0.1465, -0.4355, -0.2754],
        [-0.0583,  0.2031, -0.1177, -0.4121, -0.6367],
        [-0.3770,  0.1777, -0.0874, -0.2363, -0.3398],
        [-0.3926, -0.2715, -0.1855, -0.3750, -0.3008],
        [-0.1436,  0.2812,  0.2656, -0.3926, -0.3711],
        [-0.3086, -0.1177,  0.1826,  0.3672, -1.0078],
        [-0.3008,  0.5742,  0.2520, -0.1709, -0.0603],
        [ 0.0493,  0.1030, -0.1260, -0.4023,  0.2969],
        [-0.3086, -0.1543,  0.0210, -0.1523, -0.6680]], device='cuda:0',
       dtype=torch.bfloat16, grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None), SequenceClassifierOutput(loss=tensor(1.5532, device='cuda:0', grad_fn=<NllLossBackward0>), logits=tensor([[-0.1875,  0.4824,  0.3848, -0.0454, -0.1846],
        [-0.0172,  0.1250,  0.4941, -0.3926, -0.3809],
        [-0.2275, -0.0947, -0.1309, -0.3184, -0.3457],
        [-0.1592,  0.1172,  0.3457, -0.1895, -0.3359],
        [ 0.2080,  0.1729, -0.0071, -0.4199, -0.1787],
        [ 0.0608,  0.0830,  0.4355, -0.1846, -0.2158],
        [-0.2930,  0.3457, -0.1172, -0.0898, -0.2354],
        [-0.4512,  0.1494,  0.3105, -0.4395, -0.4004],
        [ 0.0027,  0.1094,  0.2256, -0.2373,  0.0194],
        [-0.0635,  0.1177, -0.2930, -0.5234, -0.2578],
        [ 0.0850, -0.0393,  0.3633,  0.0291, -0.5469],
        [-0.1045,  0.4902,  0.1436, -0.0742, -0.2461],
        [-0.5781,  0.2676,  0.1553, -0.1660, -0.1846],
        [-0.3320,  0.2090,  0.1113, -0.2246, -0.2178],
        [-0.3535,  0.0806, -0.2490, -0.1816,  0.1738],
        [-0.5039,  0.0232,  0.3691, -0.4609, -0.9648]], device='cuda:0',
       dtype=torch.bfloat16, grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None), SequenceClassifierOutput(loss=tensor(1.4976, device='cuda:0', grad_fn=<NllLossBackward0>), logits=tensor([[-0.3750,  0.4785,  0.0374, -0.1060, -0.5156],
        [ 0.0281, -0.2520, -0.1270, -0.4863,  0.0879],
        [ 0.0075,  0.3730,  0.2168, -0.4863, -0.2246],
        [-0.6484,  0.4551,  0.4043, -0.1875, -0.4297],
        [-0.2295,  0.2246,  0.3594, -0.0767, -0.2695],
        [-0.3535,  0.1436,  0.1943, -0.3359, -0.8672],
        [ 0.0815,  0.4766,  0.1113, -0.4766, -0.0442],
        [ 0.0161,  0.3613,  0.0359, -0.1729, -0.2676],
        [-0.0447,  0.1924,  0.3652,  0.2080, -0.3926],
        [-0.0898,  0.1543,  0.4434, -0.3145, -0.3984],
        [ 0.1157, -0.2100, -0.1069, -0.3789, -0.1001],
        [-0.3105,  0.2930,  0.6250, -0.5000, -0.6797],
        [-0.2578, -0.0188,  0.4785, -0.1797, -0.5039],
        [-0.2930,  0.1523,  0.4434, -0.2871, -0.4902],
        [-0.0520,  0.0742,  0.3613, -0.0962,  0.0564],
        [-0.3164,  0.2314,  0.3281, -0.1191, -0.3379]], device='cuda:0',
       dtype=torch.bfloat16, grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None), SequenceClassifierOutput(loss=tensor(1.4458, device='cuda:0', grad_fn=<NllLossBackward0>), logits=tensor([[ 0.3477, -0.1118,  0.1719, -0.1787, -0.1416],
        [-0.0835,  0.3594,  0.0253, -0.1436, -0.2188],
        [-0.2451, -0.1147,  0.7578, -0.3555, -0.8828],
        [-0.5195,  0.1699,  0.2031, -0.2490, -0.1309],
        [ 0.4160, -0.0762, -0.4199, -0.0045,  0.2422],
        [-0.4199,  0.3438,  0.0503, -0.4844, -0.6406],
        [ 0.2285,  0.2393, -0.0540, -0.0371, -0.0767],
        [-0.2168,  0.0054,  0.2461, -0.3828, -0.3047],
        [ 0.0557, -0.0645,  0.2051, -0.1475, -0.3613],
        [-0.2832,  0.1318, -0.0583, -0.0972, -0.4902],
        [-0.2695, -0.1689,  0.2266,  0.1299, -0.9961],
        [-0.0437,  0.2061,  0.1592, -0.3379, -0.6719],
        [-0.0967,  0.1846, -0.2500, -0.0496, -0.1875],
        [-0.0201,  0.0155,  0.3945, -0.3574,  0.1211],
        [ 0.2354,  0.3184, -0.2217, -0.2695,  0.0776],
        [ 0.0469,  0.3320,  0.3418, -0.1904, -0.3203]], device='cuda:0',
       dtype=torch.bfloat16, grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None), SequenceClassifierOutput(loss=tensor(1.4756, device='cuda:0', grad_fn=<NllLossBackward0>), logits=tensor([[ 0.0928,  0.5742,  0.3027, -0.2168, -0.2539],
        [-0.2539, -0.0309,  0.3164, -0.2090, -0.8633],
        [-0.1680,  0.1348,  0.4375, -0.1689, -0.4629],
        [-0.4258, -0.3477,  0.4590, -0.4141, -0.8828],
        [ 0.1377,  0.4629,  0.5820, -0.2070, -0.2197],
        [-0.1572,  0.2139,  0.0045,  0.0618, -0.4434],
        [ 0.0238,  0.1348,  0.2695, -0.2383, -0.0302],
        [-0.1396,  0.2617, -0.0226,  0.0172, -0.3496],
        [ 0.0583,  0.1621, -0.2988, -0.4336,  0.0315],
        [-0.1069, -0.0608,  0.5547, -0.1855, -0.2236],
        [-0.4336,  0.4785,  0.1445,  0.0259, -0.1895],
        [-0.0977,  0.3379, -0.1201, -0.4961, -0.5820],
        [-0.0145,  0.2676,  0.2988, -0.1855, -0.5859],
        [-0.3359,  0.3770,  0.2754, -0.5273, -0.5039],
        [-0.1875, -0.0723,  0.2314,  0.0593, -0.3652],
        [-0.0547,  0.3984,  0.1226, -0.4238, -0.0447]], device='cuda:0',
       dtype=torch.bfloat16, grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None), SequenceClassifierOutput(loss=tensor(1.6353, device='cuda:0', grad_fn=<NllLossBackward0>), logits=tensor([[ 0.0908,  0.1719,  0.2109, -0.5000, -0.2832],
        [-0.2871, -0.1221,  0.5195, -0.2793, -0.9570],
        [ 0.0342, -0.1157,  0.3730, -0.3574, -0.2500],
        [ 0.2539, -0.1631,  0.5820,  0.0845, -0.8672],
        [-0.2734,  0.1377,  0.1187,  0.1250, -0.0216],
        [ 0.0143,  0.1128, -0.0757, -0.4395, -0.3164],
        [-0.2969,  0.4336, -0.1396, -0.0889, -0.2793],
        [-0.2285,  0.3066,  0.3867, -0.1211, -0.2988],
        [ 0.0278,  0.0679,  0.4102, -0.2227, -0.1514],
        [-0.3730,  0.3594,  0.2520,  0.1050, -0.4941],
        [-0.0972,  0.2188,  0.1514, -0.1992, -0.3984],
        [ 0.1040, -0.2168, -0.1191, -0.2676,  0.1641],
        [ 0.0396,  0.2793,  0.2178, -0.2852, -0.2412],
        [-0.1748, -0.2598,  0.7578, -0.0537, -0.4863],
        [ 0.0459,  0.3770,  0.1572, -0.0525,  0.0840],
        [-0.0369,  0.2314,  0.2520, -0.1807, -0.8867]], device='cuda:0',
       dtype=torch.bfloat16, grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None), SequenceClassifierOutput(loss=tensor(1.5337, device='cuda:0', grad_fn=<NllLossBackward0>), logits=tensor([[ 0.1396,  0.3750, -0.2949, -0.5352,  0.0742],
        [-0.0146,  0.1387,  0.1611,  0.2002, -0.5391],
        [ 0.0073,  0.2207,  0.6367, -0.2930, -0.5195],
        [ 0.0801,  0.2598,  0.0669, -0.2930, -0.3418],
        [ 0.0488,  0.1318, -0.0356, -0.4043, -0.2031],
        [ 0.2695,  0.0928,  0.3926, -0.0588, -0.1680],
        [ 0.1777, -0.1768, -0.4160, -0.4258,  0.4375],
        [-0.1709,  0.1260,  0.5352, -0.5234, -0.7539],
        [ 0.0806,  0.2793,  0.3301, -0.4316, -0.7422],
        [-0.1641,  0.0986,  0.0077, -0.5117, -0.3535],
        [ 0.1455,  0.4395,  0.0640, -0.5469, -0.2129],
        [-0.4531, -0.4082,  0.4512, -0.3125, -1.3828],
        [-0.1006, -0.0427,  0.3359, -0.2031, -0.7969],
        [-0.0054, -0.0106,  0.3945, -0.3477, -0.4004],
        [-0.1934,  0.2930,  0.2412, -0.2520, -0.9297],
        [-0.2539, -0.0356,  0.2754,  0.1621, -0.4180]], device='cuda:0',
       dtype=torch.bfloat16, grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None), SequenceClassifierOutput(loss=tensor(1.5430, device='cuda:0', grad_fn=<NllLossBackward0>), logits=tensor([[ 7.0801e-02,  1.7188e-01, -1.1230e-01, -4.7363e-02, -5.8984e-01],
        [-2.2736e-03, -6.0303e-02, -9.5215e-02, -3.1055e-01, -1.1484e+00],
        [-1.9922e-01,  1.3086e-01, -1.8082e-03, -4.7119e-02, -1.7578e-01],
        [-2.3047e-01,  2.4219e-01,  2.1973e-01, -3.1641e-01, -9.1406e-01],
        [ 2.5024e-02,  1.4844e-01, -1.8066e-01, -1.6699e-01, -2.1582e-01],
        [-6.9336e-02,  1.1133e-01,  2.0898e-01, -2.2754e-01, -3.0859e-01],
        [ 2.9102e-01, -1.8848e-01, -6.3672e-01, -4.5312e-01,  5.0000e-01],
        [ 5.8105e-02, -1.4844e-01, -1.5918e-01, -4.6094e-01,  2.1875e-01],
        [-1.1230e-01,  5.1172e-01,  6.8750e-01, -2.2168e-01, -5.9766e-01],
        [-1.7871e-01, -4.2969e-01,  8.2031e-02, -9.9609e-02, -4.2578e-01],
        [-2.5391e-01, -1.4746e-01,  1.3281e-01, -4.1211e-01, -5.4688e-01],
        [-1.4355e-01, -9.7656e-04,  5.2344e-01, -1.1035e-01, -5.3516e-01],
        [ 8.2031e-02, -2.1362e-03,  1.6211e-01, -1.8945e-01, -2.8516e-01],
        [-1.7578e-01, -7.1411e-03,  3.3398e-01,  6.3965e-02, -4.9609e-01],
        [ 6.5918e-02,  1.9238e-01, -1.4062e-01,  7.1289e-02, -1.6406e-01],
        [-4.0820e-01,  4.4922e-01,  1.3574e-01, -5.8984e-01, -6.9922e-01]],
       device='cuda:0', dtype=torch.bfloat16, grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None), SequenceClassifierOutput(loss=tensor(1.5625, device='cuda:0', grad_fn=<NllLossBackward0>), logits=tensor([[-0.2305,  0.1494,  0.1187, -0.2461, -0.2578],
        [-0.2275,  0.0111,  0.0427, -0.0374, -0.2139],
        [-0.1826,  0.1045,  0.3887, -0.2266, -0.6992],
        [-0.1904,  0.3457,  0.1182, -0.3848, -0.2197],
        [ 0.1699,  0.0854, -0.0583, -0.4844, -0.1514],
        [-0.5859, -0.0325,  0.0791,  0.0947, -0.1885],
        [-0.0981,  0.2676,  0.0415, -0.1030, -0.5898],
        [-0.3438,  0.1455,  0.1216, -0.3164, -0.1475],
        [-0.3848,  0.5195,  0.1494, -0.1201, -0.4512],
        [-0.5938, -0.0396,  0.0583, -0.4023, -0.5664],
        [ 0.0796,  0.1099, -0.0330, -0.1768, -0.2412],
        [-0.1328,  0.3477, -0.0830, -0.6445, -0.6367],
        [-0.1943, -0.0302,  0.2412,  0.2227, -0.7188],
        [ 0.0354,  0.2373,  0.4297, -0.1992,  0.3164],
        [-0.1523,  0.2578, -0.0376, -0.4004, -0.3320],
        [-0.0144, -0.3750,  0.5000, -0.2490, -0.6172]], device='cuda:0',
       dtype=torch.bfloat16, grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None), SequenceClassifierOutput(loss=tensor(1.5996, device='cuda:0', grad_fn=<NllLossBackward0>), logits=tensor([[-0.2217, -0.0347,  0.2324, -0.2852, -0.8867],
        [-0.2041,  0.2314,  0.1138, -0.2412, -0.3672],
        [-0.1357,  0.0442,  0.4609, -0.0684, -0.3613],
        [ 0.0708,  0.1387,  0.0869, -0.0033, -0.2246],
        [ 0.1094, -0.1040,  0.1689, -0.1963,  0.1553],
        [-0.1758,  0.0569,  0.2158,  0.0500, -0.2246],
        [-0.3047,  0.6211,  0.2598, -0.0552, -0.6016],
        [ 0.1631,  0.0253,  0.2305, -0.3008, -0.2285],
        [-0.0728, -0.1226,  0.8711, -0.0400, -0.7383],
        [-0.0403,  0.0354,  0.1162, -0.3691, -0.0835],
        [-0.1074,  0.3887,  0.1465, -0.2871, -0.4199],
        [ 0.0471,  0.0176,  0.1777, -0.0053, -0.5859],
        [-0.1279, -0.0308,  0.0874, -0.0486, -0.4277],
        [-0.0840,  0.0361, -0.0608, -0.1475,  0.0261],
        [ 0.3301,  0.1533,  0.3320, -0.0476, -0.4297],
        [-0.1582, -0.0603, -0.0168, -0.2910, -0.3945]], device='cuda:0',
       dtype=torch.bfloat16, grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None), SequenceClassifierOutput(loss=tensor(1.4238, device='cuda:0', grad_fn=<NllLossBackward0>), logits=tensor([[ 0.0212,  0.3984,  0.0300, -0.3301, -0.4805],
        [-0.0776,  0.2617,  0.4961, -0.6133, -0.4102],
        [-0.3184,  0.2188,  0.2441, -0.4160, -0.0723],
        [ 0.1030,  0.3691,  0.1816,  0.1162, -0.3008],
        [ 0.0115,  0.0248,  0.0204, -0.2188, -0.8789],
        [ 0.2637, -0.0894, -0.0640,  0.0674, -0.1416],
        [-0.2812,  0.1357,  0.7383, -0.3105, -0.9336],
        [-0.0933,  0.1436,  0.4004, -0.3516, -0.2969],
        [ 0.2305,  0.2256,  0.3633, -0.4062, -0.2734],
        [ 0.1289,  0.1475,  0.0688, -0.3008, -0.6016],
        [-0.2480,  0.3730,  0.0986, -0.1084, -0.1348],
        [-0.1914,  0.2520,  0.3496, -0.1279, -0.2441],
        [-0.5547,  0.1396,  0.5742, -0.5195, -0.6953],
        [-0.1426,  0.2910, -0.0510,  0.1406, -0.6406],
        [-0.3184, -0.2354,  0.2012, -0.0869, -0.4648],
        [-0.4570,  0.3613,  0.5312, -0.2119, -0.2334]], device='cuda:0',
       dtype=torch.bfloat16, grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None), SequenceClassifierOutput(loss=tensor(1.4404, device='cuda:0', grad_fn=<NllLossBackward0>), logits=tensor([[-0.1699, -0.1060,  0.0012, -0.4512, -0.1396],
        [-0.1348,  0.2793,  0.4141, -0.2559, -0.9336],
        [ 0.0708, -0.1357,  0.1406, -0.5938, -0.7734],
        [-0.1055,  0.2129,  0.0762, -0.1162, -0.0579],
        [-0.3887,  0.5156,  0.3203, -0.4961, -0.6562],
        [-0.2578,  0.3320,  0.2559,  0.0845, -0.2910],
        [ 0.0527, -0.0386,  0.1279, -0.6992, -0.6641],
        [-0.0422,  0.2305, -0.1021, -0.2578, -0.4082],
        [ 0.0942,  0.0908,  0.1689, -0.3359, -0.4707],
        [-0.3203, -0.0698,  0.1582,  0.1475, -0.6016],
        [-0.2256,  0.2852,  0.1816, -0.0060, -0.0898],
        [-0.0165,  0.6992, -0.1777, -0.1797,  0.0146],
        [-0.1699,  0.5039,  0.3535, -0.2207, -0.3340],
        [-0.5312,  0.1177,  0.2598, -0.5742, -0.6680],
        [ 0.1729,  0.2197, -0.0400, -0.1680, -0.0708],
        [-0.1221, -0.1089, -0.0182, -0.2373, -0.7656]], device='cuda:0',
       dtype=torch.bfloat16, grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None), SequenceClassifierOutput(loss=tensor(1.4902, device='cuda:0', grad_fn=<NllLossBackward0>), logits=tensor([[ 3.2227e-01, -4.8340e-02, -6.8359e-02,  1.4746e-01,  3.3203e-01],
        [-2.9883e-01,  1.2500e-01,  6.5430e-02, -2.9102e-01, -4.4336e-01],
        [-2.4902e-02,  3.3398e-01,  1.2012e-01, -1.8457e-01, -3.4961e-01],
        [-3.0859e-01, -3.8330e-02, -1.2061e-01, -2.7539e-01, -7.5000e-01],
        [-3.2617e-01,  2.8320e-01, -2.2095e-02, -3.9062e-02, -4.1797e-01],
        [-9.3262e-02,  5.4688e-01,  3.9453e-01, -1.4062e-01, -5.2734e-01],
        [ 5.8746e-04,  3.6914e-01,  3.9844e-01, -7.0312e-01, -3.9062e-02],
        [-2.0386e-02,  2.1191e-01,  5.0391e-01, -9.8633e-02, -9.1406e-01],
        [-2.6978e-02,  4.5312e-01, -3.7354e-02, -1.6113e-01, -3.9453e-01],
        [ 2.2559e-01, -1.2207e-01, -2.2070e-01, -5.3125e-01,  2.7539e-01],
        [-3.0469e-01, -2.9907e-02,  7.5195e-02, -2.3145e-01, -3.8281e-01],
        [ 7.4707e-02,  2.5586e-01,  4.5508e-01, -3.7891e-01, -4.7656e-01],
        [ 1.1133e-01,  3.6719e-01,  2.7539e-01, -4.1016e-01, -4.1406e-01],
        [-6.1719e-01, -1.2207e-01,  3.2422e-01, -2.0508e-01, -6.2500e-01],
        [-2.7539e-01,  2.9492e-01,  9.7656e-02, -2.3438e-01, -2.2949e-01],
        [ 5.0391e-01, -1.3062e-02,  3.8818e-02, -9.7656e-02, -8.7402e-02]],
       device='cuda:0', dtype=torch.bfloat16, grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None), SequenceClassifierOutput(loss=tensor(1.5259, device='cuda:0', grad_fn=<NllLossBackward0>), logits=tensor([[-0.0498,  0.3398, -0.0178, -0.1309, -0.4766],
        [-0.2402,  0.0143, -0.3789, -0.4121, -0.3867],
        [ 0.2422,  0.1387, -0.0815, -0.0991, -0.3164],
        [-0.1592,  0.2656,  0.0008, -0.3711, -0.4121],
        [-0.0051,  0.1572,  0.0422, -0.1104, -0.4883],
        [ 0.1289,  0.1768,  0.1060,  0.0762, -0.6367],
        [ 0.1963,  0.4707,  0.1328, -0.4277, -0.7695],
        [-0.1084, -0.1270, -0.0430, -0.3848, -0.5000],
        [-0.2441,  0.1387,  0.4805, -0.3594, -0.6133],
        [-0.0688,  0.2695,  0.0190, -0.0306, -0.6602],
        [-0.0015,  0.0781, -0.2461, -0.4941, -0.0266],
        [-0.1748,  0.2246,  0.1406, -0.0664, -0.7070],
        [-0.3613,  0.2676,  0.7773, -0.2100, -0.6328],
        [-0.1030,  0.3359,  0.2539, -0.2969, -0.3438],
        [-0.2188,  0.1089,  0.2021,  0.1973, -0.3691],
        [ 0.0089,  0.1445, -0.0056, -0.6250,  0.0189]], device='cuda:0',
       dtype=torch.bfloat16, grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None), SequenceClassifierOutput(loss=tensor(1.4541, device='cuda:0', grad_fn=<NllLossBackward0>), logits=tensor([[-0.3535,  0.5898, -0.0933, -0.3184, -0.0640],
        [ 0.1514, -0.0481, -0.2500, -0.1982, -0.6875],
        [-0.0693,  0.2559,  0.3340, -0.4004, -0.0610],
        [-0.0664,  0.4668,  0.0747, -0.4062, -0.2559],
        [ 0.3359,  0.0566,  0.0330, -0.4434, -0.4121],
        [-0.3320,  0.1279,  0.2393, -0.0195, -0.1289],
        [ 0.1562,  0.3105,  0.0845, -0.3613, -0.2969],
        [-0.0593,  0.1113,  0.1963,  0.0815, -0.4316],
        [-0.0294,  0.1494,  0.3340, -0.1045, -0.4141],
        [ 0.1631, -0.1504,  0.3848, -0.1592, -0.5078],
        [-0.2461,  0.2793,  0.1260,  0.1221, -0.4277],
        [-0.0840,  0.3574, -0.0383, -0.4258, -0.2227],
        [-0.3906,  0.0559,  0.3652, -0.3828, -0.6680],
        [-0.0698,  0.1348,  0.3398, -0.0728, -0.3555],
        [-0.0564, -0.1797, -0.2334, -0.4473, -0.2969],
        [-0.2480,  0.1904,  0.4648, -0.0796, -0.3711]], device='cuda:0',
       dtype=torch.bfloat16, grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None), SequenceClassifierOutput(loss=tensor(1.4683, device='cuda:0', grad_fn=<NllLossBackward0>), logits=tensor([[-0.1436, -0.2217,  0.2354, -0.3398, -0.4219],
        [ 0.0277,  0.2578,  0.1602, -0.3477, -0.8633],
        [ 0.0277,  0.3262,  0.2070,  0.1777, -0.4258],
        [-0.0640, -0.0447,  0.4375,  0.0330, -0.6641],
        [-0.1045, -0.0923,  0.0493, -0.6406, -0.1147],
        [ 0.0292, -0.2129,  0.0352, -0.6719, -0.3535],
        [-0.1240, -0.0474, -0.1196, -0.3945, -0.5156],
        [-0.1021,  0.4023,  0.1787, -0.2695, -0.1050],
        [ 0.2021,  0.2715,  0.5039, -0.7617, -0.1777],
        [-0.2637,  0.2148,  0.2852, -0.3438, -0.5586],
        [-0.0403,  0.0708,  0.4863, -0.0364, -0.5312],
        [-0.3223,  0.5117,  0.3926, -0.4629, -0.4766],
        [-0.3926,  0.0913, -0.0608, -0.3711, -0.4492],
        [-0.6172,  0.2119, -0.0918, -0.1494, -0.5508],
        [ 0.2314,  0.2676, -0.2168, -0.1582, -0.2461],
        [ 0.0165,  0.0254,  0.1641, -0.0840, -0.2461]], device='cuda:0',
       dtype=torch.bfloat16, grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None), SequenceClassifierOutput(loss=tensor(1.3525, device='cuda:0', grad_fn=<NllLossBackward0>), logits=tensor([[-0.4434,  0.0669,  0.1250, -0.5703, -0.7656],
        [-0.2197,  0.2246,  0.1260, -0.3496, -0.9258],
        [-0.0674,  0.3281, -0.0294, -0.0608, -0.2393],
        [ 0.3320,  0.3789,  0.1846,  0.0231, -0.2227],
        [-0.2402,  0.2227,  0.2100, -0.2305, -0.6484],
        [-0.1494, -0.0588,  0.2236,  0.0256, -0.0610],
        [-0.0211,  0.3496, -0.0054, -0.3457, -0.3066],
        [-0.3203,  0.2539, -0.0811, -0.2832, -0.4570],
        [-0.5391,  0.1885, -0.1572, -0.0454, -0.3516],
        [-0.0737,  0.0708,  0.0121, -0.4043, -0.1089],
        [-0.2197,  0.3594,  0.2393, -0.2344, -0.2891],
        [-0.2012,  0.5430,  0.5742, -0.2520, -0.3574],
        [-0.0157,  0.4395,  0.3047, -0.2539, -0.2305],
        [-0.2471,  0.3652,  0.1729, -0.2373, -0.4238],
        [-0.0187,  0.2598,  0.3789, -0.3848, -0.9180],
        [-0.2500,  0.0688,  0.2305, -0.0620, -0.6016]], device='cuda:0',
       dtype=torch.bfloat16, grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None), SequenceClassifierOutput(loss=tensor(1.4365, device='cuda:0', grad_fn=<NllLossBackward0>), logits=tensor([[-2.2095e-02,  3.7891e-01,  6.5430e-02, -3.4375e-01, -2.6367e-02],
        [-4.6387e-02, -1.2207e-01, -1.6797e-01, -4.2383e-01,  2.8534e-03],
        [ 5.1270e-02,  1.6797e-01,  1.7676e-01, -1.7773e-01, -3.9062e-01],
        [-3.8867e-01,  4.0283e-02,  1.0352e-01, -8.1055e-02, -1.2354e-01],
        [-1.1865e-01,  3.6523e-01,  7.1289e-02, -4.7266e-01, -7.0312e-02],
        [-2.1289e-01,  2.3730e-01,  1.6895e-01, -6.1279e-02, -2.7344e-01],
        [-4.0771e-02,  8.7402e-02,  2.0508e-01, -4.9219e-01, -8.9062e-01],
        [-2.0801e-01,  1.3123e-02,  5.1514e-02, -1.4648e-01, -2.8516e-01],
        [-8.4961e-02,  2.8711e-01,  4.0820e-01,  4.1809e-03, -7.2266e-01],
        [-1.2695e-02,  6.3477e-02,  1.5015e-02, -6.0791e-02, -5.1953e-01],
        [-2.2461e-01,  3.4375e-01,  3.1641e-01, -5.6250e-01, -5.1172e-01],
        [-4.5898e-01,  7.8613e-02,  2.8125e-01, -4.9219e-01, -9.1797e-01],
        [ 6.5994e-04, -2.3315e-02,  1.7188e-01, -1.1816e-01, -3.9844e-01],
        [-4.2578e-01,  5.6641e-01,  5.0000e-01, -1.6992e-01, -5.0391e-01],
        [ 2.1484e-01,  6.2500e-02, -1.2598e-01, -5.9375e-01, -4.7266e-01],
        [-3.7695e-01,  3.5352e-01,  1.3965e-01, -2.8906e-01, -4.1016e-01]],
       device='cuda:0', dtype=torch.bfloat16, grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None), SequenceClassifierOutput(loss=tensor(1.4248, device='cuda:0', grad_fn=<NllLossBackward0>), logits=tensor([[-0.1279,  0.3125,  0.1729, -0.4297, -0.2715],
        [-0.2490,  0.4414, -0.2041, -0.0913, -0.1260],
        [-0.0811,  0.2090,  0.2021, -0.3008, -0.2988],
        [ 0.0791,  0.0591,  0.0947, -0.4668, -0.3867],
        [ 0.0840,  0.2314,  0.1885, -0.2354, -0.0928],
        [-0.2754,  0.2773,  0.4512, -0.0991, -0.6328],
        [ 0.0491,  0.0635, -0.1758, -0.4336, -0.1895],
        [-0.2891, -0.1216,  0.2393, -0.3242, -0.5898],
        [-0.5547,  0.0574,  0.3887, -0.0786, -0.5352],
        [-0.1709,  0.1416,  0.1338, -0.1455, -0.3965],
        [ 0.0044,  0.2188, -0.0396, -0.3242, -0.4082],
        [-0.2656,  0.2988,  0.4180, -0.2451, -0.3965],
        [ 0.2188,  0.0845,  0.2773, -0.2021, -0.2217],
        [-0.2637,  0.1494,  0.2412,  0.0952, -0.3789],
        [ 0.0098,  0.1895,  0.1797,  0.1055, -0.1299],
        [ 0.3945, -0.2754, -0.4160, -0.4727, -0.0894]], device='cuda:0',
       dtype=torch.bfloat16, grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None), SequenceClassifierOutput(loss=tensor(1.4023, device='cuda:0', grad_fn=<NllLossBackward0>), logits=tensor([[-0.0117,  0.3379, -0.1089, -0.2422,  0.1230],
        [-0.1357,  0.0571,  0.1846,  0.0427, -0.4590],
        [ 0.1270,  0.3301,  0.2100, -0.0679, -0.2178],
        [ 0.1797,  0.6133, -0.1445, -0.4238, -0.1660],
        [-0.1455, -0.1426,  0.2324, -0.1001, -0.2080],
        [ 0.0771,  0.3457, -0.1807, -0.2676, -0.1641],
        [-0.1030,  0.4453, -0.0835, -0.0170, -0.3340],
        [-0.1816,  0.3848,  0.0403, -0.2402, -0.3945],
        [-0.1650,  0.2432,  0.1230, -0.4648, -0.3477],
        [-0.0339, -0.1445, -0.0884, -0.4414, -0.1494],
        [-0.1426,  0.3359,  0.3027, -0.5273, -0.3633],
        [-0.3594, -0.0447, -0.2754, -0.0400, -0.3105],
        [-0.1118,  0.4707,  0.2539, -0.1328, -0.4180],
        [-0.1826, -0.1279,  0.4609, -0.6406, -0.9180],
        [ 0.0859,  0.1924,  0.4141, -0.1611, -0.5742],
        [-0.4492,  0.1133,  0.3516, -0.4023, -0.7578]], device='cuda:0',
       dtype=torch.bfloat16, grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None), SequenceClassifierOutput(loss=tensor(1.3745, device='cuda:0', grad_fn=<NllLossBackward0>), logits=tensor([[ 0.1299,  0.1357,  0.0752, -0.3242,  0.3770],
        [-0.1641,  0.2617, -0.0114, -0.1357, -0.3887],
        [-0.1074,  0.1582,  0.1006, -0.4492, -0.0640],
        [-0.2832,  0.3906,  0.1318, -0.3320, -0.5859],
        [-0.1040, -0.0066,  0.0518, -0.2012, -0.3652],
        [-0.1187,  0.2197,  0.1572, -0.1240, -0.3672],
        [-0.4922,  0.7227,  0.3398, -0.4453, -0.4160],
        [-0.2773,  0.3457,  0.0067, -0.1826, -0.2061],
        [ 0.1367,  0.1826, -0.0332, -0.4375,  0.2432],
        [-0.1226,  0.5078,  0.0513, -0.3105, -0.3828],
        [ 0.2578,  0.2227,  0.0908, -0.2012, -0.1396],
        [-0.1289,  0.2344,  0.3477, -0.0125, -0.3301],
        [ 0.1426,  0.5508,  0.3516, -0.1543, -0.3164],
        [ 0.1582,  0.4746,  0.0056, -0.5312, -0.3984],
        [-0.0349,  0.2598, -0.4453, -0.2383, -0.2754],
        [-0.2656,  0.4453,  0.5156, -0.4141, -0.4473]], device='cuda:0',
       dtype=torch.bfloat16, grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None), SequenceClassifierOutput(loss=tensor(1.4595, device='cuda:0', grad_fn=<NllLossBackward0>), logits=tensor([[-0.0757,  0.1816, -0.0417, -0.5195, -0.1963],
        [-0.1318,  0.2852, -0.0104, -0.3027, -0.4551],
        [-0.1953,  0.3574,  0.0095, -0.0222, -0.4199],
        [ 0.1289,  0.1533,  0.2051, -0.3887, -0.2178],
        [-0.2275,  0.5078, -0.0596, -0.2520, -0.0835],
        [ 0.0493,  0.5273,  0.1079, -0.0566, -0.3848],
        [-0.0618,  0.2773,  0.0017, -0.1338, -0.0986],
        [-0.0361,  0.3535,  0.1865, -0.3770, -0.1289],
        [ 0.0354,  0.0157,  0.2227, -0.0070, -0.3242],
        [-0.2324, -0.0564,  0.7930, -0.1650, -0.1777],
        [-0.2314,  0.1299,  0.1416, -0.4434, -0.2617],
        [-0.1797,  0.1191,  0.3164, -0.1426, -0.0762],
        [-0.2041,  0.3242,  0.1216, -0.1660, -0.2031],
        [-0.5195,  0.3477,  0.3066, -0.3418, -0.6836],
        [ 0.0361,  0.0181, -0.2031, -0.0742, -0.0669],
        [-0.6211,  0.3926, -0.0072, -0.0698, -0.5039]], device='cuda:0',
       dtype=torch.bfloat16, grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None), SequenceClassifierOutput(loss=tensor(1.4351, device='cuda:0', grad_fn=<NllLossBackward0>), logits=tensor([[-3.8086e-01,  3.4961e-01,  1.9531e-01, -2.5195e-01, -6.4844e-01],
        [ 8.5938e-02,  7.9590e-02,  2.3438e-02, -8.6914e-02, -2.2559e-01],
        [-3.5352e-01,  3.8867e-01,  1.5430e-01,  1.0889e-01, -1.6504e-01],
        [-3.7305e-01,  4.0625e-01, -1.0156e-01, -4.2773e-01, -4.4141e-01],
        [-4.8438e-01,  3.9648e-01,  3.8477e-01, -4.0430e-01, -3.2617e-01],
        [-1.5820e-01,  2.4121e-01,  3.4766e-01, -2.4414e-01, -6.1768e-02],
        [-3.4424e-02,  3.6328e-01,  9.7656e-02, -1.5625e-01, -4.9414e-01],
        [-2.9883e-01,  4.1406e-01,  3.1641e-01, -4.4531e-01, -5.1562e-01],
        [-3.4961e-01,  3.0078e-01,  4.1016e-01, -1.8164e-01, -5.3906e-01],
        [-2.0215e-01, -1.3770e-01,  3.7695e-01, -1.4160e-01, -9.9219e-01],
        [ 1.4160e-01,  5.6641e-01, -2.7734e-01, -3.6328e-01, -3.8818e-02],
        [-1.0864e-02,  2.0020e-01, -1.2329e-02, -9.5703e-02, -4.2383e-01],
        [-1.8311e-02,  3.8086e-01,  3.0859e-01, -2.4609e-01, -4.9805e-02],
        [-1.6797e-01,  4.2188e-01,  7.3730e-02, -8.7891e-02, -5.1025e-02],
        [ 2.6489e-02,  4.2773e-01,  3.1055e-01,  1.6499e-04, -1.0400e-01],
        [-4.4922e-01,  5.4297e-01,  1.0925e-02,  4.9561e-02, -1.8848e-01]],
       device='cuda:0', dtype=torch.bfloat16, grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None), SequenceClassifierOutput(loss=tensor(1.4438, device='cuda:0', grad_fn=<NllLossBackward0>), logits=tensor([[-0.1875,  0.2969, -0.4004, -0.0190, -0.4590],
        [-0.2383,  0.0894,  0.1216, -0.3555, -0.7773],
        [-0.0383,  0.3438,  0.2793,  0.0422, -0.2656],
        [-0.4961,  0.0737,  0.3262, -0.5195, -0.3008],
        [-0.2598,  0.4805,  0.3906, -0.2422, -0.3965],
        [-0.0496,  0.0620,  0.1245, -0.3496, -0.1270],
        [-0.3809,  0.0820,  0.2041, -0.2227, -0.4805],
        [-0.1426,  0.3652,  0.2793, -0.0991, -0.1064],
        [-0.5312,  0.0413,  0.4180, -0.2314, -0.5898],
        [-0.0825,  0.3164,  0.2197, -0.0972, -0.5391],
        [-0.1758,  0.2295,  0.1484, -0.3008, -0.8906],
        [-0.4062, -0.0459,  0.3047, -0.1748, -0.8047],
        [-0.1543,  0.1030, -0.4023, -0.1826, -0.0801],
        [-0.0413,  0.3047,  0.2715, -0.2812,  0.0645],
        [-0.1797,  0.2617,  0.3223, -0.2676, -0.2891],
        [ 0.2119, -0.0072,  0.1030, -0.1167, -0.6328]], device='cuda:0',
       dtype=torch.bfloat16, grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None), SequenceClassifierOutput(loss=tensor(1.4048, device='cuda:0', grad_fn=<NllLossBackward0>), logits=tensor([[-0.3145,  0.2949,  0.0908,  0.0742, -0.4766],
        [-0.2949,  0.1484,  0.2061, -0.4668, -0.9414],
        [ 0.3613,  0.1172, -0.3535, -0.5781, -0.0239],
        [-0.0056,  0.0962, -0.0620, -0.1562, -0.3242],
        [-0.0703,  0.5312,  0.1611, -0.2852, -0.4199],
        [-0.1055,  0.3965,  0.4141,  0.0277, -0.2754],
        [-0.0767,  0.0781,  0.0850, -0.0029, -0.6758],
        [-0.2520,  0.4414, -0.0820, -0.2383, -0.3984],
        [-0.3613, -0.3066,  0.4199, -0.1621, -0.2754],
        [-0.3828,  0.0571,  0.2656, -0.3086, -1.2344],
        [-0.1553,  0.3906,  0.3633, -0.2412, -1.1094],
        [-0.1807,  0.2969,  0.0332, -0.2490, -0.2734],
        [ 0.0439,  0.3457, -0.1924, -0.1426, -0.2539],
        [-0.1689,  0.4336, -0.1543, -0.1050, -0.5117],
        [ 0.1367, -0.0176, -0.1357, -0.2559, -0.2080],
        [-0.1387,  0.3652,  0.1396, -0.0400, -0.0464]], device='cuda:0',
       dtype=torch.bfloat16, grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None), SequenceClassifierOutput(loss=tensor(1.4009, device='cuda:0', grad_fn=<NllLossBackward0>), logits=tensor([[-0.1748,  0.4434, -0.1807, -0.2344, -0.2480],
        [-0.1777,  0.0972, -0.0693, -0.2168, -0.0962],
        [-0.1240,  0.0082, -0.1875,  0.0269, -0.5312],
        [-0.8867,  0.3359,  0.3945, -0.3730, -0.6680],
        [-0.2256,  0.5547,  0.0420, -0.3594, -0.4531],
        [-0.4297,  0.2637, -0.0347, -0.1240, -0.3984],
        [ 0.1025,  0.0187, -0.4609, -0.3242, -0.0146],
        [-0.3809,  0.5117, -0.2500, -0.4453, -0.4141],
        [ 0.1436,  0.2334,  0.5312,  0.0212, -0.3809],
        [ 0.2637,  0.5391,  0.7383, -0.3281, -0.4121],
        [-0.4004,  0.1245, -0.1641, -0.2793, -0.7734],
        [ 0.0297,  0.4805, -0.0879, -0.3711, -0.0996],
        [-0.0315,  0.3223, -0.1680, -0.6445, -0.4219],
        [-0.2129,  0.4043, -0.2695, -0.4277, -0.4492],
        [-0.1318,  0.0228, -0.1992, -0.3066, -0.2041],
        [-0.2217,  0.3770, -0.1240, -0.4980,  0.2100]], device='cuda:0',
       dtype=torch.bfloat16, grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None), SequenceClassifierOutput(loss=tensor(1.4463, device='cuda:0', grad_fn=<NllLossBackward0>), logits=tensor([[-0.0762,  0.1138, -0.1738, -0.3750, -0.4277],
        [-0.0554,  0.2559, -0.2539, -0.4473, -0.2871],
        [-0.1885, -0.2578,  0.6055,  0.2168, -0.1797],
        [ 0.1680,  0.3906,  0.0762, -0.3145, -0.3594],
        [ 0.0240,  0.3164, -0.0300, -0.4512, -0.6797],
        [-0.1484,  0.1826,  0.2852, -0.3477, -0.4082],
        [-0.4766, -0.0806, -0.1572, -0.1289, -0.4023],
        [-0.0112,  0.1865, -0.0147, -0.6484, -0.3574],
        [-0.1865,  0.4316,  0.3594, -0.3730, -0.3457],
        [-0.1250,  0.2061,  0.3594, -0.3086, -0.2002],
        [-0.0608,  0.5039,  0.2910, -0.4316,  0.0454],
        [ 0.0610, -0.3379,  0.8203, -0.3359, -0.4727],
        [-0.1030, -0.0308, -0.0325, -0.2812, -0.2715],
        [ 0.1182,  0.3984, -0.0679, -0.1758,  0.3281],
        [-0.3223,  0.4141, -0.2041, -0.5352, -0.4199],
        [-0.2812,  0.2695,  0.0879, -0.4277, -0.2949]], device='cuda:0',
       dtype=torch.bfloat16, grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None), SequenceClassifierOutput(loss=tensor(1.4092, device='cuda:0', grad_fn=<NllLossBackward0>), logits=tensor([[ 0.1670, -0.4570, -0.5039, -0.5859, -0.1709],
        [ 0.1230,  0.2637,  0.0820, -0.2676, -0.3691],
        [ 0.2773,  0.1934,  0.1719, -0.3535,  0.1465],
        [-0.3535,  0.4082, -0.0923,  0.0977, -0.6914],
        [-0.1128,  0.1338,  0.2422, -0.1748, -0.5586],
        [-0.0294,  0.0356,  0.1143, -0.5703, -0.5586],
        [-0.2148,  0.1211, -0.3320, -0.1719, -0.2402],
        [-0.0248,  0.4492, -0.0085,  0.0018, -0.3574],
        [-0.0347,  0.3867, -0.0186, -0.3340, -0.2637],
        [-0.3457,  0.5898, -0.0713, -0.1484, -0.3477],
        [-0.2715,  0.1436,  0.3945, -0.2617, -0.9180],
        [ 0.0874,  0.2852,  0.3125, -0.2061, -0.8281],
        [ 0.0035,  0.2930,  0.2910, -0.4160, -0.4629],
        [-0.4180,  0.5273,  0.0564,  0.1436, -0.0796],
        [-0.1777,  0.3184,  0.1069,  0.0325, -0.5391],
        [ 0.1855,  0.0547,  0.0439, -0.1582, -0.4922]], device='cuda:0',
       dtype=torch.bfloat16, grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None), SequenceClassifierOutput(loss=tensor(1.4194, device='cuda:0', grad_fn=<NllLossBackward0>), logits=tensor([[ 0.1836,  0.2930,  0.2441, -0.1699, -0.1074],
        [-0.2305,  0.0781,  0.0311, -0.1260, -0.2539],
        [ 0.3242,  0.2578, -0.4375, -0.5039, -0.0654],
        [ 0.1016,  0.3613, -0.0188, -0.2080, -0.4785],
        [-0.5898,  0.5156,  0.0815, -0.1621, -0.1846],
        [-0.3770,  0.0090,  0.4453, -0.6758, -0.4609],
        [-0.0361,  0.2539,  0.5352, -0.4121, -0.2988],
        [-0.2949,  0.4707,  0.1963, -0.6680, -0.6055],
        [-0.4590,  0.4199, -0.0977, -0.0996, -0.1680],
        [-0.3770,  0.1021,  0.1436,  0.0070, -0.7344],
        [-0.1016,  0.5156,  0.3008, -0.1787, -0.0571],
        [ 0.2041,  0.2227, -0.0278, -0.2451, -0.3418],
        [-0.0903,  0.3105, -0.0513, -0.1226, -0.3340],
        [-0.3008,  0.4609,  0.3984, -0.3652, -0.4844],
        [-0.2002,  0.6211,  0.1875, -0.2539, -0.2227],
        [-0.0562,  0.0752,  0.3887, -0.1128, -0.5586]], device='cuda:0',
       dtype=torch.bfloat16, grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None), SequenceClassifierOutput(loss=tensor(1.2974, device='cuda:0', grad_fn=<NllLossBackward0>), logits=tensor([[-0.4766, -0.2031,  0.0830, -0.2275, -0.4121],
        [ 0.0040,  0.2539,  0.3359, -0.3027, -0.3906],
        [ 0.0815,  0.4707,  0.2500, -0.3848, -0.4219],
        [-0.2383,  0.6016,  0.1953, -0.4883, -0.3379],
        [ 0.1533,  0.5352,  0.3027, -0.0693, -0.9727],
        [ 0.1562,  0.0996, -0.0388, -0.1846, -0.1221],
        [-0.4863,  0.6680,  0.2676, -0.2129, -0.5352],
        [ 0.1260,  0.5273,  0.0425, -0.2871, -0.3125],
        [ 0.1064,  0.3340, -0.0579, -0.2852, -0.2695],
        [-0.1055,  0.1016, -0.1167, -0.3418, -0.1187],
        [-0.2256,  0.3945,  0.1514,  0.1660,  0.0500],
        [-0.3242,  0.1357,  0.5039, -0.1035, -0.5781],
        [ 0.1089,  0.1494,  0.5898, -0.1201,  0.0437],
        [-0.1167,  0.5742,  0.1338, -0.2324, -0.4316],
        [-0.3145,  0.2129,  0.0044, -0.4043, -0.4531],
        [-0.0869,  0.7070,  0.1953, -0.4551, -0.1611]], device='cuda:0',
       dtype=torch.bfloat16, grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None), SequenceClassifierOutput(loss=tensor(1.2207, device='cuda:0', grad_fn=<NllLossBackward0>), logits=tensor([[-0.5312,  0.4707, -0.2021, -0.2500, -0.1982],
        [-0.0518,  0.6680,  0.2832, -0.4082, -0.6875],
        [-0.3223,  0.1631, -0.0640, -0.4785, -0.5586],
        [-0.4375,  0.4805,  0.0703, -0.4492, -0.5234],
        [ 0.2656,  0.3516, -0.2051, -0.6680,  0.0239],
        [ 0.1885,  0.4316,  0.0063, -0.3086, -0.1875],
        [-0.1377,  0.1377, -0.1650, -0.2715, -0.4570],
        [ 0.1826,  0.3613,  0.1514, -0.1650, -0.4043],
        [-0.3613,  0.1689,  0.1216, -0.1191, -0.2100],
        [-0.0845,  0.8242,  0.2471, -0.2715, -0.0027],
        [-0.1680,  0.7305,  0.2432, -0.2520, -0.0193],
        [-0.3555,  0.4961,  0.0476, -0.6836, -0.5430],
        [-0.2754,  0.5195,  0.1934, -0.1118, -0.4785],
        [-0.0610,  0.3730,  0.1592, -0.4609, -0.0654],
        [ 0.1167, -0.1016,  0.1172, -0.1318, -0.2812],
        [ 0.2246,  0.1807, -0.2090, -0.1992, -0.4375]], device='cuda:0',
       dtype=torch.bfloat16, grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None), SequenceClassifierOutput(loss=tensor(1.3081, device='cuda:0', grad_fn=<NllLossBackward0>), logits=tensor([[ 0.1543,  0.6250,  0.0359,  0.0996, -0.0708],
        [ 0.0378,  0.5820,  0.1416, -0.1406, -0.1128],
        [-0.4609,  0.2793, -0.1592, -0.0188, -0.4160],
        [ 0.1436, -0.2158,  0.3535, -0.4609, -0.4375],
        [ 0.3906,  0.1582, -0.1299, -0.4395, -0.0610],
        [-0.1758,  0.3516,  0.0640, -0.4941, -0.4727],
        [-0.0266,  0.5156, -0.0347, -0.2256, -0.0718],
        [-0.2578,  0.3320,  0.1855, -0.4141, -0.5430],
        [-0.2314,  0.5273,  0.2754, -0.2129, -0.2490],
        [-0.3828,  0.4355, -0.2754, -0.0359, -0.2637],
        [-0.0913,  0.4941,  0.2393, -0.2812, -0.5273],
        [-0.1201,  0.6328, -0.1475, -0.2852, -0.3633],
        [-0.3438,  0.3691,  0.0254, -0.5039, -0.2314],
        [ 0.0078,  0.1631,  0.4727,  0.1709, -0.6875],
        [ 0.1797,  0.1572, -0.0869, -0.2520, -0.3320],
        [-0.1787,  0.5938,  0.0972,  0.4160, -0.3926]], device='cuda:0',
       dtype=torch.bfloat16, grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None), SequenceClassifierOutput(loss=tensor(1.3301, device='cuda:0', grad_fn=<NllLossBackward0>), logits=tensor([[ 0.1416,  0.5117,  0.0566, -0.1484, -0.1211],
        [-0.3320,  0.0130,  0.2314, -0.0874, -0.3984],
        [-0.2891,  0.2129,  0.0933, -0.3691, -0.2637],
        [-0.0344,  0.2695,  0.5156, -0.0879, -0.0430],
        [-0.2188,  0.4668,  0.1680, -0.4336, -0.5664],
        [-0.2324,  0.4980,  0.2480, -0.0265, -0.4492],
        [-0.3027,  0.2500,  0.3984, -0.3281, -0.7578],
        [-0.3691,  0.1138,  0.4082, -0.1543, -0.5859],
        [-0.0752,  0.3730,  0.4766, -0.4141, -0.6992],
        [-0.0703,  0.7070,  0.1611, -0.2305, -0.0903],
        [ 0.0981,  0.4727,  0.0247,  0.0069, -0.0496],
        [-0.2617, -0.0972,  0.1396, -0.2432, -0.0464],
        [ 0.0752,  0.5156, -0.0732, -0.5234, -0.4668],
        [-0.1572,  0.3359, -0.1187, -0.0107, -0.2002],
        [-0.0977,  0.1035,  0.1758, -0.1943, -0.1943],
        [-0.0271,  0.1465, -0.1631, -0.3887,  0.0058]], device='cuda:0',
       dtype=torch.bfloat16, grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None), SequenceClassifierOutput(loss=tensor(1.4346, device='cuda:0', grad_fn=<NllLossBackward0>), logits=tensor([[-2.2754e-01,  3.2227e-01, -1.2695e-01, -5.5078e-01, -8.2812e-01],
        [ 1.2451e-01, -2.8320e-01, -4.8828e-01, -4.7852e-01,  6.0059e-02],
        [-4.5410e-02,  3.8672e-01,  1.2878e-02, -4.0234e-01,  1.3281e-01],
        [-3.2812e-01,  3.2227e-01,  2.0142e-02,  3.9062e-01, -1.4648e-01],
        [ 2.0898e-01,  5.8203e-01, -4.1580e-04, -4.0039e-01, -1.8945e-01],
        [-1.3965e-01,  5.1880e-03,  6.3672e-01, -3.3691e-02, -7.5000e-01],
        [ 4.8828e-01, -8.7402e-02, -3.5547e-01,  3.3008e-01,  5.5469e-01],
        [ 5.3711e-02,  2.0117e-01, -5.9326e-02, -3.1641e-01, -3.0078e-01],
        [ 6.0059e-02,  1.3281e-01,  3.5889e-02, -9.4238e-02, -2.4316e-01],
        [-4.5312e-01,  2.8516e-01,  1.5430e-01, -3.7891e-01, -2.5977e-01],
        [-3.1250e-01,  6.8750e-01,  2.4023e-01, -1.9043e-01, -4.0430e-01],
        [ 1.8652e-01,  3.1836e-01, -2.6367e-02,  5.2002e-02, -2.2168e-01],
        [-2.1387e-01,  3.4375e-01,  5.1270e-02, -2.5000e-01, -3.1250e-01],
        [ 4.9805e-02,  1.4062e-01, -6.3965e-02, -6.9922e-01,  2.3047e-01],
        [-1.8848e-01,  2.3145e-01,  1.8164e-01, -4.0283e-02, -7.4609e-01],
        [-7.3730e-02,  4.9414e-01,  4.6094e-01, -2.5391e-01, -7.3828e-01]],
       device='cuda:0', dtype=torch.bfloat16, grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None), SequenceClassifierOutput(loss=tensor(1.3835, device='cuda:0', grad_fn=<NllLossBackward0>), logits=tensor([[-0.1592,  0.4062,  0.2363, -0.1865, -0.3047],
        [-0.2148,  0.2021,  0.2812, -0.3906, -0.8633],
        [-0.3633,  0.1748, -0.1108, -0.0957, -0.4004],
        [-0.2715,  0.1729,  0.0623,  0.1680, -0.6914],
        [-0.2539,  0.0396, -0.0288, -0.4219, -0.6367],
        [-0.1230, -0.1348, -0.1777, -0.4004, -0.6523],
        [-0.3613,  0.2754,  0.0308, -0.0145, -0.4043],
        [-0.1865,  0.5508,  0.1299,  0.0942, -0.3242],
        [-0.0408,  0.8281,  0.1494, -0.3691, -0.1973],
        [-0.1719,  0.2021,  0.0089, -0.1187, -0.3848],
        [-0.4277,  0.4941,  0.0090, -0.3281, -0.1582],
        [-0.3359,  0.3750,  0.0781, -0.2754, -0.7109],
        [-0.4648,  0.4336, -0.0204, -0.3770, -0.1191],
        [-0.0513,  0.2178, -0.3672, -0.4492, -0.1895],
        [-0.3809,  0.3984, -0.0025, -0.4316, -0.3672],
        [-0.1738,  0.3477, -0.0280, -0.2422, -0.3828]], device='cuda:0',
       dtype=torch.bfloat16, grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None), SequenceClassifierOutput(loss=tensor(1.5083, device='cuda:0', grad_fn=<NllLossBackward0>), logits=tensor([[-0.1426,  0.4785,  0.0347, -0.1455, -0.1631],
        [-0.2383, -0.0806,  0.2715, -0.4785, -0.8516],
        [-0.3398,  0.3203,  0.2432, -0.3477, -0.7422],
        [-0.2100,  0.2217,  0.1768,  0.1240, -0.4727],
        [-0.0645,  0.1865, -0.0811,  0.0972, -0.2148],
        [ 0.2080,  0.3398,  0.1152, -0.0288, -0.0815],
        [-0.1650,  0.4551,  0.0240, -0.2910, -0.2578],
        [ 0.0630,  0.2754,  0.4824, -0.4453, -0.5352],
        [ 0.2539,  0.0737,  0.4609, -0.0981, -0.4961],
        [-0.1064,  0.4316, -0.0010, -0.0583, -0.0569],
        [-0.3496,  0.2734,  0.3691, -0.8008, -0.8398],
        [-0.0981,  0.6367,  0.2441, -0.5430, -0.1211],
        [-0.6484,  0.1631,  0.2559, -0.3066, -0.2285],
        [ 0.0498,  0.6172,  0.1113, -0.3828, -0.0894],
        [ 0.0374,  0.2363,  0.1143, -0.2656, -0.3145],
        [-0.0457, -0.0292, -0.2188, -0.4805, -0.3184]], device='cuda:0',
       dtype=torch.bfloat16, grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None), SequenceClassifierOutput(loss=tensor(1.4448, device='cuda:0', grad_fn=<NllLossBackward0>), logits=tensor([[-0.1465,  0.6562, -0.0410, -0.3770, -0.2158],
        [-0.2012,  0.4941, -0.0347, -0.4141, -0.3438],
        [ 0.0422,  0.2891,  0.2676, -0.3984, -0.7148],
        [ 0.1387,  0.0016, -0.1133, -0.4043, -0.1621],
        [ 0.5469, -0.1094, -0.4023, -0.1729,  0.5781],
        [-0.3848, -0.0194, -0.3320, -0.1235, -0.7031],
        [ 0.0330,  0.5391, -0.3027, -0.3066, -0.1504],
        [-0.0215,  0.3145,  0.1914, -0.2891, -0.8633],
        [-0.0129,  0.1670,  0.4355, -0.2891, -0.7383],
        [-0.0864,  0.2715,  0.1738, -0.3184, -0.0625],
        [-0.0762,  0.4590, -0.2715, -0.2266, -0.3730],
        [ 0.1777,  0.3203, -0.1113, -0.2910, -0.0505],
        [ 0.0015,  0.4590, -0.0237, -0.5625, -0.2949],
        [-0.2402,  0.1245,  0.0537, -0.4102, -0.7188],
        [ 0.0698,  0.1641,  0.0074, -0.3438, -0.2891],
        [ 0.0791,  0.2969,  0.2910, -0.2793, -0.9180]], device='cuda:0',
       dtype=torch.bfloat16, grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None), SequenceClassifierOutput(loss=tensor(1.2856, device='cuda:0', grad_fn=<NllLossBackward0>), logits=tensor([[-0.0559,  0.4512,  0.2119, -0.1309, -0.3066],
        [-0.1309,  0.4004, -0.1475, -0.1875, -0.1885],
        [-0.0491,  0.5859, -0.1875, -0.5664, -0.2109],
        [-0.3281,  0.3105,  0.2061, -0.5391, -0.5859],
        [-0.0771,  0.4902,  0.3008, -0.1602, -0.7656],
        [-0.1406,  0.4609,  0.0103, -0.4688, -0.6445],
        [-0.0530,  0.2617, -0.0439, -0.2988, -0.2051],
        [-0.1797,  0.6797,  0.1924, -0.3262, -0.5312],
        [-0.3203,  0.2539,  0.2949, -0.2285, -0.3086],
        [-0.0835,  0.3340, -0.0255, -0.3340, -0.1963],
        [ 0.0334,  0.6914,  0.2334, -0.3809, -0.1523],
        [ 0.2715,  0.0505,  0.0009, -0.2773, -0.4414],
        [ 0.0430,  0.3008,  0.0176, -0.1475, -0.3340],
        [ 0.3301,  0.6445,  0.0903, -0.5000, -0.3008],
        [-0.0236,  0.1182, -0.1299, -0.2871, -0.2256],
        [-0.2520,  0.6016,  0.1348, -0.3320, -0.2227]], device='cuda:0',
       dtype=torch.bfloat16, grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None), SequenceClassifierOutput(loss=tensor(1.4028, device='cuda:0', grad_fn=<NllLossBackward0>), logits=tensor([[-0.2295,  0.4297, -0.0757, -0.0610, -0.2578],
        [-0.3848, -0.1787,  0.0801, -0.3105, -0.6680],
        [-0.2852,  0.7070, -0.1504, -0.1494, -0.3340],
        [-0.0165,  0.5820, -0.0095, -0.2383, -0.1992],
        [-0.3730,  0.4414,  0.3242, -0.2090, -0.2188],
        [-0.2383,  0.1367,  0.4004, -0.5703, -1.0234],
        [-0.1328,  0.5469,  0.0115, -0.3887, -0.5000],
        [-0.1094,  0.5273, -0.0195, -0.3281, -0.1924],
        [-0.4785,  0.4316,  0.4863,  0.1406, -0.0452],
        [-0.1152,  0.7031,  0.0613, -0.1484, -0.4922],
        [ 0.1885,  0.5898,  0.2461,  0.0028, -0.0500],
        [ 0.0291,  0.4668,  0.1191, -0.0179, -0.0806],
        [-0.1562,  0.4062,  0.4883, -0.1992, -0.6680],
        [ 0.1211,  0.3203, -0.0286, -0.4629, -0.4512],
        [ 0.2891, -0.0206, -0.5039, -0.2061,  0.2539],
        [-0.3066,  0.1836, -0.0515, -0.5039, -1.1719]], device='cuda:0',
       dtype=torch.bfloat16, grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None), SequenceClassifierOutput(loss=tensor(1.2480, device='cuda:0', grad_fn=<NllLossBackward0>), logits=tensor([[ 0.1475,  0.4707, -0.2695, -0.4590,  0.1089],
        [ 0.3066,  0.5625,  0.2197, -0.3379, -0.2148],
        [ 0.1963,  0.3965, -0.1523, -0.4492, -0.3301],
        [-0.1099,  0.5352,  0.0608, -0.4219,  0.1079],
        [ 0.4160,  0.4395, -0.1797, -0.3320, -0.2754],
        [-0.0244,  0.4590, -0.1895, -0.5234, -0.4434],
        [-0.2852,  0.4648,  0.4512, -0.2324, -0.3457],
        [-0.2695,  0.2070,  0.1211, -0.3867, -0.9141],
        [-0.1055,  0.5898,  0.0815, -0.2891, -0.5742],
        [-0.3242,  0.4863,  0.1357, -0.4609, -0.2314],
        [ 0.1021,  0.4414, -0.0413, -0.0859, -0.2910],
        [-0.2236,  0.0461,  0.4902, -0.3730, -0.3613],
        [-0.1377, -0.0214,  0.0410, -0.3672, -0.4805],
        [ 0.0967,  0.7266, -0.2910, -0.4238, -0.2393],
        [ 0.3164,  0.4355, -0.2432, -0.2637, -0.4922],
        [-0.0552,  0.4219, -0.3887, -0.0014, -0.1475]], device='cuda:0',
       dtype=torch.bfloat16, grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None), SequenceClassifierOutput(loss=tensor(1.2295, device='cuda:0', grad_fn=<NllLossBackward0>), logits=tensor([[-0.0025,  0.5273,  0.1069, -0.0942, -0.3867],
        [-0.0513,  0.4688,  0.1436, -0.6484, -0.5703],
        [ 0.0258,  0.3359,  0.2354, -0.3359, -0.3848],
        [-0.1924,  0.4609,  0.4961, -0.2754, -0.3809],
        [ 0.1777,  0.3730,  0.0330, -0.4141, -0.0106],
        [-0.1348, -0.0400,  0.2891, -0.2812, -0.6836],
        [ 0.1934,  0.6562, -0.4648, -0.2520,  0.0173],
        [ 0.0574,  0.5820,  0.0830, -0.3672, -0.2275],
        [ 0.0850,  0.6211,  0.1465, -0.2451, -0.3848],
        [-0.2129,  0.6250,  0.2715,  0.0320, -0.4844],
        [-0.0981,  0.7461,  0.1514, -0.5938, -0.5625],
        [-0.1079,  0.3281, -0.3516, -0.2930, -0.4258],
        [ 0.3633, -0.2852, -0.5625, -0.4980,  0.4258],
        [ 0.0767,  0.4141,  0.1670, -0.3340, -0.6719],
        [-0.0918,  0.4688,  0.2490, -0.4277, -0.1865],
        [ 0.3125,  0.4746,  0.1128, -0.4316, -0.1768]], device='cuda:0',
       dtype=torch.bfloat16, grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None), SequenceClassifierOutput(loss=tensor(1.3755, device='cuda:0', grad_fn=<NllLossBackward0>), logits=tensor([[-0.2324,  0.4434,  0.0771, -0.2393, -0.0903],
        [-0.1367,  0.5625,  0.5039, -0.5469, -0.3418],
        [ 0.1943,  0.2441, -0.0635, -0.6445, -0.3027],
        [-0.2021,  0.5078,  0.5000, -0.4414, -0.3105],
        [-0.2070,  0.1270,  0.1641, -0.2539, -0.2158],
        [-0.1943,  0.3477,  0.1182, -0.3535, -0.1631],
        [ 0.0272,  0.4902, -0.0152, -0.3379, -0.1621],
        [ 0.1631,  0.0791,  0.3418,  0.0242, -0.3301],
        [-0.0835,  0.3770,  0.6172, -0.1914, -0.6289],
        [ 0.0928,  0.3828, -0.1030, -0.1533, -0.4551],
        [ 0.0111,  0.4785, -0.3789, -0.5781, -0.0102],
        [-0.0591,  0.5430,  0.2500, -0.4316, -0.5742],
        [-0.2734, -0.0055,  0.0315, -0.6367, -0.4531],
        [-0.0530,  0.1226,  0.1709, -0.2480, -0.2871],
        [-0.2852,  0.5078, -0.1074,  0.0043, -0.5234],
        [-0.3652,  0.4336,  0.0781, -0.2949, -0.1084]], device='cuda:0',
       dtype=torch.bfloat16, grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None), SequenceClassifierOutput(loss=tensor(1.4463, device='cuda:0', grad_fn=<NllLossBackward0>), logits=tensor([[-0.2617, -0.0505,  0.1504, -0.2930, -0.8516],
        [ 0.2217,  0.5117, -0.1787, -0.4453, -0.0713],
        [ 0.0674,  0.5273,  0.2930, -0.2256, -0.2852],
        [-0.2676,  0.5391, -0.0579, -0.1406, -0.4609],
        [-0.1079,  0.2461,  0.4180, -0.2266, -0.2227],
        [ 0.2178, -0.2021, -0.1836,  0.1260, -0.3359],
        [-0.2891,  0.7383,  0.2471,  0.0547,  0.1680],
        [ 0.2314,  0.1631,  0.3672, -0.2500, -0.7188],
        [-0.1152,  0.2852,  0.6523, -0.0405, -0.7695],
        [ 0.0311,  0.4316,  0.0022, -0.5898, -0.1641],
        [-0.1074,  0.2334,  0.0167, -0.6289, -0.3047],
        [ 0.1699,  0.3418,  0.0684, -0.6406, -0.4473],
        [-0.0164,  0.7031,  0.2324, -0.4102, -0.3730],
        [-0.1904,  0.4219,  0.2793,  0.1138, -0.3770],
        [ 0.0026,  0.1738,  0.2324, -0.1816, -0.6875],
        [-0.3379,  0.3867,  0.1709, -0.0874,  0.0300]], device='cuda:0',
       dtype=torch.bfloat16, grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None), SequenceClassifierOutput(loss=tensor(1.2444, device='cuda:0', grad_fn=<NllLossBackward0>), logits=tensor([[-0.1504,  0.0238,  0.2031, -0.2910, -0.4180],
        [ 0.0613,  0.4648, -0.1152, -0.2178, -0.3320],
        [ 0.0156,  0.8086, -0.0693, -0.1191, -0.2832],
        [-0.0884,  0.3828,  0.0786, -0.4414, -0.2490],
        [-0.3203,  0.2480,  0.1206, -0.3691, -0.8867],
        [ 0.2656,  0.2949,  0.0265, -0.3164, -0.3477],
        [-0.0869,  0.5078, -0.0356, -0.1514, -0.6758],
        [-0.0486,  0.4062,  0.3496, -0.5234, -0.6953],
        [ 0.1338,  0.3574, -0.3320, -0.5938, -0.1123],
        [-0.2480,  0.3965,  0.2275, -0.2500, -0.4785],
        [-0.3398,  0.2197, -0.1719, -0.2949, -0.3438],
        [ 0.1270,  0.2812,  0.0015, -0.4824, -0.1816],
        [-0.0388,  0.5156,  0.0815, -0.6133, -0.5781],
        [-0.2812,  0.3027,  0.2891, -0.2246, -0.4238],
        [-0.3203,  0.0454,  0.3477, -0.3652, -0.3594],
        [ 0.1328, -0.4102, -0.7539, -0.1729,  0.0957]], device='cuda:0',
       dtype=torch.bfloat16, grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None), SequenceClassifierOutput(loss=tensor(1.3489, device='cuda:0', grad_fn=<NllLossBackward0>), logits=tensor([[ 0.0349,  0.2988, -0.0234, -0.5742, -0.8281],
        [ 0.2676,  0.5039, -0.0728, -0.5078, -0.1196],
        [-0.0265,  0.0566,  0.4766, -0.0825, -0.2578],
        [-0.1729,  0.1396, -0.0554, -0.0806, -0.0986],
        [ 0.0464,  0.1963,  0.2559,  0.0532, -0.6133],
        [ 0.1875,  0.4766,  0.3203, -0.2695, -0.8203],
        [-0.3105,  0.1738,  0.2598, -0.4883, -0.0096],
        [-0.0337,  0.6992, -0.0776, -0.2910, -0.3047],
        [ 0.0137,  0.7383,  0.1289, -0.4570, -0.7070],
        [-0.3613, -0.0042,  0.0195,  0.0032, -0.5312],
        [-0.2256,  0.5039, -0.0588, -0.2334, -0.1816],
        [-0.0593,  0.5664, -0.1436, -0.4746, -0.4824],
        [ 0.1553,  0.3359, -0.0864, -0.1572, -0.4121],
        [ 0.0654,  0.3887,  0.0532,  0.1641, -0.1807],
        [ 0.1562,  0.2021, -0.2988, -0.1338, -0.1602],
        [-0.0620,  0.5273,  0.0972, -0.1089, -0.3086]], device='cuda:0',
       dtype=torch.bfloat16, grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None), SequenceClassifierOutput(loss=tensor(1.2778, device='cuda:0', grad_fn=<NllLossBackward0>), logits=tensor([[ 0.2520,  0.0199, -0.6875, -0.6289,  0.3418],
        [-0.3418,  0.3477, -0.0518, -0.3887, -0.3750],
        [-0.5469,  0.2500,  0.0256, -0.3164, -0.5391],
        [-0.2578,  0.3027,  0.1953, -0.5742, -0.4395],
        [ 0.1196,  0.3223, -0.1680, -0.2832,  0.0112],
        [-0.0884,  0.5039, -0.2422,  0.0554, -0.2539],
        [-0.4629,  0.5586,  0.1768, -0.1758, -0.6367],
        [-0.1631,  0.4062,  0.2754, -0.1699, -0.5156],
        [ 0.1011,  0.5898,  0.1250, -0.2715, -0.2344],
        [-0.4414,  0.5039,  0.4922,  0.0347, -0.7266],
        [-0.2002,  0.4941,  0.0234, -0.1631, -0.2949],
        [-0.2520,  0.3105, -0.0554, -0.2080, -0.0031],
        [ 0.0021,  0.5781,  0.1191, -0.2695, -0.2832],
        [-0.3145, -0.0688,  0.0317, -0.2402, -0.7773],
        [ 0.0124,  0.5195, -0.1465, -0.4395, -0.3418],
        [-0.2002,  0.2676, -0.0339, -0.1475, -0.6641]], device='cuda:0',
       dtype=torch.bfloat16, grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None), SequenceClassifierOutput(loss=tensor(1.3152, device='cuda:0', grad_fn=<NllLossBackward0>), logits=tensor([[-0.0060, -0.1030, -0.2520, -0.4141, -0.3750],
        [-0.2480,  0.0544,  0.0342, -0.6367, -0.7305],
        [ 0.1611,  0.5078,  0.2236, -0.2832, -0.1611],
        [-0.1147,  0.4629,  0.3242, -0.4277,  0.0094],
        [-0.1973,  0.3613, -0.1758, -0.3809, -0.3828],
        [-0.2021,  0.6914, -0.0530, -0.1309, -0.5508],
        [-0.2617,  0.6602, -0.1177, -0.1768, -0.4492],
        [ 0.2393,  0.6602,  0.2451, -0.3027, -0.0540],
        [ 0.2471,  0.0659, -0.1865, -0.4531, -0.0305],
        [-0.1045,  0.5938,  0.0352,  0.1079, -0.2197],
        [ 0.0118,  0.6914, -0.1113, -0.3359, -0.1973],
        [ 0.0270,  0.4863, -0.3340, -0.1279, -0.0977],
        [-0.5156,  0.3691,  0.2812, -0.3125, -0.3613],
        [-0.2891,  0.1855, -0.3594, -0.0913, -0.4453],
        [-0.6250,  0.6562,  0.1924, -0.2656, -0.5117],
        [-0.1128,  0.6016, -0.0063, -0.1328, -0.2637]], device='cuda:0',
       dtype=torch.bfloat16, grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None), SequenceClassifierOutput(loss=tensor(1.2300, device='cuda:0', grad_fn=<NllLossBackward0>), logits=tensor([[ 0.0039,  0.3711,  0.1045, -0.0874, -0.2793],
        [-0.2100,  0.4316, -0.0243, -0.4785, -0.2441],
        [-0.3828,  0.4668, -0.0186, -0.1836, -0.6562],
        [-0.1406,  0.6133,  0.3262, -0.1729, -0.2236],
        [-0.2539,  0.3184,  0.6562, -0.1396, -0.8633],
        [-0.0825,  0.3555, -0.3555, -0.3242, -0.5508],
        [-0.3711,  0.5547,  0.0352, -0.3828, -0.5234],
        [-0.3203,  0.0791, -0.2285, -0.2852, -0.4004],
        [-0.0069,  0.1719, -0.2090, -0.0664, -0.2695],
        [ 0.2275,  0.3223, -0.2500, -0.4473, -0.6016],
        [-0.3301,  0.6523, -0.0898, -0.3340, -0.1494],
        [-0.2354,  0.2031,  0.1455, -0.6602, -1.2031],
        [-0.1455,  0.1514,  0.3262, -0.2158, -0.9922],
        [-0.2656,  0.1069,  0.2139, -0.2002, -0.4551],
        [-0.0923,  0.6602,  0.0679, -0.4590, -0.2080],
        [-0.0684,  0.5820,  0.0674, -0.2617, -0.2139]], device='cuda:0',
       dtype=torch.bfloat16, grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None), SequenceClassifierOutput(loss=tensor(1.3247, device='cuda:0', grad_fn=<NllLossBackward0>), logits=tensor([[-0.1084,  0.0986,  0.5938, -0.0312, -0.7500],
        [ 0.1992,  0.5781, -0.2578, -0.3242, -0.3555],
        [-0.2119,  0.3457, -0.0088,  0.2412, -0.3652],
        [ 0.2246,  0.6445, -0.2090, -0.3594, -0.1562],
        [-0.0383,  0.6172, -0.1602, -0.5820, -0.4199],
        [-0.0276,  0.5977,  0.1191, -0.1934, -0.0664],
        [ 0.2344,  0.6250,  0.4805, -0.3379, -0.3262],
        [-0.1777,  0.2500,  0.1177, -0.3125, -0.2852],
        [-0.1904,  0.5156,  0.0806, -0.5547, -0.7891],
        [-0.1436,  0.1953,  0.3340,  0.1123, -0.0820],
        [-0.0649,  0.5391,  0.0371, -0.1426, -0.3789],
        [-0.0155,  0.4629, -0.1084, -0.2021, -0.3652],
        [-0.0189,  0.2119,  0.6641,  0.2236, -0.5625],
        [-0.3398,  0.6406,  0.1367, -0.2832, -0.3184],
        [-0.5898,  0.5508, -0.2988, -0.4102, -0.4512],
        [ 0.0038,  0.1221, -0.2695, -0.4570,  0.0286]], device='cuda:0',
       dtype=torch.bfloat16, grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None), SequenceClassifierOutput(loss=tensor(1.2822, device='cuda:0', grad_fn=<NllLossBackward0>), logits=tensor([[ 1.1035e-01,  3.4180e-01, -3.2227e-01, -1.5234e-01,  2.3145e-01],
        [-1.7480e-01,  5.0781e-01,  3.8867e-01, -6.7969e-01, -3.3398e-01],
        [-5.5176e-02,  6.6016e-01, -5.2246e-02, -1.0498e-01, -4.2969e-01],
        [ 5.7861e-02,  5.6641e-01, -8.9844e-02, -8.2520e-02, -1.2500e-01],
        [-1.2305e-01,  3.2617e-01,  3.5938e-01, -4.5312e-01, -6.7188e-01],
        [-2.9883e-01,  5.4688e-01, -2.4512e-01, -3.4180e-01, -2.5195e-01],
        [-2.3535e-01,  5.8203e-01, -9.7656e-02, -2.1875e-01, -2.0508e-01],
        [-9.3750e-02,  3.5156e-01,  9.8877e-03,  1.3867e-01, -1.1523e-01],
        [-3.9062e-01,  5.1953e-01,  7.0801e-02, -2.6172e-01, -3.6328e-01],
        [-3.2227e-01,  8.0859e-01, -1.4343e-02, -2.0801e-01, -3.3594e-01],
        [ 2.2168e-01,  1.6699e-01, -4.4531e-01, -4.2969e-01, -3.1836e-01],
        [ 8.1055e-02,  6.7578e-01,  7.2266e-02, -5.0781e-01, -3.4375e-01],
        [-7.2266e-02,  6.7188e-01,  2.5195e-01, -1.5625e-01, -3.8086e-02],
        [-1.3574e-01,  9.0332e-02,  1.8848e-01, -4.7852e-02, -7.5781e-01],
        [-8.9844e-02,  3.0859e-01, -2.0312e-01, -1.0352e-01, -4.4922e-01],
        [-2.4128e-04,  8.5938e-01, -3.8818e-02, -2.0117e-01, -8.2520e-02]],
       device='cuda:0', dtype=torch.bfloat16, grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None), SequenceClassifierOutput(loss=tensor(1.2924, device='cuda:0', grad_fn=<NllLossBackward0>), logits=tensor([[-0.2051,  0.5195,  0.2559, -0.0260, -0.6680],
        [ 0.0962,  0.3379,  0.0035, -0.0728, -0.2578],
        [-0.2539,  0.1855,  0.3145, -0.5781, -0.7617],
        [-0.0212,  0.3809, -0.4629, -0.2637, -0.0352],
        [-0.4883,  0.4062,  0.3926, -0.5703, -0.7812],
        [-0.0957,  0.4961,  0.0967, -0.2490, -0.3438],
        [-0.4609,  0.5742, -0.1465, -0.1416, -0.3184]], device='cuda:0',
       dtype=torch.bfloat16, grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)], [SequenceClassifierOutput(loss=tensor(1.7847, device='cuda:0', grad_fn=<NllLossBackward0>), logits=tensor([[ 0.0986,  0.7305,  0.0933, -0.3594, -0.3457],
        [ 0.2988, -0.0417, -0.7344, -0.5547,  0.3281],
        [-0.1367,  0.2314,  0.0245, -0.3164, -0.1328],
        [-0.2090,  0.5664, -0.0236, -0.0086, -0.5469],
        [-0.4902,  0.2676,  0.0334, -0.4199, -0.1611],
        [-0.3105,  0.5898,  0.3262, -0.4531, -0.6250],
        [ 0.1211,  0.1836, -0.3164, -0.3750, -0.2139],
        [ 0.0815,  0.1060, -0.3086, -0.4941, -0.1846],
        [ 0.0640,  0.3828,  0.1113, -0.4824, -0.3594],
        [-0.1143,  0.3301,  0.0498, -0.4160, -0.3926],
        [-0.2061,  0.1338,  0.3867,  0.1748, -0.2930],
        [ 0.0703,  0.4590,  0.2812, -0.1768, -0.2559],
        [-0.2559,  0.2275, -0.4336, -0.3340, -0.0884],
        [ 0.0674, -0.0986, -0.3906, -0.3242, -0.1553],
        [-0.3281,  0.2471, -0.3730, -0.1807, -0.6445],
        [ 0.0522,  0.4043,  0.0645, -0.2441, -0.5078]], device='cuda:0',
       dtype=torch.bfloat16, grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None), SequenceClassifierOutput(loss=tensor(1.8301, device='cuda:0', grad_fn=<NllLossBackward0>), logits=tensor([[ 0.0986,  0.4551,  0.2734, -0.3516, -0.3086],
        [-0.3242,  0.4453,  0.1621, -0.5156, -0.3262],
        [-0.2695,  0.2617,  0.0286, -0.2148, -0.5938],
        [ 0.0698,  0.1318,  0.0967, -0.1069, -0.0845],
        [-0.3574,  0.3711,  0.1108,  0.0679, -0.4062],
        [ 0.0757,  0.4316, -0.3223, -0.6250, -0.5039],
        [-0.1719,  0.4785, -0.0325, -0.1572, -0.2520],
        [-0.3828,  0.4785,  0.1924, -0.4531, -0.5234],
        [ 0.0339,  0.5703, -0.0913, -0.1060, -0.1826],
        [-0.2559,  0.0967,  0.2217, -0.3477, -0.5508],
        [-0.3574,  0.3633,  0.2227, -0.3828, -0.1797],
        [-0.3203,  0.3242,  0.4238, -0.5469, -0.2871],
        [ 0.0170,  0.4219, -0.2559, -0.3789,  0.0089],
        [ 0.0410,  0.1729, -0.2051, -0.4551, -0.1738],
        [-0.3125,  0.3301,  0.4160, -0.0486, -0.3906],
        [-0.2178,  0.4551,  0.3379, -0.1055, -0.1934]], device='cuda:0',
       dtype=torch.bfloat16, grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None), SequenceClassifierOutput(loss=tensor(1.6621, device='cuda:0', grad_fn=<NllLossBackward0>), logits=tensor([[ 0.3262,  0.0454, -0.7266, -0.1328, -0.0645],
        [-0.2793,  0.5859, -0.1157, -0.3047, -0.2969],
        [-0.2676,  0.0703, -0.2354, -0.6641,  0.2158],
        [ 0.1621,  0.2734, -0.1240, -0.3652, -0.1484],
        [-0.2148,  0.6406,  0.3184, -0.4219, -0.2256],
        [ 0.4258,  0.1177, -0.2852,  0.1270,  0.4570],
        [-0.2109,  0.2754, -0.0410, -0.1865, -0.6523],
        [-0.4902,  0.4238,  0.2949,  0.1143, -0.1982],
        [-0.3027, -0.1953,  0.2637, -0.0649, -0.1592],
        [-0.4160,  0.4629,  0.4863, -0.2061, -0.3633],
        [-0.0250,  0.1157, -0.1138, -0.3066, -0.0972],
        [ 0.1914,  0.0532, -0.4082, -0.3320, -0.3184],
        [-0.0113, -0.2061,  0.4727, -0.0610, -0.1680],
        [-0.1328,  0.3398, -0.2734, -0.3594, -0.2158],
        [ 0.0693,  0.4688,  0.3594, -0.6250, -0.4688],
        [ 0.0703,  0.0315, -0.7266, -0.8828,  0.0219]], device='cuda:0',
       dtype=torch.bfloat16, grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None), SequenceClassifierOutput(loss=tensor(1.8247, device='cuda:0', grad_fn=<NllLossBackward0>), logits=tensor([[-0.4297,  0.5391,  0.0028, -0.2930,  0.0131],
        [ 0.0684,  0.5703,  0.1738, -0.2793, -0.1245],
        [-0.2637,  0.1738,  0.0649, -0.6289, -0.6797],
        [-0.0123,  0.4199, -0.0728, -0.3652, -0.4277],
        [-0.3223,  0.5117,  0.0408, -0.4512, -0.2695],
        [ 0.0713,  0.6914,  0.3691, -0.1177, -0.6758],
        [-0.0540,  0.1865, -0.1211, -0.6484, -0.3496],
        [-0.1807,  0.2168,  0.0325, -0.2139, -0.2129],
        [ 0.0913, -0.1689, -0.0615, -0.1523,  0.3340],
        [ 0.1719,  0.2227, -0.0742, -0.3652, -0.1904],
        [-0.1816,  0.6797, -0.2314, -0.2988, -0.1816],
        [-0.6172,  0.0339,  0.1543, -0.5195, -0.6602],
        [ 0.2480,  0.2832, -0.2539, -0.3613, -0.3496],
        [-0.1787,  0.4531, -0.0293, -0.2324, -0.4102],
        [ 0.0486,  0.1709,  0.2139, -0.4883, -0.9141],
        [ 0.0381,  0.3613, -0.1777, -0.1553, -0.3320]], device='cuda:0',
       dtype=torch.bfloat16, grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None), SequenceClassifierOutput(loss=tensor(1.8242, device='cuda:0', grad_fn=<NllLossBackward0>), logits=tensor([[-0.4961,  0.6719, -0.0332, -0.5352, -0.6875],
        [ 0.4258,  0.4648,  0.3262, -0.1748, -0.2373],
        [-0.0044,  0.3398,  0.0640, -0.2969, -0.2061],
        [-0.3418,  0.1992,  0.0101, -0.3945, -0.1050],
        [-0.0840,  0.0547,  0.3203, -0.2480, -0.6562],
        [-0.0127, -0.1465, -0.6094, -0.2354,  0.5352],
        [-0.0099,  0.6680, -0.2578, -0.6523, -0.4980],
        [ 0.6133,  0.0635, -0.5859, -0.1250,  0.1670],
        [ 0.1660,  0.1504, -0.3809, -0.2930, -0.2988],
        [-0.1147,  0.4414, -0.0503, -0.5820, -0.6758],
        [ 0.3867,  0.1348, -0.0708, -0.4297, -0.2520],
        [ 0.0150,  0.1914, -0.2129, -0.3848, -0.3535],
        [-0.0942,  0.6016,  0.1602, -0.1797, -0.0894],
        [-0.0388,  0.0571, -0.1108, -0.6133, -0.2266],
        [-0.1934,  0.1729, -0.0583, -0.4121, -0.5508],
        [ 0.0156,  0.4414,  0.0342, -0.4121, -0.2852]], device='cuda:0',
       dtype=torch.bfloat16, grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None), SequenceClassifierOutput(loss=tensor(1.5742, device='cuda:0', grad_fn=<NllLossBackward0>), logits=tensor([[-0.0420,  0.5977,  0.1001, -0.3594, -0.6445],
        [-0.2793,  0.8477,  0.2207, -0.5273, -0.5273],
        [ 0.1074,  0.4180, -0.1787, -0.4355, -0.1807],
        [-0.1621,  0.3340, -0.0309, -0.6641, -0.3613],
        [-0.1523,  0.5273,  0.1191, -0.0588, -0.2158],
        [-0.1113,  0.4102,  0.0076, -0.1157, -0.3105],
        [-0.2812,  0.2637,  0.1543, -0.0540, -1.0156],
        [-0.2812,  0.3789,  0.4766, -0.2676, -0.7109],
        [-0.0078,  0.2754,  0.3750, -0.4141, -0.9570],
        [-0.0957,  0.5000,  0.2246, -0.4023, -0.9219],
        [ 0.0610,  0.1138, -0.2617, -0.2119, -0.6562],
        [-0.6133,  0.3887,  0.5664, -0.3066, -0.5195],
        [-0.0291,  0.3633,  0.6602, -0.2051, -0.5117],
        [-0.3008,  0.0640, -0.0143, -0.6797, -0.5664],
        [-0.3027,  0.1357, -0.0299, -0.2070, -0.7539],
        [-0.3418,  0.2168, -0.2158, -0.2949, -0.9102]], device='cuda:0',
       dtype=torch.bfloat16, grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None), SequenceClassifierOutput(loss=tensor(1.4761, device='cuda:0', grad_fn=<NllLossBackward0>), logits=tensor([[ 0.1836, -0.0540, -0.1201, -0.5703, -0.3613],
        [-0.2832,  0.6680,  0.2158, -0.1367, -0.8047],
        [-0.5586,  0.3867,  0.3633, -0.2441, -0.5117],
        [-0.4141,  0.4355,  0.0664, -0.1982, -0.6875],
        [ 0.0155,  0.1133,  0.3066, -0.2656, -0.3223],
        [-0.2520,  0.3340,  0.0542, -0.0376, -1.0312],
        [ 0.0820,  0.0049,  0.3926, -0.3613, -0.5352],
        [-0.1572,  0.6406,  0.4629,  0.0889, -1.1328],
        [-0.1177,  0.5039,  0.3320, -0.3535, -0.5352],
        [-0.0398,  0.3555,  0.3008, -0.2500, -0.7969],
        [-0.2871,  0.1611,  0.2695, -0.3535, -0.5508],
        [-0.2832,  0.4570, -0.0923, -0.6016, -0.7773],
        [-0.5625,  0.2832,  0.1201, -0.6250, -0.8984],
        [-0.1445,  0.7461,  0.5117, -0.2969, -0.7344],
        [-0.4355,  0.1797, -0.1904, -0.5938, -0.3379],
        [-0.1855,  0.4727,  0.4590, -0.0391, -0.3125]], device='cuda:0',
       dtype=torch.bfloat16, grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None), SequenceClassifierOutput(loss=tensor(1.2720, device='cuda:0', grad_fn=<NllLossBackward0>), logits=tensor([[ 0.0986,  0.2656,  0.2871, -0.3184, -0.4805],
        [ 0.0037,  0.4492,  0.2363, -0.1553, -0.7578],
        [-0.3496,  0.0266,  0.3223, -0.2012, -0.4668],
        [-0.3047,  0.0173, -0.1523,  0.0947, -0.9023],
        [-0.0962,  0.0099,  0.2793, -0.2617, -0.4277],
        [-0.0275,  0.3887, -0.3535, -0.2354, -0.2617],
        [-0.1162,  0.4160,  0.4160, -0.5469, -1.0547],
        [-0.6016,  0.5859,  0.1748, -0.3594, -0.4961],
        [-0.2041,  0.4023,  0.3594, -0.4883, -0.7070],
        [-0.2793,  0.4941,  0.5938, -0.2617, -1.0547],
        [-0.0664,  0.6328,  0.3418, -0.3008, -0.4219],
        [-0.1162,  0.1260, -0.0801, -0.3105, -0.3945],
        [-0.2256,  0.2754,  0.1562, -0.5156, -0.9688],
        [-0.0742,  0.1924,  0.2578, -0.5430, -0.7031],
        [-0.3652,  0.2676, -0.0918, -0.2969, -0.5039],
        [-0.3125,  0.4766,  0.1396, -0.3340, -0.7773]], device='cuda:0',
       dtype=torch.bfloat16, grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None), SequenceClassifierOutput(loss=tensor(1.3035, device='cuda:0', grad_fn=<NllLossBackward0>), logits=tensor([[-0.1621,  0.5625, -0.0014, -0.7422, -0.9375],
        [ 0.2227,  0.5195,  0.4492, -0.5117, -0.4844],
        [-0.3691,  0.5859,  0.4160, -0.4707, -0.2715],
        [-0.3242,  0.0659, -0.2207, -0.6641, -0.7070],
        [-0.4512, -0.0659,  0.0752, -0.2754, -0.5977],
        [-0.3262,  0.4355,  0.4766, -0.1309, -0.7617],
        [-0.5859,  0.1084,  0.1611, -0.5859, -0.6602],
        [-0.4453,  0.1807,  0.1875, -0.4180, -0.8633],
        [-0.0835,  0.0159,  0.1611, -0.4590, -0.6953],
        [-0.1660,  0.4141, -0.3613, -0.1436, -0.5938],
        [-0.4199,  0.2715,  0.2793, -0.3438, -0.5586],
        [-0.3164,  0.2158, -0.0112, -0.2266, -0.6875],
        [-0.2402,  0.0608,  0.2852, -0.7266, -0.1250],
        [-0.3809,  0.1738,  0.1357, -0.5586, -0.7266],
        [-0.2285,  0.3496,  0.0439, -0.4805, -0.1436],
        [-0.1836, -0.0776,  0.0771, -0.3125, -0.6094]], device='cuda:0',
       dtype=torch.bfloat16, grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None), SequenceClassifierOutput(loss=tensor(1.3354, device='cuda:0', grad_fn=<NllLossBackward0>), logits=tensor([[-0.3320,  0.4180,  0.0413, -0.2002, -0.4883],
        [-0.4238,  0.0320,  0.4062, -0.3926, -0.8867],
        [ 0.0825,  0.2246, -0.0610, -0.3145, -0.4023],
        [-0.2109,  0.2617,  0.2012, -0.4824, -0.5078],
        [-0.3848,  0.1523,  0.1865, -0.1768, -0.5547],
        [-0.1592, -0.1001,  0.0737, -0.2344, -0.7578],
        [ 0.1514,  0.6875,  0.1406, -0.3047, -0.4609],
        [-0.2441,  0.6953,  0.0151, -0.0554, -0.2734],
        [-0.4141,  0.3398,  0.1270, -0.4082, -0.9258],
        [-0.3027, -0.2100,  0.2969, -0.1123, -0.9180],
        [-0.2148,  0.3184,  0.2070, -0.5898, -0.9531],
        [-0.1030,  0.1128,  0.1641, -0.1445, -0.5508],
        [-0.2148,  0.2383,  0.0708, -0.4531, -0.9570],
        [-0.0398,  0.5273, -0.0381, -0.1855, -0.2949],
        [-0.3359,  0.2852, -0.1279,  0.1797, -0.6758],
        [-0.5156,  0.2334,  0.1650, -0.1445, -0.3008]], device='cuda:0',
       dtype=torch.bfloat16, grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None), SequenceClassifierOutput(loss=tensor(1.4663, device='cuda:0', grad_fn=<NllLossBackward0>), logits=tensor([[-0.8320,  0.4238, -0.0400, -0.3633, -0.7539],
        [-0.4375,  0.4473,  0.4160, -0.0981, -0.9375],
        [-0.3887,  0.0104,  0.1582, -0.1846, -1.1719],
        [-0.1475,  0.3418,  0.3164, -0.2520, -0.7852],
        [-0.3164,  0.1826,  0.1602, -0.4492, -0.8672],
        [-0.3418,  0.1660, -0.0359, -0.4180, -0.7344],
        [-0.2539,  0.1592, -0.0757, -0.0762, -0.9531],
        [-0.0618,  0.0986, -0.3164, -0.1216, -0.4023],
        [-0.2637,  0.4824,  0.0820, -0.2578, -0.5547],
        [-0.4453,  0.6289,  0.3379, -0.1768, -0.6953],
        [-0.3086,  0.1445,  0.3594, -0.5195, -0.5430],
        [-0.3750,  0.1157,  0.3984, -0.5742, -0.6328],
        [-0.1660,  0.1152,  0.1982, -0.0347, -0.6406],
        [ 0.1230,  0.3984,  0.0056, -0.1738, -0.3867],
        [-0.1953,  0.4609,  0.1953, -0.3730, -0.6602],
        [-0.2500, -0.1436, -0.0820, -0.4141, -0.3828]], device='cuda:0',
       dtype=torch.bfloat16, grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None), SequenceClassifierOutput(loss=tensor(1.3794, device='cuda:0', grad_fn=<NllLossBackward0>), logits=tensor([[-0.2002,  0.0330,  0.1377, -0.0369, -0.5430],
        [-0.0535, -0.0635, -0.0143, -0.3223, -0.6484],
        [-0.1011,  0.2139, -0.0645, -0.5586, -0.6172],
        [-0.1602,  0.1387, -0.1836, -0.4883, -0.3418],
        [-0.4121,  0.2236,  0.0011, -0.6445, -0.5391],
        [-0.2002,  0.4043,  0.4453, -0.7773, -0.6719],
        [-0.3633,  0.0486,  0.4688, -0.3574, -0.6094],
        [-0.4102,  0.3535, -0.1226, -0.1602, -0.6719],
        [-0.0540, -0.0369,  0.1143, -0.3457, -0.6211],
        [-0.2930,  0.2100, -0.0962, -0.3691, -0.7539],
        [-0.5156,  0.4395,  0.2656, -0.2002, -0.8633],
        [-0.2246, -0.3379,  0.1445, -0.2598, -0.2734],
        [-0.2598,  0.1846,  0.3105, -0.4492, -0.7305],
        [-0.1250,  0.4258,  0.0776, -0.0103, -0.5117],
        [-0.1494,  0.5508,  0.2559, -0.8750, -0.7305],
        [-0.0991,  0.2500,  0.0284, -0.3184, -0.5625]], device='cuda:0',
       dtype=torch.bfloat16, grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None), SequenceClassifierOutput(loss=tensor(1.3228, device='cuda:0', grad_fn=<NllLossBackward0>), logits=tensor([[-0.4902,  0.5078, -0.0427, -0.0869, -0.2754],
        [ 0.0874,  0.4219,  0.5156, -0.3496, -0.5586],
        [-0.0820,  0.2773,  0.4297, -0.3164, -0.8086],
        [-0.1924,  0.4961,  0.3652, -0.0737, -0.6172],
        [-0.4961,  0.6719,  0.3809, -0.1797, -0.6523],
        [-0.6211,  0.3965,  0.0571, -0.2227, -0.7148],
        [-0.3164,  0.5469,  0.2812, -0.1777, -0.8945],
        [-0.4043,  0.3555,  0.1309, -0.2715, -1.0938],
        [-0.0688,  0.2715,  0.4062, -0.0481, -0.4805],
        [-0.1367,  0.1582,  0.0564, -0.6094, -0.8516],
        [-0.7031,  0.5625,  0.3789, -0.5430, -0.6523],
        [-0.4180,  0.2373,  0.1758, -0.2949,  0.0342],
        [-0.2695,  0.3125,  0.3223, -0.4336, -0.7109],
        [-0.1943,  0.3965,  0.1348, -0.4629, -0.5977],
        [-0.2051,  0.4102,  0.3027, -0.6250, -0.6055],
        [-0.0016,  0.3457,  0.2021, -0.1826, -0.6836]], device='cuda:0',
       dtype=torch.bfloat16, grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None), SequenceClassifierOutput(loss=tensor(1.3464, device='cuda:0', grad_fn=<NllLossBackward0>), logits=tensor([[-0.0476, -0.0172, -0.3008, -0.3086, -0.2988],
        [-0.1021,  0.5039, -0.1299, -0.4258, -0.5859],
        [-0.2012,  0.2617,  0.1553, -0.1299, -0.7891],
        [-0.2363,  0.0179,  0.1719, -0.3535, -0.6836],
        [-0.2109,  0.5547,  0.3418, -0.2266, -0.5859],
        [-0.4277,  0.1807,  0.2949,  0.0339, -0.2520],
        [-0.2012,  0.0645,  0.3145, -0.4277, -0.5742],
        [-0.5039,  0.3359,  0.4727, -0.1553, -0.4746],
        [-0.0796, -0.2139, -0.0036, -0.3379, -0.5039],
        [-0.0143,  0.2891,  0.2988, -0.3066, -0.3516],
        [-0.1797,  0.1582,  0.3926, -0.2598, -0.5117],
        [-0.3008,  0.4961, -0.4004, -0.2471, -0.3340],
        [-0.4727,  0.6602,  0.1016, -0.2129, -0.6680],
        [ 0.8008, -0.0238, -0.2520, -0.4785,  0.1426],
        [-0.2471,  0.9062, -0.2236, -0.2324, -0.2637],
        [-0.0850,  0.2617,  0.1758, -0.3359, -0.5938]], device='cuda:0',
       dtype=torch.bfloat16, grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None), SequenceClassifierOutput(loss=tensor(1.2788, device='cuda:0', grad_fn=<NllLossBackward0>), logits=tensor([[-0.3477,  0.5703,  0.4004, -0.6289, -0.6250],
        [-0.0106,  0.5000,  0.0942, -0.1118, -0.6211],
        [-0.2891,  0.5117,  0.4727, -0.1025, -0.7031],
        [-0.2441,  0.4473,  0.0244, -0.7227, -0.5352],
        [-0.2852,  0.4629,  0.4727, -0.0486, -0.4863],
        [-0.4492,  0.3906,  0.0273, -0.3477, -0.4902],
        [-0.0177,  0.1445,  0.4434, -0.2793, -0.8047],
        [ 0.0630,  0.1465, -0.1069, -0.4238, -0.4648],
        [-0.4824,  0.3379,  0.1406, -0.6445, -0.8320],
        [-0.2754,  0.2207,  0.1787, -0.4062, -0.8008],
        [-0.1729,  0.3340,  0.1069, -0.6602, -0.4570],
        [-0.2295,  0.3809,  0.2119, -0.1079, -0.5117],
        [-0.0503,  0.2285,  0.1514, -0.5352, -0.8477],
        [-0.2324,  0.6289,  0.2715, -0.3926, -0.5859],
        [-0.1436,  0.1299, -0.0640, -0.4062, -0.7461],
        [ 0.1846,  0.4531, -0.1592, -0.3008, -0.6562]], device='cuda:0',
       dtype=torch.bfloat16, grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None), SequenceClassifierOutput(loss=tensor(1.2876, device='cuda:0', grad_fn=<NllLossBackward0>), logits=tensor([[-0.0087,  0.4980, -0.0102, -0.4160, -0.8594],
        [-0.1621,  0.4980,  0.4961, -0.3008, -0.5742],
        [-0.4805,  0.2119,  0.0405, -0.4648, -0.5273],
        [-0.2129,  0.6172,  0.3223, -0.2930, -0.7812],
        [-0.4707,  0.1001, -0.0049, -0.2891, -0.9023],
        [-0.1758,  0.2969,  0.3652, -0.2227, -0.1758],
        [-0.3457,  0.0203, -0.0222, -0.6133, -1.4062],
        [-0.1318,  0.0659,  0.2188, -0.1025, -0.4570],
        [-0.0869,  0.3789,  0.3047, -0.4277, -0.9062],
        [-0.0282,  0.3457,  0.1094, -0.3086, -0.5820],
        [-0.0378,  0.2344, -0.0552, -0.6836, -0.6875],
        [-0.2754,  0.3516,  0.2100, -0.4746, -0.3828],
        [ 0.0830,  0.0732,  0.1245, -0.2598, -0.2334],
        [ 0.0138,  0.0139, -0.0021, -0.3926, -1.0156],
        [-0.5156,  0.4062,  0.1787, -0.2617, -0.6328],
        [-0.3887,  0.6523,  0.0991, -0.4961, -0.7461]], device='cuda:0',
       dtype=torch.bfloat16, grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None), SequenceClassifierOutput(loss=tensor(1.3887, device='cuda:0', grad_fn=<NllLossBackward0>), logits=tensor([[-0.2461,  0.3223,  0.2852, -0.7344, -0.6289],
        [-0.2168,  0.0928,  0.0498, -0.1768, -0.7500],
        [-0.1069,  0.1387,  0.0347, -0.3945, -0.3867],
        [-0.2217,  0.0908,  0.2178, -0.4551, -0.4980],
        [ 0.4434, -0.0103, -0.3574, -0.0574,  0.3691],
        [-0.0952,  0.3867,  0.1973, -0.7031, -0.6445],
        [-0.1357,  0.3242,  0.2119, -0.2363, -0.8164],
        [-0.0173,  0.2236, -0.2031, -0.5859, -0.7500],
        [-0.2812,  0.3184,  0.1572, -0.5273, -0.1807],
        [-0.5000,  0.5625,  0.5234, -0.4492, -1.0625],
        [-0.1406,  0.1943, -0.0493, -0.1650, -0.6250],
        [-0.2500,  0.6367,  0.0344, -0.2480, -0.2305],
        [-0.4980,  0.3105,  0.3242, -0.0811, -1.0625],
        [-0.0962,  0.3242, -0.0391, -0.0464, -0.0542],
        [-0.2051,  0.3809,  0.1118, -0.4316, -0.6797],
        [ 0.1982,  0.3281,  0.0378, -0.2598, -0.2373]], device='cuda:0',
       dtype=torch.bfloat16, grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None), SequenceClassifierOutput(loss=tensor(1.1885, device='cuda:0', grad_fn=<NllLossBackward0>), logits=tensor([[-0.2188,  0.1060, -0.1904, -0.2754, -0.4434],
        [ 0.1777,  0.2754,  0.0564, -0.3496, -0.5820],
        [-0.4473,  0.3359,  0.3770, -0.5391, -0.9648],
        [-0.5117,  0.3516,  0.1553, -0.1826, -0.7695],
        [ 0.0981,  0.3867, -0.1279,  0.0908, -0.3184],
        [-0.0022,  0.0771,  0.1787, -0.4863, -0.5117],
        [ 0.1001,  0.5156, -0.1416, -0.1963, -0.1455],
        [-0.3516,  0.5586, -0.1328, -0.4629, -0.4941],
        [-0.0223,  0.4141,  0.0540, -0.6680, -0.6289],
        [ 0.0728,  0.6836, -0.0449, -0.1875, -0.3848],
        [-0.1436,  0.4160, -0.3008, -0.2520, -0.2129],
        [-0.0854,  0.3848, -0.3496, -0.3281, -0.3359],
        [-0.0845,  0.5859,  0.0325, -0.6797, -0.1963],
        [-0.4102,  0.5312, -0.0491, -0.4023, -0.3047],
        [-0.3633,  0.2871,  0.0298, -0.4688, -0.9414],
        [ 0.2539,  0.5586, -0.1826, -0.0967, -0.2754]], device='cuda:0',
       dtype=torch.bfloat16, grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None), SequenceClassifierOutput(loss=tensor(1.2300, device='cuda:0', grad_fn=<NllLossBackward0>), logits=tensor([[-0.0179,  0.8203, -0.2480, -0.5586, -0.4121],
        [-0.2148,  0.2949, -0.2812, -0.7617, -0.3867],
        [-0.2393,  0.2373, -0.3496, -0.6523, -0.8672],
        [ 0.1377,  0.3711, -0.2773, -0.0415, -0.4609],
        [ 0.1553,  0.5352,  0.4941, -0.2812, -0.6602],
        [-0.3164,  0.3145,  0.1123, -0.0471, -0.3418],
        [-0.0157,  0.4395,  0.3477, -0.1338, -0.9414],
        [ 0.0781,  0.5312,  0.3340, -0.5156, -0.2695],
        [-0.0266,  0.1221, -0.2432, -0.1025, -0.3984],
        [-0.1582,  0.7500,  0.0043, -0.4434, -0.0608],
        [-0.2578,  0.5391, -0.0942, -0.1191, -0.4004],
        [-0.1445,  0.6172, -0.3887, -0.0330, -0.1543],
        [-0.5820,  0.1621,  0.0918, -0.2793, -0.4883],
        [-0.0078,  0.6055,  0.0066, -0.1836, -0.3105],
        [ 0.3105, -0.1855, -0.2773, -0.1118, -0.0259],
        [-0.2334,  0.6484,  0.0369, -0.4199, -0.3594]], device='cuda:0',
       dtype=torch.bfloat16, grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None), SequenceClassifierOutput(loss=tensor(1.1289, device='cuda:0', grad_fn=<NllLossBackward0>), logits=tensor([[-0.3340,  0.3184,  0.2148, -0.4062, -0.6133],
        [ 0.0266,  0.6641,  0.2041, -0.1377, -0.1768],
        [-0.1143,  0.4141,  0.1045, -0.4004, -0.5430],
        [-0.0388,  0.5742, -0.0079, -0.3848, -0.3984],
        [ 0.1689,  0.3770, -0.2354, -0.3672, -0.6523],
        [-0.1064,  0.5547, -0.0786, -0.3262,  0.1157],
        [ 0.1475,  0.2988, -0.0019, -0.2266, -0.0889],
        [-0.0947,  0.6055,  0.2969, -0.2178, -0.8789],
        [ 0.0640,  0.0947, -0.4883, -0.6797, -0.5039],
        [-0.0571,  0.7500,  0.1934, -0.2021, -0.5000],
        [ 0.0386,  0.0908,  0.3340, -0.3809, -0.7070],
        [-0.1348,  0.3613, -0.2754, -0.3867, -0.3008],
        [ 0.0060,  0.6680,  0.1289, -0.3047, -0.2119],
        [-0.2451,  0.7500,  0.0540, -0.0811, -0.0908],
        [ 0.1377,  0.7305, -0.0898, -0.4824, -0.1221],
        [-0.2246,  0.7930,  0.0302, -0.2891, -0.2451]], device='cuda:0',
       dtype=torch.bfloat16, grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None), SequenceClassifierOutput(loss=tensor(1.3423, device='cuda:0', grad_fn=<NllLossBackward0>), logits=tensor([[-0.0771,  0.2021,  0.2109, -0.4375, -0.7578],
        [-0.1328,  0.4531,  0.4648, -0.2539, -0.9805],
        [-0.1040,  0.5859,  0.0718, -0.2949, -0.1455],
        [-0.1289,  0.3379,  0.0132, -0.4512, -0.8633],
        [-0.5156,  0.4902,  0.0334, -0.5430, -0.8984],
        [ 0.0562,  0.3535, -0.0806, -0.4980, -0.7734],
        [-0.0698,  0.3105,  0.0422, -0.6484, -0.6953],
        [-0.2676,  0.7617,  0.0015, -0.4355, -0.2793],
        [ 0.1289,  0.6016, -0.3535, -0.5352, -0.3965],
        [ 0.0977,  0.3945, -0.2695, -0.5117, -0.6211],
        [ 0.0330,  0.0442, -0.1963, -0.4922, -0.7500],
        [-0.2178,  0.3223,  0.3711,  0.0233, -0.2598],
        [-0.1426,  0.4414,  0.2012, -0.2852, -0.3750],
        [-0.3398,  0.5586,  0.1602, -0.2832, -0.1602],
        [-0.4043,  0.4531,  0.6016, -0.4023, -0.4082],
        [ 0.0654,  0.7500,  0.2871, -0.5039, -0.1670]], device='cuda:0',
       dtype=torch.bfloat16, grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None), SequenceClassifierOutput(loss=tensor(1.2175, device='cuda:0', grad_fn=<NllLossBackward0>), logits=tensor([[ 0.0439,  0.4707, -0.1592, -0.4023, -0.2383],
        [-0.3516,  0.6641, -0.0515, -0.1045, -0.4336],
        [ 0.0120,  0.3965, -0.0996, -0.4629, -0.8164],
        [-0.4414,  0.5156, -0.1396, -0.4355, -0.5078],
        [-0.0923,  0.6797,  0.0640, -0.1875, -0.8516],
        [-0.0830,  0.4414, -0.2949, -0.2344, -0.0649],
        [-0.1562,  0.3242, -0.1729,  0.0723, -0.4980],
        [-0.2324,  0.3223,  0.3145, -0.3770, -0.6484],
        [-0.0874,  0.6562, -0.1162, -0.0923, -0.3789],
        [ 0.0747,  0.5039,  0.5078, -0.4688, -0.8242],
        [ 0.1064,  0.6484, -0.1729, -0.5742, -0.0718],
        [ 0.0933,  0.4863, -0.2227, -0.2949, -0.4512],
        [ 0.2002,  0.1689, -0.5234, -0.4238,  0.2930],
        [-0.0879,  0.9141, -0.0342, -0.2168, -0.2930],
        [ 0.0222,  0.5117,  0.0491,  0.1943, -0.4922],
        [ 0.0452,  0.7031,  0.0405, -0.3945, -0.3750]], device='cuda:0',
       dtype=torch.bfloat16, grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None), SequenceClassifierOutput(loss=tensor(1.1953, device='cuda:0', grad_fn=<NllLossBackward0>), logits=tensor([[ 0.0120,  0.6953, -0.1299, -0.4180, -0.1089],
        [ 0.0413,  0.4199,  0.3828, -0.2246, -0.8125],
        [ 0.0211,  0.6953, -0.3457, -0.4473, -0.6172],
        [-0.3945,  0.6758, -0.4590, -0.2734, -0.5703],
        [-0.4199,  0.5508,  0.3555, -0.6367, -0.9570],
        [-0.0317,  0.7461, -0.0114, -0.1777, -0.0928],
        [-0.0615,  0.5508, -0.3281, -0.1875, -0.3770],
        [-0.0366,  0.7188, -0.1162, -0.4707, -0.1108],
        [-0.0156,  0.5703,  0.4688, -0.3789, -0.6914],
        [-0.0371,  0.5508, -0.0132, -0.2422, -0.5312],
        [-0.0486,  0.3105, -0.1123,  0.0762, -0.4004],
        [ 0.1157,  0.5273, -0.2715, -0.1396, -0.3242],
        [-0.2227,  0.7188,  0.0540, -0.0208, -0.1836],
        [-0.1377,  0.2139,  0.0630, -0.0742, -0.2676],
        [ 0.1699,  0.7383, -0.1934, -0.1729, -0.2949],
        [ 0.0023,  0.5781,  0.2598, -0.0500, -0.7578]], device='cuda:0',
       dtype=torch.bfloat16, grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None), SequenceClassifierOutput(loss=tensor(1.0745, device='cuda:0', grad_fn=<NllLossBackward0>), logits=tensor([[ 0.0630,  0.6367, -0.0613, -0.5898, -0.0383],
        [ 0.0383,  0.0664,  0.4219, -0.0913, -0.7422],
        [ 0.0344,  0.4102,  0.0275, -0.2754, -0.4844],
        [-0.1230,  0.6758,  0.3145, -0.3262, -0.8750],
        [ 0.0933,  1.0156,  0.3086, -0.0674, -0.2969],
        [ 0.1396,  0.6250,  0.3398, -0.5156, -1.0234],
        [-0.1050,  0.3945,  0.1099, -0.3672, -0.5703],
        [-0.3008,  0.7695, -0.2910, -0.1709, -0.1982],
        [-0.0352,  0.5117, -0.0967, -0.0947, -0.3711],
        [-0.4199,  0.2969, -0.0023, -0.3848, -0.1787],
        [-0.1338,  0.4668, -0.2188, -0.1836, -0.6211],
        [-0.0403,  0.6797, -0.0708, -0.3145, -0.0082],
        [-0.2773,  0.4766, -0.1167, -0.2852, -0.5977],
        [-0.0615,  0.9102, -0.0500,  0.0361, -0.2168],
        [-0.3301,  0.5312,  0.0859, -0.4004, -0.4258],
        [-0.2480,  0.7656, -0.0444, -0.1416, -0.4199]], device='cuda:0',
       dtype=torch.bfloat16, grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None), SequenceClassifierOutput(loss=tensor(1.3540, device='cuda:0', grad_fn=<NllLossBackward0>), logits=tensor([[-0.3086,  0.4727,  0.3770, -0.5078, -0.9219],
        [-0.2500, -0.3750,  0.0100, -0.4316, -0.7188],
        [-0.3789,  0.5742,  0.0019, -0.4570, -1.0781],
        [-0.0522,  0.4746,  0.0835, -0.2393, -0.4863],
        [-0.0603,  0.1943,  0.0540, -0.4141, -0.5586],
        [-0.3477,  0.8828, -0.0591, -0.2100, -0.0913],
        [ 0.0505,  0.8750, -0.2217, -0.1885, -0.2158],
        [ 0.6641, -0.1118, -0.7812, -0.0352,  0.1055],
        [ 0.0981,  0.6992,  0.0596, -0.4785, -0.3848],
        [-0.1680,  0.1279,  0.3301,  0.0030, -0.5859],
        [ 0.2051,  0.5078,  0.0728, -0.0070, -0.3672],
        [-0.2910,  0.5000, -0.2188, -0.1309, -0.4492],
        [ 0.0295,  0.5508, -0.4043, -0.4492, -0.2812],
        [-0.2520,  0.1650, -0.2734,  0.0126, -0.4414],
        [-0.0583,  0.4531, -0.1699, -0.1084,  0.0923],
        [-0.1660,  0.4902, -0.0488, -0.0175, -0.4941]], device='cuda:0',
       dtype=torch.bfloat16, grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None), SequenceClassifierOutput(loss=tensor(1.1826, device='cuda:0', grad_fn=<NllLossBackward0>), logits=tensor([[-0.5000,  0.3926, -0.2246, -0.1855, -0.5820],
        [-0.4531,  0.5039,  0.1572, -0.3594, -0.4180],
        [-0.1680,  0.6016,  0.1406, -0.4160, -0.9219],
        [ 0.1011,  0.4199, -0.1992, -0.3457, -0.1836],
        [-0.1475,  0.5781, -0.2100, -0.4590, -0.1709],
        [ 0.0752,  0.4922, -0.1318, -0.3340, -0.4316],
        [-0.1147,  0.4238, -0.2012, -0.0149, -0.5312],
        [-0.2090,  0.2520, -0.2461, -0.4512, -0.1631],
        [ 0.2344,  0.6484, -0.1377, -0.4941, -0.1670],
        [ 0.1396,  0.4395,  0.4473, -0.3730, -0.5625],
        [-0.2598,  0.8086,  0.1279, -0.2969, -0.1191],
        [-0.0483,  0.5625,  0.2832, -0.1523, -0.4551],
        [ 0.2207,  0.5664, -0.1982,  0.0206, -0.1611],
        [-0.1426,  0.3516, -0.0127, -0.6289, -0.4180],
        [ 0.2539,  0.5078,  0.0713, -0.0874, -0.3750],
        [-0.3086,  0.4668, -0.3516, -0.2041, -0.3555]], device='cuda:0',
       dtype=torch.bfloat16, grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None), SequenceClassifierOutput(loss=tensor(1.1099, device='cuda:0', grad_fn=<NllLossBackward0>), logits=tensor([[-1.5332e-01,  5.7422e-01,  4.6082e-03, -6.4453e-01, -8.1641e-01],
        [-4.6680e-01,  4.9023e-01, -1.3281e-01, -3.8867e-01, -1.1484e+00],
        [ 8.3008e-02,  5.1953e-01,  1.6699e-01, -5.0391e-01, -3.8086e-01],
        [ 1.6992e-01,  4.5508e-01,  4.7852e-01, -1.5137e-02, -1.3086e-01],
        [-7.4219e-02,  6.7188e-01,  5.9814e-02,  9.7656e-03, -1.7676e-01],
        [-4.5898e-01,  3.9453e-01,  4.8828e-01, -9.1797e-02, -5.3125e-01],
        [ 3.8330e-02,  7.0703e-01, -7.8125e-02, -2.6562e-01, -3.1250e-01],
        [ 1.3657e-03,  4.9805e-01,  1.5527e-01, -2.6123e-02, -6.2500e-01],
        [-3.0859e-01,  8.9453e-01, -2.8516e-01, -3.4180e-01, -2.5000e-01],
        [ 1.2109e-01,  7.3438e-01,  9.9182e-04, -2.6758e-01, -3.1836e-01],
        [-3.5547e-01,  3.7500e-01,  2.5977e-01, -2.1484e-01, -4.3555e-01],
        [ 8.0078e-02,  8.6328e-01, -2.6562e-01, -1.4832e-02, -2.2461e-01],
        [-9.9609e-02,  7.7734e-01,  1.6895e-01, -3.8672e-01, -6.7969e-01],
        [ 1.7969e-01,  8.2812e-01,  2.5781e-01, -4.1211e-01, -3.1641e-01],
        [ 2.2461e-01,  3.9453e-01,  5.5078e-01, -2.7344e-01, -4.4922e-01],
        [-1.5918e-01,  3.8281e-01,  5.0537e-02, -5.2734e-01, -7.2656e-01]],
       device='cuda:0', dtype=torch.bfloat16, grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None), SequenceClassifierOutput(loss=tensor(1.1589, device='cuda:0', grad_fn=<NllLossBackward0>), logits=tensor([[-0.2969,  0.4238, -0.4414,  0.2041, -0.3613],
        [-0.3789,  0.4414,  0.0708, -0.5820, -0.5352],
        [-0.1875,  0.8398, -0.2432, -0.1406, -0.5352],
        [-0.3223,  0.5664, -0.3652, -0.3770, -0.6406],
        [ 0.1299,  0.5352, -0.2734, -0.6484, -0.4297],
        [ 0.0161,  0.6406, -0.0737, -0.3359, -0.1855],
        [-0.0087,  0.8984, -0.0544, -0.3047, -0.1074],
        [ 0.0139,  0.5000,  0.3516, -0.4160, -0.2539],
        [-0.2158,  0.7734,  0.0425, -0.3301, -0.3242],
        [-0.1836,  0.2852, -0.0344, -0.4316, -0.3281],
        [-0.1221,  0.4434,  0.2695, -0.5000, -0.7734],
        [-0.3418,  0.4980,  0.0204, -0.1504, -0.6484],
        [-0.0344,  0.5586, -0.0879, -0.0366, -0.6328],
        [-0.1396,  0.3906,  0.3770, -0.1201, -0.8516],
        [-0.0933,  0.3320, -0.5781,  0.0608, -0.7539],
        [-0.4473,  0.8398,  0.3535, -0.8633, -0.6484]], device='cuda:0',
       dtype=torch.bfloat16, grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None), SequenceClassifierOutput(loss=tensor(1.0874, device='cuda:0', grad_fn=<NllLossBackward0>), logits=tensor([[-0.1836,  0.7695,  0.1816, -0.4082, -0.4727],
        [-0.1621,  0.5039, -0.2109,  0.0542, -0.5547],
        [-0.0635,  0.6484, -0.3145, -0.4902, -0.0250],
        [-0.0664,  0.4707, -0.0508, -0.5000, -0.3789],
        [-0.2559,  0.5352, -0.0089, -0.2734, -0.5273],
        [-0.1221,  0.5781, -0.4219, -0.3652, -0.1777],
        [-0.0879,  0.2217, -0.4551, -0.4766, -0.5352],
        [-0.1104,  0.4805,  0.0591, -0.0344, -0.5742],
        [-0.1426,  0.5586,  0.2207, -0.1689, -0.5078],
        [ 0.0967,  0.6445,  0.0273, -0.2930, -0.5078],
        [-0.1396,  0.5820,  0.1016, -0.3555, -0.4590],
        [-0.1865,  0.7617, -0.2207, -0.2236, -0.3379],
        [ 0.2598,  0.6055,  0.1719, -0.7109,  0.0732],
        [ 0.0103,  0.6484, -0.3730, -0.4492, -0.2969],
        [-0.0510,  0.5312, -0.1416, -0.4219, -0.2637],
        [-0.1128,  0.3887, -0.0771, -0.5586, -0.5664]], device='cuda:0',
       dtype=torch.bfloat16, grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None), SequenceClassifierOutput(loss=tensor(1.2351, device='cuda:0', grad_fn=<NllLossBackward0>), logits=tensor([[-0.0952,  0.3379,  0.1064, -0.1953, -0.5469],
        [ 0.3184,  0.5039, -0.0059, -0.1875, -0.4531],
        [ 0.4707,  0.7031, -0.1611, -0.8516, -0.2812],
        [-0.3223,  0.2266,  0.3301, -0.5938, -0.7109],
        [ 0.1797,  0.2373, -0.6211, -0.5664,  0.2148],
        [-0.1387,  0.2969,  0.5586, -0.1826, -0.7930],
        [ 0.1021,  0.3242, -0.0835, -0.4922, -0.3438],
        [-0.0228,  0.3926, -0.0557, -0.5938, -0.4629],
        [-0.1855,  0.3184, -0.0898, -0.5938, -0.4375],
        [-0.1494,  0.4766,  0.0322, -0.0703, -0.3848],
        [-0.0547,  0.4980, -0.2773, -0.0898, -0.3184],
        [ 0.3125,  0.9375, -0.1611, -0.3809,  0.0508],
        [-0.6797,  0.7344, -0.0131, -0.2695, -0.6211],
        [-0.1436,  0.6797,  0.0688, -0.3457, -0.3691],
        [-0.4629,  0.6289,  0.1060, -0.0198, -0.4590],
        [ 0.1768,  0.3828, -0.0791, -0.3652, -0.2441]], device='cuda:0',
       dtype=torch.bfloat16, grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None), SequenceClassifierOutput(loss=tensor(1.1387, device='cuda:0', grad_fn=<NllLossBackward0>), logits=tensor([[-0.0928,  0.5352, -0.3887, -0.2988, -0.3184],
        [-0.0854,  0.7539,  0.0371, -0.5312, -0.1016],
        [-0.1582,  0.4961, -0.1299, -0.3516, -0.3027],
        [-0.4395,  0.4434, -0.0903, -0.6055, -0.4746],
        [ 0.0417,  0.5586, -0.2793, -0.2051, -0.3887],
        [ 0.1055,  0.6875, -0.3730, -0.4336, -0.2734],
        [ 0.2852,  0.6211,  0.0317, -0.2793, -0.4238],
        [ 0.0830,  0.6953,  0.0781, -0.1670, -0.4453],
        [-0.2031,  0.6016,  0.3066,  0.1318, -0.6406],
        [ 0.2012,  0.4570, -0.1147, -0.7891, -0.5625],
        [ 0.0977,  0.5117, -0.2637, -0.2090, -0.3086],
        [ 0.0781,  0.6562, -0.2031, -0.3574, -0.1289],
        [-0.0342,  0.3496, -0.3047, -0.3555, -0.1357],
        [-0.2715,  0.6406,  0.0228, -0.1670, -0.4062],
        [ 0.0583,  0.6406, -0.1895, -0.4316, -0.3477],
        [-0.3105,  0.6836, -0.0977, -0.2871, -0.3242]], device='cuda:0',
       dtype=torch.bfloat16, grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None), SequenceClassifierOutput(loss=tensor(1.2815, device='cuda:0', grad_fn=<NllLossBackward0>), logits=tensor([[-0.2051,  0.2949,  0.1328, -0.4277, -0.7188],
        [ 0.2012,  0.9180, -0.0024, -0.4141, -0.5859],
        [ 0.1504,  0.7656,  0.3203, -0.1240, -0.4844],
        [-0.0757,  0.6875, -0.0154, -0.0522, -0.3633],
        [-0.2324,  0.6445, -0.2295, -0.3027, -0.2637],
        [-0.1641,  0.8828,  0.0952, -0.5781, -0.4844],
        [ 0.0427,  0.4883,  0.0403, -0.5859, -0.6445],
        [-0.2158,  0.5977, -0.4785, -0.3906, -0.8164],
        [-0.3340,  0.2217, -0.0349, -0.8398, -0.8242],
        [-0.3672,  0.5742, -0.1387, -0.2275, -0.5078],
        [-0.1943,  0.3496,  0.2354, -0.1123,  0.0498],
        [-0.1455,  0.6992, -0.1226, -0.1738, -0.1621],
        [-0.2812,  0.5352, -0.3066, -0.2275, -0.2051],
        [-0.2256,  0.4316,  0.0559, -0.1934, -0.1348],
        [-0.0898,  0.4980, -0.2402, -0.2598, -0.2119],
        [ 0.1807,  0.6641,  0.1504, -0.2285, -0.4102]], device='cuda:0',
       dtype=torch.bfloat16, grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None), SequenceClassifierOutput(loss=tensor(1.2505, device='cuda:0', grad_fn=<NllLossBackward0>), logits=tensor([[ 0.2422,  0.7539,  0.0356, -0.1021, -0.0688],
        [ 0.0317,  0.6172, -0.1152, -0.2363, -0.4023],
        [ 0.1069,  0.5898,  0.1582, -0.2217, -0.2432],
        [-0.2451,  0.7188, -0.2285,  0.0405, -0.2676],
        [-0.0491,  0.9766, -0.2461, -0.5469, -0.2871],
        [ 0.0728,  0.6484, -0.4277, -0.5742, -0.3320],
        [ 0.2559,  1.0312,  0.5195, -0.4375,  0.0181],
        [-0.1196,  0.5586, -0.2637, -0.1235, -0.5664],
        [ 0.1826,  0.2266, -0.0918, -0.5586, -0.2539],
        [-0.3750,  0.6055, -0.4570, -0.3828,  0.1069],
        [-0.0967,  0.5000, -0.1602, -0.2832, -0.3691],
        [ 0.2373,  0.2129,  0.2100,  0.0278, -0.0732],
        [-0.2383,  0.6602, -0.0153, -0.1147, -0.1738],
        [ 0.0791,  0.6641,  0.0403, -0.5547, -0.5664],
        [ 0.0159,  0.7070, -0.3594, -0.3848, -0.3086],
        [-0.0013,  0.3613, -0.2598, -0.1914,  0.0364]], device='cuda:0',
       dtype=torch.bfloat16, grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None), SequenceClassifierOutput(loss=tensor(1.1838, device='cuda:0', grad_fn=<NllLossBackward0>), logits=tensor([[-4.6875e-02,  6.6406e-01, -4.0234e-01, -2.0996e-01, -2.9297e-01],
        [ 1.1133e-01,  6.0156e-01, -4.5654e-02, -3.6523e-01, -1.3184e-01],
        [-3.0078e-01,  8.5156e-01, -7.5684e-02, -1.9531e-01,  1.1169e-02],
        [-2.7539e-01,  2.5586e-01,  1.5234e-01, -5.5859e-01, -8.9844e-01],
        [-1.6699e-01,  5.0000e-01, -9.0820e-02, -3.6719e-01, -6.0938e-01],
        [-2.8906e-01,  5.3906e-01, -6.2988e-02, -3.0469e-01, -3.5938e-01],
        [-2.0752e-02,  3.6328e-01,  1.0742e-02, -2.9297e-01,  6.5430e-02],
        [-2.7539e-01,  2.5000e-01, -5.3711e-02, -6.7188e-01, -5.6250e-01],
        [ 2.2339e-02,  3.6914e-01, -5.7812e-01, -1.2793e-01, -4.5898e-01],
        [-3.7500e-01,  4.8828e-01, -1.5527e-01, -4.3555e-01, -1.4160e-01],
        [-3.8867e-01,  1.4844e-01, -1.0693e-01, -3.0078e-01, -4.5703e-01],
        [-3.9368e-03,  5.7812e-01,  5.3711e-03, -4.0820e-01, -5.1953e-01],
        [ 2.7084e-04,  5.5859e-01,  5.6885e-02,  1.5991e-02, -8.3984e-01],
        [-6.3477e-02,  7.4609e-01, -1.4941e-01, -4.1602e-01, -1.8066e-01],
        [ 7.6660e-02,  4.9609e-01, -4.9561e-02, -5.4932e-02, -3.1641e-01],
        [ 3.3447e-02,  3.0859e-01, -1.4941e-01, -7.0801e-02, -3.1055e-01]],
       device='cuda:0', dtype=torch.bfloat16, grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None), SequenceClassifierOutput(loss=tensor(1.2625, device='cuda:0', grad_fn=<NllLossBackward0>), logits=tensor([[-0.0903,  0.5508,  0.1836, -0.1050, -0.3672],
        [ 0.1582,  0.6914,  0.1758, -0.4805, -0.2432],
        [-0.1338,  0.5391, -0.1904, -0.1396, -0.1060],
        [-0.2988,  0.4199,  0.0126, -0.4023, -0.3574],
        [ 0.1006,  0.4668,  0.0359, -0.5391, -0.6641],
        [ 0.1279,  0.4922,  0.0300, -0.0221, -0.1201],
        [-0.2334,  0.8789, -0.2109, -0.1377, -0.1279],
        [ 0.1060,  0.1719,  0.3125, -0.6797, -0.8750],
        [-0.0082,  0.6523, -0.2197, -0.2520, -0.1377],
        [-0.5156,  0.6797,  0.0175, -0.4551, -0.4668],
        [ 0.1875,  0.3730,  0.3789, -0.0205, -0.8594],
        [ 0.0522,  0.5156, -0.1934, -0.0520, -0.2969],
        [-0.0664,  0.3379,  0.0284, -0.1167, -0.5195],
        [ 0.2207,  0.3945, -0.1699, -0.2871, -0.3828],
        [-0.2285,  0.8711, -0.2100, -0.5625, -0.0532],
        [-0.4277,  0.4492,  0.1738, -0.5391, -1.0312]], device='cuda:0',
       dtype=torch.bfloat16, grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None), SequenceClassifierOutput(loss=tensor(1.1780, device='cuda:0', grad_fn=<NllLossBackward0>), logits=tensor([[-3.2031e-01,  6.5625e-01, -2.3730e-01, -6.9336e-02, -3.9453e-01],
        [ 1.8457e-01,  6.3281e-01,  1.7944e-02, -2.9297e-01, -6.5430e-02],
        [-5.0659e-03,  7.8125e-01,  1.5527e-01, -3.7695e-01, -2.6562e-01],
        [-3.7891e-01,  9.3359e-01,  4.7656e-01, -2.2266e-01, -4.4727e-01],
        [-8.3008e-02,  8.7109e-01, -6.8848e-02,  1.3574e-01, -2.7734e-01],
        [-4.7852e-01,  3.3398e-01,  3.9551e-02, -2.9492e-01, -9.4922e-01],
        [ 2.0508e-01,  9.3750e-01,  1.1182e-01, -3.2031e-01, -1.1816e-01],
        [-3.0859e-01,  8.0859e-01, -1.8945e-01, -3.3984e-01, -7.7637e-02],
        [-3.5938e-01,  6.7578e-01, -3.1445e-01, -4.3945e-01, -3.5547e-01],
        [-7.7148e-02,  1.0000e+00,  2.1875e-01, -2.9102e-01, -3.0273e-01],
        [ 2.9492e-01,  5.1562e-01, -3.1250e-01, -3.6133e-01, -2.5195e-01],
        [ 2.8931e-02,  3.2031e-01, -5.0293e-02, -2.9688e-01, -5.0781e-01],
        [-4.2383e-01,  4.4141e-01,  3.9307e-02, -4.6875e-01, -6.1719e-01],
        [ 3.7598e-02,  4.5312e-01,  9.9121e-02, -3.4961e-01, -6.5234e-01],
        [-1.6504e-01,  6.8359e-01,  6.2012e-02, -3.2812e-01, -4.4336e-01],
        [-2.2461e-02,  8.0078e-01, -2.6703e-04, -1.0547e-01, -4.1602e-01]],
       device='cuda:0', dtype=torch.bfloat16, grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None), SequenceClassifierOutput(loss=tensor(1.1279, device='cuda:0', grad_fn=<NllLossBackward0>), logits=tensor([[ 0.2812,  0.5859,  0.0845, -0.4648, -0.0203],
        [-0.5117,  0.7148, -0.0491, -0.4004, -0.3496],
        [ 0.0571,  0.4648,  0.3828, -0.0771, -0.6875],
        [-0.4297,  0.7617,  0.3145, -0.5352, -0.4902],
        [ 0.4941,  0.0698, -0.5703,  0.0618,  0.1514],
        [ 0.0396,  0.3887,  0.0153, -0.1562, -0.7188],
        [ 0.0058,  0.9727, -0.0601, -0.1152, -0.5078],
        [-0.2090,  0.3574,  0.1426, -0.2949, -0.6680],
        [-0.1147,  0.5859, -0.0679, -0.4824, -0.2324],
        [-0.2275,  0.2695, -0.0515, -0.3105, -0.4785],
        [-0.1758,  0.3164,  0.1245, -0.0977, -1.1250],
        [-0.1836,  0.7188,  0.2520, -0.7773, -0.4414],
        [ 0.0066,  0.6602, -0.2451, -0.2441, -0.2988],
        [ 0.0222,  0.5703,  0.2676, -0.3652, -0.0190],
        [-0.0845,  0.8438, -0.0981, -0.3008, -0.1670],
        [-0.0177,  0.6758,  0.0200, -0.1328, -0.6133]], device='cuda:0',
       dtype=torch.bfloat16, grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None), SequenceClassifierOutput(loss=tensor(1.2239, device='cuda:0', grad_fn=<NllLossBackward0>), logits=tensor([[-2.0312e-01,  1.1016e+00, -1.9434e-01, -1.8457e-01, -6.2500e-01],
        [-4.2969e-02,  8.3594e-01,  3.5547e-01, -1.4941e-01, -8.2422e-01],
        [-4.3750e-01,  7.1094e-01,  3.2617e-01, -4.0625e-01, -8.1250e-01],
        [-2.4805e-01,  2.5195e-01, -7.4005e-04, -6.2109e-01, -9.4531e-01],
        [ 5.0049e-02,  7.6562e-01,  1.0693e-01, -3.0273e-01, -7.1777e-02],
        [-3.1445e-01,  5.1953e-01, -2.9053e-02, -3.1738e-02, -7.3828e-01],
        [-4.7852e-02,  5.8984e-01,  1.1670e-01, -1.1865e-01, -1.6211e-01],
        [-1.0791e-01,  8.3203e-01, -2.4316e-01, -1.0986e-01, -3.4766e-01],
        [-2.1606e-02,  7.9688e-01, -6.8359e-03, -3.9844e-01, -3.9062e-01],
        [-4.0430e-01,  3.2422e-01,  1.5039e-01, -3.5547e-01, -5.4297e-01],
        [-1.6602e-01,  9.2188e-01, -3.0273e-01,  3.1738e-02, -5.8105e-02],
        [-1.0840e-01,  4.9609e-01,  1.5991e-02, -4.6680e-01, -3.6523e-01],
        [-6.6895e-02,  8.5156e-01,  3.4912e-02, -1.5527e-01, -4.2969e-01],
        [-1.3000e-02,  6.9531e-01,  1.7773e-01, -3.0664e-01, -4.9609e-01],
        [-1.1523e-01,  8.8281e-01,  1.8750e-01, -2.2949e-01, -3.7500e-01],
        [-4.3750e-01,  6.8750e-01, -2.8320e-01, -5.7812e-01,  4.7119e-02]],
       device='cuda:0', dtype=torch.bfloat16, grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None), SequenceClassifierOutput(loss=tensor(1.4297, device='cuda:0', grad_fn=<NllLossBackward0>), logits=tensor([[ 4.2725e-02,  6.9141e-01, -1.3379e-01, -1.0693e-01, -1.2451e-01],
        [-1.6797e-01,  2.5977e-01, -1.7676e-01, -3.9453e-01, -3.4570e-01],
        [ 2.3242e-01,  4.2188e-01,  4.5508e-01, -2.9688e-01, -4.0527e-02],
        [-5.5176e-02,  5.7422e-01,  3.4766e-01, -1.8848e-01, -1.1250e+00],
        [-1.6602e-01,  7.8125e-01,  1.1084e-01, -2.4805e-01, -3.8477e-01],
        [-1.2012e-01,  5.7422e-01, -2.1387e-01, -3.3594e-01, -1.6699e-01],
        [-4.8438e-01,  6.4453e-01, -1.8555e-01, -2.4121e-01, -2.4121e-01],
        [ 2.0020e-02,  5.5078e-01,  9.2773e-02, -2.7930e-01, -3.6719e-01],
        [ 1.3184e-01,  5.6250e-01,  1.2598e-01, -1.4648e-01, -8.3542e-04],
        [-5.5420e-02,  6.1328e-01, -1.1841e-02, -1.3379e-01, -3.1055e-01],
        [ 8.5449e-02,  5.0391e-01, -5.3906e-01, -3.3984e-01, -1.5332e-01],
        [ 3.0664e-01,  3.1445e-01, -2.8711e-01, -4.9219e-01, -2.0312e-01],
        [-9.2773e-02,  9.3359e-01, -1.9434e-01, -5.3516e-01, -1.8945e-01],
        [-3.3594e-01,  2.3047e-01,  4.5312e-01, -2.6562e-01, -5.5859e-01],
        [ 3.7500e-01,  4.9023e-01, -8.6212e-04, -1.3281e-01,  3.5889e-02],
        [-1.2256e-01,  5.8203e-01, -2.5195e-01, -1.1279e-01, -6.6016e-01]],
       device='cuda:0', dtype=torch.bfloat16, grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None), SequenceClassifierOutput(loss=tensor(1.2334, device='cuda:0', grad_fn=<NllLossBackward0>), logits=tensor([[-0.0518,  0.2871, -0.2305, -0.5156,  0.1279],
        [-0.0101,  0.3320,  0.1250, -0.0432, -0.5391],
        [ 0.0957,  0.5938,  0.2656, -0.2891, -0.4414],
        [ 0.1494,  1.0547,  0.0123, -0.6719, -0.3906],
        [ 0.1006,  0.8867, -0.1582, -0.5430, -0.3809],
        [ 0.3711,  0.4824, -0.3652, -0.7695,  0.0315],
        [ 0.4082,  0.8398, -0.2676, -0.1943, -0.3867],
        [ 0.1147,  0.5742,  0.3789, -0.2080, -0.8086],
        [-0.1885,  0.5508, -0.2246, -0.1943, -0.5859],
        [-0.1387,  0.6914, -0.3418, -0.7500, -0.5391],
        [ 0.2158,  0.6719, -0.3730, -0.6523, -0.5273],
        [-0.0028,  0.5273,  0.1797, -0.3887, -0.9570],
        [-0.0703,  0.6250,  0.0864, -0.1514, -0.7461],
        [ 0.0469,  0.4590,  0.2100, -0.6289, -0.5664],
        [-0.0608,  0.7930,  0.1147, -0.2539, -0.7930],
        [-0.1768,  0.3711, -0.3398, -0.2266, -0.4473]], device='cuda:0',
       dtype=torch.bfloat16, grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None), SequenceClassifierOutput(loss=tensor(1.1499, device='cuda:0', grad_fn=<NllLossBackward0>), logits=tensor([[ 0.1680,  0.4004, -0.2393, -0.1875, -0.5430],
        [-0.1738,  0.5117,  0.0457, -0.6016, -1.2500],
        [-0.0515,  0.7227, -0.1631, -0.1719, -0.2109],
        [-0.1108,  0.6016, -0.0420, -0.4590, -0.8398],
        [ 0.1807,  0.5820, -0.1069, -0.2754, -0.4473],
        [-0.1357,  0.7656,  0.0593, -0.3906, -0.5898],
        [ 0.1797,  0.7500, -0.2676, -0.5898, -0.2578],
        [ 0.1533,  0.3926, -0.3691, -0.4863, -0.1816],
        [-0.2578,  0.6875,  0.4785, -0.3789, -0.5391],
        [-0.3320,  0.0972, -0.2207, -0.1475, -0.4004],
        [-0.1924,  0.4453, -0.1318, -0.6016, -0.5078],
        [-0.2969,  0.4004,  0.2578, -0.0059, -0.4102],
        [ 0.3203,  0.3926,  0.0505, -0.5195, -0.3652],
        [-0.1738,  0.5156, -0.1650, -0.3848, -0.0991],
        [-0.1816,  0.5469, -0.0432, -0.2812, -0.3008],
        [ 0.1416,  0.6680, -0.1768, -0.3125, -0.5547]], device='cuda:0',
       dtype=torch.bfloat16, grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None), SequenceClassifierOutput(loss=tensor(1.2788, device='cuda:0', grad_fn=<NllLossBackward0>), logits=tensor([[-0.1670,  0.5547, -0.0640, -0.6250, -0.2812],
        [-0.2773,  0.6641, -0.4160, -0.3418, -0.3613],
        [-0.0256,  0.4609, -0.1924, -0.4629, -0.3887],
        [-0.3711,  0.7344,  0.0894, -0.2773, -0.1719],
        [ 0.1328,  0.4336, -0.5703, -0.8867, -0.0098],
        [-0.2422,  0.4219, -0.2676, -0.1328, -0.2070],
        [-0.1377,  0.4180, -0.1387, -0.0605, -0.5078],
        [-0.0659,  0.6289, -0.0796, -0.4043, -0.1250],
        [-0.0806,  0.6094,  0.0096, -0.0659, -0.3789],
        [-0.2432,  0.4062, -0.0094, -0.3965, -0.8398],
        [ 0.0659,  0.4805, -0.4551,  0.0854, -0.3750],
        [ 0.0664,  0.5547, -0.1865, -0.4863, -0.6680],
        [-0.2656,  0.2383,  0.2002, -0.1729, -0.5000],
        [ 0.0126,  0.9805,  0.0454, -0.2324,  0.2891],
        [-0.1250,  0.6484, -0.3652, -0.5547, -0.0143],
        [ 0.2109,  0.2559,  0.2207, -0.4141, -0.3359]], device='cuda:0',
       dtype=torch.bfloat16, grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None), SequenceClassifierOutput(loss=tensor(1.1650, device='cuda:0', grad_fn=<NllLossBackward0>), logits=tensor([[-0.0500,  0.7461,  0.1895, -0.4551, -0.7852],
        [-0.0299,  0.5938,  0.0142, -0.6133, -0.3262],
        [-0.2910,  0.2178,  0.1934, -0.3184, -0.5430],
        [-0.2520,  0.7617, -0.2041, -0.2041, -0.3223],
        [ 0.1138,  0.9414,  0.4121, -0.1943, -0.0649],
        [-0.2559,  0.9141, -0.1191, -0.1543, -0.3945],
        [-0.4434,  0.9141, -0.0167, -0.4121, -0.8320],
        [-0.1201,  0.8359, -0.3340, -0.3945, -0.1924],
        [ 0.1416,  0.3398,  0.5312,  0.3984, -0.6094],
        [ 0.0236,  0.9258,  0.2148, -0.3750, -0.0811],
        [-0.2676,  0.5781, -0.1289, -0.3750, -0.4316],
        [ 0.0864,  0.7383, -0.3516, -0.2695, -0.3223],
        [-0.2578,  0.4043, -0.5859, -0.3125, -0.6953],
        [-0.0908,  0.3574, -0.3125, -0.1670, -0.1216],
        [ 0.1206,  0.8281, -0.0928, -0.3555, -0.2334],
        [-0.0386,  0.8438, -0.1426, -0.3047, -0.4258]], device='cuda:0',
       dtype=torch.bfloat16, grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None), SequenceClassifierOutput(loss=tensor(1.1091, device='cuda:0', grad_fn=<NllLossBackward0>), logits=tensor([[ 0.0413,  0.7734, -0.0083, -0.6484, -0.5820],
        [-0.1680,  0.8633, -0.2061, -0.8047, -0.7461],
        [-0.1245,  0.9141,  0.1357, -0.6172, -0.1934],
        [ 0.1094,  0.9922, -0.3828, -0.2275, -0.2969],
        [-0.1709,  0.6445, -0.0928, -0.4316, -0.7852],
        [ 0.1943,  0.5938, -0.3340, -0.4629, -0.4609],
        [-0.3652,  0.4453,  0.1201, -0.3340, -0.7617],
        [-0.2852,  0.9062, -0.1904, -0.2520, -0.2734],
        [ 0.1719,  0.8750,  0.0308, -0.2334, -0.1562],
        [-0.1689,  0.6289, -0.1641, -0.4180, -0.3242],
        [-0.0732,  0.9453, -0.0116, -0.2695, -0.4922],
        [-0.2598,  0.9727, -0.0732, -0.3438, -0.1934],
        [-0.4297,  0.5078,  0.0352, -0.4512, -0.6758],
        [-0.2217,  0.5000, -0.1846, -0.1138, -0.5078],
        [-0.3203,  0.1240,  0.2080, -0.2324, -0.8125],
        [-0.2637,  0.5859,  0.3125, -0.4258, -0.4102]], device='cuda:0',
       dtype=torch.bfloat16, grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None), SequenceClassifierOutput(loss=tensor(1.2039, device='cuda:0', grad_fn=<NllLossBackward0>), logits=tensor([[-0.3789,  0.4453, -0.0361, -0.4668, -0.1021],
        [-0.3496,  0.6914,  0.2295, -0.4336, -0.6602],
        [-0.0118,  0.6523,  0.3516, -0.3555, -0.8320],
        [-0.1904,  1.0000, -0.4199, -0.1611, -0.2051],
        [-0.1523,  0.8594,  0.1709, -0.6641, -0.6484],
        [-0.1318,  0.8008,  0.1436, -0.1475, -0.4043],
        [ 0.0137,  0.4023, -0.0532, -0.6016, -0.4336],
        [-0.0075,  0.7070, -0.2070, -0.2061, -0.2949],
        [ 0.2471,  0.4238, -0.2559, -0.3848, -0.3828],
        [-0.3145,  0.2451, -0.3262, -0.0211, -0.5469],
        [-0.2969,  0.7852, -0.1523, -0.1680, -0.4883],
        [-0.3281,  1.0781,  0.0223, -0.4297, -0.0337],
        [-0.1758,  0.8750, -0.0547, -0.2715, -0.4082],
        [-0.1973,  0.2373,  0.1787, -0.6211, -0.5430],
        [ 0.1826,  0.8672, -0.1914, -0.2715, -0.1426],
        [ 0.0771,  0.7656, -0.0026, -0.3164, -0.5430]], device='cuda:0',
       dtype=torch.bfloat16, grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None), SequenceClassifierOutput(loss=tensor(1.2344, device='cuda:0', grad_fn=<NllLossBackward0>), logits=tensor([[ 0.1816, -0.0459, -0.4062,  0.1494,  0.5234],
        [-0.3398,  0.7227,  0.1123, -0.4023, -0.3320],
        [ 0.0067,  0.7734, -0.2236, -0.4141, -0.3008],
        [-0.3008,  0.3574, -0.3145, -0.5859, -0.9062],
        [-0.0649,  0.6172, -0.2383, -0.1230, -0.7383],
        [-0.0400,  0.7812,  0.1484, -0.3008, -0.7188],
        [ 0.0625,  0.7969,  0.0640, -0.6094, -0.0820],
        [ 0.1128,  0.6445,  0.2891, -0.3340, -0.7695],
        [-0.2119,  0.6875, -0.1729, -0.4082, -0.5352],
        [ 0.2910, -0.1260, -0.4277, -0.6016,  0.2773],
        [-0.1719,  0.6172, -0.1406, -0.4395, -0.2949],
        [ 0.0033,  0.6641, -0.1094, -0.5391, -0.4102],
        [-0.0123,  0.7188, -0.2285, -0.3633, -0.4980],
        [-0.3809, -0.1099,  0.1030, -0.2812, -0.6875],
        [-0.3809,  0.9414,  0.0366, -0.1855, -0.5508],
        [ 0.1147,  0.6719,  0.1748, -0.3965, -0.1523]], device='cuda:0',
       dtype=torch.bfloat16, grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None), SequenceClassifierOutput(loss=tensor(1.3206, device='cuda:0', grad_fn=<NllLossBackward0>), logits=tensor([[ 0.3379,  0.5430,  0.2236,  0.0835, -0.4766],
        [-0.1797,  0.4492, -0.4043, -0.3535, -0.6094],
        [ 0.1553,  0.6133, -0.2227, -0.3652, -0.6836],
        [-0.1621,  0.5664, -0.2441, -0.4160, -0.9961],
        [ 0.1318,  0.5742, -0.1514, -0.2070, -0.5352],
        [-0.1660,  0.6172, -0.0293, -0.1514, -0.4316],
        [ 0.2314,  0.6992, -0.0476, -0.2598, -0.7695],
        [ 0.0742,  0.6797, -0.1260, -0.3242, -0.3672],
        [-0.0211,  0.3555, -0.0080, -0.4453, -0.4668],
        [-0.3027,  0.5000, -0.2236, -0.0298, -0.5508],
        [ 0.1250,  0.3926, -0.6367, -0.6016,  0.2324],
        [-0.1670,  0.4629, -0.0271, -0.1250, -0.5898],
        [-0.3633,  0.8164,  0.4629, -0.3594, -0.6914],
        [-0.1504,  0.7656,  0.2031, -0.3242, -0.1855],
        [-0.1797,  0.6328, -0.1279, -0.2949, -0.3223],
        [-0.1191,  0.8281, -0.1465, -0.6953, -0.1289]], device='cuda:0',
       dtype=torch.bfloat16, grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None), SequenceClassifierOutput(loss=tensor(1.1062, device='cuda:0', grad_fn=<NllLossBackward0>), logits=tensor([[-0.1396,  0.7695, -0.4395, -0.5664, -0.3867],
        [-0.1260,  0.7305, -0.2637, -0.2656, -0.7422],
        [ 0.0305,  0.7773,  0.1816, -0.4941, -0.1099],
        [-0.1689,  0.9844,  0.0786, -0.2949, -0.5078],
        [ 0.3984,  0.7422, -0.0359, -0.5117, -0.4473],
        [-0.3379,  0.4727,  0.4160, -0.3047, -0.4121],
        [-0.0221,  0.7031,  0.0366, -0.4551, -0.1562],
        [-0.1904,  0.4707,  0.1670, -0.0574, -0.5391],
        [-0.0132,  0.7773,  0.2656, -0.3418, -0.4980],
        [-0.0312,  0.4570,  0.0664, -0.3887, -0.6523],
        [-0.1030,  0.7539, -0.0674, -0.2432, -0.4629],
        [-0.0664,  0.7617, -0.0547, -0.3789, -0.4238],
        [-0.0879,  0.4863,  0.1709, -0.5938, -0.7812],
        [ 0.4121,  0.6914,  0.2422, -0.2734, -0.4395],
        [-0.3398,  0.0466, -0.3516, -0.4980, -0.2354],
        [-0.3262,  0.8203, -0.0554, -0.1729, -0.4023]], device='cuda:0',
       dtype=torch.bfloat16, grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None), SequenceClassifierOutput(loss=tensor(1.0872, device='cuda:0', grad_fn=<NllLossBackward0>), logits=tensor([[-0.0376,  0.6016,  0.0275, -0.5273, -0.4160],
        [ 0.2422,  0.7539, -0.0080, -0.4297, -0.7031],
        [ 0.1582,  0.6562,  0.2852, -0.0066, -0.4551],
        [-0.0747,  0.5078,  0.2266, -0.3516, -0.8828],
        [-0.1729,  0.5391, -0.4062, -0.4531, -0.2598],
        [-0.1367,  0.6055,  0.0105, -0.6875, -0.9688],
        [-0.1050,  0.5938, -0.1787, -0.3145, -0.5391],
        [-0.0820,  0.8086, -0.0107, -0.2617, -0.0275],
        [ 0.0312,  0.4297,  0.1235, -0.7500, -0.5859],
        [-0.4590,  0.6562, -0.1377, -0.1738, -0.5234],
        [-0.1006,  0.6289,  0.1436, -0.2070, -0.7422],
        [-0.0045,  1.0391, -0.0850, -0.0469, -0.1553],
        [-0.2373,  0.8203, -0.1953, -0.3906, -0.3418],
        [-0.1128,  0.5820, -0.4238, -0.3086, -0.6602],
        [ 0.1543,  0.9023, -0.3750, -0.3906, -0.5273],
        [ 0.3789,  0.0918, -0.4043, -0.1060, -0.4434]], device='cuda:0',
       dtype=torch.bfloat16, grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None), SequenceClassifierOutput(loss=tensor(1.0796, device='cuda:0', grad_fn=<NllLossBackward0>), logits=tensor([[-0.3066,  0.5664,  0.0554, -0.2441, -0.6914],
        [-0.5234,  0.7969,  0.0250, -0.2500, -0.7852],
        [-0.0115,  0.7617,  0.1406, -0.1357, -0.4629],
        [ 0.3301,  0.6016, -0.2930, -0.2432, -0.3184],
        [-0.1104,  0.5703, -0.0405, -0.3281, -0.7383],
        [ 0.1318,  0.4199, -0.4648, -0.2051, -0.2832],
        [-0.1030,  0.9492, -0.2402, -0.4473, -0.2471],
        [-0.1699,  0.5234, -0.4062, -0.0928, -0.2041],
        [-0.4570,  0.8086,  0.0354, -0.0466, -0.4102],
        [-0.2793,  0.6641, -0.3613, -0.4160, -0.4141],
        [-0.1226,  0.6133, -0.0928, -0.4512, -0.2832],
        [-0.2480,  0.8242,  0.3887, -0.1113, -0.3418],
        [-0.3281,  0.6797,  0.1973, -0.1650, -0.4688],
        [-0.2852,  0.6016, -0.1484, -0.5312, -0.4512],
        [-0.4395,  0.3691,  0.0315, -0.4531, -0.9297],
        [-0.1953,  0.2441,  0.0825, -0.1001, -0.6562]], device='cuda:0',
       dtype=torch.bfloat16, grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None), SequenceClassifierOutput(loss=tensor(1.1448, device='cuda:0', grad_fn=<NllLossBackward0>), logits=tensor([[ 0.0042,  0.7305, -0.1719, -0.3711, -0.2695],
        [ 0.0869,  0.4844, -0.2148, -0.1865, -0.1719],
        [ 0.0195,  0.6133, -0.1680, -0.5820, -0.3613],
        [-0.3652,  0.5234, -0.3145, -0.0688, -0.3496],
        [-0.0510,  0.8516, -0.2949, -0.6016, -0.3984],
        [ 0.0972,  0.7383, -0.1553, -0.3945, -0.3750],
        [-0.1875,  0.3086,  0.1963, -0.3887, -0.7617],
        [-0.1475,  0.4590, -0.5039, -0.0237, -0.2246],
        [-0.1582,  0.6758,  0.1240, -0.2402, -0.9375],
        [-0.1670,  0.7070, -0.2354, -0.2734, -0.2871],
        [-0.1992,  0.5273,  0.0068, -0.5508, -0.4629],
        [-0.1123,  0.4434, -0.4336, -0.5078, -0.5859],
        [-0.4414,  0.6523, -0.2354, -0.1079, -0.5938],
        [-0.0718,  0.4961, -0.1406, -0.5781, -0.5078],
        [ 0.1895,  0.9883, -0.3047, -0.2773, -0.3887],
        [-0.4277,  0.7383,  0.0486, -0.3750, -0.3887]], device='cuda:0',
       dtype=torch.bfloat16, grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None), SequenceClassifierOutput(loss=tensor(1.0901, device='cuda:0', grad_fn=<NllLossBackward0>), logits=tensor([[-3.9795e-02,  6.2500e-01, -8.0566e-03, -5.6250e-01, -4.1797e-01],
        [-6.5430e-02,  7.7734e-01, -4.1211e-01,  6.7444e-03, -1.1475e-01],
        [ 1.0254e-02,  4.9805e-01, -3.6523e-01, -2.9688e-01, -5.1172e-01],
        [ 1.6016e-01,  6.8750e-01, -7.9590e-02, -5.2344e-01, -1.2891e-01],
        [ 8.8867e-02,  3.8477e-01, -5.0293e-02, -2.5586e-01, -2.5586e-01],
        [-2.3730e-01,  4.4531e-01,  1.0449e-01, -2.9688e-01, -7.5781e-01],
        [ 7.4219e-02,  8.1641e-01, -5.9082e-02, -4.1406e-01, -4.2773e-01],
        [-2.1484e-01,  4.1602e-01,  1.9043e-02, -6.6797e-01, -7.3047e-01],
        [-3.5889e-02,  7.4609e-01, -2.8198e-02, -3.7695e-01, -1.4258e-01],
        [-1.5625e-01,  5.4297e-01,  6.9824e-02, -4.3945e-01, -8.6328e-01],
        [ 3.2715e-02,  6.3672e-01, -4.1016e-01, -3.9453e-01, -4.6484e-01],
        [-1.1328e-01,  4.9414e-01,  4.5898e-02, -3.6133e-01, -5.5469e-01],
        [ 9.9609e-02,  6.3672e-01,  1.9531e-01, -3.0469e-01, -3.8281e-01],
        [-4.8633e-01,  5.8594e-01,  2.6703e-04, -1.7578e-01, -6.1719e-01],
        [ 1.9238e-01,  8.1641e-01,  5.6641e-02, -3.1445e-01, -5.3125e-01],
        [ 4.8438e-01,  8.5449e-02, -8.2031e-01, -4.5703e-01,  3.1055e-01]],
       device='cuda:0', dtype=torch.bfloat16, grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None), SequenceClassifierOutput(loss=tensor(1.0801, device='cuda:0', grad_fn=<NllLossBackward0>), logits=tensor([[ 0.0996,  0.5859, -0.3418, -0.5469, -0.3398],
        [ 0.1484,  0.6445, -0.0898, -0.1475, -0.4102],
        [ 0.0214,  0.6367,  0.0369, -0.2432, -0.5117],
        [ 0.0430,  0.6992, -0.2852, -0.2969, -0.3320],
        [ 0.2988,  0.7109,  0.0405, -0.4316, -0.2812],
        [-0.0825,  0.4668, -0.1787, -0.5039, -0.5273],
        [-0.1895,  0.7852, -0.0791, -0.2734, -0.3203],
        [ 0.0302,  0.6836, -0.1206, -0.3555, -0.2451],
        [-0.3164,  0.6797, -0.2334, -0.3125, -0.4668],
        [-0.2031,  0.9336, -0.0178, -0.1025, -0.4590],
        [ 0.0618,  0.7188, -0.1279, -0.5859, -0.4590],
        [-0.4863,  0.5469, -0.0996, -0.3984, -0.2949],
        [-0.1060,  0.7539, -0.1729, -0.1816, -0.2412],
        [-0.0806,  0.4922, -0.0732, -0.5117, -0.4824],
        [ 0.2969,  0.7383,  0.0194, -0.4551, -0.4941],
        [-0.3828,  0.5938,  0.2061, -0.3262, -0.4980]], device='cuda:0',
       dtype=torch.bfloat16, grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None), SequenceClassifierOutput(loss=tensor(1.1003, device='cuda:0', grad_fn=<NllLossBackward0>), logits=tensor([[ 0.1143,  0.9102, -0.2422, -0.3008, -0.0835],
        [ 0.1973,  0.7344, -0.0884, -0.3164, -0.3008],
        [ 0.4004,  0.7734, -0.3730, -0.6406, -0.4688],
        [-0.1177,  0.9141, -0.1797, -0.1953, -0.4883],
        [-0.3262,  0.6914, -0.2949, -0.4512, -0.1426],
        [-0.3867,  0.5234, -0.1475,  0.1523, -0.5820],
        [-0.5586,  1.1406, -0.2266, -0.3984, -0.4629],
        [-0.2314,  1.0391, -0.0698, -0.4961, -0.4629],
        [-0.0569,  0.4746, -0.2002, -0.3125, -0.0801],
        [-0.1963,  0.3652, -0.2656, -0.3418, -0.6719],
        [ 0.2441,  0.6953,  0.1455, -0.2070, -0.3340],
        [ 0.1226,  0.8516, -0.1729, -0.3398, -0.3496],
        [ 0.2002,  0.7539,  0.1787, -0.3457, -0.4492],
        [ 0.2832,  0.8672, -0.0566, -0.2793, -0.4004],
        [ 0.0129,  0.6562,  0.0164, -0.2891, -0.4961],
        [-0.1680,  0.8320,  0.0957, -0.3984, -0.2715]], device='cuda:0',
       dtype=torch.bfloat16, grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None), SequenceClassifierOutput(loss=tensor(1.2295, device='cuda:0', grad_fn=<NllLossBackward0>), logits=tensor([[-0.0231,  0.4941, -0.1260, -0.6289, -0.2354],
        [ 0.1279,  0.7891, -0.4062, -0.6406, -0.2617],
        [ 0.0148,  0.8906, -0.1982, -0.3535, -0.5859],
        [-0.0659,  0.8398,  0.0952, -0.1543, -0.2061],
        [-0.0265,  0.7734, -0.3223, -0.2002, -0.1992],
        [-0.3496,  1.0156, -0.1885, -0.2988, -0.3887],
        [-0.2119,  0.6445, -0.3262, -0.2432, -0.4492],
        [-0.2520,  0.7227, -0.0376, -0.3379, -0.2480],
        [-0.0615,  0.6055, -0.1387, -0.3301, -0.3184],
        [-0.0288,  0.4961,  0.1289, -0.2773, -0.0154],
        [-0.0996,  0.7070, -0.2676, -0.7734, -0.3047],
        [-0.2080,  0.2871,  0.0684, -0.2871, -0.1279],
        [-0.0454,  0.7891,  0.0884, -0.1250, -0.2324],
        [-0.1680,  0.7500,  0.0728, -0.3066, -0.6406],
        [-0.1836,  0.5547,  0.1641, -0.3340, -0.3145],
        [-0.2832,  0.5859,  0.0593, -0.0620, -0.4727]], device='cuda:0',
       dtype=torch.bfloat16, grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None), SequenceClassifierOutput(loss=tensor(1.2361, device='cuda:0', grad_fn=<NllLossBackward0>), logits=tensor([[ 7.1777e-02,  5.9375e-01, -6.4941e-02, -5.2734e-01, -6.7969e-01],
        [-9.6680e-02,  7.0312e-01, -2.5781e-01, -1.2988e-01, -1.8750e-01],
        [-1.7188e-01,  5.2734e-01, -3.9795e-02, -4.5898e-01, -3.4375e-01],
        [-1.2598e-01,  7.9688e-01, -4.4250e-03, -4.9805e-01, -6.0547e-01],
        [-1.3867e-01,  8.2422e-01,  5.3955e-02, -4.1406e-01, -4.4141e-01],
        [ 1.0498e-01,  7.6953e-01,  6.3965e-02, -2.6367e-01, -3.2422e-01],
        [ 4.8828e-02,  8.0078e-01, -4.4922e-02, -1.2793e-01, -2.6758e-01],
        [-1.3477e-01,  6.9141e-01,  1.0791e-01, -4.9219e-01, -2.5586e-01],
        [-8.6670e-03,  4.4922e-01, -1.7383e-01, -3.5938e-01, -2.3340e-01],
        [-2.1553e-04,  1.7773e-01,  2.8320e-01, -1.2207e-01, -8.1250e-01],
        [ 1.2061e-01,  6.9922e-01, -4.0039e-01, -5.6250e-01, -1.4453e-01],
        [ 7.4707e-02,  4.7070e-01, -4.6875e-01, -4.2188e-01, -4.2969e-01],
        [ 4.5654e-02,  9.5312e-01, -6.2988e-02, -5.3906e-01, -1.7285e-01],
        [ 2.3730e-01,  4.9609e-01, -9.2285e-02, -1.7480e-01, -1.9531e-01],
        [ 2.0264e-02,  9.7266e-01,  5.9509e-03,  1.1597e-02, -2.9297e-01],
        [-5.1562e-01,  9.3750e-01, -2.6172e-01, -1.7480e-01, -3.9062e-01]],
       device='cuda:0', dtype=torch.bfloat16, grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None), SequenceClassifierOutput(loss=tensor(1.0918, device='cuda:0', grad_fn=<NllLossBackward0>), logits=tensor([[-0.2773,  0.7539, -0.1602, -0.1846, -0.2715],
        [-0.2910,  0.5977,  0.1611, -0.4473, -0.5938],
        [ 0.0084,  0.8438,  0.2305, -0.3906, -0.2119],
        [-0.1133,  0.9844,  0.0684, -0.5352, -0.0957],
        [-0.2715,  0.8711,  0.0076, -0.6484, -0.2734],
        [-0.0208,  0.5586, -0.1904, -0.4219, -0.3887],
        [-0.1738,  0.6719, -0.3301, -0.3008, -0.5312],
        [ 0.0588,  0.7852, -0.2139, -0.4238, -0.3066],
        [-0.5742,  0.2217, -0.1953, -0.6680, -0.6289],
        [-0.1099,  0.7539, -0.2793, -0.3828, -0.3125],
        [-0.2715,  0.4785,  0.0040, -0.5430, -1.0391],
        [-0.3945,  0.6211,  0.3633, -0.3027, -0.6055],
        [-0.3438,  0.6641, -0.3379, -0.3574, -0.2031],
        [-0.1030,  1.1719, -0.1709, -0.5703, -0.3379],
        [ 0.1338,  0.7383, -0.0300, -0.0728, -0.5586],
        [ 0.3398,  0.6953,  0.1455, -0.0688, -0.6758]], device='cuda:0',
       dtype=torch.bfloat16, grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None), SequenceClassifierOutput(loss=tensor(1.1179, device='cuda:0', grad_fn=<NllLossBackward0>), logits=tensor([[-0.2773,  0.8359, -0.0742,  0.1787, -0.3945],
        [-0.2344,  0.4844,  0.0299, -0.6602, -0.4980],
        [ 0.1079,  0.8477, -0.3340, -0.5234, -0.6055],
        [ 0.3516,  0.8086, -0.0640, -0.3574, -0.3867],
        [ 0.1934,  0.6250, -0.0845, -0.1348, -0.0776],
        [-0.2617,  0.8906,  0.0166, -0.1562, -0.5117],
        [-0.3418,  0.6055,  0.1328, -0.1562, -0.6992],
        [-0.4141,  0.5977, -0.0732, -0.3262, -0.4336],
        [ 0.1611,  0.5547, -0.0623, -0.4102, -0.5469],
        [-0.4492,  0.3398,  0.0466, -0.2949, -1.0781],
        [-0.1348,  0.6250, -0.2100, -0.1592, -0.9648],
        [-0.0466,  0.9453, -0.3086, -0.2480, -0.3867],
        [-0.1050,  0.5742, -0.0203, -0.2578, -0.3809],
        [-0.0413,  0.6719, -0.2041, -0.5898, -0.3359],
        [-0.2324,  0.6172, -0.2354, -0.4434, -0.4551],
        [ 0.0288,  0.8828, -0.4199,  0.0410, -0.1523]], device='cuda:0',
       dtype=torch.bfloat16, grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None), SequenceClassifierOutput(loss=tensor(1.1252, device='cuda:0', grad_fn=<NllLossBackward0>), logits=tensor([[ 0.1094,  0.8555, -0.4199, -0.4473, -0.7617],
        [-0.1069,  0.7891, -0.1040, -0.2969, -0.2422],
        [ 0.1230,  0.5273, -0.4121,  0.0164, -0.3008],
        [-0.6289,  0.5977,  0.1660, -0.6016, -0.7188],
        [-0.2832,  0.6914, -0.0013, -0.3008, -0.4531],
        [-0.1611,  0.7539, -0.1523, -0.1094, -0.4746],
        [ 0.1680,  0.7266, -0.5078, -0.4883, -0.3105],
        [ 0.0654,  0.8125, -0.2285, -0.2812, -0.4648],
        [-0.2344,  0.4551,  0.3008, -0.4141, -0.6133],
        [ 0.1484,  0.9297,  0.3652, -0.2578, -0.4961],
        [-0.1494,  0.5664,  0.1064, -0.3750, -0.8477],
        [-0.0247,  0.9531, -0.1436, -0.5430, -0.2969],
        [ 0.1045,  0.6484, -0.0728, -0.5586, -0.5898],
        [-0.1030,  0.8359, -0.5078, -0.0801, -0.6602],
        [-0.2314,  1.0781, -0.3281, -0.1299, -0.3340],
        [-0.1455,  0.9805, -0.0209, -0.4453,  0.0309]], device='cuda:0',
       dtype=torch.bfloat16, grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None), SequenceClassifierOutput(loss=tensor(1.1550, device='cuda:0', grad_fn=<NllLossBackward0>), logits=tensor([[-0.1523,  0.3008, -0.3262, -0.5508, -0.4004],
        [-0.0811,  0.8516, -0.0659, -0.5859, -0.4395],
        [ 0.0184,  0.4023, -0.5078, -0.1611, -0.0654],
        [-0.2891,  1.0312, -0.0349, -0.3789, -0.4941],
        [-0.0302,  0.8789, -0.3789, -0.5938, -0.5898],
        [-0.0337,  0.8516, -0.0442, -0.4395, -0.4336],
        [-0.1719,  0.4707, -0.1484, -0.5078, -0.7773],
        [-0.0698,  0.9727, -0.1768, -0.5742, -0.4355],
        [-0.0147,  0.7773, -0.1797, -0.4473, -0.6211],
        [ 0.0147,  1.0703,  0.0928, -0.3203, -0.2988],
        [-0.0227,  1.2812, -0.0386, -0.2334, -0.1338],
        [ 0.0820,  0.5352,  0.0698, -0.4238, -0.5312],
        [-0.2354,  0.2490, -0.2148, -0.3398, -0.4023],
        [ 0.1670,  0.8086, -0.2158, -0.4512, -0.3496],
        [-0.2334,  1.1484, -0.0137, -0.3301, -0.3008],
        [-0.1904,  1.0469, -0.6250, -0.4766, -0.2432]], device='cuda:0',
       dtype=torch.bfloat16, grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None), SequenceClassifierOutput(loss=tensor(1.0369, device='cuda:0', grad_fn=<NllLossBackward0>), logits=tensor([[-2.3633e-01,  4.3164e-01,  1.2695e-02, -5.6641e-01, -5.7031e-01],
        [ 9.7656e-02,  5.5859e-01, -3.9844e-01, -3.2031e-01, -4.4727e-01],
        [-1.6699e-01,  1.0312e+00, -3.5352e-01, -2.5879e-02, -2.8906e-01],
        [-2.4023e-01,  8.2422e-01, -2.1240e-02, -9.2773e-02, -4.8828e-01],
        [ 1.8677e-02,  4.4336e-01,  7.6172e-02, -5.1562e-01, -3.7500e-01],
        [-2.2363e-01,  1.0391e+00, -1.8066e-01, -6.1328e-01, -6.2109e-01],
        [-5.1514e-02,  9.0234e-01, -1.4941e-01, -3.1445e-01, -3.5352e-01],
        [ 1.4453e-01,  5.7422e-01, -4.7266e-01, -1.8262e-01, -8.4961e-02],
        [-2.5977e-01,  1.2344e+00, -3.0664e-01, -6.9531e-01,  1.9287e-02],
        [-1.4648e-01,  5.1562e-01, -4.2188e-01, -5.6396e-02, -5.3125e-01],
        [-2.0630e-02,  5.9375e-01, -1.4355e-01, -4.0234e-01, -6.3281e-01],
        [ 4.3213e-02,  5.9766e-01,  5.4550e-04, -2.6367e-01, -7.9688e-01],
        [-3.5352e-01,  6.2891e-01,  8.9844e-02, -3.5938e-01, -5.6641e-01],
        [-1.3574e-01,  7.1484e-01, -2.2363e-01, -2.1680e-01, -3.0469e-01],
        [-4.0283e-02,  5.1953e-01, -2.0508e-01, -5.1172e-01, -2.8906e-01],
        [-1.3770e-01,  3.3398e-01, -1.9727e-01, -3.2227e-01, -3.9062e-01]],
       device='cuda:0', dtype=torch.bfloat16, grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None), SequenceClassifierOutput(loss=tensor(1.1526, device='cuda:0', grad_fn=<NllLossBackward0>), logits=tensor([[-0.0356,  0.8359,  0.0581, -0.3223, -0.3027],
        [-0.0640,  0.8047, -0.2520, -0.3516, -0.2539],
        [ 0.3105,  0.6719, -0.3770, -0.5508, -0.1895],
        [ 0.0254,  0.6289, -0.6094, -0.3418, -0.6055],
        [-0.2676,  0.5586,  0.0364, -0.2578, -0.4961],
        [-0.0806,  0.3887, -0.1177, -0.4902, -0.3535],
        [-0.1553,  0.8359,  0.3125, -0.5898, -0.3887],
        [-0.1748,  0.8867, -0.1758, -0.6523, -0.4590],
        [-0.0894,  0.8594, -0.0674, -0.1157, -0.3145],
        [-0.2354,  0.6523, -0.3008, -0.2930, -0.6641],
        [ 0.0374,  1.1016,  0.0515, -0.3613, -0.3320],
        [ 0.2461,  0.5781,  0.1167, -0.3301, -0.2441],
        [ 0.1914,  0.8672, -0.3652, -0.1289, -0.1748],
        [-0.0022,  0.7617,  0.2178, -0.2178, -0.3535],
        [-0.1104,  1.3516, -0.0300, -0.5430, -0.1924],
        [-0.2656,  0.5391, -0.0825, -0.3770, -0.6367]], device='cuda:0',
       dtype=torch.bfloat16, grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None), SequenceClassifierOutput(loss=tensor(0.8760, device='cuda:0', grad_fn=<NllLossBackward0>), logits=tensor([[-2.1387e-01,  3.8281e-01, -2.1973e-02, -7.6562e-01, -5.9766e-01],
        [ 5.4688e-02,  8.6719e-01, -9.7656e-02, -2.3633e-01, -3.3008e-01],
        [ 3.1128e-02,  9.0625e-01, -3.1982e-02, -6.3672e-01, -4.4141e-01],
        [ 3.5889e-02,  9.7266e-01, -2.6953e-01, -1.7676e-01, -2.3535e-01],
        [-1.8457e-01,  5.3906e-01, -2.6953e-01, -2.5586e-01, -1.0469e+00],
        [-2.2656e-01,  1.0938e+00, -2.2949e-01,  1.4355e-01, -1.4062e-01],
        [-4.5898e-01,  9.9609e-01, -2.4609e-01, -4.5312e-01, -4.0430e-01],
        [-5.2344e-01,  8.3594e-01, -5.5176e-02, -3.9062e-01, -3.4180e-01],
        [-2.2852e-01,  7.3438e-01, -1.5723e-01, -2.9102e-01, -2.2168e-01],
        [ 1.0254e-01,  9.1016e-01, -1.5625e-01, -4.9805e-01, -8.2031e-02],
        [-8.2031e-02,  9.9609e-01, -1.4258e-01, -4.0039e-01, -3.3984e-01],
        [-3.3398e-01,  1.0781e+00,  1.0498e-01, -3.3594e-01, -8.6328e-01],
        [-3.1445e-01,  9.1406e-01,  1.9238e-01, -6.1035e-02, -1.3281e-01],
        [ 2.7148e-01,  9.0234e-01, -3.6914e-01, -6.0156e-01, -5.5078e-01],
        [ 3.3760e-04,  1.0781e+00, -1.7480e-01, -3.0273e-01, -6.2891e-01],
        [-2.2168e-01,  8.9062e-01, -1.5723e-01, -3.7109e-01, -4.3359e-01]],
       device='cuda:0', dtype=torch.bfloat16, grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None), SequenceClassifierOutput(loss=tensor(0.9263, device='cuda:0', grad_fn=<NllLossBackward0>), logits=tensor([[-0.5625,  0.7969, -0.3164, -0.2070,  0.0583],
        [-0.1582,  0.9570, -0.0654, -0.4395, -0.2500],
        [-0.0215,  0.5703, -0.1982, -0.6094, -0.4395],
        [-0.1992,  1.1250, -0.0359, -0.6836, -0.1455],
        [-0.2266,  0.8359, -0.4844, -0.2471, -0.2891],
        [ 0.0664,  0.7461, -0.2559, -0.2412, -0.4941],
        [-0.1992,  0.7695, -0.4141, -0.1348, -0.3906],
        [ 0.1396,  0.9766, -0.2539, -0.3574, -0.5742],
        [-0.2305,  0.3984, -0.0143, -0.1426, -0.4043],
        [-0.0723,  1.1016,  0.0075, -0.4434, -0.1611],
        [ 0.2441,  0.8086, -0.1797, -0.2871, -0.1533],
        [-0.1245,  0.9922, -0.1787, -0.4844, -0.3125],
        [-0.2402,  0.8594, -0.0386, -0.3750, -0.4980],
        [-0.1367,  0.6406, -0.1582, -0.6016, -0.1904],
        [-0.1030,  0.6758, -0.1592, -0.2930, -0.5078],
        [-0.0938,  0.9922, -0.1191, -0.4766, -0.3535]], device='cuda:0',
       dtype=torch.bfloat16, grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None), SequenceClassifierOutput(loss=tensor(1.0303, device='cuda:0', grad_fn=<NllLossBackward0>), logits=tensor([[-1.5918e-01,  1.0938e+00, -1.7480e-01, -2.4023e-01, -4.2188e-01],
        [ 1.8848e-01,  1.0781e+00, -4.0820e-01, -2.8320e-01, -1.6309e-01],
        [-1.8164e-01,  6.4453e-01, -2.8320e-01, -1.4551e-01, -5.4688e-01],
        [-8.0566e-02,  3.2422e-01, -1.8555e-01, -7.1875e-01, -5.3125e-01],
        [-1.4648e-02,  6.4844e-01, -1.5332e-01, -4.2383e-01,  2.4292e-02],
        [ 5.6396e-02,  7.6172e-01,  6.2500e-02, -3.0664e-01, -5.8203e-01],
        [ 6.1768e-02,  1.1094e+00, -1.2256e-01, -1.3965e-01, -4.9805e-01],
        [-2.2363e-01,  8.1250e-01, -1.4844e-01, -5.8203e-01, -2.8320e-01],
        [-2.8906e-01,  9.1016e-01, -2.1582e-01, -2.6953e-01, -4.6094e-01],
        [-3.3789e-01,  8.7109e-01, -3.4570e-01, -1.9336e-01, -2.6758e-01],
        [ 1.9550e-04,  6.8750e-01, -2.8711e-01, -3.2617e-01, -5.1562e-01],
        [ 2.2266e-01,  1.2734e+00, -3.8086e-01, -5.8984e-01, -3.9258e-01],
        [-1.8262e-01,  5.5469e-01, -1.7578e-02, -6.1328e-01, -1.3086e-01],
        [-5.6152e-02,  4.9023e-01,  4.5312e-01, -2.3047e-01, -7.5391e-01],
        [ 1.6992e-01,  7.2656e-01, -6.0547e-01, -3.6719e-01, -5.5859e-01],
        [-4.1016e-01,  4.5117e-01, -8.3496e-02, -3.7598e-02, -6.3672e-01]],
       device='cuda:0', dtype=torch.bfloat16, grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None), SequenceClassifierOutput(loss=tensor(0.9966, device='cuda:0', grad_fn=<NllLossBackward0>), logits=tensor([[-0.1650,  0.8320, -0.1494, -0.1357, -0.6289],
        [-0.3086,  0.4707, -0.0231, -0.3340, -0.4883],
        [-0.1377,  0.8750, -0.1719, -0.5352, -0.2520],
        [-0.2734,  0.8281,  0.1147, -0.3516, -0.2500],
        [-0.3301,  0.8359, -0.1963, -0.4238, -0.5039],
        [-0.1602,  0.5430,  0.1807, -0.2393, -0.3711],
        [-0.1602,  0.6367,  0.0105, -0.4297, -0.7773],
        [-0.2969,  0.5078,  0.1865,  0.1040, -0.6250],
        [-0.2275,  0.5547,  0.1670, -0.4844, -0.6914],
        [ 0.0742,  0.8047,  0.1221, -0.4062, -0.3164],
        [ 0.0167,  0.9102,  0.1001, -0.5352, -0.2578],
        [-0.4922,  0.4980,  0.0737, -0.3418, -0.6758],
        [-0.1416,  0.6602, -0.0032, -0.5391, -0.6250],
        [ 0.1084,  0.6602, -0.4238, -0.0825, -0.5547],
        [ 0.0703,  0.7656, -0.3496, -0.2070, -0.3418],
        [ 0.0267,  1.0703, -0.1133, -0.3730, -0.1206]], device='cuda:0',
       dtype=torch.bfloat16, grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None), SequenceClassifierOutput(loss=tensor(1.2559, device='cuda:0', grad_fn=<NllLossBackward0>), logits=tensor([[-0.0391,  0.6484, -0.3984, -0.6172, -0.6484],
        [ 0.2656,  0.4609, -0.2559, -0.3008,  0.0051],
        [ 0.1445,  0.9258, -0.4688, -0.3340,  0.0996],
        [-0.3633,  0.9570, -0.3828, -0.1157, -0.2949],
        [ 0.0854,  0.9492, -0.2793, -0.5430, -0.0630],
        [-0.3477,  0.2793,  0.1836, -0.4609, -0.8516],
        [ 0.1357,  0.2324, -0.5430,  0.0461,  0.1855],
        [ 0.2275,  0.6875,  0.0361, -0.5156, -0.4668],
        [-0.2832,  0.8984, -0.2910, -0.1650, -0.1465],
        [ 0.0396,  0.8203, -0.1904, -0.3242, -0.3320],
        [-0.1641,  1.0156, -0.1953, -0.4609, -0.5273],
        [ 0.2656,  0.9102, -0.1514, -0.2061, -0.2871],
        [ 0.0037,  0.7656, -0.1738, -0.3066, -0.2275],
        [ 0.3164,  0.5469, -0.0928, -0.5938,  0.1670],
        [-0.6211,  0.5391,  0.2676, -0.1436, -0.9375],
        [ 0.0160,  0.8906,  0.1436, -0.4219, -0.8281]], device='cuda:0',
       dtype=torch.bfloat16, grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None), SequenceClassifierOutput(loss=tensor(1.1243, device='cuda:0', grad_fn=<NllLossBackward0>), logits=tensor([[-8.7891e-02,  5.9375e-01, -4.0894e-03, -4.0820e-01, -2.7148e-01],
        [ 1.0645e-01,  8.2812e-01,  3.6133e-02, -2.5586e-01, -8.6719e-01],
        [-2.0215e-01,  1.0703e+00, -6.8359e-02, -4.0039e-01, -2.8125e-01],
        [-1.3477e-01,  7.0312e-01,  8.9355e-02, -1.2598e-01, -1.1016e+00],
        [-9.9121e-02,  8.5156e-01,  8.2520e-02, -6.7578e-01, -9.8828e-01],
        [-4.4922e-02,  3.8672e-01, -6.4844e-01, -4.2188e-01, -7.5000e-01],
        [-1.7853e-03,  5.2734e-01, -5.4932e-02, -6.7383e-02, -3.1055e-01],
        [-1.3574e-01,  8.9844e-01, -1.0205e-01, -2.4023e-01, -4.7852e-01],
        [ 1.4648e-01,  9.1406e-01, -2.8125e-01, -3.7305e-01, -3.9844e-01],
        [ 2.8442e-02,  5.7422e-01, -6.7383e-02, -1.8262e-01, -4.4922e-01],
        [-2.5000e-01,  8.7891e-01, -2.7734e-01, -5.7031e-01, -4.2188e-01],
        [-3.6914e-01,  6.6406e-01, -1.6113e-01, -7.6562e-01, -8.5156e-01],
        [-4.5312e-01,  8.9453e-01, -3.3008e-01, -2.2949e-01, -5.1172e-01],
        [-4.1748e-02,  8.3984e-01, -2.9688e-01, -3.0859e-01, -1.6968e-02],
        [-1.6113e-01,  1.1016e+00, -7.9346e-04, -3.3203e-01, -4.0430e-01],
        [-4.8828e-02,  8.0469e-01, -3.2812e-01, -4.8242e-01, -2.3438e-01]],
       device='cuda:0', dtype=torch.bfloat16, grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None), SequenceClassifierOutput(loss=tensor(1.2529, device='cuda:0', grad_fn=<NllLossBackward0>), logits=tensor([[ 0.2158,  0.4707, -0.3633, -0.3809, -0.3105],
        [-0.2383,  0.6484,  0.3164, -0.4785, -0.8750],
        [-0.2734,  0.6797,  0.3203, -0.5898, -0.6680],
        [-0.0369,  0.8867, -0.5273, -0.0825, -0.2012],
        [ 0.1953,  0.4199, -0.4180, -0.3027, -0.4160],
        [ 0.0781,  0.8711, -0.1865, -0.5508, -0.3691],
        [ 0.0103,  0.5234, -0.4395, -0.1553, -0.2949],
        [ 0.5039,  0.9062, -0.1128, -0.4277, -0.3848],
        [ 0.0035,  0.7266, -0.1914, -0.2734, -0.6602],
        [-0.0166,  0.7344, -0.1030, -0.1377, -0.1934],
        [-0.1855,  0.5273,  0.3535, -0.4590, -0.6680],
        [ 0.1875,  0.9609, -0.2402, -0.4590, -0.3164],
        [-0.7148,  0.5938, -0.0400, -0.4336, -0.4668],
        [ 0.0245,  1.0781, -0.1484, -0.5625, -0.4531],
        [ 0.0330,  0.6211,  0.0037, -0.5430, -0.2910],
        [-0.0635,  0.8789, -0.1885, -0.4277, -0.2988]], device='cuda:0',
       dtype=torch.bfloat16, grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None), SequenceClassifierOutput(loss=tensor(1.1321, device='cuda:0', grad_fn=<NllLossBackward0>), logits=tensor([[-0.1768,  0.5703,  0.1162, -0.2559, -0.1030],
        [-0.0022,  0.8320, -0.3125, -0.7617, -0.6328],
        [ 0.0162,  0.4512,  0.1494, -0.3496, -0.8281],
        [-0.0371,  0.9570, -0.4219, -0.3848, -0.5273],
        [ 0.1465,  0.1279, -0.4023, -0.3594, -0.1348],
        [-0.2393,  0.7266, -0.5469, -0.2188, -0.5312],
        [ 0.1514,  0.9453, -0.2930, -0.5430, -0.1768],
        [-0.1357,  0.6992, -0.1152, -0.4160, -0.7500],
        [ 0.0654,  0.8438,  0.2129, -0.3008, -1.1719],
        [-0.2891,  0.8555, -0.0845, -0.6680, -0.1895],
        [-0.0972,  0.9609, -0.1357, -0.0405, -0.3594],
        [-0.3887,  0.8203, -0.0664, -0.4043, -0.2217],
        [ 0.0261,  0.9141, -0.2295, -0.6680, -0.3652],
        [-0.7344,  0.8359, -0.4102, -0.4531, -0.5781],
        [ 0.3359,  0.8047, -0.2773, -0.6055, -0.4043],
        [-0.0146,  0.8906,  0.0654, -0.4316, -0.8750]], device='cuda:0',
       dtype=torch.bfloat16, grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None), SequenceClassifierOutput(loss=tensor(1.0293, device='cuda:0', grad_fn=<NllLossBackward0>), logits=tensor([[-0.1582,  0.6641,  0.1416, -0.0415, -0.4902],
        [-0.0243,  0.8281, -0.1934, -0.2295, -0.3203],
        [-0.0028,  0.6758, -0.3164, -0.5898, -0.4707],
        [-0.1279,  0.8516, -0.3027, -0.2324, -0.5977],
        [ 0.0559,  0.9023, -0.0120, -0.3574, -0.2266],
        [-0.0747,  0.5859, -0.4961, -0.5742, -0.4316],
        [-0.2334,  0.5742, -0.1660, -0.4238, -0.2148],
        [-0.1191,  0.8594, -0.1699, -0.4277, -0.6094],
        [-0.3730,  1.0781,  0.1328, -0.6445, -0.1865],
        [-0.1816,  0.8516, -0.3750, -0.4238, -0.1787],
        [-0.1318,  0.9023,  0.1553, -0.4824, -0.2432],
        [ 0.1758,  0.3418,  0.1299, -0.4355, -0.5352],
        [ 0.0476,  0.6953, -0.2910, -0.3633, -0.7734],
        [ 0.2178,  1.0859,  0.0361, -0.4785, -0.1826],
        [ 0.0165,  0.6523, -0.1787, -0.0728, -0.3145],
        [-0.2793,  0.7422, -0.2314, -0.2598, -0.4531]], device='cuda:0',
       dtype=torch.bfloat16, grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None), SequenceClassifierOutput(loss=tensor(1.1460, device='cuda:0', grad_fn=<NllLossBackward0>), logits=tensor([[ 1.2158e-01,  7.1484e-01, -2.7734e-01, -2.0020e-01, -2.6562e-01],
        [-4.4141e-01,  9.2285e-02, -7.2754e-02, -6.4062e-01, -5.3516e-01],
        [-7.6660e-02,  1.1406e+00, -1.9531e-03, -3.0859e-01, -2.7148e-01],
        [-2.9883e-01,  8.0859e-01, -4.2188e-01, -3.1250e-01, -5.3906e-01],
        [-3.5938e-01,  1.0156e+00, -1.6403e-03, -3.6523e-01, -5.8746e-04],
        [ 9.2773e-02,  7.1484e-01,  2.8125e-01, -4.2188e-01, -8.7109e-01],
        [ 1.1780e-02,  8.0859e-01, -8.8379e-02, -3.0273e-01, -3.2031e-01],
        [-2.1680e-01,  5.3516e-01, -1.9434e-01, -2.7930e-01, -3.5547e-01],
        [-1.9775e-02,  8.2031e-01,  4.9805e-02, -3.6328e-01, -1.4551e-01],
        [-1.7969e-01,  1.0938e+00, -6.0303e-02, -1.6699e-01, -1.4844e-01],
        [-5.5176e-02,  8.6719e-01, -4.7302e-03, -2.1484e-01, -5.1562e-01],
        [-4.5898e-01,  9.8438e-01, -4.0820e-01, -1.5625e-01, -3.3008e-01],
        [-3.4912e-02,  9.4141e-01, -3.8818e-02, -6.0547e-01, -7.6562e-01],
        [-1.5918e-01,  7.6562e-01, -5.7031e-01, -4.9609e-01, -5.4297e-01],
        [ 4.0234e-01,  2.1387e-01, -4.1602e-01, -6.3672e-01, -5.3125e-01],
        [-1.8945e-01,  5.1172e-01, -2.8125e-01, -6.5234e-01, -9.5312e-01]],
       device='cuda:0', dtype=torch.bfloat16, grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None), SequenceClassifierOutput(loss=tensor(0.9243, device='cuda:0', grad_fn=<NllLossBackward0>), logits=tensor([[-2.4121e-01,  1.2578e+00, -1.3770e-01, -4.2188e-01, -2.7344e-01],
        [ 1.2793e-01,  9.1797e-01,  1.8457e-01, -1.6211e-01, -4.4141e-01],
        [ 2.4902e-01,  9.8828e-01, -3.4766e-01, -4.3555e-01, -2.5195e-01],
        [-6.0059e-02,  1.0703e+00, -3.0469e-01, -4.3945e-01, -2.4512e-01],
        [ 4.1406e-01,  8.5156e-01, -4.6875e-01, -3.9844e-01, -1.5527e-01],
        [ 3.4766e-01,  9.7656e-01, -5.3516e-01, -5.0781e-01, -3.7109e-01],
        [-7.9102e-02,  1.0156e+00, -1.7480e-01, -3.8086e-01,  8.5938e-02],
        [ 5.4688e-02,  1.0547e+00,  1.9043e-01, -4.7266e-01, -1.0547e+00],
        [ 3.5095e-04,  1.1562e+00,  1.8555e-01, -2.0996e-01, -7.9688e-01],
        [-3.9453e-01,  7.5391e-01, -1.0742e-01, -6.4062e-01, -6.0938e-01],
        [ 2.3242e-01,  6.6016e-01, -3.3594e-01, -4.3750e-01, -3.5742e-01],
        [-2.2168e-01,  8.0859e-01,  1.0742e-01, -6.4453e-01, -6.4062e-01],
        [-5.5420e-02,  5.2344e-01, -1.4648e-01, -5.5469e-01, -8.7109e-01],
        [ 3.2227e-02,  1.1250e+00, -2.5781e-01, -2.6758e-01, -4.3164e-01],
        [ 3.5938e-01,  1.0625e+00, -2.4512e-01, -5.5078e-01, -3.0469e-01],
        [ 5.0781e-02,  8.5938e-01, -5.5859e-01, -3.3398e-01, -4.7070e-01]],
       device='cuda:0', dtype=torch.bfloat16, grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None), SequenceClassifierOutput(loss=tensor(1.0115, device='cuda:0', grad_fn=<NllLossBackward0>), logits=tensor([[-0.0615,  0.7734, -0.1885, -0.7422, -0.6484],
        [-0.0232,  1.1250, -0.1338, -0.8633, -0.5938],
        [ 0.1172,  0.4258, -0.2197, -0.2344, -0.3711],
        [-0.2324,  0.7734,  0.0635, -0.3867, -0.4004],
        [-0.1289,  0.8945, -0.1045, -0.1494, -0.2988],
        [-0.1226,  0.5547, -0.1191, -0.5000, -0.4082],
        [ 0.0728,  0.8516, -0.4727, -0.6797, -0.2373],
        [ 0.2578,  0.9297, -0.1030, -0.5586, -0.5703],
        [ 0.2354,  0.6016, -0.1729, -0.4199, -0.2021],
        [-0.2520,  0.7109, -0.1050, -0.2695, -0.6328],
        [-0.3184,  0.9961, -0.0791, -0.4707, -0.7148],
        [-0.1699,  0.6055, -0.1719, -0.2324, -0.4434],
        [ 0.3730,  0.0297, -0.7109, -0.6523,  0.5156],
        [-0.0981,  0.6094, -0.4082, -0.4629, -0.5664],
        [ 0.0220,  0.7734,  0.1016, -0.6211, -0.2393],
        [ 0.0535,  0.7852, -0.1016, -0.5273, -0.2061]], device='cuda:0',
       dtype=torch.bfloat16, grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None), SequenceClassifierOutput(loss=tensor(0.9990, device='cuda:0', grad_fn=<NllLossBackward0>), logits=tensor([[-0.0508,  0.8516, -0.5430, -0.3008, -0.0723],
        [-0.4668,  1.0938, -0.0850, -0.8398, -0.3398],
        [-0.2197,  1.1641, -0.2168, -0.4395, -0.3203],
        [-0.2637,  1.0703,  0.2070, -0.5039, -0.2695],
        [ 0.1611,  0.7930, -0.6055, -0.5195, -0.3145],
        [ 0.0879,  1.0156, -0.2500, -0.4863, -0.4180],
        [ 0.0124,  0.9023, -0.2988, -0.3340, -0.1572],
        [ 0.0669,  0.6914,  0.0287, -0.2910, -0.1953],
        [-0.0105,  1.0469,  0.4297, -0.6367, -0.4375],
        [ 0.2969,  0.4141,  0.0228, -0.4707, -0.1953],
        [ 0.0386,  0.9688, -0.1934, -0.6523, -0.3770],
        [-0.2656,  1.1953, -0.2256, -0.2637, -0.5391],
        [ 0.0977,  0.5391, -0.1279, -0.6445, -0.5703],
        [-0.2773,  0.6445, -0.2207, -0.4590, -0.3672],
        [-0.1260,  1.0156, -0.3848, -0.0859, -0.5586],
        [ 0.1523,  0.6289,  0.0723, -0.4492, -0.2676]], device='cuda:0',
       dtype=torch.bfloat16, grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None), SequenceClassifierOutput(loss=tensor(1.2578, device='cuda:0', grad_fn=<NllLossBackward0>), logits=tensor([[-0.2178,  0.4746, -0.0312, -0.4551, -0.8750],
        [ 0.0085,  1.0547,  0.0767, -0.4492, -0.0530],
        [ 0.1206,  0.8477, -0.2715, -0.5469, -0.5273],
        [-0.1699,  0.8359, -0.3770, -0.1533, -0.3008],
        [ 0.0039,  0.6406,  0.2256, -0.4375, -0.4688],
        [-0.1504,  0.5703, -0.4180, -0.1494, -0.4609],
        [-0.1240,  0.9336,  0.1172, -0.2598, -0.6875],
        [ 0.0991,  0.4141,  0.1426, -0.3770, -0.6055],
        [ 0.1348,  0.8750,  0.2500, -0.4688, -0.6680],
        [ 0.1885,  0.9531, -0.1035, -0.5508, -0.3027],
        [-0.0791,  0.8867, -0.2441, -0.3848, -0.2246],
        [ 0.2305,  0.5977, -0.1426, -0.3359, -0.3066],
        [-0.0121,  0.8867, -0.3691, -0.4258, -0.4590],
        [-0.0640,  0.4160,  0.1118, -0.5078, -0.3906],
        [ 0.1016,  0.4727,  0.0630, -0.2969, -0.5312],
        [-0.1416,  0.9297, -0.0222, -0.0669,  0.0542]], device='cuda:0',
       dtype=torch.bfloat16, grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None), SequenceClassifierOutput(loss=tensor(0.9795, device='cuda:0', grad_fn=<NllLossBackward0>), logits=tensor([[ 0.3223,  0.3105, -0.0708, -0.6914, -0.7070],
        [ 0.1689,  0.7539, -0.2910, -0.4805, -0.2480],
        [ 0.0461,  1.0234, -0.4258, -0.4727, -0.3125],
        [-0.1680,  0.6445, -0.3066, -0.3828, -0.2227],
        [-0.2041,  0.5898,  0.1387, -0.4746, -0.5352],
        [ 0.1270,  0.7852, -0.2246, -0.5195, -0.4297],
        [-0.2520,  0.8438,  0.0264, -0.3008, -0.3750],
        [-0.0325,  0.9844,  0.3223, -0.3535, -0.8633],
        [-0.0320,  0.9062, -0.0498, -0.4082, -0.1494],
        [-0.0096,  0.9531,  0.0303, -0.2832, -0.6133],
        [-0.0571,  0.7852, -0.3770, -0.0518, -0.3809],
        [-0.0269,  0.8086, -0.1055, -0.6250, -0.3535],
        [-0.0398,  0.6797, -0.4277, -0.4512, -0.7383],
        [-0.0942,  0.8750, -0.2324, -0.6016, -0.6758],
        [-0.3730,  0.8047,  0.0071, -0.7578, -0.2812],
        [ 0.1177, -0.1699, -0.5859, -0.2578, -0.1758]], device='cuda:0',
       dtype=torch.bfloat16, grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None), SequenceClassifierOutput(loss=tensor(1.0916, device='cuda:0', grad_fn=<NllLossBackward0>), logits=tensor([[-0.0432,  0.9648, -0.2412, -0.2891, -0.2148],
        [ 0.1768,  1.0469, -0.3008, -0.4766, -0.2832],
        [-0.3438,  0.8516, -0.1030, -0.5508, -0.1465],
        [-0.0322,  0.8203, -0.6250, -0.4199, -0.3477],
        [-0.1309,  0.6758,  0.0874, -0.0466, -0.6289],
        [ 0.0771,  0.8750, -0.0245, -0.3164, -0.6484],
        [ 0.0214,  0.8320,  0.0288, -0.5625, -0.1533],
        [-0.1543,  0.7461, -0.2520, -0.1953, -0.0654],
        [-0.0698,  1.0234, -0.2988, -0.5273, -0.5469],
        [ 0.0493,  0.0884, -0.2617, -0.5898, -0.6641],
        [-0.3047,  0.8477, -0.2539, -0.1475, -0.2910],
        [ 0.2539,  0.9023, -0.3418, -0.5195, -0.2100],
        [ 0.1816,  0.4824, -0.2617, -0.1826, -0.3281],
        [ 0.1914,  0.8906, -0.2520, -0.3809, -0.1816],
        [ 0.2041,  0.3301, -0.5625, -0.2324, -0.4609],
        [-0.1328,  0.7344, -0.1904, -0.5703, -0.4238]], device='cuda:0',
       dtype=torch.bfloat16, grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None), SequenceClassifierOutput(loss=tensor(0.9817, device='cuda:0', grad_fn=<NllLossBackward0>), logits=tensor([[ 0.1562,  0.4297, -0.7695, -0.6406, -0.0041],
        [-0.2871,  1.1406,  0.1768, -0.2949, -0.4180],
        [-0.5273,  0.5703,  0.0052, -0.4688, -0.3340],
        [-0.1865,  0.6406,  0.0439, -0.6680, -0.3477],
        [-0.0065,  0.8750, -0.2354, -0.3184, -0.3828],
        [-0.1045,  0.9922, -0.4004, -0.3555, -0.4785],
        [-0.1582,  0.7539, -0.3984, -0.1079, -0.7188],
        [-0.4102,  0.8164, -0.1689, -0.3809, -0.5391],
        [ 0.0400,  0.9688, -0.4551, -0.3105, -0.3594],
        [-0.5898,  0.4121, -0.1904, -0.4590, -0.7109],
        [-0.4395,  0.8320, -0.2910, -0.2285, -0.5781],
        [-0.0515,  0.8164, -0.3809, -0.5781, -0.4082],
        [ 0.0070,  1.0156, -0.4980, -0.6641, -0.1592],
        [-0.4180,  0.7969, -0.2305, -0.6133, -0.7969],
        [ 0.0457,  1.1641, -0.3574, -0.4688, -0.2871],
        [-0.0859,  0.6211, -0.0864, -0.2969, -0.5117]], device='cuda:0',
       dtype=torch.bfloat16, grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None), SequenceClassifierOutput(loss=tensor(1.0049, device='cuda:0', grad_fn=<NllLossBackward0>), logits=tensor([[ 8.6426e-02,  7.4609e-01, -5.6250e-01, -5.8203e-01, -2.8125e-01],
        [-1.3965e-01,  5.0781e-01,  5.3406e-03, -5.5078e-01, -8.1250e-01],
        [-2.3828e-01,  9.7266e-01, -1.8262e-01, -6.2891e-01, -6.0938e-01],
        [ 1.6699e-01,  8.3984e-01, -1.2061e-01, -4.5703e-01, -2.5195e-01],
        [-1.1670e-01,  8.3594e-01, -4.9805e-01, -6.9922e-01, -5.8203e-01],
        [ 3.5645e-02,  9.8828e-01, -3.2227e-01, -3.0029e-02, -4.1797e-01],
        [-3.7305e-01,  8.3203e-01, -7.5684e-02, -2.2461e-01, -3.6328e-01],
        [-2.1289e-01,  9.2188e-01, -1.6113e-01, -5.7031e-01, -5.3516e-01],
        [ 5.3955e-02,  7.1875e-01, -1.0889e-01, -2.4512e-01, -5.0781e-01],
        [-2.1680e-01,  7.6172e-01, -3.3594e-01, -3.4375e-01, -3.4375e-01],
        [ 6.5918e-02,  1.0859e+00, -3.3203e-01, -4.0039e-01, -1.4062e-01],
        [-6.9824e-02,  8.3594e-01, -4.3555e-01, -3.3203e-01, -3.6914e-01],
        [-2.5195e-01,  6.0156e-01, -8.7357e-04, -6.9531e-01, -8.9453e-01],
        [-3.4375e-01,  1.0938e+00, -5.3516e-01, -2.0703e-01, -4.6875e-01],
        [-1.4453e-01,  9.4922e-01, -2.4121e-01, -4.1016e-01, -5.1953e-01],
        [-1.6895e-01,  9.6484e-01, -4.6289e-01, -5.1953e-01, -2.8125e-01]],
       device='cuda:0', dtype=torch.bfloat16, grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None), SequenceClassifierOutput(loss=tensor(1.0325, device='cuda:0', grad_fn=<NllLossBackward0>), logits=tensor([[-0.1279,  0.7656, -0.2090, -0.0408, -0.6055],
        [-0.0067,  0.9336, -0.1167, -0.7734, -0.4688],
        [-0.2217,  0.8516, -0.2539, -0.4941, -0.8555],
        [-0.1016,  0.7539, -0.1089, -0.2715, -0.7148],
        [-0.2002,  0.6367,  0.3730, -0.2471, -0.6133],
        [ 0.2266,  0.8438, -0.5859, -0.2852, -0.2949],
        [-0.2754,  0.6211, -0.1641, -0.8281, -0.4277],
        [-0.2109,  0.4805, -0.3574, -0.1592, -0.5312],
        [-0.0273,  0.8906, -0.3887, -0.4980, -0.6641],
        [ 0.2500,  0.9062, -0.2617, -0.5586, -0.6680],
        [ 0.0415,  1.1641, -0.0928, -0.5234, -0.4121],
        [-0.0500,  0.6562, -0.0884, -0.5078, -1.2734],
        [-0.1338,  0.5938,  0.2676, -0.7656, -1.0469],
        [-0.1475,  0.4961,  0.1084, -0.3906, -0.5625],
        [-0.0645,  1.1719, -0.0018, -0.5508, -0.4980],
        [ 0.0432,  0.7227, -0.0214, -0.4492, -0.5742]], device='cuda:0',
       dtype=torch.bfloat16, grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None), SequenceClassifierOutput(loss=tensor(1.0508, device='cuda:0', grad_fn=<NllLossBackward0>), logits=tensor([[ 0.1177,  0.5352, -0.0315, -0.2578, -0.5391],
        [ 0.0898,  1.0547, -0.2168, -0.3750, -0.2793],
        [-0.2109,  0.8867, -0.2891, -0.0188, -0.3418],
        [ 0.0199,  0.8984,  0.0236, -0.4082, -0.4082],
        [ 0.0476,  1.0078, -0.1064, -0.8594, -0.3281],
        [ 0.1030,  0.9492,  0.0320, -0.3223, -0.1904],
        [ 0.0518,  1.0156, -0.2949, -0.4512, -0.2119],
        [-0.2891,  1.0391, -0.3359, -0.5820, -0.5352],
        [ 0.1436,  0.8906,  0.1660, -0.4219, -0.7344],
        [-0.0566,  0.5703,  0.1602, -0.2852, -0.4629],
        [ 0.2051,  1.0547, -0.4648, -0.2812, -0.2793],
        [ 0.0564,  0.9609, -0.5352, -0.5195, -0.3535],
        [ 0.0491,  0.6875,  0.2656, -0.2461, -0.3965],
        [-0.0542,  0.7461, -0.0635, -0.2051, -0.1387],
        [-0.2100,  0.8477, -0.4434, -0.1719, -0.5430],
        [ 0.2178,  0.6758, -0.0449, -0.4883, -0.4590]], device='cuda:0',
       dtype=torch.bfloat16, grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None), SequenceClassifierOutput(loss=tensor(1.0310, device='cuda:0', grad_fn=<NllLossBackward0>), logits=tensor([[ 0.2021,  0.8945, -0.4180, -0.3809, -0.1396],
        [-0.2383,  0.8906,  0.1738, -0.9727, -0.0378],
        [-0.0591,  0.9219, -0.2656, -0.4355, -0.1396],
        [-0.2139,  1.3047, -0.0157, -0.3301, -0.2637],
        [ 0.0820,  0.8867, -0.1963, -0.3672, -0.5195],
        [-0.1177,  0.9570, -0.4961, -0.0928, -0.3867],
        [-0.3809,  0.8125, -0.3027, -0.3398, -0.4668],
        [ 0.1338,  0.9648, -0.2734, -0.2432, -0.4141],
        [-0.3848,  0.7461, -0.3105, -0.2305, -0.2969],
        [-0.4883,  1.1094, -0.3066, -0.5586, -0.6562],
        [ 0.3027,  0.5234, -0.5391, -0.5742, -0.6484],
        [ 0.0113,  1.1562, -0.1709, -0.3555, -0.4688],
        [-0.0048,  1.1016, -0.4023, -0.4941, -0.3711],
        [-0.1216,  0.7188,  0.0742, -0.3867, -0.8164],
        [-0.3438,  0.4844, -0.4746, -0.3027, -0.5391],
        [-0.1621,  1.0312, -0.1602, -0.4805, -0.2773]], device='cuda:0',
       dtype=torch.bfloat16, grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None), SequenceClassifierOutput(loss=tensor(1.0831, device='cuda:0', grad_fn=<NllLossBackward0>), logits=tensor([[-0.0205,  1.0625, -0.0177, -0.3438, -0.5195],
        [-0.0410,  0.7344, -0.1924, -0.1494, -0.2832],
        [-0.4531,  0.5586,  0.1162, -0.7461, -0.6094],
        [ 0.2637,  0.6719, -0.3242, -0.5469, -0.3320],
        [-0.3262,  1.1094,  0.5352, -0.5625, -0.7578],
        [ 0.0830,  0.6797, -0.0288, -0.5234, -0.5547],
        [-0.1074,  0.6094, -0.4277, -0.3086, -0.4453]], device='cuda:0',
       dtype=torch.bfloat16, grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)], [SequenceClassifierOutput(loss=tensor(1.9453, device='cuda:0', grad_fn=<NllLossBackward0>), logits=tensor([[ 0.1147,  1.0234, -0.0796, -0.3711, -0.2832],
        [ 0.4766,  0.5195, -0.9258, -0.6016, -0.2080],
        [-0.2119,  0.5625, -0.2832, -0.5898, -0.0991],
        [-0.0747,  0.4902,  0.0471, -0.6250, -1.1094],
        [-0.3086,  0.6094, -0.2354, -0.4219, -0.5039],
        [-0.0058,  0.7773, -0.1128, -0.4102, -0.4668],
        [-0.0315,  0.8477, -0.2793, -0.4492, -0.3945],
        [ 0.1191,  0.8828, -0.4609, -0.6250, -0.4492],
        [-0.1396,  1.1094,  0.1030, -0.3594, -0.3574],
        [-0.2344,  0.6641, -0.1050, -0.4941, -0.5859],
        [-0.1689,  0.3125,  0.1865, -0.0223, -0.4082],
        [-0.0496,  0.9531,  0.0908, -0.1973, -0.5977],
        [-0.0786,  0.6992, -0.1709, -0.4746, -0.3438],
        [ 0.3223,  0.0278, -0.6367, -0.4785,  0.1865],
        [-0.3887,  0.5234, -0.3320, -0.5156, -0.6953],
        [-0.1572,  0.9414, -0.0654, -0.3418, -0.2910]], device='cuda:0',
       dtype=torch.bfloat16, grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None), SequenceClassifierOutput(loss=tensor(1.9351, device='cuda:0', grad_fn=<NllLossBackward0>), logits=tensor([[ 0.2832,  0.9219, -0.1826, -0.2930, -0.2021],
        [-0.4199,  0.5977, -0.4355, -0.7383, -0.2617],
        [ 0.2432,  0.6992, -0.5625, -0.6445, -0.1611],
        [-0.0933,  0.5938, -0.3203, -0.2676, -0.6133],
        [-0.0156,  0.8398, -0.5820, -0.2295, -0.3008],
        [ 0.0728,  0.6992, -0.2207, -0.6797, -0.3203],
        [ 0.1836,  1.0859, -0.2754, -0.5898, -0.5625],
        [ 0.3066,  0.8125, -0.1396, -0.3398, -0.1904],
        [ 0.2168,  0.8086, -0.0815, -0.3730, -0.7930],
        [-0.3008,  0.5117, -0.2969, -0.4551, -0.6875],
        [-0.2773,  0.6172, -0.5898, -0.4512, -0.1748],
        [ 0.0305,  1.0312,  0.1484, -0.6133, -0.4180],
        [ 0.3125,  0.5234, -0.6680, -0.6133,  0.3145],
        [ 0.0082,  0.3574, -0.7812, -0.4727, -0.0374],
        [-0.2090,  0.5430,  0.1094, -0.1260, -0.5312],
        [-0.1172,  0.5312,  0.0452, -0.2354, -0.3496]], device='cuda:0',
       dtype=torch.bfloat16, grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None), SequenceClassifierOutput(loss=tensor(1.7896, device='cuda:0', grad_fn=<NllLossBackward0>), logits=tensor([[ 0.0192,  0.1465, -0.3184, -0.1270, -0.2891],
        [-0.2334,  1.0234, -0.3359, -0.5664, -0.5273],
        [-0.0820,  0.5977, -0.1260, -0.3555, -0.5352],
        [-0.1533,  0.8438, -0.2910, -0.3125, -0.4785],
        [-0.0718,  0.6602, -0.1445, -0.4551, -0.2969],
        [ 0.0879,  0.1846, -0.6602, -0.5312, -0.5508],
        [-0.1787,  0.3379, -0.1611, -0.0859, -0.2754],
        [-0.2793,  0.9375,  0.2246, -0.1367, -0.1768],
        [-0.1709,  0.2227, -0.1021, -0.1318, -0.2119],
        [-0.2598,  0.9805,  0.1206, -0.3066, -0.2793],
        [ 0.1055,  0.5664, -0.4648, -0.4395, -0.5391],
        [ 0.1445,  0.3594, -0.4844, -0.5234, -0.2246],
        [ 0.0806,  0.3164, -0.2471, -0.2236, -0.1348],
        [-0.0250,  0.8281, -0.3457, -0.6328, -0.1709],
        [-0.2402,  0.9141,  0.0801, -0.5586, -0.3945],
        [ 0.1348,  0.2773, -0.6562, -0.8477, -0.0383]], device='cuda:0',
       dtype=torch.bfloat16, grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None), SequenceClassifierOutput(loss=tensor(1.8984, device='cuda:0', grad_fn=<NllLossBackward0>), logits=tensor([[ 0.0364,  0.7109, -0.2832, -0.5312, -0.1807],
        [-0.0212,  0.9609, -0.2891, -0.2617, -0.1875],
        [ 0.0101,  0.6836, -0.3633, -0.7695, -0.5000],
        [ 0.0713,  0.7383, -0.2910, -0.4141, -0.4688],
        [-0.0752,  0.8203, -0.4414, -0.5781, -0.4141],
        [ 0.1621,  0.7617, -0.1904, -0.5234, -0.3359],
        [ 0.1099,  0.8828, -0.2314, -0.5039, -0.2412],
        [ 0.0771,  0.4004,  0.0271, -0.2891, -0.2227],
        [-0.1001,  0.7227, -0.2539, -0.5508, -0.1025],
        [ 0.4199,  0.6602, -0.2910, -0.3340, -0.4219],
        [-0.1699,  0.9961, -0.4258, -0.5898, -0.3906],
        [-0.1338,  0.5391, -0.3867, -0.4844, -0.3613],
        [ 0.1660,  0.2012, -0.2734, -0.0291, -0.2383],
        [-0.2598,  0.6836, -0.2891, -0.3379, -0.3477],
        [ 0.3008,  0.4629, -0.0104, -0.2930, -0.7656],
        [ 0.1465,  0.8320, -0.2910, -0.3672, -0.3262]], device='cuda:0',
       dtype=torch.bfloat16, grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None), SequenceClassifierOutput(loss=tensor(2.0815, device='cuda:0', grad_fn=<NllLossBackward0>), logits=tensor([[-0.0072,  1.0547, -0.0713, -0.5352, -0.2871],
        [ 0.2227,  1.1484,  0.0415, -0.3984, -0.3281],
        [-0.0728,  0.8438,  0.0674, -0.3926, -0.3496],
        [-0.0645,  0.6406, -0.2275, -0.4355, -0.1177],
        [-0.1943,  0.7461, -0.2402, -0.6758, -0.5625],
        [ 0.5078,  0.0173, -0.8164, -0.2305, -0.4785],
        [-0.2344,  0.9492, -0.4629, -0.8789, -0.7656],
        [ 0.3027,  0.2344, -0.7148, -0.5938,  0.2227],
        [-0.0102,  0.6719, -0.5469, -0.4238, -0.4043],
        [-0.1875,  0.7422, -0.3184, -0.5039, -0.6641],
        [ 0.0249,  0.6172, -0.0107, -0.4102, -0.5273],
        [-0.0635,  0.5430, -0.2422, -0.4473, -0.3691],
        [-0.0045,  0.9336, -0.1089, -0.6953, -0.4941],
        [ 0.0962,  0.8516, -0.1289, -0.4590, -0.3652],
        [-0.0659,  0.6172, -0.3398, -0.8281,  0.0077],
        [-0.1191,  0.6719, -0.4121, -0.5664, -0.7305]], device='cuda:0',
       dtype=torch.bfloat16, grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None), SequenceClassifierOutput(loss=tensor(1.4353, device='cuda:0', grad_fn=<NllLossBackward0>), logits=tensor([[-0.3164,  0.5664, -0.1445, -0.5273, -0.3457],
        [-0.0747,  0.6914, -0.3398, -0.5234, -0.3984],
        [ 0.2812,  1.1016,  0.0781, -0.1328, -0.4629],
        [-0.1807,  0.7969, -0.2910, -0.7812, -0.4570],
        [-0.2168,  1.0156, -0.3223, -0.1279, -0.5391],
        [ 0.2520,  0.7031, -0.4199, -0.1660, -0.2988],
        [-0.0391,  0.5938, -0.0815, -0.2197, -1.0156],
        [ 0.1084,  0.5625, -0.1377, -0.6016, -0.6328],
        [ 0.0903,  0.6055, -0.3848, -0.3438, -0.6016],
        [-0.0168,  0.6836, -0.2891, -0.5273, -0.7656],
        [ 0.1709,  0.7578, -0.2441, -0.4277, -0.6016],
        [-0.3574,  0.8945,  0.0131, -0.5000, -0.9258],
        [-0.0522,  0.8008,  0.1904, -0.3516, -0.5898],
        [ 0.0217,  0.2871, -0.1680, -0.3613, -0.6523],
        [-0.4180,  0.6758, -0.0188, -0.5664, -0.7500],
        [ 0.1992,  0.4570, -0.2393, -0.4258, -0.7422]], device='cuda:0',
       dtype=torch.bfloat16, grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None), SequenceClassifierOutput(loss=tensor(1.3098, device='cuda:0', grad_fn=<NllLossBackward0>), logits=tensor([[-0.0972,  0.5859, -0.5117, -0.7344, -0.3027],
        [-0.0059,  0.6484,  0.0654, -0.6758, -0.9492],
        [-0.4023,  0.5117,  0.1963, -0.5078, -0.4746],
        [-0.1592,  0.9453, -0.1592, -0.4375, -0.6016],
        [-0.2051,  0.6523,  0.2168, -0.4238, -0.6914],
        [-0.2617,  0.9102,  0.0723, -0.2617, -1.0312],
        [ 0.1816,  0.8125,  0.1553, -0.2354, -0.6094],
        [-0.0488,  0.6211,  0.1167, -0.4727, -1.0469],
        [-0.1543,  0.4121,  0.0840, -0.5742, -0.6680],
        [ 0.0278,  0.9844,  0.1279, -0.4062, -0.6367],
        [ 0.3613,  0.4062, -0.1211, -0.4492, -0.6641],
        [-0.4492,  0.6562, -0.1367, -0.7148, -0.6836],
        [-0.4102,  0.4590, -0.0297, -0.5078, -0.8750],
        [-0.1689,  0.6719, -0.0260, -0.3770, -0.5391],
        [-0.5000,  0.4805, -0.2539, -0.6953, -0.6641],
        [-0.3203,  0.6914,  0.1426, -0.2598, -0.5469]], device='cuda:0',
       dtype=torch.bfloat16, grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None), SequenceClassifierOutput(loss=tensor(0.9900, device='cuda:0', grad_fn=<NllLossBackward0>), logits=tensor([[-0.0654,  0.7422,  0.1699, -0.3555, -0.8867],
        [ 0.1128,  0.5156,  0.1465, -0.4219, -0.9961],
        [-0.2119,  0.6328,  0.2891, -0.5195, -0.6250],
        [-0.1387,  0.6914, -0.0238, -0.2676, -0.8945],
        [-0.1836,  0.4590,  0.1719, -0.2471, -0.5078],
        [-0.2070,  1.1641, -0.3770, -0.5039, -0.4414],
        [-0.2334,  0.9492,  0.5195, -0.4980, -0.8398],
        [-0.2148,  0.7383,  0.0043, -0.4258, -0.7930],
        [-0.3848,  0.9141,  0.2012, -0.7539, -0.9219],
        [-0.1514,  0.9297,  0.1035, -0.7617, -1.0859],
        [ 0.0289,  0.7422, -0.0107, -0.6719, -0.5547],
        [-0.0820,  0.4355, -0.0250, -0.3008, -0.4102],
        [-0.1475,  0.8008,  0.1523, -0.6758, -0.7852],
        [-0.1206,  0.4453,  0.1582, -0.5352, -0.8164],
        [-0.3105,  0.8164, -0.1030, -0.5820, -0.6914],
        [ 0.0024,  0.5547, -0.1201, -0.6602, -1.1406]], device='cuda:0',
       dtype=torch.bfloat16, grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None), SequenceClassifierOutput(loss=tensor(0.9841, device='cuda:0', grad_fn=<NllLossBackward0>), logits=tensor([[-0.3281,  0.6133, -0.1396, -0.7969, -0.9453],
        [ 0.0386,  0.9648,  0.1348, -0.4922, -0.3281],
        [-0.1060,  0.9844,  0.1680, -0.4688, -0.5898],
        [-0.0374,  0.8281, -0.2910, -0.8438, -0.4609],
        [-0.2324,  0.4121,  0.0129, -0.6211, -0.7422],
        [-0.2178,  0.7031,  0.3457, -0.4941, -1.0703],
        [-0.2539,  0.3809,  0.1094, -0.5273, -0.5664],
        [-0.2539,  0.8164, -0.0342, -0.6367, -0.6445],
        [-0.0068,  0.8945,  0.0603, -0.6250, -0.7695],
        [-0.2012,  0.8750, -0.0403, -0.4336, -0.5039],
        [-0.3672,  0.7930,  0.0334, -0.4082, -0.4883],
        [ 0.0146,  0.7344,  0.0493, -0.4316, -0.9102],
        [-0.1416,  0.6680,  0.1553, -0.7031, -0.4004],
        [-0.2734,  0.7969,  0.0913, -0.6523, -0.6172],
        [-0.0154,  0.9297, -0.1069, -0.5391, -0.5664],
        [-0.1738,  0.4863, -0.5625, -0.6797, -0.8203]], device='cuda:0',
       dtype=torch.bfloat16, grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None), SequenceClassifierOutput(loss=tensor(1.0771, device='cuda:0', grad_fn=<NllLossBackward0>), logits=tensor([[-0.0496,  0.4512, -0.0069, -0.6914, -0.7383],
        [-0.2969,  0.7578,  0.1055, -0.7031, -0.8789],
        [ 0.0388,  0.7578, -0.3594, -0.2305, -0.1904],
        [-0.1768,  0.7539,  0.1367, -0.5039, -0.4297],
        [-0.1099,  0.8516,  0.2393, -0.6211, -0.4531],
        [-0.2500,  0.6875,  0.1094, -0.2539, -1.0000],
        [ 0.2490,  0.7500,  0.0781, -0.3320, -0.6289],
        [-0.3633,  1.0859, -0.3613, -0.5039, -0.3047],
        [-0.1118,  0.7461, -0.0623, -0.8047, -0.8945],
        [-0.3633,  0.5391, -0.1260, -0.2402, -0.9375],
        [ 0.0952,  0.3203,  0.1514, -0.5938, -0.7695],
        [-0.2188,  0.6953,  0.0084, -0.1455, -0.7266],
        [-0.2246,  0.5195, -0.1689, -0.8477, -0.6562],
        [-0.0466,  0.9570, -0.1050, -0.0219, -0.3555],
        [-0.2559,  0.7188, -0.3340, -0.4238, -0.3594],
        [-0.1514,  0.9180, -0.0530, -0.3008, -0.6484]], device='cuda:0',
       dtype=torch.bfloat16, grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None), SequenceClassifierOutput(loss=tensor(1.2266, device='cuda:0', grad_fn=<NllLossBackward0>), logits=tensor([[-0.4648,  0.5000, -0.0981, -0.7305, -0.9414],
        [-0.1445,  0.7656,  0.0767, -0.2695, -0.8633],
        [-0.2334,  0.5703, -0.1230, -0.3965, -1.2344],
        [ 0.1514,  0.5781,  0.3457, -0.8047, -0.5039],
        [-0.2812,  0.9414,  0.1016, -0.6133, -0.7461],
        [-0.4258,  0.4219,  0.1021, -0.2988, -0.8750],
        [-0.2051,  0.6836, -0.2930, -0.1118, -0.8438],
        [ 0.0500,  0.7812, -0.3398, -0.5000, -0.4551],
        [-0.2285,  0.6680,  0.1396, -0.5625, -0.7422],
        [-0.1963,  0.6562, -0.2227, -0.4023, -0.3887],
        [ 0.1157,  0.7461,  0.1846, -0.3438, -0.7227],
        [-0.3398,  0.3965, -0.0359, -0.5664, -0.5586],
        [-0.2500,  0.5117, -0.0447, -0.1680, -0.5742],
        [ 0.0618,  1.3047, -0.0449, -0.6641, -0.6406],
        [-0.0728,  0.4258, -0.1914, -0.6016, -0.7266],
        [-0.0859,  0.1885, -0.3145, -0.9492, -0.2695]], device='cuda:0',
       dtype=torch.bfloat16, grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None), SequenceClassifierOutput(loss=tensor(1.1746, device='cuda:0', grad_fn=<NllLossBackward0>), logits=tensor([[-0.4258,  0.2256, -0.2559, -0.5977, -0.6875],
        [-0.0942,  0.7734, -0.1514, -0.2432, -0.6758],
        [-0.1040,  0.5781, -0.3809, -0.6523, -0.6992],
        [-0.3008,  0.5156, -0.0474, -0.8086, -0.6016],
        [-0.1748,  0.5547,  0.0025, -0.6992, -0.4785],
        [-0.3555,  0.8633,  0.2490, -0.8516, -0.9453],
        [-0.3379,  0.3477, -0.1299, -0.5977, -0.4648],
        [-0.2734,  0.8750,  0.2734, -0.4902, -0.8125],
        [-0.1953,  0.7461,  0.4102, -0.4492, -0.4785],
        [-0.2754,  0.7500, -0.1875, -0.5898, -1.0703],
        [-0.6016,  0.3945,  0.4062, -0.6055, -0.7773],
        [ 0.0327,  0.6719,  0.0293, -0.4453, -0.5234],
        [ 0.1855,  0.2910, -0.3438, -0.7109, -0.2988],
        [ 0.0115,  0.9648,  0.2832, -0.5898, -0.6602],
        [-0.2178,  0.9141,  0.0075, -0.7969, -0.6367],
        [-0.2539,  0.8086, -0.1680, -0.4336, -0.8320]], device='cuda:0',
       dtype=torch.bfloat16, grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None), SequenceClassifierOutput(loss=tensor(1.0859, device='cuda:0', grad_fn=<NllLossBackward0>), logits=tensor([[-0.4355,  0.6094, -0.1084, -0.3145, -0.5117],
        [-0.0476,  0.6680,  0.3418, -0.6328, -0.6914],
        [-0.0630,  0.4375,  0.0503, -0.1934, -1.0000],
        [-0.1260,  0.9961, -0.0199, -0.4141, -0.5508],
        [-0.1260,  1.1406,  0.1670, -0.4648, -0.4141],
        [-0.0378,  0.4355,  0.1543, -0.6562, -0.6797],
        [ 0.1104,  0.9219,  0.0317, -0.5977, -1.0234],
        [-0.1504,  0.6016, -0.1895, -0.5273, -0.6836],
        [-0.0332,  0.8164,  0.3652, -0.3770, -0.5547],
        [ 0.0864,  0.5234, -0.1348, -0.7930, -0.6328],
        [-0.3008,  0.8867,  0.1475, -0.2793, -0.5977],
        [ 0.0311,  0.3398, -0.0060, -0.4277,  0.1177],
        [-0.3574,  0.4746,  0.2578, -0.6719, -0.5742],
        [ 0.1699,  0.6641, -0.2832, -0.5312, -0.6484],
        [-0.1445,  0.5742,  0.4355, -0.4570, -0.5664],
        [-0.4043,  0.8398,  0.4238, -0.5820, -0.5117]], device='cuda:0',
       dtype=torch.bfloat16, grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None), SequenceClassifierOutput(loss=tensor(1.0571, device='cuda:0', grad_fn=<NllLossBackward0>), logits=tensor([[-0.0121,  0.9297, -0.3535, -0.1934, -0.4180],
        [ 0.1465,  0.7227, -0.0825, -0.6055, -0.5977],
        [-0.1846,  0.5352, -0.1455, -0.2285, -0.9570],
        [-0.2061,  0.5664,  0.1895, -0.4590, -0.6641],
        [ 0.0981,  0.8945,  0.1494, -0.4844, -0.7266],
        [-0.2227,  0.3535,  0.0659, -0.3887, -0.6211],
        [-0.0850,  0.2656,  0.1543, -0.4805, -0.3906],
        [-0.3535,  0.4336, -0.0209, -0.3633, -0.6445],
        [ 0.0087,  0.6562,  0.0957, -0.3398, -0.6719],
        [ 0.1582,  0.7344, -0.1133, -0.3398, -0.4180],
        [-0.4004,  0.4199, -0.0986, -0.6641, -0.8672],
        [-0.4004,  1.1094, -0.3984, -0.2285, -0.4375],
        [-0.5078,  0.6875, -0.0061, -0.4609, -0.5547],
        [ 0.7266,  0.1670, -0.3965, -0.4141,  0.2090],
        [-0.1992,  1.0781, -0.3398, -0.7422, -0.1846],
        [-0.2178,  0.7539, -0.1680, -0.4141, -0.6836]], device='cuda:0',
       dtype=torch.bfloat16, grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None), SequenceClassifierOutput(loss=tensor(1.0500, device='cuda:0', grad_fn=<NllLossBackward0>), logits=tensor([[-0.3867,  0.5781,  0.1152, -0.5820, -0.6836],
        [-0.1533,  0.7266, -0.0505, -0.2080, -0.9727],
        [-0.3652,  0.8086,  0.1182, -0.4551, -0.7852],
        [-0.0840,  0.7188,  0.0299, -0.2422, -0.4668],
        [-0.3086,  0.3574,  0.2168, -0.4414, -0.1934],
        [-0.4922,  0.8008, -0.0742, -0.3281, -0.5586],
        [ 0.1953,  0.5781,  0.1934, -0.4336, -0.7812],
        [-0.0747,  0.3613, -0.1240, -0.4922, -0.4473],
        [-0.2451,  0.7617, -0.1182, -0.8398, -0.8945],
        [ 0.0806,  0.4629,  0.1143, -0.6914, -0.6055],
        [-0.0425,  0.7656, -0.0127, -0.6172, -0.9219],
        [-0.2754,  0.6797,  0.0190, -0.3184, -0.7734],
        [-0.3574,  0.8750,  0.0737, -0.5000, -0.6758],
        [-0.0447,  0.7539,  0.1113, -0.8359, -0.7539],
        [-0.2422,  0.6797, -0.2490, -0.3711, -0.3594],
        [-0.1387,  0.8398, -0.4473, -0.7656, -0.9414]], device='cuda:0',
       dtype=torch.bfloat16, grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None), SequenceClassifierOutput(loss=tensor(1.0850, device='cuda:0', grad_fn=<NllLossBackward0>), logits=tensor([[-0.2061,  0.7227,  0.0771, -0.0654, -0.5234],
        [ 0.0405,  0.8047,  0.2070, -0.4453, -0.7539],
        [-0.2393,  0.6758, -0.3281, -0.5938, -0.6172],
        [ 0.0894,  0.7461, -0.1084, -0.5078, -0.6211],
        [ 0.0222,  0.4570,  0.0064, -0.4395, -1.0391],
        [-0.1079,  0.6172,  0.2412, -0.2227, -0.2715],
        [-0.0771,  0.5977, -0.2227, -0.7930, -1.2188],
        [-0.1260,  0.6172, -0.1235, -0.3223, -0.5000],
        [ 0.1187,  0.9531, -0.0244, -0.8828, -0.9141],
        [-0.0049,  0.5938, -0.0854, -0.3125, -0.5703],
        [-0.0718,  0.8398,  0.1582, -0.4824, -1.1172],
        [ 0.1816,  0.4219, -0.0898, -0.2275, -0.8125],
        [-0.1553,  0.4824, -0.1416, -0.5156, -0.6602],
        [-0.1113,  0.6406,  0.0111, -0.6914, -0.7969],
        [-0.4395,  0.8906,  0.2158, -0.4316, -0.6250],
        [-0.2773,  0.6016,  0.0535, -0.1206, -0.6836]], device='cuda:0',
       dtype=torch.bfloat16, grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None), SequenceClassifierOutput(loss=tensor(1.1172, device='cuda:0', grad_fn=<NllLossBackward0>), logits=tensor([[-4.3701e-02,  4.6484e-01,  2.0410e-01, -3.4375e-01, -4.7070e-01],
        [-2.7539e-01,  7.3438e-01,  5.0293e-02, -4.3164e-01, -7.1875e-01],
        [-2.0215e-01,  8.8672e-01, -4.3457e-02, -5.2344e-01, -6.2109e-01],
        [-6.8054e-03,  7.9297e-01,  2.2168e-01, -7.4219e-01, -6.4844e-01],
        [ 5.0391e-01,  3.4961e-01, -5.7422e-01, -5.9766e-01,  1.0742e-01],
        [ 3.6719e-01,  1.0312e+00,  1.6016e-01, -5.4688e-01, -6.7188e-01],
        [ 1.9409e-02,  8.2031e-01, -4.3945e-01, -7.3438e-01, -6.7969e-01],
        [-7.5195e-02,  7.4609e-01,  3.8574e-02, -7.4219e-01, -6.6016e-01],
        [ 2.8801e-04,  8.8672e-01, -3.6914e-01, -4.5312e-01, -2.3730e-01],
        [-4.1406e-01,  6.6016e-01,  1.3770e-01, -5.8203e-01, -1.0938e+00],
        [-2.9688e-01,  6.7969e-01, -1.5527e-01, -5.5078e-01, -7.8906e-01],
        [-1.8066e-01,  1.0781e+00, -1.3062e-02, -6.5918e-02, -5.1953e-01],
        [-1.7285e-01,  6.0547e-01, -2.5977e-01, -5.3906e-01, -1.1250e+00],
        [-1.3086e-01,  8.1641e-01, -5.5078e-01, -5.3906e-01,  4.2480e-02],
        [-2.7930e-01,  6.6016e-01,  3.6377e-02, -5.4688e-01, -6.7969e-01],
        [-1.6602e-01,  7.9688e-01, -2.1094e-01, -7.3438e-01, -5.1562e-01]],
       device='cuda:0', dtype=torch.bfloat16, grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None), SequenceClassifierOutput(loss=tensor(0.8743, device='cuda:0', grad_fn=<NllLossBackward0>), logits=tensor([[-0.2080,  0.3711,  0.0781, -0.5117, -0.5859],
        [ 0.1050,  0.9297, -0.0762, -0.7422, -1.0312],
        [-0.0139,  0.7461, -0.0364, -0.5898, -0.6523],
        [-0.3301,  0.8047,  0.0552, -0.7539, -1.0078],
        [ 0.3086,  1.0078, -0.3633, -0.3301, -0.2617],
        [-0.0403,  0.4238, -0.0535, -0.5312, -0.7148],
        [-0.1060,  0.9570, -0.2969, -0.7461, -0.0708],
        [-0.3125,  0.8633, -0.4902, -0.8125, -0.6211],
        [-0.0815,  1.0625, -0.1396, -0.4707, -0.6016],
        [-0.1416,  1.1797, -0.3027, -0.4023, -0.4180],
        [-0.0371,  0.9453, -0.2012, -0.4453, -0.3594],
        [-0.3633,  0.8828, -0.1377, -0.4707, -0.3730],
        [-0.0640,  1.0078, -0.0688, -0.3164, -0.5156],
        [-0.3438,  1.0156, -0.4062, -0.4082, -0.3047],
        [-0.3965,  0.6523, -0.0476, -0.3867, -0.9297],
        [-0.0026,  1.0781, -0.1914, -0.0295, -0.2100]], device='cuda:0',
       dtype=torch.bfloat16, grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None), SequenceClassifierOutput(loss=tensor(0.9985, device='cuda:0', grad_fn=<NllLossBackward0>), logits=tensor([[ 0.1299,  0.8789, -0.3789, -0.6562, -0.6836],
        [ 0.0491,  0.7227, -0.5117, -0.5273, -0.2891],
        [-0.2451,  0.5938, -0.2969, -0.4453, -0.8398],
        [-0.0091,  0.9023, -0.5547,  0.0298, -0.4258],
        [ 0.0957,  0.6758,  0.1992, -0.5312, -0.7656],
        [ 0.0811,  0.8242, -0.4473, -0.3672, -0.3789],
        [ 0.0111,  0.7891,  0.0596, -0.5703, -0.6484],
        [ 0.0120,  0.7070,  0.0044, -0.3379, -0.6328],
        [-0.4238,  0.6406, -0.2314, -0.2773, -0.6523],
        [-0.5859,  1.0000, -0.2422, -0.8672, -0.1875],
        [-0.2773,  0.3340, -0.4883, -0.2422, -0.6523],
        [ 0.1494,  0.8594, -0.2715, -0.4473, -0.6172],
        [-0.3184,  0.6562, -0.3340, -0.2676, -0.5820],
        [ 0.1011,  0.7656, -0.2871, -0.2734, -0.2852],
        [ 0.1650,  0.7461, -0.2910, -0.2715, -0.6367],
        [ 0.0121,  0.6953, -0.3848, -0.5273, -0.5195]], device='cuda:0',
       dtype=torch.bfloat16, grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None), SequenceClassifierOutput(loss=tensor(0.8054, device='cuda:0', grad_fn=<NllLossBackward0>), logits=tensor([[ 0.0330,  1.1641,  0.1001, -0.5430, -0.8398],
        [-0.0688,  0.9336, -0.2812, -0.4004, -0.4199],
        [-0.2754,  0.8047, -0.1406, -0.5117, -0.8125],
        [-0.2363,  0.9414, -0.0679, -0.4023, -0.4512],
        [-0.0635,  1.0547, -0.1016, -0.4688, -0.6836],
        [ 0.0034,  1.0625, -0.4121, -0.3164, -0.2754],
        [ 0.1875,  0.9570, -0.1719, -0.3125, -0.2539],
        [ 0.2617,  0.4863, -0.1660, -0.2422, -0.6523],
        [ 0.1006,  0.3516, -0.6836, -0.6055, -0.7305],
        [ 0.0110,  1.1172, -0.0840, -0.5430, -0.5430],
        [ 0.0630,  0.5977, -0.3320, -0.5625, -0.6914],
        [-0.1484,  0.9336, -0.2197, -0.5977, -0.0303],
        [ 0.0884,  1.0781, -0.1221, -0.3730, -0.4941],
        [-0.1074,  1.0703, -0.3066, -0.5117, -0.5664],
        [-0.0233,  1.0078, -0.1650, -0.4473, -0.4141],
        [ 0.0162,  1.1484, -0.0601, -0.5195, -0.2676]], device='cuda:0',
       dtype=torch.bfloat16, grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None), SequenceClassifierOutput(loss=tensor(1.1389, device='cuda:0', grad_fn=<NllLossBackward0>), logits=tensor([[ 0.1943,  0.6133, -0.4863, -0.4844, -0.6367],
        [-0.0684,  0.8945,  0.2676, -0.4629, -0.8320],
        [-0.1113,  1.1328, -0.2812, -0.1523, -0.5703],
        [-0.1953,  0.6562, -0.1387, -0.6914, -0.6211],
        [-0.2734,  0.4180, -0.3672, -0.4473, -0.6484],
        [ 0.0337,  0.8984, -0.1455, -0.4648, -0.3691],
        [-0.1543,  0.4512, -0.1816, -0.5156, -0.8008],
        [-0.0383,  1.2500, -0.3262, -0.4688, -0.2441],
        [-0.0830,  1.0625, -0.2148, -0.6875, -0.6328],
        [ 0.0425,  1.2031, -0.2637, -0.6328, -0.4824],
        [-0.0938,  0.4141,  0.0240, -0.4980, -0.8438],
        [-0.1680,  0.7656,  0.0400, -0.0850, -0.6289],
        [-0.1855,  0.7305,  0.1738, -0.3535, -0.4688],
        [-0.3379,  0.8750, -0.0845, -0.2793, -0.1099],
        [-0.3340,  0.8711,  0.0157, -0.7812, -0.5938],
        [-0.0952,  0.9766,  0.0226, -0.8242, -0.1895]], device='cuda:0',
       dtype=torch.bfloat16, grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None), SequenceClassifierOutput(loss=tensor(1.0129, device='cuda:0', grad_fn=<NllLossBackward0>), logits=tensor([[-0.0292,  0.9883, -0.2500, -0.5117, -0.4707],
        [-0.2773,  1.2578, -0.1924, -0.3750, -0.2578],
        [ 0.1089,  0.6250, -0.1758, -0.4277, -0.8516],
        [-0.2422,  0.8398, -0.2852, -0.4375, -0.2578],
        [-0.0942,  0.9766,  0.2490, -0.3262, -0.4180],
        [-0.2793,  1.0078, -0.0500, -0.3242, -0.3965],
        [ 0.1030,  1.1016, -0.4609, -0.3320, -0.4355],
        [ 0.1768,  0.4531, -0.0386, -0.3652, -0.7773],
        [ 0.2852,  1.0547, -0.1777, -0.4688, -0.2461],
        [ 0.4648,  1.1797,  0.1650, -0.3379, -0.7422],
        [ 0.0908,  0.7852, -0.4102, -0.4395, -0.2422],
        [ 0.1245,  0.7383, -0.1699, -0.1904, -0.4043],
        [-0.0092,  0.3047, -0.6719, -0.3164,  0.1074],
        [ 0.0110,  1.0000, -0.2285, -0.1226, -0.2637],
        [-0.1416,  1.0234, -0.3965, -0.2695, -0.3320],
        [ 0.1777,  1.1484, -0.3926, -0.5664, -0.2754]], device='cuda:0',
       dtype=torch.bfloat16, grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None), SequenceClassifierOutput(loss=tensor(0.9800, device='cuda:0', grad_fn=<NllLossBackward0>), logits=tensor([[-0.0194,  1.3438,  0.0287, -0.7188, -0.3398],
        [-0.0156,  0.8672,  0.0386, -0.3438, -1.0000],
        [ 0.1875,  0.9414, -0.2314, -0.4297, -0.2451],
        [-0.0811,  0.9023, -0.5078, -0.4668, -0.2969],
        [ 0.1543,  0.8125, -0.2891, -0.5312, -0.7930],
        [-0.0649,  0.9297, -0.1787, -0.4688,  0.0879],
        [-0.1084,  0.6406, -0.6523, -0.4004, -0.3438],
        [-0.0043,  1.0469, -0.1289, -0.4941, -0.0972],
        [ 0.1143,  0.7266,  0.0786, -0.2949, -0.6992],
        [ 0.1553,  1.0078, -0.5234, -0.3398, -0.3555],
        [-0.3340,  0.6914, -0.4688, -0.4590, -0.4238],
        [ 0.1631,  0.9375, -0.5664, -0.5977, -0.3438],
        [-0.0016,  0.8008, -0.3359, -0.4141, -0.5352],
        [-0.0879,  0.5078, -0.1069, -0.3184, -0.3320],
        [-0.0986,  1.0391, -0.4180, -0.4316, -0.1289],
        [ 0.0796,  0.9453,  0.1904, -0.4336, -0.7266]], device='cuda:0',
       dtype=torch.bfloat16, grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None), SequenceClassifierOutput(loss=tensor(0.7961, device='cuda:0', grad_fn=<NllLossBackward0>), logits=tensor([[-0.3145,  0.9180, -0.4375, -0.2637, -0.4238],
        [-0.0640,  0.7188,  0.2207, -0.5117, -0.8047],
        [ 0.1025,  1.0703, -0.3828, -0.4160, -0.2871],
        [-0.0854,  0.8984, -0.1738, -0.4785, -1.0156],
        [ 0.2256,  1.0312, -0.0835, -0.6367, -0.2773],
        [ 0.1992,  1.1797, -0.0374, -0.7344, -0.6406],
        [-0.0062,  0.8477, -0.1553, -0.2139, -0.0552],
        [-0.0586,  0.9961, -0.3574, -0.1914, -0.3438],
        [ 0.0884,  1.2422, -0.1611, -0.0498, -0.3945],
        [-0.1562,  0.8555, -0.3613, -0.7812, -0.3730],
        [-0.0918,  0.7539, -0.4941, -0.3750, -0.7422],
        [-0.0581,  0.9727,  0.1416, -0.2715, -0.2812],
        [-0.1611,  0.8242, -0.3047, -0.5273, -0.6562],
        [-0.2617,  1.1250, -0.3223, -0.3848, -0.2266],
        [-0.1904,  0.7969, -0.0471, -0.4258, -0.4121],
        [ 0.1738,  0.9766, -0.0742, -0.1201, -0.3906]], device='cuda:0',
       dtype=torch.bfloat16, grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None), SequenceClassifierOutput(loss=tensor(1.1169, device='cuda:0', grad_fn=<NllLossBackward0>), logits=tensor([[-0.2285,  0.8828, -0.0265, -0.3809, -0.8906],
        [-0.7383,  0.6367,  0.0125, -0.6328, -0.3535],
        [-0.1973,  0.6133, -0.1934, -0.6328, -1.0234],
        [-0.2119,  1.1406, -0.3926, -0.3652, -0.7734],
        [-0.2383,  0.7305,  0.3047, -0.5117, -0.5938],
        [-0.2598,  0.7305, -0.1895, -0.6211, -0.3691],
        [-0.0469,  1.0391, -0.3203, -0.4727, -0.2910],
        [ 0.6641,  0.4355, -0.6797,  0.1084,  0.3535],
        [ 0.1011,  0.7852, -0.2617, -0.8008, -0.4434],
        [-0.2275,  0.5898,  0.0908, -0.0459, -0.7656],
        [ 0.1074,  0.9531, -0.5312, -0.7109, -0.1494],
        [ 0.0043,  0.8125, -0.2451, -0.4062, -0.5000],
        [-0.0962,  1.1172, -0.2656, -0.6953, -0.3125],
        [-0.0742,  0.6836, -0.4883,  0.0317, -0.3516],
        [ 0.2832,  1.2109, -0.3301, -0.3555, -0.3652],
        [-0.3516,  1.1406, -0.0928, -0.3906, -0.6367]], device='cuda:0',
       dtype=torch.bfloat16, grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None), SequenceClassifierOutput(loss=tensor(0.9011, device='cuda:0', grad_fn=<NllLossBackward0>), logits=tensor([[-0.1030,  0.7383, -0.2041, -0.3320, -0.7617],
        [-0.3262,  0.6875, -0.2559, -0.5352, -0.5586],
        [-0.0613,  0.8242, -0.2383, -0.4629, -0.8945],
        [ 0.1992,  0.8438, -0.4863, -0.4531, -0.3145],
        [ 0.1406,  1.0000, -0.4258, -0.4473, -0.2148],
        [ 0.0454,  1.1328, -0.3066, -0.5156, -0.3047],
        [ 0.0806,  1.0469, -0.3887, -0.3770, -0.4531],
        [-0.2295,  0.9219, -0.2910, -0.3809, -0.6484],
        [ 0.2109,  1.0625, -0.2090, -0.2656, -0.4160],
        [-0.0752,  0.7148,  0.0513, -0.5195, -0.8594],
        [-0.0062,  1.2344, -0.1826, -0.1865, -0.4160],
        [-0.1074,  0.9648, -0.3652, -0.6250, -0.1865],
        [ 0.1396,  0.9805, -0.1787, -0.0525, -0.3652],
        [-0.3223,  0.8086, -0.1245, -0.6523, -0.4590],
        [ 0.1416,  0.9570, -0.2051, -0.2432, -0.6367],
        [ 0.0996,  0.9219, -0.5625, -0.7305, -0.3730]], device='cuda:0',
       dtype=torch.bfloat16, grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None), SequenceClassifierOutput(loss=tensor(0.8711, device='cuda:0', grad_fn=<NllLossBackward0>), logits=tensor([[-0.0845,  1.0391, -0.1602, -0.4941, -0.5508],
        [ 0.0537,  0.6875,  0.0344, -0.5586, -1.1406],
        [-0.0259,  0.9375, -0.4375, -0.5859, -0.2930],
        [-0.0708,  0.9258,  0.1611, -0.5625, -0.9102],
        [ 0.0728,  0.9180, -0.0233, -0.3105,  0.0576],
        [-0.1367,  0.6016,  0.0674, -0.2852, -0.5312],
        [-0.0253,  1.1406, -0.1660, -0.3945, -0.3535],
        [-0.1060,  0.5195, -0.3242, -0.0586, -0.6172],
        [ 0.0469,  0.9922, -0.0869, -0.2852, -0.1670],
        [ 0.3066,  0.8047,  0.1279, -0.5469, -0.6719],
        [-0.3496,  0.8828, -0.0703, -0.4609, -0.3672],
        [ 0.1299,  1.0625, -0.4219, -0.2314, -0.5898],
        [-0.0179,  1.1562, -0.2461, -0.7422, -0.5352],
        [-0.1709,  1.0469, -0.1338, -0.3730, -0.2871],
        [ 0.0427,  0.9961,  0.3242, -0.6016, -0.6914],
        [-0.2832,  0.6719, -0.0535, -0.6523, -0.7227]], device='cuda:0',
       dtype=torch.bfloat16, grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None), SequenceClassifierOutput(loss=tensor(0.9385, device='cuda:0', grad_fn=<NllLossBackward0>), logits=tensor([[-0.4395,  1.1250, -0.3828, -0.3535, -0.5586],
        [-0.2793,  0.6758, -0.4414, -0.4648, -0.3457],
        [-0.0591,  1.1641, -0.3301, -0.3652, -0.5938],
        [-0.0088,  0.6445, -0.4629, -0.4980, -0.3594],
        [ 0.1455,  0.9453, -0.2637, -0.6562, -0.3164],
        [ 0.0557,  0.9531, -0.1514, -0.5352, -0.4375],
        [ 0.1226,  1.3750, -0.1260, -0.4609, -0.2490],
        [ 0.1128,  0.7617, -0.0923, -0.4297, -0.3086],
        [ 0.0188,  0.9727, -0.3574, -0.4863, -0.2070],
        [-0.4082,  0.7070,  0.2070, -0.5625, -0.5469],
        [-0.1885,  0.7539, -0.0869, -0.3867, -1.0391],
        [-0.3926,  0.8125, -0.2891, -0.1582, -0.5469],
        [ 0.1006,  0.9375, -0.1455, -0.4492, -0.3262],
        [ 0.0129,  0.8906,  0.0305, -0.4980, -0.8203],
        [-0.3027,  0.7578, -0.4102, -0.1611, -0.6133],
        [-0.1738,  0.9414,  0.1309, -1.0312, -0.9336]], device='cuda:0',
       dtype=torch.bfloat16, grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None), SequenceClassifierOutput(loss=tensor(0.7944, device='cuda:0', grad_fn=<NllLossBackward0>), logits=tensor([[-0.0308,  1.4766, -0.0232, -0.5234, -0.2520],
        [-0.0361,  0.8594, -0.2559, -0.0518, -0.4668],
        [ 0.3086,  1.0703, -0.3203, -0.3809, -0.3906],
        [ 0.4121,  0.9961, -0.3770, -0.6562, -0.3594],
        [-0.2598,  1.2656, -0.0879, -0.2598, -0.3770],
        [-0.0315,  0.8359, -0.5430, -0.6367, -0.2598],
        [ 0.0781,  0.5938, -0.1338, -0.3848, -0.5898],
        [ 0.0625,  1.1094, -0.2422, -0.1943, -0.4902],
        [-0.1797,  0.9531, -0.3047, -0.1641, -0.4062],
        [ 0.2256,  1.0547, -0.1709, -0.3711, -0.5898],
        [-0.0630,  1.1641, -0.0036, -0.4453, -0.2930],
        [-0.0762,  1.0938, -0.2188, -0.5820, -0.3730],
        [ 0.2754,  1.3125, -0.2070, -0.7539, -0.1865],
        [ 0.1865,  0.9727, -0.3340, -0.7148, -0.3965],
        [-0.1797,  0.8672, -0.2363, -0.2695, -0.2520],
        [-0.2988,  0.8984, -0.6328, -0.5898, -0.7109]], device='cuda:0',
       dtype=torch.bfloat16, grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None), SequenceClassifierOutput(loss=tensor(1.0471, device='cuda:0', grad_fn=<NllLossBackward0>), logits=tensor([[-0.4043,  0.8945, -0.1641, -0.5977, -0.9453],
        [-0.0393,  0.7500, -0.4043, -0.2236, -0.5117],
        [ 0.1533,  1.0391,  0.0415, -0.5742, -0.3262],
        [-0.1445,  0.6836,  0.2451, -0.6641, -0.9688],
        [ 0.3379,  0.6016, -0.4961, -0.4277, -0.4434],
        [-0.2305,  0.8555,  0.0454, -0.4082, -0.6680],
        [-0.1865,  0.5742, -0.3320, -0.7188, -0.4570],
        [ 0.0918,  0.9922, -0.4277, -0.3320, -0.3398],
        [ 0.1104,  0.6875, -0.3750, -0.6289, -0.7383],
        [ 0.0383,  1.2031, -0.1328, -0.3730, -0.2891],
        [ 0.0698,  1.2578,  0.1157, -0.3125, -0.5625],
        [ 0.1147,  1.2188, -0.4121, -0.2451, -0.1846],
        [ 0.0947,  0.8906, -0.4609, -0.4941, -0.4316],
        [-0.2891,  0.8438, -0.3008, -0.4219, -0.5273],
        [-0.1934,  0.5977, -0.1494, -0.1270, -0.7031],
        [ 0.2227,  0.8984, -0.6406, -0.7148, -0.3027]], device='cuda:0',
       dtype=torch.bfloat16, grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None), SequenceClassifierOutput(loss=tensor(0.9224, device='cuda:0', grad_fn=<NllLossBackward0>), logits=tensor([[-0.2148,  1.1719, -0.5703, -0.1108, -0.1787],
        [-0.0564,  0.9492, -0.4531, -0.5664, -0.0669],
        [-0.1250,  1.0781, -0.3105, -0.5898, -0.5117],
        [-0.1079,  1.1094, -0.5352, -0.6094, -0.4355],
        [-0.1084,  0.8594, -0.5586, -0.2041, -0.4688],
        [-0.1934,  1.3438, -0.5820, -0.5781, -0.1943],
        [-0.0674,  0.9609, -0.2539, -0.5039, -0.3809],
        [ 0.0410,  0.7969, -0.4512, -0.6953, -0.2930],
        [-0.3945,  0.8242, -0.1367, -0.2969, -0.4238],
        [ 0.1138,  0.8477, -0.3223, -0.5781, -0.4102],
        [ 0.1475,  1.0391, -0.2773, -0.2949, -0.2139],
        [ 0.0703,  1.0391, -0.3789, -0.3672, -0.2871],
        [ 0.0977,  0.5820, -0.5742, -0.5273, -0.3223],
        [-0.1040,  0.8906, -0.1943, -0.5469, -0.5312],
        [ 0.3418,  1.1250, -0.3438, -0.6328, -0.4316],
        [-0.1367,  0.7773, -0.1729, -0.2949, -0.2412]], device='cuda:0',
       dtype=torch.bfloat16, grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None), SequenceClassifierOutput(loss=tensor(1.0647, device='cuda:0', grad_fn=<NllLossBackward0>), logits=tensor([[-2.6172e-01,  7.7734e-01, -2.2266e-01, -6.2500e-01, -1.0000e+00],
        [-6.4941e-02,  8.9844e-01, -2.5000e-01, -4.1211e-01, -3.4766e-01],
        [ 2.1680e-01,  9.1797e-01, -2.2754e-01, -3.3789e-01, -6.9922e-01],
        [-2.6562e-01,  8.8281e-01, -2.4609e-01, -4.7461e-01, -3.3008e-01],
        [ 8.9844e-02,  9.0234e-01, -5.0000e-01, -6.0156e-01, -6.7871e-02],
        [-3.8086e-01,  1.1484e+00, -3.4180e-02, -6.2500e-01, -1.5039e-01],
        [-6.8054e-03,  9.0625e-01, -3.5400e-02, -7.7734e-01, -4.4727e-01],
        [-4.4922e-01,  7.8906e-01, -5.7373e-02, -5.8203e-01, -8.2422e-01],
        [-3.7305e-01,  4.7070e-01, -2.2070e-01, -7.2656e-01, -6.6797e-01],
        [-1.2891e-01,  6.7188e-01,  3.6865e-02, -1.4844e-01, -4.0820e-01],
        [ 4.1797e-01,  1.8848e-01, -6.0547e-01, -3.8086e-01,  4.5898e-02],
        [ 3.0273e-01,  1.0938e+00, -2.9297e-01, -5.4688e-01, -4.0234e-01],
        [ 1.3281e-01,  8.9453e-01, -3.8086e-01, -2.4609e-01, -3.4961e-01],
        [-1.7071e-04,  5.8594e-01, -2.6758e-01, -5.0000e-01, -5.2734e-01],
        [ 4.0039e-02,  9.2188e-01, -6.0156e-01, -8.3496e-02, -1.5430e-01],
        [ 1.9043e-02,  1.0938e+00,  1.7871e-01, -2.1387e-01, -5.0781e-01]],
       device='cuda:0', dtype=torch.bfloat16, grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None), SequenceClassifierOutput(loss=tensor(1.0479, device='cuda:0', grad_fn=<NllLossBackward0>), logits=tensor([[ 0.3418,  1.1875, -0.2871, -0.2441, -0.3691],
        [ 0.1982,  0.7656, -0.1953, -0.8242, -0.4102],
        [ 0.1963,  0.8594, -0.2051, -0.6602, -0.5898],
        [-0.2207,  1.1484, -0.5820, -0.4141, -0.3750],
        [-0.2285,  1.0078, -0.1260, -0.4336, -0.3320],
        [-0.1787,  1.0312, -0.2188, -0.3477, -0.5117],
        [ 0.1934,  1.3438, -0.3926, -1.0234,  0.0203],
        [-0.4336,  0.8438, -0.2949,  0.0093, -0.6445],
        [ 0.2285,  0.9297, -0.3184, -0.2617, -0.3262],
        [-0.1543,  1.0078, -0.7109, -0.0747, -0.2656],
        [ 0.2539,  1.0781, -0.2793, -0.7305, -0.3984],
        [ 0.2715,  0.9922, -0.2598, -0.4336, -0.0625],
        [ 0.0840,  0.9297, -0.3125, -0.4922, -0.3730],
        [ 0.1992,  0.9336, -0.0835, -0.2402, -0.6133],
        [-0.0310,  1.1406, -0.2090, -0.5117, -0.4453],
        [ 0.1006,  0.7578, -0.1875, -0.4629, -0.4883]], device='cuda:0',
       dtype=torch.bfloat16, grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None), SequenceClassifierOutput(loss=tensor(0.9114, device='cuda:0', grad_fn=<NllLossBackward0>), logits=tensor([[-0.1484,  0.9492, -0.2617, -0.4980, -0.5508],
        [ 0.0461,  1.1016, -0.4492, -0.3789, -0.0640],
        [-0.0918,  1.2422, -0.3340, -0.7344, -0.2930],
        [-0.0182,  0.9414, -0.1001, -0.9727, -0.7305],
        [-0.0476,  0.7188, -0.2217, -0.4160, -0.7812],
        [-0.1748,  0.8711, -0.2051, -0.3809, -0.3516],
        [-0.1270,  1.0312, -0.1206, -0.0020, -0.1650],
        [ 0.1494,  1.0312, -0.1992, -0.6797, -0.2617],
        [ 0.3945,  0.6367, -0.4316, -0.2559, -0.5820],
        [-0.1943,  0.9219, -0.1689, -0.6172, -0.8203],
        [-0.2031,  0.6602, -0.2734, -0.6094, -0.6250],
        [ 0.0554,  0.9414, -0.1484, -0.3535, -0.4551],
        [-0.3145,  0.7578,  0.1494,  0.0198, -0.7070],
        [-0.3418,  0.8594, -0.2676, -0.1982, -0.5391],
        [ 0.1826,  0.9766,  0.0815, -0.6016, -0.1865],
        [-0.0913,  0.5039, -0.4492, -0.6445, -0.5820]], device='cuda:0',
       dtype=torch.bfloat16, grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None), SequenceClassifierOutput(loss=tensor(1.0659, device='cuda:0', grad_fn=<NllLossBackward0>), logits=tensor([[-0.2246,  1.2422, -0.1660, -0.3262, -0.5234],
        [ 0.0850,  0.8320, -0.1357, -0.6562, -0.6797],
        [ 0.2578,  1.0078, -0.2490, -0.4863, -0.4531],
        [-0.1367,  0.9141,  0.0693, -0.2129, -0.4492],
        [ 0.0334,  1.1484, -0.2197, -0.5391, -0.3945],
        [ 0.0576,  0.8555, -0.2432, -0.4336, -0.0825],
        [-0.1206,  1.4062, -0.2871, -0.4082, -0.2256],
        [-0.3008,  0.6523, -0.1230, -0.6836, -0.6680],
        [ 0.0654,  0.9609, -0.2363, -0.4980, -0.0033],
        [-0.1670,  1.0156, -0.4297, -0.5625, -0.1807],
        [ 0.1167,  0.7344, -0.1836, -0.5820, -0.7969],
        [-0.0762,  1.0547, -0.1602, -0.5000, -0.2178],
        [ 0.2354,  0.2793, -0.3652, -0.1309, -0.5000],
        [ 0.1963,  0.9688, -0.3301, -0.5000, -0.3125],
        [-0.1338,  1.0469, -0.4785, -0.7695, -0.5234],
        [-0.1377,  0.9102, -0.2031, -0.4707, -0.9570]], device='cuda:0',
       dtype=torch.bfloat16, grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None), SequenceClassifierOutput(loss=tensor(1.0623, device='cuda:0', grad_fn=<NllLossBackward0>), logits=tensor([[-0.3809,  1.1562, -0.1621, -0.6445, -0.4609],
        [-0.0284,  0.6602, -0.5586, -0.2852, -0.1167],
        [-0.2393,  1.0625, -0.1216, -0.4824, -0.3281],
        [-0.2129,  1.1953, -0.0549, -0.6055, -0.4258],
        [-0.1992,  1.1797, -0.3496, -0.2539, -0.3242],
        [-0.3184,  0.6367, -0.2031, -0.2266, -0.4141],
        [-0.0109,  1.1719, -0.0669, -0.5117, -0.2812],
        [-0.0952,  0.9766, -0.0126, -0.2910, -0.3262],
        [-0.2178,  1.0000, -0.2832, -0.3359, -0.3730],
        [ 0.0154,  1.4609, -0.2422, -0.6953, -0.5781],
        [ 0.0510,  0.9062, -0.5195, -0.4609, -0.2520],
        [ 0.0618,  1.1875, -0.0388, -0.5117, -0.2676],
        [ 0.2139,  0.6211,  0.0693, -0.2793, -0.2451],
        [-0.0654,  0.7422, -0.4160, -0.8672, -0.3945],
        [ 0.0947,  0.9023, -0.5820, -0.6758, -0.4141],
        [-0.0898,  0.9062, -0.1572, -0.4512, -0.1758]], device='cuda:0',
       dtype=torch.bfloat16, grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None), SequenceClassifierOutput(loss=tensor(0.9038, device='cuda:0', grad_fn=<NllLossBackward0>), logits=tensor([[ 0.2559,  1.1484, -0.2354, -0.5391, -0.5938],
        [-0.4121,  0.8555, -0.0835, -0.6367, -0.3145],
        [-0.3047,  0.7266,  0.4570, -0.3184, -1.0312],
        [-0.0452,  1.0703, -0.2012, -0.6133, -0.3027],
        [ 0.3906,  0.0854, -0.5898, -0.2578,  0.4961],
        [-0.4062,  0.4492, -0.0184, -0.5938, -0.4297],
        [ 0.3203,  1.1562, -0.3945, -0.3281, -0.2578],
        [ 0.0034,  0.7070,  0.0981, -0.4160, -0.5391],
        [ 0.1963,  1.0859, -0.2139, -0.2500, -0.3574],
        [-0.0053,  0.6445, -0.1885, -0.3184, -1.1484],
        [-0.0869,  0.7578, -0.0128, -0.6641, -0.8867],
        [ 0.0903,  0.9805, -0.1387, -0.4941, -0.2969],
        [ 0.0630,  1.0859, -0.3145, -0.4570, -0.3574],
        [-0.1045,  0.9688, -0.0269, -0.3613, -0.2676],
        [ 0.2139,  1.1016, -0.5078, -0.5703, -0.0933],
        [-0.0613,  1.1484, -0.2676, -0.4805, -0.4023]], device='cuda:0',
       dtype=torch.bfloat16, grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None), SequenceClassifierOutput(loss=tensor(1.0198, device='cuda:0', grad_fn=<NllLossBackward0>), logits=tensor([[-0.1455,  0.8320, -0.3086, -0.3730, -0.6328],
        [ 0.0170,  1.0000, -0.1182, -0.4844, -0.5938],
        [ 0.0259,  0.7305, -0.2344, -0.4922, -0.7305],
        [-0.3086,  0.4805,  0.0947, -0.5781, -1.1250],
        [-0.0645,  1.2734, -0.2354, -0.6328, -0.4941],
        [-0.3828,  0.7227, -0.5039, -0.0255, -0.3555],
        [ 0.0796,  0.9258, -0.2656, -0.6055, -0.2637],
        [ 0.2500,  0.9336, -0.3262, -0.4473, -0.5703],
        [ 0.3652,  0.9883, -0.3184, -0.3359, -0.5859],
        [-0.1494,  0.7695,  0.0796, -0.7266, -0.6641],
        [ 0.0928,  1.1719, -0.3535, -0.6836, -0.4160],
        [-0.0356,  1.0000,  0.0530, -0.6289, -0.4043],
        [ 0.0850,  0.9805, -0.3496, -0.5312, -0.4922],
        [ 0.0311,  0.8359, -0.2334, -0.3965, -0.5859],
        [ 0.0825,  0.9766, -0.0786, -0.4648, -0.5195],
        [-0.2080,  1.0938, -0.1187, -0.7500, -0.3359]], device='cuda:0',
       dtype=torch.bfloat16, grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None), SequenceClassifierOutput(loss=tensor(1.2341, device='cuda:0', grad_fn=<NllLossBackward0>), logits=tensor([[ 2.9297e-01,  8.4375e-01, -3.4961e-01, -9.6191e-02, -4.4922e-01],
        [-3.7500e-01,  4.7852e-01, -1.1230e-01, -4.8242e-01, -8.8281e-01],
        [ 1.0303e-01,  9.5312e-01, -3.5938e-01, -3.0078e-01,  1.0157e-04],
        [ 2.3730e-01,  1.0156e+00,  6.6895e-02, -2.1484e-01, -6.4844e-01],
        [-1.2256e-01,  1.1484e+00, -6.9580e-03, -8.3984e-02, -3.5352e-01],
        [ 1.7285e-01,  7.0312e-01, -2.7148e-01, -4.5117e-01, -5.2344e-01],
        [-3.5547e-01,  8.9062e-01, -4.4336e-01, -2.0020e-01, -4.7852e-01],
        [-1.5039e-01,  1.0625e+00, -2.0801e-01, -7.3047e-01, -6.6016e-01],
        [ 4.3945e-02,  8.5156e-01,  2.8564e-02, -5.8984e-01, -3.8086e-01],
        [-4.3945e-01,  1.0312e+00, -2.5781e-01, -1.1768e-01, -4.8047e-01],
        [-1.7334e-02,  1.1562e+00, -2.6367e-01, -4.9609e-01, -3.7500e-01],
        [ 2.9492e-01,  1.0859e+00, -4.1992e-01, -6.2500e-01, -3.1445e-01],
        [-3.2959e-02,  1.0781e+00, -4.1211e-01, -6.1719e-01, -3.4766e-01],
        [-8.6914e-02,  5.0391e-01,  1.8164e-01, -5.5078e-01, -6.3672e-01],
        [-5.6396e-02,  1.0391e+00, -5.8838e-02, -4.5898e-01, -3.6523e-01],
        [-1.3867e-01,  9.9219e-01, -2.2852e-01, -5.0781e-01, -8.9062e-01]],
       device='cuda:0', dtype=torch.bfloat16, grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None), SequenceClassifierOutput(loss=tensor(1.0486, device='cuda:0', grad_fn=<NllLossBackward0>), logits=tensor([[ 0.0947,  0.8555, -0.6445, -0.4766,  0.0708],
        [ 0.0500,  0.8633, -0.2148, -0.2695, -0.3945],
        [-0.0075,  1.3828, -0.1523, -0.8633, -0.3086],
        [-0.0118,  1.2266, -0.4199, -0.8945, -0.5977],
        [ 0.2539,  1.0391, -0.3398, -0.6172, -0.6133],
        [-0.0405,  1.3203, -0.3438, -0.7969, -0.6875],
        [ 0.2246,  1.2812, -0.2891, -0.6445, -0.2520],
        [-0.1924,  0.8594,  0.0825, -0.6914, -0.7500],
        [-0.1553,  0.9766, -0.2578, -0.5586, -0.7539],
        [-0.0283,  0.7734, -0.3984, -0.8008, -0.3105],
        [ 0.3535,  0.9766, -0.6367, -0.7422, -0.4082],
        [ 0.0664,  0.8320,  0.2354, -0.1572, -1.1328],
        [ 0.0444,  1.0469, -0.0737, -0.0090, -0.4824],
        [ 0.1514,  0.9141, -0.0208, -0.3906, -0.4668],
        [-0.0025,  0.9688, -0.1895, -0.6602, -1.0234],
        [-0.3203,  1.0312, -0.3848, -0.4297, -0.5781]], device='cuda:0',
       dtype=torch.bfloat16, grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None), SequenceClassifierOutput(loss=tensor(0.9204, device='cuda:0', grad_fn=<NllLossBackward0>), logits=tensor([[ 0.1318,  0.8164, -0.2832, -0.3730, -0.5156],
        [-0.2021,  0.8359,  0.0835, -0.6602, -0.8906],
        [-0.0654,  0.7852, -0.2305, -0.1797, -0.4238],
        [ 0.2012,  0.8516, -0.2578, -0.4805, -0.6953],
        [-0.0400,  1.0391, -0.2461, -0.5352, -0.2256],
        [-0.2012,  0.9102, -0.2891, -0.2910, -0.3750],
        [ 0.3340,  0.7852, -0.3887, -0.4902, -0.0566],
        [ 0.0020,  0.8828, -0.6836, -0.2949, -0.1758],
        [ 0.1250,  1.3750,  0.1982, -0.5586, -0.4668],
        [-0.0194,  0.8125, -0.2217, -0.5312, -0.3770],
        [ 0.1289,  0.7969, -0.3262, -0.6953, -0.4727],
        [-0.0608,  0.6719, -0.0039, -0.3594, -0.5195],
        [-0.0366,  1.1094,  0.0266, -0.6250, -0.4883],
        [-0.1689,  0.7422, -0.1187, -0.4902, -0.7812],
        [-0.1357,  1.0625, -0.2217, -0.6445, -0.3730],
        [-0.1572,  0.8516, -0.5508, -0.7578, -0.6523]], device='cuda:0',
       dtype=torch.bfloat16, grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None), SequenceClassifierOutput(loss=tensor(1.0532, device='cuda:0', grad_fn=<NllLossBackward0>), logits=tensor([[-0.0325,  1.3359, -0.4336, -0.6094, -0.2656],
        [-0.0045,  1.1484, -0.4590, -0.4023, -0.2061],
        [-0.0302,  0.9375, -0.0287, -0.3203, -0.5039],
        [ 0.0273,  0.7930, -0.0864, -0.5625, -0.0194],
        [ 0.4941,  0.6250, -0.4785, -0.5625,  0.4961],
        [-0.0532,  1.1953, -0.8320, -0.3691, -0.2676],
        [-0.2930,  0.8359, -0.2168, -0.2734, -0.8828],
        [ 0.2891,  0.5391, -0.3340, -0.5234, -0.1738],
        [ 0.0840,  0.9609, -0.2432, -0.5703, -0.2344],
        [-0.1357,  0.9336, -0.1094, -0.9258, -0.7969],
        [ 0.2246,  1.0000, -0.5469, -0.2334, -0.4805],
        [ 0.1543,  0.8594, -0.3301, -0.4082, -0.5938],
        [-0.2305,  0.7148,  0.3418, -0.4492, -0.8750],
        [-0.1758,  1.5078, -0.2285, -0.4395, -0.0564],
        [ 0.0198,  1.0781, -0.4844, -0.4961, -0.3770],
        [ 0.0918,  1.0703, -0.2080, -0.6875, -0.7227]], device='cuda:0',
       dtype=torch.bfloat16, grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None), SequenceClassifierOutput(loss=tensor(0.9790, device='cuda:0', grad_fn=<NllLossBackward0>), logits=tensor([[ 0.3887,  1.0781,  0.0226, -0.7539, -0.7070],
        [-0.2148,  0.8164, -0.2734, -0.7617, -0.5508],
        [-0.1709,  0.9258, -0.4453, -0.6523, -0.4902],
        [-0.2363,  0.8672, -0.4082, -0.4531, -0.2812],
        [ 0.2734,  0.9844, -0.2139, -0.4902, -0.0918],
        [-0.4453,  1.0703,  0.0055, -0.4355, -0.5469],
        [ 0.2539,  0.8438, -0.2891, -0.5859, -0.4629],
        [ 0.3594,  1.1875, -0.4199, -0.3203,  0.0874],
        [-0.1484,  0.6406,  0.2930,  0.1465, -0.6992],
        [ 0.2695,  1.2578, -0.1680, -0.6094, -0.2080],
        [-0.2949,  0.9531, -0.4570, -0.5703, -0.4258],
        [ 0.2598,  1.0938, -0.3789, -0.2793, -0.4512],
        [-0.0410,  1.1719, -0.5625, -0.6523, -0.4824],
        [ 0.0547,  1.0859, -0.5078, -0.5195, -0.1963],
        [ 0.0031,  1.1016, -0.4258, -0.4375, -0.2812],
        [-0.0742,  0.9336, -0.4453, -0.5078, -0.5312]], device='cuda:0',
       dtype=torch.bfloat16, grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None), SequenceClassifierOutput(loss=tensor(0.9587, device='cuda:0', grad_fn=<NllLossBackward0>), logits=tensor([[-0.0728,  0.9727, -0.0391, -0.9336, -0.4902],
        [ 0.1475,  0.8633, -0.3770, -0.7344, -0.4609],
        [-0.1436,  1.1484, -0.2520, -0.5703, -0.4746],
        [ 0.0161,  1.1953, -0.2930, -0.3750, -0.3301],
        [ 0.1357,  0.8320,  0.0544, -0.2451, -0.9023],
        [ 0.3418,  0.9883, -0.4844, -0.2539, -0.2109],
        [-0.1118,  0.5469, -0.0708, -0.3770, -0.8125],
        [ 0.0830,  0.9609, -0.1895, -0.5039, -0.3340],
        [ 0.0635,  1.0078, -0.3672, -0.3867, -0.2031],
        [ 0.0491,  0.8789, -0.2354, -0.4453, -0.4688],
        [-0.1699,  0.8711, -0.4492, -0.3984, -0.4102],
        [ 0.0640,  0.8477, -0.3320, -0.0649, -0.1904],
        [-0.2041,  0.6172,  0.1016, -0.7070, -0.9531],
        [ 0.0284,  0.6875, -0.4336, -0.3047, -0.3418],
        [-0.1201,  0.8164, -0.2832, -0.5430, -0.3379],
        [ 0.1963,  1.2500, -0.2598, -0.4570, -0.3359]], device='cuda:0',
       dtype=torch.bfloat16, grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None), SequenceClassifierOutput(loss=tensor(1.0310, device='cuda:0', grad_fn=<NllLossBackward0>), logits=tensor([[-0.1543,  0.6367, -0.4043, -0.3203, -0.3477],
        [-0.1250,  1.0859, -0.0439, -0.4336, -0.5742],
        [ 0.1050,  0.8398,  0.0608, -0.7773, -0.8594],
        [ 0.1128,  1.0938, -0.2754, -0.5312, -0.3262],
        [-0.1621,  0.6289, -0.3418, -0.9102, -0.6836],
        [-0.2061,  0.6211, -0.4102, -0.1152, -0.5820],
        [ 0.0228,  0.8906, -0.1572, -0.5039, -0.7891],
        [-0.0540,  1.1406, -0.1914, -0.4336, -0.2832],
        [-0.0879,  1.0781, -0.1260, -0.6055, -0.5078],
        [-0.5586,  0.7188, -0.2852, -0.5039, -0.5273],
        [-0.2871,  1.1328, -0.2002, -0.4492, -0.4824],
        [-0.2207,  1.2578, -0.1602, -0.5977,  0.0334],
        [-0.3301,  1.0391, -0.2715, -0.2324, -0.6719],
        [-0.3066,  0.7734, -0.1602, -0.8633, -0.7930],
        [ 0.2451,  1.1328, -0.2373, -0.3652, -0.3477],
        [ 0.0610,  1.2109, -0.2988, -0.3184, -0.8125]], device='cuda:0',
       dtype=torch.bfloat16, grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None), SequenceClassifierOutput(loss=tensor(1.0461, device='cuda:0', grad_fn=<NllLossBackward0>), logits=tensor([[ 0.6719,  0.2451, -0.6523,  0.0811,  0.2490],
        [-0.2080,  0.9648, -0.3203, -0.3887, -0.3789],
        [-0.2754,  1.1719, -0.2969, -0.4824, -0.2012],
        [-0.0981,  0.6016, -0.4727, -0.5469, -0.9375],
        [-0.2949,  1.2500, -0.6367, -0.4355, -0.6484],
        [-0.1226,  0.7031, -0.2891, -0.0981, -0.7148],
        [ 0.1660,  1.0156, -0.4453, -0.7695, -0.0297],
        [ 0.0271,  0.8750,  0.1396, -0.5352, -0.8828],
        [-0.1226,  1.1172, -0.3418, -0.6875, -0.7930],
        [ 0.3145,  0.3867, -0.9531, -0.6016,  0.1367],
        [ 0.0962,  0.8594, -0.3223, -0.5312, -0.6719],
        [ 0.2793,  1.0078, -0.1650, -0.7031, -0.4590],
        [-0.0057,  0.8750, -0.1631, -0.6914, -0.6562],
        [-0.4238,  0.3535,  0.1338, -0.5078, -0.5078],
        [-0.2393,  0.8438, -0.6016, -0.0108, -0.4277],
        [-0.0630,  0.8984, -0.2637, -0.4629, -0.2285]], device='cuda:0',
       dtype=torch.bfloat16, grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None), SequenceClassifierOutput(loss=tensor(1.1421, device='cuda:0', grad_fn=<NllLossBackward0>), logits=tensor([[ 0.0635,  0.8750, -0.4434, -0.0493, -0.4414],
        [ 0.0547,  1.1172, -0.6094, -0.7930, -0.5156],
        [ 0.2236,  0.7891, -0.5898, -0.5430, -0.3223],
        [-0.1128,  0.9023, -0.1367, -0.5273, -0.6758],
        [ 0.1455,  1.2500, -0.2266, -0.6445, -0.4434],
        [-0.1846,  0.8359, -0.0266, -0.3457, -0.6953],
        [ 0.2773,  1.1797, -0.1357, -0.4629, -0.7266],
        [ 0.0496,  0.9609, -0.6680, -0.6133, -0.3652],
        [-0.0835,  0.9102,  0.0796, -0.1572, -0.4883],
        [-0.2793,  0.8203, -0.3516, -0.3867, -0.4336],
        [ 0.3379,  0.6133, -0.4492, -0.4453, -0.1992],
        [-0.0227,  1.0781, -0.3223, -0.0593, -0.7227],
        [-0.3398,  1.1172,  0.0356, -0.6680, -0.6836],
        [-0.0532,  1.2422, -0.0796, -0.4316, -0.4590],
        [ 0.1211,  0.7305, -0.1875, -0.2490, -0.5742],
        [ 0.1060,  1.0156, -0.6133, -0.6914, -0.1035]], device='cuda:0',
       dtype=torch.bfloat16, grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None), SequenceClassifierOutput(loss=tensor(0.8967, device='cuda:0', grad_fn=<NllLossBackward0>), logits=tensor([[-0.3281,  0.9375, -0.6211, -0.3105, -0.4805],
        [ 0.0747,  0.8047, -0.3691, -0.4980, -0.7656],
        [ 0.1514,  1.1406, -0.3320, -0.9180, -0.3574],
        [-0.5156,  1.0234, -0.3945, -0.5391, -0.1631],
        [-0.0332,  0.8828, -0.3262, -0.9336, -0.5586],
        [ 0.0674,  0.7773, -0.0065, -0.5430, -0.4180],
        [-0.1367,  0.7891, -0.3984, -0.3555, -0.2637],
        [-0.0339,  0.7188,  0.1147, -0.2695, -0.2695],
        [-0.0199,  0.8867, -0.0106, -0.1079, -0.6562],
        [-0.0135,  0.8984, -0.0996, -0.5898, -0.6094],
        [ 0.0075,  1.1797, -0.6250, -0.5039, -0.2988],
        [-0.2871,  1.1953, -0.0298, -0.4980, -0.5586],
        [-0.1885,  1.0078, -0.0312, -0.3652, -0.6992],
        [ 0.2178,  1.0078, -0.3145, -0.5938, -0.6406],
        [-0.2227,  0.8086, -0.0500, -0.3613, -0.5938],
        [-0.1592,  0.9766, -0.1865, -0.5469, -0.4023]], device='cuda:0',
       dtype=torch.bfloat16, grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None), SequenceClassifierOutput(loss=tensor(0.9297, device='cuda:0', grad_fn=<NllLossBackward0>), logits=tensor([[-0.2129,  0.6953, -0.0398, -0.5703, -0.3398],
        [ 0.2109,  0.8477, -0.6211, -0.6953, -0.7773],
        [ 0.0474,  0.9219, -0.2734, -0.2793, -0.5156],
        [ 0.1758,  0.9922,  0.1270, -0.5586, -0.8906],
        [-0.2295,  0.8242, -0.4336, -0.8594, -0.5430],
        [-0.0461,  1.0547, -0.0403, -0.7500, -0.6992],
        [-0.0214,  0.7383, -0.1963, -0.6328, -0.4238],
        [-0.3145,  1.0234, -0.3105, -0.5234, -0.3848],
        [ 0.0359,  1.0312, -0.0791, -0.7266, -0.3672],
        [-0.0540,  0.9141, -0.2559, -0.5898, -0.4531],
        [ 0.0432,  0.9805, -0.1719, -0.2266, -0.5977],
        [-0.2295,  1.2969, -0.1846, -0.2832, -0.0811],
        [ 0.0645,  1.1016, -0.4160, -0.8203, -0.4453],
        [-0.0913,  0.7891, -0.8164, -0.6133, -0.4609],
        [ 0.0417,  1.1719, -0.1426, -0.4941, -0.5625],
        [-0.1128,  0.6992, -0.4570, -0.3652, -0.7578]], device='cuda:0',
       dtype=torch.bfloat16, grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None), SequenceClassifierOutput(loss=tensor(0.8660, device='cuda:0', grad_fn=<NllLossBackward0>), logits=tensor([[-0.1406,  0.9844, -0.1299, -0.7500, -0.6250],
        [-0.1504,  0.8008, -0.2930, -0.5469, -0.7188],
        [-0.1631,  1.0234, -0.1260, -0.3477, -0.4590],
        [-0.1660,  0.9141,  0.0474, -0.7188, -0.1631],
        [-0.1494,  0.5430, -0.2178, -0.4785, -0.6875],
        [ 0.1133,  0.8594, -0.6094, -0.4102, -0.5117],
        [-0.2520,  1.0625, -0.2256, -0.6133, -0.3203],
        [-0.3320,  1.0859, -0.3457, -0.3711, -0.5742],
        [-0.3320,  0.9570, -0.4023, -0.2852, -0.7500],
        [ 0.1748,  0.8945, -0.2002, -0.0762, -0.3379],
        [-0.0288,  1.1172, -0.2520, -0.5195, -0.3516],
        [-0.4336,  0.9336, -0.4414, -0.5156, -0.3594],
        [-0.1133,  0.9297, -0.1064, -0.3906, -0.3730],
        [-0.3555,  0.9883, -0.4199, -0.6016, -0.5469],
        [-0.0044,  0.6562, -0.3438, -0.7227, -0.8359],
        [-0.0099,  0.6641,  0.1016, -0.2891, -0.4883]], device='cuda:0',
       dtype=torch.bfloat16, grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None), SequenceClassifierOutput(loss=tensor(0.9695, device='cuda:0', grad_fn=<NllLossBackward0>), logits=tensor([[ 0.0564,  1.1875, -0.3848, -0.5742, -0.1982],
        [ 0.1787,  1.0391, -0.2871, -0.8320, -0.2930],
        [ 0.0049,  0.8945, -0.3164, -0.5117, -0.4121],
        [-0.4277,  0.6172, -0.3926, -0.2002, -0.3594],
        [-0.0101,  1.2578, -0.3418, -0.7266, -0.3477],
        [ 0.2490,  1.0078, -0.2559, -0.4238, -0.4395],
        [-0.3672,  0.9492,  0.2109, -0.5742, -0.7500],
        [-0.1523,  1.0469, -0.3652, -0.3848, -0.5430],
        [ 0.0947,  1.0547, -0.0354, -0.4043, -0.7227],
        [-0.0143,  1.2266, -0.3223, -0.2490, -0.7500],
        [-0.2109,  0.8125,  0.1396, -0.7695, -0.4902],
        [-0.1875,  0.8516, -0.2832, -0.9648, -0.8711],
        [-0.0037,  0.6758, -0.5039, -0.4082, -0.5352],
        [-0.1836,  0.7734, -0.1953, -0.3203, -0.5742],
        [-0.0703,  1.1719, -0.5156, -0.6914, -0.2754],
        [-0.3223,  0.7227, -0.4648, -0.3867, -0.4043]], device='cuda:0',
       dtype=torch.bfloat16, grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None), SequenceClassifierOutput(loss=tensor(0.8154, device='cuda:0', grad_fn=<NllLossBackward0>), logits=tensor([[ 0.1216,  0.8750, -0.2793, -0.5039, -0.4492],
        [-0.1387,  1.2891, -0.3516, -0.2676, -0.2617],
        [ 0.1670,  1.0547, -0.3418, -0.4316, -0.3789],
        [ 0.1621,  0.9570, -0.4785, -0.6641, -0.3652],
        [-0.0231,  1.0469, -0.0684, -0.6016, -0.2363],
        [-0.3242,  0.9375, -0.0417, -0.4707, -0.9492],
        [ 0.0996,  1.2344, -0.3145, -0.7500, -0.3672],
        [-0.3320,  0.8438,  0.1426, -0.4961, -0.8320],
        [-0.0569,  0.9023, -0.3125, -0.5000, -0.4941],
        [ 0.0378,  0.7344, -0.3887, -0.4844, -0.8164],
        [-0.0596,  1.1797, -0.3398, -0.8438, -0.3477],
        [-0.1060,  0.9531, -0.0718, -0.5391, -0.3262],
        [ 0.0757,  0.9844, -0.2109, -0.3652, -0.2266],
        [-0.1709,  0.7383, -0.4062, -0.0204, -0.8438],
        [ 0.1680,  1.1094,  0.0045, -0.3750, -0.6992],
        [ 0.2852,  0.8945, -0.2393, -0.3652, -0.4434]], device='cuda:0',
       dtype=torch.bfloat16, grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None), SequenceClassifierOutput(loss=tensor(0.8762, device='cuda:0', grad_fn=<NllLossBackward0>), logits=tensor([[ 0.0664,  1.1719, -0.4961, -0.8516, -0.3906],
        [-0.0850,  0.9375, -0.3555, -0.1904, -0.6602],
        [ 0.0811,  1.1250, -0.2412, -0.3398, -0.3926],
        [-0.0613,  0.9805, -0.1885, -0.7266, -0.4512],
        [ 0.3320,  0.9648, -0.2578, -0.6797, -0.2500],
        [-0.0128,  0.7852, -0.6602, -0.6797, -0.1060],
        [-0.2129,  0.9492, -0.2441, -0.3027, -0.3027],
        [-0.2041,  0.8164, -0.2812, -0.6250, -0.2090],
        [ 0.1689,  1.0156, -0.2402, -0.3184, -0.3438],
        [ 0.0840,  1.2422, -0.1377, -0.7188, -0.4492],
        [ 0.2041,  0.8789, -0.0391, -0.7422, -0.5430],
        [-0.1982,  0.7422, -0.2441, -0.3184, -0.2930],
        [ 0.0457,  1.1328, -0.1943, -0.5977,  0.0074],
        [ 0.0601,  0.9453, -0.2637, -0.5430, -0.9414],
        [ 0.3711,  1.5156, -0.4082, -0.6562, -0.3672],
        [-0.3184,  1.1172, -0.0742, -0.8203, -0.7773]], device='cuda:0',
       dtype=torch.bfloat16, grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None), SequenceClassifierOutput(loss=tensor(0.9087, device='cuda:0', grad_fn=<NllLossBackward0>), logits=tensor([[ 0.2969,  1.1484,  0.0461, -0.6328, -0.2393],
        [ 0.1914,  0.9844, -0.3145, -0.4727, -0.2109],
        [ 0.2715,  1.4531, -0.6562, -0.6992, -0.6016],
        [-0.0859,  1.2031, -0.1416, -0.8711, -0.4551],
        [-0.0488,  0.9492, -0.6367, -0.2031, -0.3086],
        [-0.5625,  0.9961, -0.4160, -0.1709, -0.4844],
        [-0.2930,  1.1172, -0.3027, -0.6602, -0.4160],
        [-0.1245,  1.3281, -0.1001, -0.6602, -0.4531],
        [ 0.1318,  0.7852, -0.7109, -0.7031,  0.3750],
        [ 0.0173,  0.9141, -0.2148, -0.2754, -0.5156],
        [ 0.0972,  1.0703, -0.0776, -0.4023, -0.4512],
        [ 0.0250,  1.0000, -0.2393, -0.4512, -0.4258],
        [ 0.1289,  0.9883, -0.3672, -0.5781, -0.3945],
        [-0.0898,  0.9414, -0.3164, -0.6875, -0.4023],
        [-0.2734,  1.2422, -0.1650, -0.3828, -0.5039],
        [-0.1060,  1.0234,  0.0684, -0.2969, -0.3555]], device='cuda:0',
       dtype=torch.bfloat16, grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None), SequenceClassifierOutput(loss=tensor(1.1228, device='cuda:0', grad_fn=<NllLossBackward0>), logits=tensor([[ 0.1885,  0.9961, -0.3789, -0.5195, -0.2227],
        [ 0.2656,  1.0547, -0.5156, -0.4746, -0.4512],
        [-0.3320,  1.3047, -0.2051, -0.3691, -0.8281],
        [ 0.0275,  1.1406, -0.1748, -0.5195, -0.6367],
        [ 0.0337,  1.0703, -0.3730, -0.4102, -0.5781],
        [-0.1367,  1.0469, -0.0732, -0.4961, -0.7539],
        [-0.1162,  1.0156, -0.3633, -0.5039, -0.8984],
        [ 0.1504,  1.0781, -0.2715, -0.8359, -0.4277],
        [-0.0349,  1.0391, -0.3340, -0.3340, -0.1982],
        [ 0.0503,  1.0703, -0.0569, -0.3203, -0.2871],
        [ 0.2617,  1.1016, -0.6406, -1.0312, -0.5156],
        [-0.1494,  0.9961, -0.1885, -0.3340, -0.5352],
        [-0.1270,  0.8281, -0.2129, -0.4004, -0.2871],
        [-0.1621,  0.8516, -0.2930, -0.7344, -0.7891],
        [ 0.0625,  1.1797, -0.2412, -0.4688, -0.3633],
        [-0.2490,  0.8281, -0.3340, -0.2598, -0.4980]], device='cuda:0',
       dtype=torch.bfloat16, grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None), SequenceClassifierOutput(loss=tensor(1.1060, device='cuda:0', grad_fn=<NllLossBackward0>), logits=tensor([[-6.2866e-03,  1.2109e+00, -1.9141e-01, -5.6250e-01, -5.0000e-01],
        [ 1.5430e-01,  1.1172e+00, -1.2891e-01, -1.8262e-01, -5.7617e-02],
        [ 8.3984e-02,  1.1094e+00, -4.1602e-01, -2.7148e-01, -1.9238e-01],
        [-2.4512e-01,  9.5703e-01, -3.0078e-01, -8.2422e-01, -7.3828e-01],
        [-2.0801e-01,  1.0547e+00, -1.5332e-01, -7.7734e-01, -1.9434e-01],
        [ 8.7891e-02,  1.1328e+00, -5.7129e-02, -2.6953e-01, -4.9414e-01],
        [ 7.8613e-02,  1.0547e+00, -2.0605e-01, -3.2617e-01, -6.2500e-01],
        [-6.4941e-02,  9.2578e-01, -6.3477e-02, -5.4688e-01, -3.4961e-01],
        [ 5.7602e-04,  1.0156e+00, -3.1641e-01, -2.8711e-01, -4.1016e-01],
        [-2.1094e-01,  4.7656e-01, -2.7710e-02, -4.1992e-01, -7.7344e-01],
        [-9.3384e-03,  1.2500e+00, -2.1191e-01, -6.1328e-01, -1.5625e-01],
        [ 3.7500e-01,  5.9375e-01, -5.5078e-01, -5.3516e-01, -5.7812e-01],
        [ 2.5000e-01,  9.1016e-01, -6.7188e-01, -5.5469e-01, -3.2422e-01],
        [-2.0605e-01,  1.2969e+00, -1.5918e-01, -5.6250e-01, -5.2344e-01],
        [ 1.6211e-01,  1.2188e+00, -3.1641e-01, -3.4961e-01, -3.7598e-02],
        [-2.0020e-01,  1.3906e+00, -7.2754e-02, -3.2422e-01, -4.8438e-01]],
       device='cuda:0', dtype=torch.bfloat16, grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None), SequenceClassifierOutput(loss=tensor(0.9148, device='cuda:0', grad_fn=<NllLossBackward0>), logits=tensor([[-0.2383,  1.4375, -0.1738, -1.0156, -0.6289],
        [-0.0107,  0.7422, -0.2354, -0.4355, -0.5625],
        [-0.3418,  1.1172, -0.0315, -0.4590, -0.4629],
        [-0.2012,  0.9805, -0.6523, -0.4395, -0.2119],
        [ 0.1396,  1.1953, -0.0081, -0.6367, -0.3594],
        [-0.0157,  0.8906, -0.3027, -0.5391, -0.3047],
        [ 0.0080,  1.0859, -0.2109, -0.5508, -0.5391],
        [ 0.2275,  0.8438, -0.3359, -0.5312, -0.3828],
        [-0.2969,  0.4785,  0.0513, -0.3438, -0.6289],
        [-0.0693,  1.0312, -0.2500, -0.6133, -0.4434],
        [-0.2891,  0.7852, -0.2949, -0.6406, -0.7656],
        [-0.0491,  0.9414, -0.1699, -0.5039, -0.5938],
        [-0.1611,  1.0547, -0.4648, -0.3613, -0.4941],
        [ 0.1611,  1.2031, -0.1099, -0.4316, -0.4121],
        [-0.1680,  1.2031, -0.2715, -0.5938, -0.5430],
        [ 0.0021,  0.8750,  0.0102, -0.6367, -0.6602]], device='cuda:0',
       dtype=torch.bfloat16, grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None), SequenceClassifierOutput(loss=tensor(0.9438, device='cuda:0', grad_fn=<NllLossBackward0>), logits=tensor([[-0.3086,  1.0312, -0.4980, -0.0854, -0.5664],
        [-0.0815,  0.5781, -0.1729, -0.7227, -0.5938],
        [ 0.1631,  1.0156, -0.3652, -0.8242, -0.5156],
        [ 0.0684,  1.1719, -0.3359, -0.6836, -0.5156],
        [-0.0579,  0.9727,  0.0723, -0.5039, -0.2578],
        [ 0.0354,  1.1797, -0.0588, -0.1650, -0.4434],
        [-0.1016,  1.0547, -0.3184, -0.5430, -0.7070],
        [-0.2559,  0.8750, -0.4434, -0.3770, -0.4121],
        [ 0.2314,  0.7695, -0.1582, -0.8438, -0.6562],
        [ 0.1328,  0.5625,  0.0157, -0.4199, -1.0703],
        [-0.0481,  0.9102, -0.2637, -0.5820, -1.1875],
        [ 0.0398,  1.1719, -0.4590, -0.6211, -0.3711],
        [-0.2451,  1.0156, -0.4043, -0.4590, -0.4277],
        [ 0.0221,  0.8516, -0.1768, -0.1416, -0.6172],
        [ 0.1069,  0.8594, -0.6602, -0.6250, -0.4355],
        [ 0.0474,  1.2656,  0.0273,  0.0040, -0.2949]], device='cuda:0',
       dtype=torch.bfloat16, grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None), SequenceClassifierOutput(loss=tensor(0.9521, device='cuda:0', grad_fn=<NllLossBackward0>), logits=tensor([[ 0.4492,  1.2969, -0.3867, -0.8711, -0.3066],
        [ 0.0147,  1.0000,  0.0476, -0.4668, -0.5508],
        [-0.2695,  1.0000, -0.6016, -0.4082, -0.6289],
        [-0.4980,  0.8320, -0.0057, -0.6484, -0.7578],
        [-0.0674,  1.0469, -0.2207, -0.5508, -0.6836],
        [-0.1045,  0.9727, -0.1943, -0.6172, -0.5117],
        [ 0.2207,  1.1328, -0.4141, -0.6641, -0.4395],
        [-0.3145,  1.1328, -0.3594, -0.4668, -0.5742],
        [-0.1523,  0.8516,  0.0270, -0.6719, -0.8164],
        [ 0.3164,  1.5078,  0.0250, -0.5625, -0.5234],
        [-0.1475,  1.0469, -0.3340, -0.5781, -0.4629],
        [ 0.1030,  1.0234, -0.4590, -0.6133, -0.4961],
        [-0.0500,  0.9414, -0.3066, -0.6172, -0.4941],
        [-0.1021,  1.0859, -0.2490, -0.7422, -0.7891],
        [-0.4414,  1.0078, -0.1504, -0.4883, -0.4277],
        [ 0.0065,  1.2969, -0.3535, -1.0000, -0.3086]], device='cuda:0',
       dtype=torch.bfloat16, grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None), SequenceClassifierOutput(loss=tensor(1.1055, device='cuda:0', grad_fn=<NllLossBackward0>), logits=tensor([[-0.1113,  0.6250, -0.4785, -0.2949, -0.2500],
        [-0.0918,  0.8047,  0.2012, -0.4688, -0.4844],
        [-0.0493,  0.8438, -0.4043, -0.6328, -0.3418],
        [ 0.1060,  1.2344, -0.3770, -0.7891, -0.5156],
        [-0.0566,  0.9883, -0.6328, -0.6016, -0.4785],
        [ 0.1377,  0.7695, -0.0352, -0.1328, -0.3320],
        [-0.2314,  0.5820, -0.5195, -0.6289, -0.5742],
        [ 0.0187,  1.1172, -0.4746, -0.7227, -0.6094],
        [-0.0258,  1.0781, -0.3203, -0.4531, -0.2051],
        [ 0.2363,  0.9844, -0.4707, -0.6875, -0.4043],
        [ 0.1270,  1.0625, -0.2100, -0.8438, -0.1084],
        [-0.1250,  0.8906, -0.1855, -0.6133, -0.4512],
        [-0.0767,  0.5625, -0.5742, -0.5039, -0.2559],
        [ 0.0908,  0.8594, -0.5078, -0.9336, -0.2227],
        [ 0.0282,  1.2266, -0.3223, -0.4863, -0.4082],
        [ 0.0825,  1.2344, -0.2949, -0.2793, -0.2559]], device='cuda:0',
       dtype=torch.bfloat16, grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None), SequenceClassifierOutput(loss=tensor(0.9177, device='cuda:0', grad_fn=<NllLossBackward0>), logits=tensor([[-0.1953,  0.9922, -0.2490, -0.8359, -0.8320],
        [ 0.0889,  0.9844, -0.2812, -0.4805, -0.5469],
        [ 0.0825,  1.2188, -0.4043, -0.5430,  0.0048],
        [-0.0864,  1.0703, -0.3535, -0.2285, -0.8438],
        [-0.0116,  0.9531, -0.5078, -0.2773, -0.3633],
        [ 0.0757,  0.9805, -0.2324, -0.5078, -0.7852],
        [ 0.0238,  0.8555, -0.3281, -0.4062, -0.4570],
        [ 0.0325,  1.2422, -0.4336, -0.4102, -0.4180],
        [-0.4805,  1.0625, -0.3496, -0.6875, -0.2852],
        [-0.2373,  0.9844, -0.3398, -0.1338, -0.4102],
        [ 0.0164,  0.9141, -0.0359, -0.6641, -0.9453],
        [ 0.0121,  0.6758, -0.1157, -0.5117, -0.8281],
        [-0.1113,  0.7109, -0.2188, -0.5859, -0.6289],
        [-0.0238,  1.2656, -0.3164, -0.3555, -0.3496],
        [-0.0549,  0.8828, -0.3926, -0.3047, -0.4395],
        [ 0.2676,  0.6992, -0.2090, -0.5664, -0.0889]], device='cuda:0',
       dtype=torch.bfloat16, grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None), SequenceClassifierOutput(loss=tensor(1.0166, device='cuda:0', grad_fn=<NllLossBackward0>), logits=tensor([[ 0.2236,  0.8945, -0.1826, -0.5430, -0.1895],
        [-0.1982,  1.1250, -0.2793, -0.5430, -0.3184],
        [ 0.3203,  0.5820, -0.7812, -0.8086, -0.2100],
        [ 0.0425,  0.8320, -0.2812, -0.4258, -0.8086],
        [-0.0894,  1.0234, -0.5586, -0.5469, -0.4844],
        [ 0.0520,  0.3828, -0.3320, -0.5859, -0.4648],
        [-0.1045,  0.8047, -0.1924, -0.6094, -0.3184],
        [-0.2480,  0.9180, -0.3281, -0.8320, -0.5977],
        [ 0.1494,  0.9531, -0.0996, -0.2354, -0.3672],
        [-0.4062,  0.7930, -0.2539, -0.2773, -0.6172],
        [ 0.2422,  1.3203, -0.3945, -0.8633, -0.3496],
        [ 0.1943,  1.0703, -0.3066, -0.3320, -0.1235],
        [-0.0327,  1.0938, -0.3945, -0.3730, -0.4355],
        [ 0.0889,  0.5664, -0.3145, -0.6367, -0.0913],
        [-0.1865,  1.1719, -0.4141, -0.3145, -0.1025],
        [-0.2520,  0.8906, -0.1729, -0.5586, -0.6328]], device='cuda:0',
       dtype=torch.bfloat16, grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None), SequenceClassifierOutput(loss=tensor(0.7356, device='cuda:0', grad_fn=<NllLossBackward0>), logits=tensor([[-0.0830,  0.6172, -0.2051, -0.7305, -0.5586],
        [-0.0542,  1.0781, -0.3379, -0.3672, -0.6992],
        [-0.0581,  1.1172, -0.2490, -0.8086, -0.4941],
        [-0.1426,  1.0703, -0.3613, -0.4961, -0.2383],
        [ 0.2314,  1.1875, -0.1895, -0.6602, -1.2109],
        [-0.1230,  1.1250, -0.4492, -0.5742, -0.4473],
        [-0.2812,  1.0156, -0.2910, -0.4395, -0.3242],
        [ 0.1689,  1.0078, -0.2734, -0.6797, -0.3945],
        [ 0.1523,  1.1953, -0.2617, -0.7148, -0.4883],
        [ 0.0071,  1.2344, -0.2949, -0.3711, -0.3164],
        [-0.0131,  1.0312, -0.3047, -0.2871, -0.3711],
        [-0.2656,  1.1250,  0.0356, -0.5898, -0.5039],
        [ 0.2676,  1.0938, -0.3086, -0.5430, -0.1846],
        [ 0.1079,  1.2266, -0.3770, -0.7070, -0.8242],
        [-0.1367,  1.2578, -0.3438, -0.6211, -0.4062],
        [-0.0806,  0.9336, -0.2852, -0.8164, -0.6172]], device='cuda:0',
       dtype=torch.bfloat16, grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None), SequenceClassifierOutput(loss=tensor(0.7739, device='cuda:0', grad_fn=<NllLossBackward0>), logits=tensor([[-0.4727,  1.1562, -0.3848, -0.3945, -0.4297],
        [ 0.0112,  0.9766, -0.3594, -0.7266, -0.3887],
        [-0.0625,  0.8984, -0.3906, -0.4863, -0.6367],
        [-0.0195,  1.0703, -0.3066, -0.3652, -0.4121],
        [ 0.0464,  0.9375, -0.2676, -0.4648, -0.3672],
        [ 0.1426,  1.1953, -0.7344, -0.0991, -0.4668],
        [-0.0166,  1.1172, -0.2988, -0.2236, -0.4648],
        [-0.0732,  1.2578, -0.5352, -0.4609, -0.5898],
        [-0.0532,  1.1016, -0.2227, -0.2812, -0.3418],
        [ 0.0442,  1.3516, -0.3418, -0.9766, -0.4082],
        [ 0.0093,  1.2109, -0.1719, -0.4551, -0.3828],
        [-0.3379,  1.0703, -0.1680, -0.6914, -0.5469],
        [-0.2314,  0.9844, -0.2539, -0.5664, -0.3711],
        [-0.0781,  0.9883, -0.4082, -0.5586, -0.3145],
        [ 0.2090,  1.1250, -0.4121, -0.5820, -0.5156],
        [ 0.0713,  0.9297, -0.3340, -0.6680, -0.3984]], device='cuda:0',
       dtype=torch.bfloat16, grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None), SequenceClassifierOutput(loss=tensor(0.8972, device='cuda:0', grad_fn=<NllLossBackward0>), logits=tensor([[-0.2695,  1.0469, -0.2197, -0.1553, -0.2734],
        [ 0.2246,  1.3672, -0.5117, -0.3223, -0.0845],
        [-0.1064,  0.8633, -0.2969, -0.1523, -0.4980],
        [ 0.0564,  0.7773, -0.2139, -0.5938, -0.6406],
        [ 0.0598,  0.9609, -0.3359, -0.6094, -0.3105],
        [ 0.0067,  0.9688, -0.3320, -0.4355, -0.3613],
        [-0.0732,  1.2500, -0.3496, -1.0625, -0.3359],
        [-0.3887,  1.2031, -0.3320, -0.8125, -0.4297],
        [-0.2500,  1.0938,  0.0874, -0.5703, -0.6523],
        [ 0.1670,  1.2500, -0.3594, -0.5391, -0.1436],
        [ 0.0908,  1.1328, -0.3203, -0.2334, -0.3789],
        [ 0.2344,  1.1250, -0.4863, -0.8047, -0.5430],
        [-0.3906,  0.7383, -0.3184, -0.4023, -0.5352],
        [-0.2402,  0.8789,  0.1338, -0.5234, -0.6523],
        [ 0.0488,  0.8477, -0.3691, -0.5742, -0.3789],
        [-0.2734,  1.0234, -0.2178, -0.2041, -0.3477]], device='cuda:0',
       dtype=torch.bfloat16, grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None), SequenceClassifierOutput(loss=tensor(0.7561, device='cuda:0', grad_fn=<NllLossBackward0>), logits=tensor([[-0.0200,  1.2500,  0.0303, -0.1865, -0.5781],
        [ 0.1348,  0.6953, -0.5000, -0.3730, -0.2852],
        [-0.0072,  1.1875, -0.4609, -0.4219, -0.0334],
        [ 0.0679,  1.1094, -0.1963, -0.3828, -0.3535],
        [-0.3418,  1.2500, -0.2246, -0.3945, -0.3691],
        [ 0.0332,  0.7539, -0.1816, -0.3105, -0.5156],
        [-0.0649,  1.0312, -0.0640, -0.7812, -1.2812],
        [-0.4102,  1.0469, -0.1768, -0.4219, -0.7422],
        [-0.1611,  0.9961, -0.1553, -0.6328, -0.4238],
        [ 0.0114,  1.1875, -0.2559, -0.5312, -0.4258],
        [ 0.1934,  1.0781, -0.3242, -0.8555, -0.7227],
        [-0.0972,  0.9258, -0.2734, -0.2617, -0.6797],
        [ 0.4082,  1.0625, -0.2178, -0.5391, -0.4531],
        [-0.1504,  0.9141, -0.6172, -0.3770, -0.6055],
        [ 0.0908,  1.0625, -0.6719, -0.5117, -0.6211],
        [ 0.2969,  1.2422, -0.6211, -0.4785, -0.5234]], device='cuda:0',
       dtype=torch.bfloat16, grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None), SequenceClassifierOutput(loss=tensor(1.1904, device='cuda:0', grad_fn=<NllLossBackward0>), logits=tensor([[-0.1387,  0.8320, -0.2930, -0.6445, -0.7773],
        [ 0.2090,  0.8047, -0.5234, -0.3242, -0.2373],
        [ 0.2305,  1.1641, -0.1914, -0.5156, -0.2637],
        [-0.3379,  1.2422, -0.3945, -0.3105, -0.4355],
        [ 0.1230,  1.0859, -0.3418, -0.6133, -0.3828],
        [-0.2266,  0.9180,  0.3340, -0.4512, -0.5859],
        [ 0.5273,  0.2598, -0.9609, -0.4141,  0.1738],
        [ 0.0293,  0.7695, -0.1050, -0.6602, -0.5547],
        [ 0.0505,  1.1172, -0.2471, -0.3887, -0.5977],
        [-0.0332,  1.1016, -0.1348, -0.8164, -0.4434],
        [ 0.0222,  0.8867, -0.4609, -0.7305, -0.6523],
        [-0.0396,  1.3125, -0.4375, -0.5781, -0.2402],
        [-0.2051,  1.0312, -0.3809, -0.5898, -0.5039],
        [ 0.2031,  0.8672, -0.3848, -0.6055, -0.4297],
        [ 0.0195,  1.0156, -0.0640, -0.4375, -0.7656],
        [ 0.1494,  1.2656, -0.0615, -0.5469, -0.5898]], device='cuda:0',
       dtype=torch.bfloat16, grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None), SequenceClassifierOutput(loss=tensor(1.0369, device='cuda:0', grad_fn=<NllLossBackward0>), logits=tensor([[-0.1943,  1.1094, -0.5195, -0.3398, -0.5547],
        [-0.1523,  0.9297, -0.0801, -0.5117, -0.6953],
        [-0.2539,  1.0156, -0.0488, -0.4316, -0.4844],
        [-0.0537,  1.0469, -0.1777, -0.3770, -0.7422],
        [-0.0469,  0.8164, -0.1641, -0.6367, -0.6484],
        [-0.0070,  0.6914, -0.8242, -0.5469, -0.7266],
        [ 0.0708,  1.0234, -0.1709, -0.3535, -0.4688],
        [-0.0035,  1.2031, -0.5391, -0.2949, -0.3633],
        [ 0.3555,  1.2422, -0.4219, -0.6562, -0.2344],
        [ 0.1768,  1.0938, -0.2812, -0.3965, -0.4199],
        [-0.1680,  1.2500, -0.5039, -0.3652, -0.4297],
        [-0.0308,  1.1094, -0.3184, -0.9102, -0.6367],
        [-0.1069,  1.0234, -0.2910, -0.3906, -0.4219],
        [ 0.2041,  1.3672, -0.2100, -0.5938, -0.4180],
        [-0.2734,  1.2344, -0.2930, -0.5117, -0.4004],
        [ 0.0752,  1.2734, -0.4766, -0.5039, -0.4648]], device='cuda:0',
       dtype=torch.bfloat16, grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None), SequenceClassifierOutput(loss=tensor(1.1309, device='cuda:0', grad_fn=<NllLossBackward0>), logits=tensor([[-0.0039,  1.2812, -0.3066, -0.4648, -0.3301],
        [ 0.0737,  0.7031,  0.0045, -0.6406, -0.5938],
        [-0.3750,  0.9258,  0.1328, -0.6797, -0.8750],
        [ 0.0298,  0.9023, -0.2305, -0.3984, -0.7227],
        [ 0.1758,  0.9570, -0.3887, -0.6211, -0.5234],
        [ 0.2852,  1.5469, -0.3555, -0.6992, -0.2383],
        [-0.1885,  0.8789, -0.3848, -0.5469, -0.0972],
        [ 0.0713,  0.9023, -0.2832, -0.9727, -0.6641],
        [ 0.0952,  0.8672, -0.1807, -0.4590, -0.6328],
        [ 0.0322,  0.9102, -0.1377, -0.4453, -0.0659],
        [ 0.0879,  0.8477,  0.0869, -0.2969, -0.4531],
        [ 0.1240,  1.1641, -0.3027, -0.7852, -0.3477],
        [-0.4863,  0.9727, -0.0115, -0.5859, -0.5586],
        [ 0.0884,  1.2422, -0.3223, -0.7539, -0.1504],
        [-0.1582,  0.9492, -0.2617, -0.5859, -0.3906],
        [-0.1235,  0.9219, -0.6055, -0.8203, -0.6133]], device='cuda:0',
       dtype=torch.bfloat16, grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None), SequenceClassifierOutput(loss=tensor(0.9875, device='cuda:0', grad_fn=<NllLossBackward0>), logits=tensor([[-0.1099,  0.8516, -0.3477, -0.7422, -0.2520],
        [ 0.0542,  1.2656, -0.4590, -0.5469, -0.4160],
        [ 0.0967,  0.8047, -0.1602, -0.6133, -1.0625],
        [ 0.1641,  0.9961, -0.4570, -0.6914, -0.4395],
        [ 0.3438,  0.3672, -0.5898,  0.0194,  0.2412],
        [ 0.2002,  1.0312, -0.5078, -0.3594, -0.5742],
        [ 0.0549,  1.4375, -0.2871, -1.0156, -0.5234],
        [-0.4473,  1.0156, -0.3320, -0.2275, -0.6523],
        [-0.0204,  0.8203,  0.0542, -0.3594, -1.2891],
        [ 0.1367,  0.9844, -0.1309, -0.6680, -0.3535],
        [ 0.4395,  0.8594, -0.1602, -0.6953, -0.1621],
        [-0.0391,  1.0703, -0.1865, -0.7188, -0.1167],
        [ 0.1602,  1.1641, -0.5352, -0.8867, -0.1523],
        [-0.2275,  0.9883, -0.4824, -0.7109, -0.6484],
        [ 0.0593,  1.0781, -0.4473, -0.2715, -0.3145],
        [ 0.0659,  1.1797,  0.0437, -0.6641, -0.8672]], device='cuda:0',
       dtype=torch.bfloat16, grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None), SequenceClassifierOutput(loss=tensor(0.8757, device='cuda:0', grad_fn=<NllLossBackward0>), logits=tensor([[ 0.0596,  0.8477, -0.2314, -0.3750, -0.5078],
        [ 0.0688,  1.0547, -0.3496, -0.2451, -0.5859],
        [-0.0913,  1.0000, -0.4512, -0.8555, -0.4375],
        [-0.1001,  1.1328, -0.2080, -0.4883, -0.5703],
        [-0.1377,  1.1797, -0.3008, -0.3496, -0.4180],
        [ 0.0104,  1.2266, -0.6797, -0.6211, -0.3652],
        [-0.0574,  1.0156, -0.2480, -0.3809, -0.1699],
        [-0.0112,  0.9805, -0.3105, -0.6289, -0.6523],
        [-0.3809,  0.9844,  0.2197, -1.0000, -0.4004],
        [ 0.0400,  0.9609, -0.1650, -0.3711, -0.4258],
        [-0.1982,  1.3594, -0.0352, -0.7812, -0.4570],
        [-0.0449,  0.8438, -0.3027, -0.8164, -0.5430],
        [-0.0579,  1.1875, -0.3613, -0.5547, -0.4863],
        [ 0.3340,  1.1172, -0.2363, -0.8516, -0.3223],
        [ 0.1230,  1.1328, -0.4727, -0.3750, -0.3594],
        [-0.1807,  0.9531, -0.3887, -0.4082, -0.3145]], device='cuda:0',
       dtype=torch.bfloat16, grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None), SequenceClassifierOutput(loss=tensor(1.0339, device='cuda:0', grad_fn=<NllLossBackward0>), logits=tensor([[-0.2100,  1.0625, -0.2422, -0.3965, -0.2773],
        [ 0.2451,  0.4883, -0.4355, -0.9414, -0.8242],
        [ 0.0366,  1.2656, -0.3496, -0.7422, -0.5586],
        [-0.1260,  1.0703, -0.5117, -0.2695, -0.5820],
        [-0.5078,  1.0703, -0.0586, -0.6367, -0.2275],
        [-0.2197,  1.0000,  0.0303, -0.5195, -0.9414],
        [ 0.0527,  1.0781, -0.3223, -0.3516, -0.4238],
        [ 0.3203,  0.8359, -0.2988, -0.4648, -0.2715],
        [-0.0332,  0.8906, -0.2305, -0.5352, -0.4609],
        [-0.1069,  1.3672, -0.2969, -0.5742, -0.2559],
        [-0.0356,  0.9258, -0.4062, -0.4277, -0.1914],
        [-0.3652,  0.9414, -0.5078, -0.6367, -0.6523],
        [ 0.0137,  1.0547, -0.3184, -0.5508, -0.8906],
        [ 0.2695,  0.9961, -0.4414, -0.6680, -0.4141],
        [ 0.5195,  0.5430, -0.9180, -0.9258, -0.2988],
        [ 0.0574,  0.6289, -0.3848, -0.6094, -0.7266]], device='cuda:0',
       dtype=torch.bfloat16, grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None), SequenceClassifierOutput(loss=tensor(0.7681, device='cuda:0', grad_fn=<NllLossBackward0>), logits=tensor([[-0.0483,  1.1797, -0.4492, -0.5820, -0.5430],
        [-0.0024,  0.9375,  0.1289, -0.5977, -0.4160],
        [ 0.0972,  1.2578, -0.3789, -0.8203, -0.4863],
        [ 0.1602,  1.3594, -0.5977, -0.4805, -0.2002],
        [ 0.0972,  1.4609, -0.3828, -1.1016, -0.3223],
        [ 0.0928,  0.9375, -0.6875, -0.6641, -0.3906],
        [-0.1309,  1.2422, -0.1299, -0.4375, -0.6445],
        [ 0.0603,  0.9297, -0.3711, -0.5352, -0.8164],
        [-0.0923,  1.3047, -0.0476, -0.4922, -0.7305],
        [-0.2559,  0.9258, -0.2812, -0.6602, -0.1689],
        [ 0.1060,  1.3984, -0.2656, -0.7734, -0.4004],
        [ 0.0835,  0.9727, -0.1982, -0.7344, -0.8086],
        [ 0.0981,  0.4551, -0.4688, -0.7148, -0.8672],
        [ 0.0713,  1.3438, -0.1006, -0.3594, -0.3105],
        [ 0.2432,  1.2500, -0.3789, -0.8594, -0.3105],
        [ 0.1328,  1.4766, -0.5078, -0.4395, -0.3555]], device='cuda:0',
       dtype=torch.bfloat16, grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None), SequenceClassifierOutput(loss=tensor(0.8992, device='cuda:0', grad_fn=<NllLossBackward0>), logits=tensor([[ 1.9727e-01,  1.0312e+00, -5.2344e-01, -5.2344e-01, -4.9414e-01],
        [-2.7832e-02,  1.0469e+00, -4.1211e-01, -8.9062e-01, -5.0391e-01],
        [-9.7168e-02,  1.2266e+00, -1.4453e-01, -7.0703e-01, -6.9141e-01],
        [-3.4766e-01,  1.1250e+00, -5.1270e-02, -1.4355e-01, -3.3984e-01],
        [ 1.0059e-01,  1.1641e+00, -3.5352e-01, -5.3516e-01, -4.5117e-01],
        [ 8.1543e-02,  7.7734e-01, -3.2031e-01, -9.3750e-01, -6.3672e-01],
        [ 2.9883e-01,  1.2500e+00, -2.7539e-01, -1.0156e+00, -3.2812e-01],
        [-4.7852e-02,  1.3750e+00, -2.4121e-01, -8.9062e-01, -3.4766e-01],
        [-1.0449e-01,  9.1797e-01, -2.4121e-01, -6.9531e-01, -4.6875e-01],
        [-4.6387e-02,  9.2578e-01, -2.2266e-01, -3.5742e-01, -3.2031e-01],
        [ 2.8125e-01,  8.9453e-01,  9.7656e-02, -7.5781e-01, -7.1875e-01],
        [-5.0049e-02,  8.9453e-01, -1.6797e-01, -6.4062e-01, -6.2891e-01],
        [ 4.8047e-01,  2.0312e-01, -1.1875e+00, -8.8281e-01,  4.7266e-01],
        [ 1.4746e-01,  9.8828e-01, -3.0273e-01, -5.5469e-01, -5.1172e-01],
        [-2.5195e-01,  1.2266e+00,  2.3804e-02, -8.5938e-01, -7.9688e-01],
        [ 5.6458e-04,  1.1406e+00, -1.9727e-01, -5.7422e-01, -3.9844e-01]],
       device='cuda:0', dtype=torch.bfloat16, grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None), SequenceClassifierOutput(loss=tensor(0.9375, device='cuda:0', grad_fn=<NllLossBackward0>), logits=tensor([[ 0.0918,  1.1250, -0.5508, -0.6836, -0.0080],
        [-0.1138,  1.1094, -0.2051, -0.7266, -0.5859],
        [-0.1021,  1.3594, -0.3613, -0.7383, -0.5273],
        [-0.0771,  1.0781,  0.1523, -0.7578, -0.5352],
        [-0.0337,  0.8281, -0.4629, -0.6875, -0.5312],
        [ 0.1240,  1.0625, -0.0669, -0.4473, -0.3125],
        [ 0.1025,  1.0234, -0.5703, -0.7852, -0.0815],
        [ 0.1504,  1.1250, -0.5703, -0.4844, -0.3516],
        [-0.1670,  1.5156, -0.0090, -0.7539, -0.5391],
        [ 0.1738,  0.9922, -0.1123, -0.5391, -0.6133],
        [-0.0947,  1.2656, -0.3262, -0.9531, -0.3652],
        [-0.0884,  1.0703, -0.2617, -0.7891, -0.5352],
        [-0.0209,  0.6133, -0.4062, -0.6602, -0.5117],
        [-0.0889,  1.0234, -0.1069, -0.5508, -0.3340],
        [-0.3789,  0.8633, -0.3984, -0.1050, -0.7734],
        [-0.2129,  1.0859, -0.1904, -0.6680, -0.4727]], device='cuda:0',
       dtype=torch.bfloat16, grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None), SequenceClassifierOutput(loss=tensor(1.1421, device='cuda:0', grad_fn=<NllLossBackward0>), logits=tensor([[-0.2197,  0.8281, -0.2969, -0.8008, -0.8633],
        [ 0.1011,  1.1328, -0.4629, -0.7188, -0.4336],
        [-0.1152,  1.1172, -0.3164, -0.6367, -0.5352],
        [-0.1680,  1.2891, -0.2539, -0.7031, -0.4277],
        [-0.3301,  1.0312,  0.1621, -0.4199, -0.4609],
        [-0.0210,  0.9336, -0.5078, -0.2812, -0.4219],
        [-0.1357,  1.2188, -0.2891, -0.5000, -0.2500],
        [-0.2383,  0.7891,  0.0669, -0.7109, -0.4453],
        [ 0.0608,  1.3203, -0.2021, -0.3828, -0.8750],
        [-0.0845,  1.3828, -0.3496, -0.7734, -0.1904],
        [ 0.1006,  1.1094, -0.4336, -0.6641, -0.4316],
        [ 0.0065,  1.1562, -0.4453, -0.6406, -0.6367],
        [ 0.2656,  1.1719, -0.5508, -0.7812, -0.6797],
        [ 0.2891,  0.7070, -0.2461, -0.4941, -0.5078],
        [ 0.3613,  0.9844, -0.0918, -0.2373, -0.5469],
        [-0.1973,  1.1484, -0.5977, -0.5234, -0.0728]], device='cuda:0',
       dtype=torch.bfloat16, grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None), SequenceClassifierOutput(loss=tensor(0.8596, device='cuda:0', grad_fn=<NllLossBackward0>), logits=tensor([[-0.0615,  0.6172,  0.0200, -0.6250, -0.6523],
        [-0.0101,  1.0078, -0.2773, -0.8672, -0.4199],
        [ 0.3340,  1.1250, -0.2754, -0.5312, -0.5117],
        [ 0.1934,  0.9336, -0.5742, -0.4863, -0.2676],
        [-0.3203,  0.9258, -0.2520, -0.5117, -0.5938],
        [ 0.1196,  1.0312, -0.2188, -0.4492, -0.5977],
        [ 0.0894,  1.0234, -0.2363, -0.4727, -0.5312],
        [ 0.1572,  1.2031,  0.2734, -0.4844, -0.8164],
        [ 0.0344,  1.1406, -0.5156, -0.5977, -0.3770],
        [-0.0581,  1.2188, -0.1562, -0.6484, -0.5859],
        [ 0.1631,  1.2266, -0.2539, -0.4629, -0.3848],
        [-0.0030,  0.9688, -0.6289, -0.9805, -0.3418],
        [-0.2246,  0.9023, -0.4570, -0.5039, -0.9805],
        [-0.0645,  1.1250, -0.3672, -0.5664, -0.7891],
        [-0.3359,  1.0156, -0.3379, -0.8555, -0.4961],
        [ 0.5938,  0.0250, -0.5508, -0.3633,  0.2471]], device='cuda:0',
       dtype=torch.bfloat16, grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None), SequenceClassifierOutput(loss=tensor(1.0098, device='cuda:0', grad_fn=<NllLossBackward0>), logits=tensor([[-0.1416,  1.1719, -0.4668, -0.8711, -0.3848],
        [ 0.1050,  1.2969, -0.5195, -0.6719, -0.3086],
        [-0.0576,  1.0391, -0.3301, -0.6680, -0.1523],
        [ 0.0850,  1.3672, -0.5156, -0.5508, -0.3867],
        [-0.0327,  1.0234, -0.2969, -0.2324, -0.5039],
        [-0.0459,  1.1172, -0.3047, -0.5273, -0.5703],
        [-0.0923,  1.3594, -0.1465, -0.6680, -0.2559],
        [-0.1143,  0.8750, -0.1816, -0.4316, -0.3164],
        [ 0.1138,  1.0781, -0.0398, -0.6172, -0.6719],
        [ 0.1338,  0.2969, -0.5156, -0.3711, -0.1943],
        [ 0.0225,  1.2344, -0.2832, -0.2598, -0.6016],
        [ 0.2344,  1.1719, -0.4902, -0.6406, -0.7578],
        [ 0.2266,  1.4219, -0.1465, -0.8789, -0.4941],
        [ 0.3242,  1.3594, -0.5508, -0.7305, -0.5703],
        [-0.1338,  1.0234, -0.3457, -0.3965, -0.4941],
        [ 0.0645,  1.1562, -0.6562, -1.0078, -0.6484]], device='cuda:0',
       dtype=torch.bfloat16, grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None), SequenceClassifierOutput(loss=tensor(0.8376, device='cuda:0', grad_fn=<NllLossBackward0>), logits=tensor([[ 0.3164,  0.7148, -1.1016, -0.9766,  0.3398],
        [-0.1118,  1.1797, -0.1855, -0.5938, -0.3574],
        [-0.3262,  1.2344, -0.4590, -0.5938, -0.4609],
        [-0.0330,  0.9883, -0.1318, -0.7031, -0.4395],
        [-0.1387,  1.1875, -0.4199, -0.6719, -0.3691],
        [-0.2578,  1.2969, -0.4922, -0.8594, -0.6953],
        [-0.2344,  0.8828, -0.5000, -0.4727, -0.7539],
        [-0.4688,  1.0391, -0.2256, -0.4492, -0.5117],
        [-0.1670,  1.3750, -0.6992, -0.6211, -0.5000],
        [-0.2158,  0.8125, -0.4883, -0.5781, -0.5469],
        [ 0.0461,  1.1016, -0.5117, -0.6133, -0.7812],
        [ 0.0574,  1.2812, -0.5312, -1.0938, -0.7617],
        [ 0.2158,  1.3047, -0.5156, -0.9023, -0.3945],
        [-0.3008,  0.9805, -0.3828, -0.6172, -0.3262],
        [ 0.0938,  1.3281, -0.1357, -0.7500, -0.3320],
        [-0.0210,  1.1406, -0.3164, -0.4414, -0.5977]], device='cuda:0',
       dtype=torch.bfloat16, grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None), SequenceClassifierOutput(loss=tensor(0.8557, device='cuda:0', grad_fn=<NllLossBackward0>), logits=tensor([[ 0.0874,  1.0312, -0.4219, -0.9492, -0.5742],
        [-0.0554,  0.7734, -0.2656, -0.7773, -0.6016],
        [ 0.0054,  1.2500, -0.3789, -0.7148, -0.6406],
        [-0.0145,  1.3906, -0.2754, -0.4824, -0.5898],
        [ 0.0137,  0.9844, -0.4727, -1.0000, -0.3398],
        [ 0.1475,  1.2344, -0.3281, -0.4180, -0.4082],
        [-0.2178,  0.9375, -0.4004, -0.5664, -0.3711],
        [ 0.2266,  0.9492, -0.6250, -0.4297, -0.4121],
        [ 0.2256,  1.1172, -0.2451, -0.4961, -0.6250],
        [ 0.0291,  1.0625, -0.4844, -0.5938, -0.2070],
        [-0.0771,  1.0000, -0.6328, -0.7969, -0.4297],
        [ 0.1445,  1.4062, -0.4004, -0.6172,  0.0898],
        [-0.1025,  0.7188, -0.7148, -0.9453, -0.5352],
        [-0.2197,  1.2500, -0.3047, -0.4824, -0.5430],
        [-0.3223,  1.4375, -0.3965, -0.6172, -0.4082],
        [-0.0898,  1.5078, -0.5547, -0.8164, -0.1504]], device='cuda:0',
       dtype=torch.bfloat16, grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None), SequenceClassifierOutput(loss=tensor(0.9237, device='cuda:0', grad_fn=<NllLossBackward0>), logits=tensor([[-0.0952,  1.1484, -0.6758, -0.5312, -0.6875],
        [ 0.2949,  0.9297, -0.5820, -0.9102, -0.5117],
        [-0.4043,  1.1406, -0.2617, -0.4648, -0.3789],
        [-0.2832,  1.1406, -0.2832, -0.3730, -0.4590],
        [-0.2715,  1.0703,  0.2031, -0.2715, -0.7383],
        [ 0.1187,  0.9766, -0.6875, -0.2520, -0.7461],
        [-0.0403,  0.8594, -0.1445, -0.8672, -0.3633],
        [-0.3438,  0.6875, -0.3223, -0.4180, -0.5977],
        [-0.1934,  1.1484, -0.4258, -0.7422, -0.8477],
        [ 0.1162,  1.1641, -0.5859, -0.7305, -0.7539],
        [-0.0894,  1.2188, -0.2354, -0.8516, -0.5117],
        [ 0.0483,  0.8359,  0.2061, -0.6211, -0.9648],
        [-0.2158,  0.8477,  0.1270, -0.7109, -0.8594],
        [-0.2344,  0.6211, -0.3184, -0.3691, -0.7148],
        [-0.1025,  1.4141, -0.2617, -1.3125, -0.5820],
        [-0.1445,  1.1641, -0.4609, -0.6250, -0.5820]], device='cuda:0',
       dtype=torch.bfloat16, grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None), SequenceClassifierOutput(loss=tensor(0.9312, device='cuda:0', grad_fn=<NllLossBackward0>), logits=tensor([[ 0.2188,  0.8125, -0.1846, -0.8242, -0.6875],
        [ 0.1064,  1.2734, -0.6602, -0.4219, -0.0603],
        [-0.2236,  1.2891, -0.6016, -0.1084, -0.6211],
        [-0.1602,  1.1328, -0.4668, -0.6211, -0.3496],
        [-0.1011,  1.2969, -0.2168, -1.0078, -0.6172],
        [ 0.4766,  1.1094, -0.1963, -0.3398, -0.2969],
        [-0.0317,  1.1719, -0.4297, -0.6914, -0.1973],
        [ 0.0337,  1.2969, -0.3535, -0.4395, -0.4375],
        [ 0.1299,  1.0469, -0.0227, -0.3945, -0.8711],
        [ 0.0330,  0.8164, -0.2070, -0.4785, -0.2148],
        [ 0.0630,  1.3203,  0.0135, -0.3945, -0.4824],
        [-0.0309,  1.2656, -0.1021, -0.9180, -0.2832],
        [ 0.1177,  1.1016, -0.3105, -0.8359, -0.2871],
        [-0.0275,  0.9609, -0.7227, -0.4316, -0.3574],
        [-0.4355,  0.9141, -0.4727, -0.0688, -0.4668],
        [ 0.2266,  0.9492, -0.5977, -0.7344, -0.1279]], device='cuda:0',
       dtype=torch.bfloat16, grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None), SequenceClassifierOutput(loss=tensor(0.9136, device='cuda:0', grad_fn=<NllLossBackward0>), logits=tensor([[-0.1406,  1.4141, -0.5391, -0.5430, -0.3340],
        [-0.2578,  1.2734, -0.1533, -0.9570, -0.5391],
        [-0.0364,  1.0625, -0.5234, -0.5312, -0.6445],
        [-0.2793,  1.3203, -0.4375, -0.4785, -0.3633],
        [ 0.3301,  0.4219, -0.5781, -0.3008,  0.5352],
        [ 0.0581,  1.2812, -0.3965, -0.7578, -0.5664],
        [-0.1309,  1.2969, -0.3164, -0.6836, -0.5312],
        [ 0.1201,  1.3047, -0.2930, -0.4414, -0.2178],
        [-0.5938,  1.0781, -0.3477, -0.6172, -0.3809],
        [-0.0981,  1.3047, -0.2480, -0.6953, -0.4375],
        [-0.0098,  0.9453, -0.3633, -0.7539, -0.7422],
        [ 0.0498,  1.4375, -0.3613, -0.5859, -0.2559],
        [-0.0063,  1.3047, -0.4941, -0.6992, -0.4023],
        [-0.1436,  1.0938, -0.1465, -0.7930, -0.6250],
        [-0.0957,  0.8555, -0.4414, -0.2295, -0.6523],
        [-0.0098,  1.2578, -0.4004, -0.6680, -0.3730]], device='cuda:0',
       dtype=torch.bfloat16, grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None), SequenceClassifierOutput(loss=tensor(0.9252, device='cuda:0', grad_fn=<NllLossBackward0>), logits=tensor([[ 0.2148,  1.3203, -0.4160, -0.5625, -0.3789],
        [-0.0598,  1.0078, -0.4668, -0.6094, -0.2969],
        [-0.2871,  0.7578, -0.0579, -0.9258, -0.4043],
        [ 0.2012,  1.1406, -0.3438, -0.6914, -0.4629],
        [-0.3438,  0.9727, -0.0088, -0.7031, -0.7539],
        [ 0.1973,  1.1641, -0.5547, -0.4961, -0.4258],
        [-0.1123,  1.1406, -0.0056, -0.5625, -0.5234]], device='cuda:0',
       dtype=torch.bfloat16, grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)], [SequenceClassifierOutput(loss=tensor(2.0635, device='cuda:0', grad_fn=<NllLossBackward0>), logits=tensor([[ 0.0327,  1.1562, -0.5117, -0.8438, -0.3086],
        [ 0.3750,  0.6289, -0.9453, -0.9102,  0.1299],
        [ 0.0325,  0.7227, -0.3066, -0.6133, -0.4043],
        [-0.1738,  0.8008, -0.0137, -0.6445, -0.9883],
        [-0.1572,  0.9375, -0.3242, -0.5352, -0.7148],
        [-0.1846,  0.9141, -0.4316, -0.6641, -0.4336],
        [ 0.1118,  1.0703, -0.5703, -0.8086, -0.3281],
        [ 0.1201,  1.0078, -0.6211, -0.6289, -0.4922],
        [-0.0050,  1.0078, -0.1787, -0.7852, -0.5820],
        [ 0.0398,  0.8086, -0.3535, -0.2832, -0.8789],
        [-0.2637,  0.3809, -0.1089, -0.2109, -0.1455],
        [-0.0291,  0.9453,  0.0193, -0.5898, -0.6133],
        [ 0.1318,  0.9766, -0.4141, -0.4414, -0.1631],
        [ 0.2207,  0.6641, -0.5586, -0.5273,  0.0776],
        [ 0.0131,  0.9062, -0.7500, -0.7031, -0.7109],
        [ 0.0923,  1.0391, -0.1719, -0.4082, -0.5078]], device='cuda:0',
       dtype=torch.bfloat16, grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None), SequenceClassifierOutput(loss=tensor(2.1074, device='cuda:0', grad_fn=<NllLossBackward0>), logits=tensor([[ 0.1836,  1.1719, -0.3184, -0.5586, -0.5820],
        [-0.1914,  1.0859, -0.4609, -0.7227, -0.3809],
        [ 0.1299,  0.8945, -0.4512, -0.6133, -0.4746],
        [ 0.0261,  0.8906, -0.2500, -0.4414, -0.6406],
        [ 0.1494,  1.1328, -0.3398, -0.3086, -0.5391],
        [-0.1816,  0.7852, -0.5352, -0.4238, -0.5547],
        [-0.0142,  1.0469, -0.3906, -0.8086, -0.5742],
        [-0.2021,  0.9102, -0.0713, -0.8047, -0.6133],
        [-0.0222,  1.0156, -0.3594, -0.6914, -0.3750],
        [-0.2041,  0.8242, -0.3281, -0.8477, -0.6250],
        [-0.3203,  0.9844, -0.2988, -0.8125, -0.3809],
        [-0.4434,  1.1484, -0.2910, -0.7891, -0.3906],
        [ 0.3516,  0.5000, -0.9141, -0.8398,  0.2559],
        [ 0.1328,  0.6289, -0.8320, -0.5430, -0.2754],
        [ 0.1235,  0.8594, -0.0334, -0.1699, -0.1216],
        [-0.0801,  0.9883, -0.2500, -0.4941, -0.4512]], device='cuda:0',
       dtype=torch.bfloat16, grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None), SequenceClassifierOutput(loss=tensor(1.8530, device='cuda:0', grad_fn=<NllLossBackward0>), logits=tensor([[ 0.1211,  0.6055, -0.7461, -0.2100, -0.1299],
        [-0.3359,  0.7031, -0.3828, -0.3496, -0.4980],
        [ 0.2490,  0.8672, -0.3730, -0.6289, -0.1670],
        [-0.0957,  1.3047, -0.2832, -0.3262, -0.2578],
        [ 0.0508,  0.8125, -0.4375, -0.6523, -0.3789],
        [ 0.4121,  0.4961, -0.8984, -0.5625, -0.0674],
        [-0.0850,  1.0078, -0.0977, -0.4199, -0.5977],
        [-0.2656,  1.0234, -0.1123, -0.0981, -0.2451],
        [-0.1641,  0.8516, -0.3965, -0.1748, -0.2256],
        [-0.2393,  1.1484, -0.1152, -0.6797, -0.6914],
        [-0.0547,  0.9141, -0.5820, -0.4609, -0.5898],
        [ 0.3281,  0.4746, -0.8359, -0.6250,  0.3320],
        [-0.1040,  0.8594, -0.2793, -0.1504, -0.3555],
        [ 0.3184,  0.8906, -0.6367, -0.5312, -0.1846],
        [-0.2480,  1.2500, -0.2158, -0.6406, -0.2832],
        [ 0.3418,  0.6797, -0.7109, -0.8945,  0.2656]], device='cuda:0',
       dtype=torch.bfloat16, grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None), SequenceClassifierOutput(loss=tensor(2.0894, device='cuda:0', grad_fn=<NllLossBackward0>), logits=tensor([[-2.4414e-01,  1.0391e+00, -3.7891e-01, -9.5703e-01, -2.9688e-01],
        [-8.3618e-03,  1.0469e+00, -1.9629e-01, -6.1719e-01, -4.6289e-01],
        [-7.0801e-02,  1.0703e+00, -3.4332e-03, -6.1328e-01, -5.1562e-01],
        [ 1.6846e-02,  9.0625e-01, -4.0234e-01, -6.7969e-01, -6.3672e-01],
        [ 6.5430e-02,  1.0078e+00, -3.7500e-01, -9.9609e-01, -4.7266e-01],
        [ 2.7539e-01,  8.9844e-01, -1.0205e-01, -7.2266e-01, -6.3672e-01],
        [-2.1777e-01,  1.0078e+00, -3.1445e-01, -6.2500e-01, -4.0039e-01],
        [ 3.5156e-02,  6.3672e-01, -3.5352e-01, -4.5898e-01, -3.0664e-01],
        [ 3.1836e-01,  5.2734e-01, -6.2891e-01, -5.7812e-01,  5.8984e-01],
        [ 3.3203e-01,  8.7891e-01, -3.8086e-01, -4.4922e-01, -4.5703e-01],
        [-3.3188e-04,  1.4219e+00, -5.1953e-01, -4.2188e-01, -6.9531e-01],
        [-1.3086e-01,  7.6172e-01, -5.8984e-01, -6.6797e-01, -5.9766e-01],
        [ 2.6367e-01,  5.3906e-01, -4.8438e-01, -5.7812e-01, -3.4180e-01],
        [-2.8125e-01,  1.2422e+00, -5.5859e-01, -7.3047e-01, -5.5469e-01],
        [ 3.0664e-01,  7.8906e-01, -1.0156e-01, -5.2734e-01, -5.4297e-01],
        [-4.5898e-02,  1.0781e+00, -5.3125e-01, -6.9531e-01, -7.0703e-01]],
       device='cuda:0', dtype=torch.bfloat16, grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None), SequenceClassifierOutput(loss=tensor(2.2061, device='cuda:0', grad_fn=<NllLossBackward0>), logits=tensor([[-0.0762,  1.6016, -0.6211, -0.5781, -0.3145],
        [ 0.3047,  1.1719, -0.3691, -0.6094, -0.2949],
        [-0.1455,  0.9531, -0.6133, -0.7305, -0.3887],
        [ 0.2383,  1.1797, -0.4297, -0.3750, -0.0366],
        [-0.2695,  0.9180, -0.3242, -0.7617, -0.5430],
        [ 0.2656,  0.3418, -0.6914, -0.4238,  0.2412],
        [-0.0364,  1.2500, -0.4062, -0.9688, -0.5938],
        [ 0.4492,  0.7227, -0.6523, -0.6406,  0.3984],
        [ 0.4766,  0.9883, -0.6406, -0.8281, -0.4355],
        [-0.5156,  1.1641, -0.2559, -0.7461, -0.7266],
        [ 0.4590,  0.7266, -0.4082, -0.9414, -0.4219],
        [-0.0520,  0.7070, -0.4180, -0.7891, -0.5312],
        [-0.3672,  1.0156,  0.0245, -0.4434, -0.3262],
        [ 0.0608,  1.0156,  0.0249, -0.8555, -0.3242],
        [ 0.1211,  0.6836, -0.6055, -0.7773, -0.3125],
        [ 0.2559,  0.8203, -0.5820, -0.5273, -0.4336]], device='cuda:0',
       dtype=torch.bfloat16, grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None), SequenceClassifierOutput(loss=tensor(1.3203, device='cuda:0', grad_fn=<NllLossBackward0>), logits=tensor([[ 0.0132,  0.9023, -0.2422, -0.6797, -0.4863],
        [-0.0781,  0.8438, -0.0532, -0.7578, -0.5391],
        [ 0.2598,  1.2266, -0.4199, -0.5234, -0.3770],
        [ 0.0544,  0.9219, -0.4609, -0.7578, -0.3105],
        [-0.1533,  0.9062, -0.6758, -0.6406, -0.4824],
        [ 0.0278,  0.9375, -0.3262, -0.7852, -0.3574],
        [-0.1221,  0.9453, -0.1289, -0.2754, -1.1094],
        [ 0.0221,  1.1641, -0.1172, -0.8164, -0.5078],
        [-0.2031,  1.2422, -0.2500, -0.9141, -0.9102],
        [-0.0986,  1.0078, -0.2852, -0.5938, -0.5664],
        [-0.1128,  0.9141, -0.2734, -0.8633, -0.4355],
        [-0.1729,  0.7734,  0.0703, -0.6875, -0.7383],
        [-0.1187,  1.3203,  0.0422, -0.6875, -0.4883],
        [-0.0938,  0.4668, -0.4570, -0.8086, -0.5469],
        [-0.1162,  1.0234, -0.2520, -0.7070, -0.5352],
        [-0.0110,  0.8516, -0.3320, -0.3887, -0.7305]], device='cuda:0',
       dtype=torch.bfloat16, grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None), SequenceClassifierOutput(loss=tensor(1.1396, device='cuda:0', grad_fn=<NllLossBackward0>), logits=tensor([[ 0.2793,  1.1484, -0.4922, -0.6289, -0.3691],
        [-0.0381,  0.8906, -0.0630, -0.8047, -0.9922],
        [-0.1289,  0.4805, -0.2773, -0.5820, -0.3340],
        [ 0.0142,  1.0938, -0.2344, -0.6641, -0.8828],
        [-0.1050,  0.8789, -0.1250, -0.8398, -0.6328],
        [-0.3555,  1.0625, -0.1416, -0.3262, -0.9336],
        [-0.0859,  1.1641,  0.0139, -0.5742, -0.7070],
        [-0.3652,  0.8359, -0.2354, -0.1367, -1.0234],
        [-0.0122,  1.0938,  0.1230, -0.8281, -0.8477],
        [ 0.1377,  1.1641, -0.1924, -0.9180, -0.7930],
        [-0.1709,  0.8320, -0.1865, -0.2988, -0.5312],
        [-0.5391,  0.7109, -0.4824, -0.6133, -0.6719],
        [-0.2412,  1.0859, -0.3281, -0.6641, -1.0000],
        [ 0.1865,  1.1562,  0.0684, -0.6953, -0.8555],
        [-0.3262,  0.8164, -0.6367, -1.0703, -0.4863],
        [ 0.0398,  0.7930, -0.0791, -0.1738, -0.4453]], device='cuda:0',
       dtype=torch.bfloat16, grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None), SequenceClassifierOutput(loss=tensor(0.8330, device='cuda:0', grad_fn=<NllLossBackward0>), logits=tensor([[ 0.1885,  0.7539,  0.1992, -0.7344, -0.7891],
        [-0.1235,  1.0781, -0.2334, -0.6836, -1.0000],
        [ 0.2314,  0.9648, -0.0752, -0.5273, -0.5781],
        [-0.0933,  1.0391, -0.1436, -0.6719, -0.9023],
        [-0.3359,  0.8594,  0.0099, -0.6562, -0.7344],
        [ 0.0659,  0.8594, -0.4570, -0.5859, -0.3984],
        [ 0.1484,  0.9805, -0.2500, -0.6523, -0.4570],
        [-0.2256,  0.7812,  0.1055, -0.2793, -0.7266],
        [-0.2275,  1.0781, -0.2148, -0.6523, -0.4941],
        [-0.2773,  0.9258, -0.1299, -0.5273, -0.9258],
        [ 0.2441,  0.6211, -0.2070, -1.0625, -0.5391],
        [-0.1416,  0.7812, -0.3809, -0.7969, -0.4121],
        [-0.2178,  1.2734, -0.1826, -0.8828, -0.7031],
        [-0.0403,  0.7539, -0.1709, -0.7266, -0.6875],
        [-0.2080,  0.8711, -0.2578, -0.7812, -0.7227],
        [-0.1016,  0.5000, -0.2217, -0.5820, -0.8438]], device='cuda:0',
       dtype=torch.bfloat16, grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None), SequenceClassifierOutput(loss=tensor(0.8269, device='cuda:0', grad_fn=<NllLossBackward0>), logits=tensor([[ 0.0488,  0.6406, -0.4785, -0.7188, -0.8320],
        [ 0.2852,  0.9336, -0.0811, -0.5742, -0.7500],
        [-0.3750,  1.2734,  0.3184, -0.7812, -0.7461],
        [-0.1143,  0.7383, -0.1172, -0.7930, -0.5586],
        [-0.2080,  0.5508, -0.3594, -0.4570, -0.7656],
        [ 0.0684,  1.0078, -0.1992, -0.7578, -1.0391],
        [-0.2344,  0.9492, -0.0654, -0.8320, -0.8281],
        [-0.3047,  1.0781, -0.2109, -0.8672, -0.9258],
        [ 0.0479,  1.1016, -0.1934, -0.7070, -0.3984],
        [ 0.0425,  0.9648, -0.2852, -0.8125, -0.6523],
        [-0.4570,  1.2109, -0.0093, -0.6133, -0.3789],
        [-0.1562,  0.7422, -0.1367, -0.5195, -0.9062],
        [-0.1387,  1.1562,  0.0552, -0.5547, -0.4727],
        [-0.1973,  0.8555, -0.2539, -0.5273, -0.6992],
        [-0.0625,  0.7617, -0.3281, -0.7148, -0.4980],
        [-0.4746,  1.1953, -0.5000, -1.1484, -0.3984]], device='cuda:0',
       dtype=torch.bfloat16, grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None), SequenceClassifierOutput(loss=tensor(0.8997, device='cuda:0', grad_fn=<NllLossBackward0>), logits=tensor([[-0.0547,  1.0312, -0.2051, -0.6406, -0.6445],
        [-0.3398,  0.7148, -0.1245, -0.8945, -0.6758],
        [ 0.0737,  0.9766, -0.5273, -0.3574, -0.4375],
        [-0.0476,  0.9727, -0.0967, -0.7031, -0.5312],
        [ 0.0183,  0.9375, -0.3906, -0.7812, -0.7383],
        [-0.1885,  0.8047, -0.1816, -0.6367, -0.6680],
        [ 0.0728,  1.3359, -0.0237, -0.8164, -0.6523],
        [-0.3750,  1.1562, -0.3613, -0.9414, -0.4844],
        [-0.2891,  0.9180, -0.0786, -0.5469, -0.9297],
        [-0.2559,  0.8594, -0.0703, -0.6055, -0.8125],
        [-0.2002,  0.8750, -0.2334, -1.0234, -0.6016],
        [-0.2266,  0.7500, -0.0034, -0.3164, -0.6328],
        [ 0.2139,  0.7344, -0.4277, -0.9570, -0.5898],
        [ 0.1738,  1.3984, -0.4512, -0.4629, -0.1367],
        [ 0.1592,  0.8398, -0.6133, -0.6875, -0.5391],
        [-0.2129,  1.2656, -0.0168, -0.8828, -0.8516]], device='cuda:0',
       dtype=torch.bfloat16, grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None), SequenceClassifierOutput(loss=tensor(1.0432, device='cuda:0', grad_fn=<NllLossBackward0>), logits=tensor([[-2.0508e-01,  8.5156e-01, -1.8164e-01, -1.0234e+00, -7.1484e-01],
        [-6.0059e-02,  8.3594e-01, -8.0566e-02, -5.2344e-01, -9.2188e-01],
        [-1.1865e-01,  6.0938e-01, -4.0234e-01, -5.3906e-01, -1.0859e+00],
        [-1.9043e-02,  9.5312e-01, -1.6113e-01, -9.2969e-01, -8.3984e-01],
        [-2.8906e-01,  1.2422e+00,  1.8359e-01, -6.5625e-01, -6.4062e-01],
        [-1.5039e-01,  5.7031e-01,  9.7168e-02, -5.5859e-01, -9.8438e-01],
        [-1.4941e-01,  6.7969e-01, -1.7285e-01, -4.7070e-01, -1.1562e+00],
        [ 1.3867e-01,  1.0938e+00, -2.7148e-01, -9.7266e-01, -5.7031e-01],
        [-5.1270e-02,  7.3438e-01, -3.7109e-01, -9.8047e-01, -4.8438e-01],
        [-3.5352e-01,  1.0156e+00,  2.1362e-02, -7.1484e-01, -3.7109e-01],
        [ 2.7539e-01,  1.3125e+00,  1.7676e-01, -4.8047e-01, -3.3789e-01],
        [-5.3467e-02,  1.1875e+00, -1.7871e-01, -7.8906e-01, -5.6250e-01],
        [-1.2695e-01,  7.6562e-01, -1.3477e-01, -4.0820e-01, -7.3828e-01],
        [ 1.1139e-03,  1.2109e+00, -3.2812e-01, -9.0625e-01, -7.2656e-01],
        [-3.0273e-01,  1.0469e+00,  1.1816e-01, -9.6875e-01, -7.0312e-01],
        [-1.8799e-02,  5.0781e-01, -3.7891e-01, -8.3203e-01, -2.6953e-01]],
       device='cuda:0', dtype=torch.bfloat16, grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None), SequenceClassifierOutput(loss=tensor(1.0598, device='cuda:0', grad_fn=<NllLossBackward0>), logits=tensor([[ 0.0771,  0.6523, -0.4668, -0.8164, -0.4980],
        [-0.1089,  1.0312, -0.2119, -0.6562, -0.7617],
        [ 0.0737,  0.9414, -0.3887, -0.8086, -0.8633],
        [-0.1777,  0.9922, -0.3086, -1.1719, -0.5742],
        [-0.1787,  0.6758, -0.1699, -0.5078, -0.5586],
        [ 0.0264,  1.1406, -0.0461, -0.7031, -0.6641],
        [-0.1621,  0.8711, -0.1885, -0.8867, -0.4805],
        [-0.1187,  0.6289,  0.2598, -0.5195, -0.9961],
        [ 0.0317,  0.8984,  0.0586, -0.5508, -0.6016],
        [-0.2305,  1.1406, -0.2314, -0.6914, -0.7227],
        [-0.5469,  0.7070, -0.0281, -0.8008, -0.9219],
        [-0.0131,  0.8945, -0.0830, -0.4102, -0.5938],
        [-0.0393,  0.9727, -0.1738, -0.9727, -0.6875],
        [-0.2344,  1.1719, -0.0258, -0.7148, -0.7266],
        [-0.2480,  1.1953, -0.1748, -1.0156, -0.4902],
        [-0.0459,  0.9727, -0.0067, -0.7930, -0.5352]], device='cuda:0',
       dtype=torch.bfloat16, grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None), SequenceClassifierOutput(loss=tensor(0.9453, device='cuda:0', grad_fn=<NllLossBackward0>), logits=tensor([[-0.2539,  1.0156,  0.0347, -0.4492, -0.5859],
        [ 0.1436,  1.0781, -0.0272, -0.6367, -0.6641],
        [ 0.3301,  1.0469,  0.2119, -0.7227, -0.6641],
        [-0.2891,  1.1094,  0.0605, -0.4746, -0.6484],
        [ 0.0811,  1.4297, -0.1484, -0.5234, -0.4668],
        [-0.3516,  0.8438, -0.2188, -0.5547, -0.7031],
        [-0.1260,  1.2422, -0.1572, -0.4863, -0.8906],
        [ 0.0708,  0.6562, -0.2695, -0.4980, -0.4941],
        [ 0.1455,  1.0781, -0.1094, -0.6680, -0.4941],
        [-0.1641,  0.9688, -0.4473, -1.2500, -0.4492],
        [-0.1099,  1.0547, -0.0415, -0.5547, -0.4492],
        [-0.2002,  0.9727, -0.2275, -0.7148, -0.3438],
        [-0.0776,  0.9062, -0.1416, -0.5625, -0.8867],
        [-0.2910,  0.9219,  0.1416, -0.6289, -0.8203],
        [ 0.1016,  1.0391, -0.0415, -0.9258, -0.7656],
        [-0.0513,  1.2266, -0.2119, -0.6484, -0.6484]], device='cuda:0',
       dtype=torch.bfloat16, grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None), SequenceClassifierOutput(loss=tensor(0.8876, device='cuda:0', grad_fn=<NllLossBackward0>), logits=tensor([[-0.0957,  1.2734, -0.4023, -0.4551, -0.6445],
        [ 0.0850,  0.8945, -0.2021, -0.8789, -0.5820],
        [ 0.2969,  1.0469, -0.1777, -0.3789, -0.7969],
        [-0.0190,  0.9844, -0.2852, -0.6562, -0.7148],
        [-0.2793,  1.0859,  0.0220, -0.4199, -0.9492],
        [-0.0796,  0.5742, -0.0525, -0.3184, -0.6211],
        [-0.2793,  0.5273, -0.0168, -0.4648, -0.5195],
        [-0.1157,  0.9062, -0.1172, -0.6406, -0.5352],
        [ 0.0576,  1.0312, -0.0320, -0.6875, -0.6602],
        [-0.1016,  1.0156, -0.3535, -0.7422, -0.2910],
        [-0.1377,  1.0078, -0.2109, -0.7734, -0.5195],
        [-0.3418,  1.2656, -0.6211, -0.5156, -0.1777],
        [-0.2969,  1.1406, -0.1338, -0.6055, -0.6562],
        [ 0.6406,  0.7422, -0.6211, -1.0859, -0.4160],
        [-0.2051,  1.4766, -0.2812, -0.7734, -0.4238],
        [ 0.0330,  0.8672, -0.2539, -0.6992, -0.5586]], device='cuda:0',
       dtype=torch.bfloat16, grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None), SequenceClassifierOutput(loss=tensor(0.8616, device='cuda:0', grad_fn=<NllLossBackward0>), logits=tensor([[ 0.1445,  1.3125, -0.1572, -0.8477, -0.5117],
        [-0.0522,  1.1875, -0.2090, -0.8477, -0.7930],
        [-0.1426,  0.9688, -0.3789, -0.4766, -0.5312],
        [-0.2334,  1.1250, -0.1367, -0.7539, -0.5938],
        [-0.2520,  1.1797, -0.1436, -0.6250, -0.4121],
        [-0.0840,  0.9883,  0.1768, -0.7812, -0.6797],
        [ 0.4512,  0.8398,  0.1562, -0.7188, -0.5508],
        [ 0.0256,  0.6953, -0.2695, -0.8125, -0.8281],
        [-0.1689,  0.6758, -0.3379, -0.8164, -0.8242],
        [ 0.1069,  0.9922,  0.0352, -0.6992, -0.5859],
        [-0.0466,  0.9570, -0.1650, -0.6406, -0.5781],
        [-0.0220,  1.0078, -0.0664, -0.6328, -0.7656],
        [-0.2891,  0.9961, -0.2305, -0.8164, -0.6602],
        [-0.2217,  0.8359, -0.1387, -0.6914, -0.7305],
        [-0.1787,  0.9375, -0.2002, -0.6602, -1.0547],
        [ 0.2793,  1.0469, -0.6406, -0.8047, -0.7422]], device='cuda:0',
       dtype=torch.bfloat16, grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None), SequenceClassifierOutput(loss=tensor(0.9182, device='cuda:0', grad_fn=<NllLossBackward0>), logits=tensor([[-0.0625,  0.9609,  0.1084, -0.4746, -0.9180],
        [-0.1533,  1.5000,  0.2441, -0.4688, -0.7383],
        [-0.4004,  1.1719, -0.5234, -0.7266, -0.7031],
        [-0.1611,  1.0234, -0.2676, -0.6562, -0.6055],
        [-0.1621,  0.7617, -0.1016, -0.8516, -1.1797],
        [-0.0237,  0.9727, -0.3125, -0.5703, -0.2012],
        [-0.1436,  0.6914, -0.3320, -0.7812, -0.9453],
        [ 0.0957,  0.7578, -0.0679, -0.2930, -0.6367],
        [ 0.2256,  0.9258, -0.2051, -0.6016, -0.6641],
        [ 0.1123,  0.6211, -0.3398, -0.6367, -0.4199],
        [ 0.1177,  1.1875, -0.2373, -0.9844, -0.6992],
        [ 0.0728,  0.8672, -0.0354, -0.9414, -0.6016],
        [ 0.1846,  0.6680, -0.6484, -0.6914, -0.4238],
        [ 0.0452,  0.8008, -0.0317, -0.4551, -0.8867],
        [-0.2891,  1.1797,  0.1963, -0.4004, -0.4844],
        [-0.4473,  0.7812, -0.2119, -0.5195, -1.1406]], device='cuda:0',
       dtype=torch.bfloat16, grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None), SequenceClassifierOutput(loss=tensor(1.0112, device='cuda:0', grad_fn=<NllLossBackward0>), logits=tensor([[-0.1484,  0.9883,  0.1582, -0.5547, -0.8281],
        [-0.3320,  1.0703,  0.0933, -0.7852, -0.6172],
        [-0.4062,  1.2031, -0.1270, -0.7852, -0.5469],
        [-0.1289,  0.8477, -0.3047, -0.7031, -0.7539],
        [ 0.9102,  0.6250, -0.4199, -0.2676,  0.3066],
        [ 0.0776,  1.0156,  0.0107, -0.8125, -1.0156],
        [ 0.2383,  1.0000, -0.2432, -0.5898, -0.8594],
        [-0.0029,  1.1328, -0.0908, -0.8945, -0.7188],
        [-0.1118,  1.2734, -0.2520, -0.7578, -0.4648],
        [-0.0079,  0.8945, -0.0277, -0.8945, -1.0703],
        [-0.4121,  0.6992, -0.0747, -0.6133, -0.6680],
        [-0.3789,  1.1172, -0.3496, -0.5508, -0.5195],
        [-0.3789,  1.0156,  0.0250, -0.4336, -1.0234],
        [-0.1455,  0.9570, -0.5117, -0.6484, -0.3320],
        [-0.0069,  0.8750,  0.0923, -0.5859, -0.6680],
        [ 0.1250,  0.8242, -0.5898, -0.5352, -0.3086]], device='cuda:0',
       dtype=torch.bfloat16, grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None), SequenceClassifierOutput(loss=tensor(0.7368, device='cuda:0', grad_fn=<NllLossBackward0>), logits=tensor([[ 0.1216,  0.7500, -0.2217, -0.5859, -0.6172],
        [ 0.0376,  1.0781, -0.1001, -0.6797, -0.9336],
        [-0.1250,  0.7500, -0.1523, -0.7891, -0.4453],
        [-0.0454,  1.0859, -0.4043, -0.6797, -0.9727],
        [ 0.0491,  0.9766, -0.2969, -0.6875, -0.4316],
        [ 0.1709,  1.0625, -0.2910, -0.9648, -0.7812],
        [ 0.1641,  1.3359, -0.1797, -0.4277, -0.3555],
        [-0.1729,  1.0234, -0.2871, -0.7383, -0.3848],
        [-0.2559,  1.2109, -0.2793, -0.9492, -0.8945],
        [-0.0135,  1.2969,  0.0583, -0.7617, -0.1157],
        [ 0.1787,  0.9922, -0.5352, -0.4473, -0.5234],
        [-0.3008,  1.2891, -0.3809, -0.8438, -0.4941],
        [ 0.2197,  1.2422, -0.3652, -0.7344, -0.4863],
        [-0.1138,  1.3125, -0.2598, -0.8164, -0.4941],
        [-0.0579,  0.7070, -0.4043, -0.4453, -0.8047],
        [ 0.0510,  1.0234, -0.3262, -0.5586, -0.3906]], device='cuda:0',
       dtype=torch.bfloat16, grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None), SequenceClassifierOutput(loss=tensor(0.8105, device='cuda:0', grad_fn=<NllLossBackward0>), logits=tensor([[ 0.1836,  1.1641, -0.6641, -0.6250, -0.5625],
        [ 0.2695,  0.8125, -0.5078, -0.9453, -0.3984],
        [-0.3301,  1.0078, -0.6172, -0.4199, -0.5664],
        [-0.0564,  1.0312, -0.3086, -0.0732, -0.4160],
        [-0.2363,  1.1484, -0.0645, -0.4238, -0.4922],
        [-0.1416,  1.0625, -0.6992, -0.4531, -0.3770],
        [ 0.0139,  1.0312, -0.0474, -0.6211, -0.7539],
        [ 0.1162,  1.2344, -0.1484, -0.6133, -0.3730],
        [-0.0747,  0.9375, -0.5664, -0.3848, -0.3281],
        [-0.0903,  0.9531, -0.4297, -0.5820, -0.3828],
        [-0.0125,  1.0156, -0.3496, -0.6367, -0.6875],
        [ 0.1846,  1.4766, -0.4785, -0.6523, -0.6680],
        [-0.3438,  1.2656, -0.5391, -0.6250, -0.5195],
        [ 0.0312,  0.9336, -0.5234, -0.5430, -0.3730],
        [ 0.3945,  0.9570, -0.7695, -0.8281, -0.7070],
        [ 0.1377,  1.1406, -0.4668, -0.6406, -0.5938]], device='cuda:0',
       dtype=torch.bfloat16, grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None), SequenceClassifierOutput(loss=tensor(0.6370, device='cuda:0', grad_fn=<NllLossBackward0>), logits=tensor([[ 0.0039,  1.0859, -0.1816, -0.6914, -0.8984],
        [ 0.0063,  1.0547, -0.3711, -0.4062, -0.5195],
        [-0.2354,  1.2344, -0.4219, -0.9648, -0.7812],
        [ 0.0547,  1.3984, -0.4785, -0.6484, -0.4316],
        [ 0.1709,  1.2891, -0.3125, -0.5898, -0.4180],
        [ 0.1777,  1.2578, -0.5742, -0.9453, -0.5586],
        [ 0.0723,  1.3828, -0.4355, -0.6992, -0.0981],
        [-0.0996,  0.9766, -0.1494, -0.5352, -0.6445],
        [-0.0513,  0.6250, -0.7148, -1.1406, -0.6914],
        [-0.2090,  1.5938, -0.3242, -0.7305, -0.7383],
        [ 0.3535,  0.9375, -0.1406, -0.5312, -0.5820],
        [-0.2637,  1.1406, -0.3848, -0.4395, -0.2871],
        [-0.0228,  1.3125, -0.3320, -0.4004, -0.3340],
        [ 0.0679,  1.1641, -0.2793, -0.5352, -0.4062],
        [ 0.3203,  1.4609, -0.2471, -0.7188, -0.4375],
        [-0.1689,  0.9844, -0.2891, -0.5078, -0.3711]], device='cuda:0',
       dtype=torch.bfloat16, grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None), SequenceClassifierOutput(loss=tensor(0.9933, device='cuda:0', grad_fn=<NllLossBackward0>), logits=tensor([[ 0.0061,  0.9414, -0.5938, -0.6875, -0.8516],
        [-0.0186,  0.9375,  0.0854, -0.6055, -0.7695],
        [-0.0015,  1.3516, -0.5938, -0.4277, -0.5664],
        [-0.2256,  0.8203, -0.0325, -0.8750, -0.7969],
        [-0.3906,  0.8672, -0.0967, -0.8945, -0.5195],
        [ 0.0135,  1.0547, -0.1621, -0.7305, -0.2070],
        [ 0.1089,  0.7812, -0.1621, -0.5234, -0.8984],
        [-0.0732,  1.2734, -0.3379, -0.6602, -0.3418],
        [ 0.0452,  1.2969, -0.2070, -0.6641, -0.4785],
        [ 0.0530,  1.4297, -0.5078, -0.5742, -0.7344],
        [ 0.0410,  0.8477, -0.2168, -0.8047, -0.8711],
        [ 0.0518,  1.1719, -0.3945, -0.4668, -0.5859],
        [ 0.0079,  0.8203, -0.1826, -0.6016, -0.2129],
        [-0.2520,  1.1016, -0.2773, -0.7930, -0.6484],
        [-0.1318,  1.0703, -0.0996, -0.4883, -0.7695],
        [-0.0072,  1.3125, -0.3281, -0.9961, -0.1992]], device='cuda:0',
       dtype=torch.bfloat16, grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None), SequenceClassifierOutput(loss=tensor(0.9331, device='cuda:0', grad_fn=<NllLossBackward0>), logits=tensor([[ 0.0830,  1.3672, -0.2109, -0.4922, -0.7109],
        [-0.2949,  1.5078, -0.3945, -0.6094, -0.3789],
        [-0.0762,  0.9766, -0.4121, -0.8164, -1.0469],
        [ 0.0986,  0.9219, -0.3926, -0.5195, -0.5078],
        [-0.0330,  1.3750, -0.0498, -0.8203, -1.0312],
        [-0.1572,  0.9375, -0.2793, -0.6133, -0.5312],
        [-0.1924,  0.9844, -0.7070, -0.4297, -0.5117],
        [ 0.1245,  1.1562, -0.3105, -0.4629, -1.0469],
        [ 0.2432,  1.2656, -0.1680, -0.7188, -0.6211],
        [ 0.2930,  1.4531, -0.1924, -0.8164, -0.8203],
        [ 0.3281,  1.2969, -0.4473, -0.6875, -0.2539],
        [-0.1719,  1.1797, -0.3398, -0.4453, -0.8047],
        [ 0.1406,  0.3613, -0.6016, -0.7812,  0.1309],
        [-0.0432,  1.2422, -0.1689, -0.7148, -0.5078],
        [-0.0830,  1.0703, -0.4453, -0.4023, -0.4453],
        [-0.0417,  1.1562, -0.6250, -0.9062, -0.3926]], device='cuda:0',
       dtype=torch.bfloat16, grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None), SequenceClassifierOutput(loss=tensor(0.8032, device='cuda:0', grad_fn=<NllLossBackward0>), logits=tensor([[ 0.2715,  1.3203, -0.4883, -0.7344, -0.5859],
        [-0.2051,  1.0078, -0.0371, -0.5625, -0.7461],
        [-0.1196,  1.2188, -0.3535, -0.6758, -0.3359],
        [ 0.1025,  1.0547, -0.7031, -0.9609, -0.5234],
        [-0.0796,  1.1953, -0.3438, -0.8789, -0.8711],
        [ 0.1396,  1.1719, -0.6836, -0.7188, -0.3750],
        [ 0.4434,  1.1094, -0.4746, -0.5625, -0.1455],
        [-0.1455,  1.1016, -0.3535, -0.8828, -0.6562],
        [ 0.1377,  1.1484,  0.0157, -0.2363, -0.8750],
        [-0.2773,  1.2812, -0.5859, -0.7422, -0.4980],
        [-0.2207,  0.9648, -0.7070, -0.7188, -0.4844],
        [ 0.0728,  1.0781, -0.4121, -0.4766, -0.5938],
        [-0.0537,  1.0938, -0.1157, -0.4023, -0.4375],
        [-0.1406,  1.0234, -0.0703, -0.3652, -0.5391],
        [-0.0894,  1.3750, -0.5547, -0.6719, -0.2891],
        [-0.1748,  1.0078,  0.0559, -0.7930, -1.0078]], device='cuda:0',
       dtype=torch.bfloat16, grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None), SequenceClassifierOutput(loss=tensor(0.6750, device='cuda:0', grad_fn=<NllLossBackward0>), logits=tensor([[ 0.0864,  1.0781, -0.6328, -0.6641, -0.4102],
        [-0.0605,  1.4844,  0.0330, -0.6016, -0.7812],
        [ 0.3145,  1.2266, -0.2852, -0.7500, -0.3887],
        [ 0.1777,  0.8672, -0.0258, -0.8594, -0.9375],
        [ 0.0649,  1.2891, -0.2412, -0.5977, -0.5664],
        [ 0.5352,  1.1172, -0.1992, -0.8398, -0.8281],
        [-0.1543,  1.1328, -0.5273, -0.5898, -0.5039],
        [-0.1748,  0.8828, -0.3105, -0.4492, -0.3203],
        [ 0.0378,  1.1562, -0.3516, -0.5391, -0.5312],
        [-0.2969,  1.0859, -0.1245, -0.9531, -0.3047],
        [-0.0762,  1.1875, -0.5469, -0.5625, -0.5117],
        [ 0.0413,  0.9961, -0.2129, -0.3984, -0.1895],
        [-0.0623,  1.1406, -0.5312, -0.6406, -0.3359],
        [ 0.0342,  1.2031, -0.5586, -0.5742, -0.3750],
        [-0.0128,  1.1641, -0.2109, -0.4746, -0.3457],
        [-0.0226,  0.9023, -0.2393, -0.7578, -0.7695]], device='cuda:0',
       dtype=torch.bfloat16, grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None), SequenceClassifierOutput(loss=tensor(0.9988, device='cuda:0', grad_fn=<NllLossBackward0>), logits=tensor([[-0.2988,  0.9922,  0.2021, -0.2812, -1.0391],
        [-0.0571,  0.9062, -0.0952, -0.7031, -0.6445],
        [-0.0850,  1.1562, -0.3848, -0.6797, -0.9648],
        [-0.1807,  0.7812, -0.3574, -0.7461, -0.4766],
        [ 0.3086,  1.1172, -0.1787, -0.5703, -0.7070],
        [-0.1387,  1.1250, -0.2148, -0.7305, -0.3125],
        [ 0.0505,  1.1562, -0.9141, -0.8242, -0.0417],
        [ 0.5742,  0.3008, -0.7227,  0.0332,  0.1562],
        [-0.1260,  1.2188, -0.1465, -0.6016, -0.6836],
        [-0.3633,  0.6484, -0.4512, -0.2715, -0.4180],
        [ 0.2021,  1.0156, -0.8008, -0.8320, -0.4531],
        [ 0.1885,  1.1562, -0.1982, -0.5586, -0.3164],
        [-0.0693,  1.1172, -0.5117, -0.6914, -0.6055],
        [ 0.1143,  0.9336, -0.5117, -0.4922, -0.4297],
        [-0.0811,  1.2969, -0.7500, -0.6367, -0.3359],
        [-0.0184,  0.8828, -0.3105, -0.7695, -0.4785]], device='cuda:0',
       dtype=torch.bfloat16, grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None), SequenceClassifierOutput(loss=tensor(0.8423, device='cuda:0', grad_fn=<NllLossBackward0>), logits=tensor([[-0.0679,  0.9766, -0.5547, -0.2432, -0.7969],
        [-0.1611,  1.2188, -0.1270, -0.7148, -0.5859],
        [ 0.0503,  1.2188, -0.2832, -0.6484, -0.6094],
        [ 0.2363,  1.1797, -0.4844, -0.5469, -0.3516],
        [ 0.0215,  1.2422, -0.1650, -0.7461, -0.4902],
        [ 0.2891,  1.0156, -0.4141, -0.5000, -0.4453],
        [-0.1992,  1.1406, -0.4805, -0.5625, -0.6641],
        [-0.0425,  1.0859, -0.7344, -0.8047, -0.6484],
        [-0.0142,  1.2188, -0.3203, -0.4160, -0.2891],
        [ 0.1357,  0.8281,  0.1504, -0.8359, -0.5430],
        [ 0.2139,  1.2422, -0.3770, -0.7266, -0.1064],
        [-0.0439,  0.8633, -0.1338, -0.4824, -0.2178],
        [ 0.0718,  1.4688, -0.1748, -0.1309, -0.4180],
        [ 0.0645,  1.0391, -0.4707, -0.6797, -0.2949],
        [-0.1040,  1.1484, -0.2559, -0.5547, -0.4883],
        [ 0.0349,  1.0625, -0.6055, -0.5703, -0.2598]], device='cuda:0',
       dtype=torch.bfloat16, grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None), SequenceClassifierOutput(loss=tensor(0.7534, device='cuda:0', grad_fn=<NllLossBackward0>), logits=tensor([[-0.2207,  1.1328, -0.4844, -0.6992, -0.2949],
        [ 0.0136,  1.1328, -0.1895, -0.6055, -1.3906],
        [ 0.4141,  1.5156, -0.4277, -0.8320, -0.2617],
        [ 0.1641,  0.9922,  0.1143, -0.7500, -0.7227],
        [ 0.0454,  0.9492, -0.5469, -0.3047, -0.3789],
        [-0.1455,  1.0312, -0.0557, -0.5664, -0.4512],
        [ 0.0532,  1.3125, -0.1504, -0.7812, -0.4062],
        [-0.3418,  0.9883,  0.0481, -0.9648, -0.8164],
        [ 0.1260,  1.0703, -0.3398, -0.8594, -0.3652],
        [ 0.1104,  1.2891, -0.3672, -0.5977, -0.3262],
        [-0.0659,  1.1406, -0.1250, -0.6914, -0.5117],
        [-0.0674,  1.4453, -0.3926, -0.3633, -0.7461],
        [-0.0255,  1.1094, -0.4121, -0.5664, -0.5078],
        [ 0.1875,  1.1406,  0.0752, -0.7773, -0.5430],
        [ 0.1455,  1.2344, -0.0781, -0.7734, -0.5859],
        [ 0.0850,  0.6992, -0.3867, -0.8398, -0.5625]], device='cuda:0',
       dtype=torch.bfloat16, grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None), SequenceClassifierOutput(loss=tensor(0.7787, device='cuda:0', grad_fn=<NllLossBackward0>), logits=tensor([[-0.1328,  1.1953, -0.5508, -0.3574, -0.3398],
        [-0.1064,  1.0625, -0.5039, -0.7969, -0.8359],
        [ 0.0149,  1.4297, -0.5312, -0.7891, -0.4551],
        [-0.4297,  0.5898, -0.5664, -0.3906, -0.5703],
        [ 0.0337,  1.3438, -0.3203, -0.9609, -0.7109],
        [-0.0884,  1.0234, -0.5195, -0.8477, -0.4277],
        [ 0.1133,  1.4141, -0.1582, -0.8359, -0.1455],
        [-0.0596,  1.0312, -0.1807, -0.6055, -0.4180],
        [-0.0192,  0.9219, -0.4727, -0.6367, -0.5703],
        [-0.1328,  0.9609,  0.0121, -0.6758, -0.7305],
        [-0.0173,  0.9883,  0.0172, -0.4297, -0.7812],
        [-0.3711,  0.9492, -0.4043, -0.5508, -0.4648],
        [ 0.5703,  0.4727, -1.1328, -0.7578,  0.2305],
        [ 0.0466,  1.0234, -0.0398, -0.3828, -0.8555],
        [-0.2061,  1.3047, -0.3574, -0.4004, -0.5352],
        [ 0.0103,  1.0547, -0.0864, -0.9141, -0.7305]], device='cuda:0',
       dtype=torch.bfloat16, grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None), SequenceClassifierOutput(loss=tensor(0.6917, device='cuda:0', grad_fn=<NllLossBackward0>), logits=tensor([[ 0.1074,  1.3828, -0.3789, -0.8945, -0.4219],
        [ 0.0060,  1.1172, -0.3008, -0.1357, -0.6719],
        [-0.0035,  1.1328, -0.5938, -0.8398, -0.1523],
        [ 0.1270,  1.2734, -0.5586, -0.9062, -0.4336],
        [-0.0023,  1.4141, -0.3242, -0.6016, -0.7109],
        [-0.0046,  1.0234, -0.2988, -0.7773, -0.5273],
        [ 0.1338,  0.8867, -0.4902, -0.7969, -0.5820],
        [-0.2773,  1.6562, -0.1797, -0.6328, -0.4023],
        [-0.2051,  0.7344, -0.5469, -0.4668, -0.3613],
        [ 0.1553,  1.0078, -0.5898, -0.6211, -0.4023],
        [-0.3086,  1.1797, -0.4473, -0.7969, -0.6055],
        [ 0.1045,  1.2734, -0.3750, -0.7383, -0.4238],
        [-0.0025,  1.8281, -0.3594, -0.8203, -0.0791],
        [ 0.0747,  1.0703, -0.3730, -0.8086, -0.5195],
        [-0.0123,  1.3984, -0.6875, -0.7656, -0.3926],
        [-0.0498,  0.8672, -0.6367, -0.5312, -0.3809]], device='cuda:0',
       dtype=torch.bfloat16, grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None), SequenceClassifierOutput(loss=tensor(0.9082, device='cuda:0', grad_fn=<NllLossBackward0>), logits=tensor([[ 0.1069,  1.1719, -0.0583, -0.9375, -0.8125],
        [ 0.0640,  1.1094, -0.6758, -0.7695, -0.5977],
        [-0.2109,  0.9922, -0.3926, -0.8906, -0.4277],
        [-0.2949,  1.2188, -0.0110, -0.6055, -0.9062],
        [ 0.1807,  0.6211, -0.9258, -0.7969,  0.1494],
        [ 0.2773,  0.8906, -0.0483, -0.4863, -0.3340],
        [-0.3672,  1.0547, -0.2129, -0.7773, -0.7422],
        [-0.0432,  1.1328, -0.4238, -0.4746, -0.5898],
        [ 0.2852,  1.2500, -0.2500, -0.6445, -0.3555],
        [-0.3086,  0.9141, -0.1055, -0.5938, -0.7383],
        [-0.0708,  1.2188, -0.5664, -0.6914, -0.2773],
        [ 0.0618,  1.4766, -0.3691, -0.6953, -0.6992],
        [ 0.3574,  0.9727, -0.3301, -0.6328, -0.5664],
        [ 0.0547,  1.4219, -0.5312, -0.9062, -0.6133],
        [-0.0835,  1.1641, -0.1709, -0.1875, -0.3691],
        [-0.0635,  1.3516, -0.2256, -0.8008, -0.5938]], device='cuda:0',
       dtype=torch.bfloat16, grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None), SequenceClassifierOutput(loss=tensor(0.7963, device='cuda:0', grad_fn=<NllLossBackward0>), logits=tensor([[-1.3672e-01,  1.3359e+00, -8.7891e-01, -7.3438e-01, -4.5117e-01],
        [-1.0400e-01,  1.3828e+00, -5.7031e-01, -7.9688e-01, -7.1875e-01],
        [-9.9609e-02,  1.0859e+00, -2.9492e-01, -8.7891e-01, -4.9609e-01],
        [-2.6172e-01,  1.1562e+00, -5.4297e-01, -7.3047e-01, -6.0156e-01],
        [-4.8096e-02,  1.4141e+00, -6.2500e-01, -2.7344e-01, -5.0391e-01],
        [ 9.2285e-02,  1.2734e+00, -6.2891e-01, -6.4062e-01, -3.3008e-01],
        [ 1.3379e-01,  8.9453e-01, -3.9062e-01, -5.1562e-01, -3.5742e-01],
        [ 6.9824e-02,  1.1797e+00, -3.2617e-01, -4.4727e-01, -3.2617e-01],
        [-3.3008e-01,  1.0547e+00, -2.7539e-01, -4.1797e-01, -3.3008e-01],
        [ 2.7148e-01,  9.9609e-01, -6.5625e-01, -1.0234e+00, -5.9375e-01],
        [ 2.0117e-01,  8.9844e-01, -3.0469e-01, -5.1953e-01, -6.2891e-01],
        [-2.1240e-02,  1.3984e+00, -2.5586e-01, -5.4297e-01, -5.0391e-01],
        [ 1.6309e-01,  1.0391e+00, -6.6797e-01, -4.7266e-01, -4.3164e-01],
        [-2.5000e-01,  1.0312e+00, -4.3945e-01, -6.3672e-01, -5.2344e-01],
        [-1.8164e-01,  1.0000e+00, -4.8633e-01, -6.1719e-01, -5.3125e-01],
        [ 4.0771e-02,  1.3828e+00, -6.5231e-04, -5.9375e-01, -3.1445e-01]],
       device='cuda:0', dtype=torch.bfloat16, grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None), SequenceClassifierOutput(loss=tensor(1.0117, device='cuda:0', grad_fn=<NllLossBackward0>), logits=tensor([[-0.1660,  1.0703, -0.2383, -0.9102, -1.1328],
        [ 0.1074,  1.2578, -0.3809, -0.8555, -0.6562],
        [ 0.2734,  1.2031, -0.2256, -0.3965, -0.4980],
        [-0.2021,  1.1953, -0.4590, -0.8125, -0.4316],
        [ 0.0381,  1.0938, -0.6094, -0.6367, -0.7305],
        [-0.1553,  1.2500, -0.0588, -0.4375, -0.3711],
        [-0.1650,  0.9414, -0.3184, -0.6523, -0.7578],
        [-0.2734,  0.7578, -0.3184, -0.5273, -0.6836],
        [-0.0728,  1.0000, -0.1523, -0.8750, -0.9297],
        [-0.2188,  1.1797, -0.4004, -0.6211, -0.3789],
        [ 0.0728,  0.9336, -0.5664, -0.3867, -0.3262],
        [ 0.1650,  1.0859, -0.6250, -0.6250, -0.4434],
        [-0.0791,  1.4062, -0.5508, -0.3848, -0.4590],
        [ 0.0393,  1.0391, -0.4863, -0.4414, -0.2773],
        [ 0.1826,  1.1172, -0.5391, -0.6523, -0.4766],
        [-0.1377,  1.4141, -0.3594, -0.4277, -0.3711]], device='cuda:0',
       dtype=torch.bfloat16, grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None), SequenceClassifierOutput(loss=tensor(0.9492, device='cuda:0', grad_fn=<NllLossBackward0>), logits=tensor([[-0.0386,  1.1016, -0.5898, -0.4844, -0.6211],
        [-0.2119,  1.0234, -0.6875, -0.7852, -0.5156],
        [ 0.2148,  0.8984, -0.3945, -0.5938, -0.4062],
        [ 0.0781,  1.5469, -0.3145, -0.5117, -0.4512],
        [-0.1787,  1.3359, -0.2812, -0.5117, -0.8359],
        [ 0.1035,  1.1250, -0.5469, -0.7891, -0.4570],
        [ 0.2930,  1.4453, -0.2217, -0.9141, -0.3047],
        [-0.0356,  1.1328, -0.4297, -0.8008, -0.2637],
        [ 0.1914,  0.8711, -0.5273, -0.7227, -0.3184],
        [-0.2080,  1.2344, -0.6445, -0.5430, -0.3867],
        [ 0.6836,  1.1250, -0.2305, -0.8633, -0.5117],
        [ 0.0486,  1.3984, -0.4531, -0.6523, -0.3262],
        [-0.0071,  0.9375, -0.4082, -0.7031, -0.4609],
        [ 0.1494,  1.3125, -0.1777, -0.4883, -0.4883],
        [-0.1689,  1.6719, -0.3457, -0.8320, -0.2578],
        [ 0.2119,  1.1719, -0.2422, -0.6328, -0.3535]], device='cuda:0',
       dtype=torch.bfloat16, grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None), SequenceClassifierOutput(loss=tensor(0.7749, device='cuda:0', grad_fn=<NllLossBackward0>), logits=tensor([[-0.0233,  1.2109, -0.1982, -0.3613, -0.5508],
        [ 0.0574,  1.5547, -0.3867, -0.7109, -0.1660],
        [-0.0344,  1.2109, -0.6016, -0.7109, -0.6484],
        [ 0.1611,  0.8789, -0.3105, -0.7109, -0.3984],
        [-0.0649,  1.2578, -0.1729, -0.7344, -0.5938],
        [-0.0054,  0.7383, -0.7656, -0.5781, -0.3105],
        [ 0.2344,  1.3828, -0.3730, -0.4219, -0.1543],
        [-0.0659,  1.2031, -0.3672, -0.7695, -0.3750],
        [ 0.3770,  0.8125, -0.1816, -0.4434, -0.7578],
        [-0.1914,  0.8477, -0.2383, -0.6406, -0.7031],
        [-0.4434,  1.2656, -0.4570, -0.4844, -0.4941],
        [ 0.2832,  1.2031, -0.4727, -0.9258, -0.1523],
        [-0.3359,  1.2266,  0.0718, -0.2656, -0.7461],
        [-0.0991,  1.2266, -0.4844, -0.7852, -0.3262],
        [ 0.0270,  1.1406, -0.2383, -0.6172, -0.4355],
        [ 0.1235,  0.5508, -0.5000, -0.7891, -0.6094]], device='cuda:0',
       dtype=torch.bfloat16, grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None), SequenceClassifierOutput(loss=tensor(0.9497, device='cuda:0', grad_fn=<NllLossBackward0>), logits=tensor([[ 0.0493,  1.3594, -0.4219, -0.8711, -0.3750],
        [-0.1245,  1.1562, -0.3008, -0.7891, -0.5547],
        [ 0.1187,  1.1875, -0.4238, -0.5859, -0.3125],
        [ 0.0079,  1.0469, -0.1270, -0.4395, -0.5820],
        [-0.1436,  1.3516, -0.4082, -0.8242, -0.3750],
        [ 0.1138,  1.3906, -0.4023, -0.4043, -0.4375],
        [ 0.3027,  1.4609, -0.5352, -0.6367, -0.2734],
        [-0.1338,  0.9219, -0.0361, -1.0156, -0.9492],
        [ 0.1016,  1.2656, -0.4043, -0.5547, -0.3086],
        [ 0.0510,  1.3438, -0.5977, -0.9336, -0.6523],
        [ 0.2891,  1.1016, -0.2324, -0.4023, -0.7617],
        [-0.0767,  1.1406, -0.3105, -0.7344, -0.1953],
        [-0.0072,  0.9258, -0.4336, -0.2617, -0.5703],
        [ 0.1221,  1.1953, -0.4824, -0.5859, -0.5195],
        [-0.1318,  1.5703, -0.7461, -0.8359, -0.1846],
        [-0.0962,  0.9688, -0.2520, -0.6797, -0.7070]], device='cuda:0',
       dtype=torch.bfloat16, grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None), SequenceClassifierOutput(loss=tensor(0.9276, device='cuda:0', grad_fn=<NllLossBackward0>), logits=tensor([[-0.3125,  1.2812, -0.2324, -0.5039, -0.4375],
        [ 0.2539,  1.0234, -0.5586, -0.6641, -0.4922],
        [-0.0369,  1.2031, -0.0251, -0.9062, -0.5430],
        [ 0.0400,  1.0859, -0.3477, -0.7031, -0.5078],
        [-0.0981,  1.3281, -0.2598, -0.5391, -0.3652],
        [ 0.0347,  0.9609, -0.6641, -0.7578, -0.4668],
        [ 0.0679,  1.6172, -0.4355, -0.8828, -0.1543],
        [-0.1865,  1.2656, -0.3574, -0.4238, -0.4434],
        [-0.4414,  1.1562, -0.2344, -0.7930, -0.3535],
        [ 0.0537,  1.5156, -0.4023, -0.9492, -0.5664],
        [ 0.1133,  1.4609, -0.3125, -0.7695, -0.2988],
        [-0.1089,  1.0469, -0.6836, -0.8789, -0.6133],
        [-0.3516,  0.8789,  0.2148, -0.5234, -0.3965],
        [-0.1309,  0.7109, -0.3496, -0.8633, -0.6172],
        [ 0.3398,  0.9453, -0.7969, -0.6602, -0.0791],
        [-0.0063,  0.9414, -0.2793, -0.6562, -0.2695]], device='cuda:0',
       dtype=torch.bfloat16, grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None), SequenceClassifierOutput(loss=tensor(0.7335, device='cuda:0', grad_fn=<NllLossBackward0>), logits=tensor([[ 0.3457,  1.4609, -0.5273, -0.8320, -0.3457],
        [-0.4199,  1.0469, -0.5938, -0.7305, -0.5391],
        [ 0.0288,  0.8203, -0.0117, -0.5117, -0.7227],
        [-0.0359,  1.3438, -0.5781, -0.9609, -0.6211],
        [ 0.6719,  0.1270, -0.6992, -0.0393,  0.5703],
        [-0.6016,  1.0234, -0.3730, -0.6367, -0.4375],
        [ 0.2061,  1.3359, -0.5859, -0.7930, -0.3770],
        [-0.1826,  0.9023, -0.1138, -0.7109, -0.5312],
        [ 0.0830,  1.3516, -0.0354, -0.5156, -0.7305],
        [-0.0535,  1.3438, -0.3340, -0.5352, -0.3535],
        [-0.3984,  0.9023, -0.0947, -0.6562, -1.1484],
        [-0.0317,  1.0625, -0.6055, -0.5938, -0.5234],
        [ 0.0145,  1.0938, -0.3301, -0.5703, -0.3398],
        [-0.0132,  1.1719, -0.2217, -1.0547, -0.3906],
        [ 0.0491,  1.2031, -0.9258, -0.8086, -0.3418],
        [-0.1221,  1.2500, -0.4453, -0.7148, -0.4395]], device='cuda:0',
       dtype=torch.bfloat16, grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None), SequenceClassifierOutput(loss=tensor(0.8796, device='cuda:0', grad_fn=<NllLossBackward0>), logits=tensor([[ 0.0347,  1.2500, -0.7109, -0.8867, -0.7461],
        [-0.0045,  1.3047, -0.2891, -0.7773, -0.5664],
        [-0.0762,  0.9766, -0.0854, -0.6758, -0.6250],
        [-0.0347,  0.8164, -0.1875, -1.0234, -0.9375],
        [ 0.0239,  1.5000, -0.2314, -0.9219, -0.4941],
        [-0.0347,  1.2188, -0.4141, -0.2031, -0.6445],
        [ 0.0143,  1.2656, -0.1973, -0.6367, -0.1060],
        [ 0.1611,  0.9844, -0.6602, -0.6172, -0.5508],
        [ 0.2266,  1.1562, -0.5469, -0.4863, -0.5508],
        [ 0.1211,  0.7305, -0.3359, -0.9688, -0.6055],
        [ 0.0457,  1.1953, -0.3320, -0.8047, -0.3945],
        [ 0.0762,  1.2422, -0.1748, -0.9414, -0.4277],
        [ 0.2051,  1.5312, -0.2969, -0.6016, -0.2012],
        [ 0.0073,  1.2500, -0.5352, -0.4512, -0.2158],
        [ 0.2188,  1.3438, -0.1660, -0.2373, -0.6094],
        [-0.2559,  1.2969, -0.7422, -0.6328, -0.4531]], device='cuda:0',
       dtype=torch.bfloat16, grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None), SequenceClassifierOutput(loss=tensor(1.1777, device='cuda:0', grad_fn=<NllLossBackward0>), logits=tensor([[ 0.0664,  1.2109, -0.4102, -0.7266, -0.4922],
        [-0.1836,  0.9883, -0.3398, -0.6445, -0.7188],
        [ 0.3262,  1.0547, -0.2832, -0.3828, -0.0261],
        [ 0.1426,  1.1484,  0.0080, -0.6328, -0.8555],
        [-0.0679,  1.3594, -0.4824, -0.4414, -0.4648],
        [ 0.1982,  0.8906, -0.6367, -0.7148, -0.5586],
        [-0.2480,  1.2578, -0.2285, -0.3887, -0.5117],
        [ 0.1377,  1.0391, -0.3203, -0.7969, -0.6680],
        [ 0.0854,  1.2266, -0.4238, -0.6328, -0.3867],
        [-0.2480,  1.1328, -0.5117, -0.4082, -0.7070],
        [ 0.1250,  1.3047, -0.5273, -0.8320, -0.4453],
        [ 0.0757,  1.1797, -0.4512, -0.7969, -0.6484],
        [-0.0171,  1.0078, -0.2100, -1.0781, -0.5000],
        [-0.2617,  1.1016, -0.3164, -0.7070, -0.4531],
        [ 0.1855,  1.2188, -0.2852, -0.4316, -0.3320],
        [-0.1250,  0.9688, -0.5000, -0.7461, -0.5586]], device='cuda:0',
       dtype=torch.bfloat16, grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None), SequenceClassifierOutput(loss=tensor(0.9819, device='cuda:0', grad_fn=<NllLossBackward0>), logits=tensor([[ 0.0654,  1.4375, -0.4082, -0.6406, -0.0869],
        [ 0.2305,  0.9102, -0.2559, -0.1299, -0.3984],
        [ 0.1885,  1.5000, -0.6719, -0.7266, -0.2676],
        [ 0.0947,  1.4609, -0.6094, -1.1953, -0.6680],
        [ 0.0057,  1.3438, -0.4961, -0.9258, -0.6172],
        [ 0.3242,  1.2656, -0.4570, -0.8047, -0.4355],
        [ 0.1748,  1.5078, -0.6328, -1.0469, -0.5938],
        [-0.2598,  0.9648, -0.1748, -0.8320, -0.7305],
        [-0.0806,  0.9766, -0.3691, -0.5859, -0.8164],
        [ 0.2070,  1.0391, -0.3652, -0.8281, -0.4883],
        [ 0.2305,  1.4062, -0.7188, -0.8164, -0.5547],
        [-0.0586,  1.2500, -0.1338, -0.6562, -0.7656],
        [ 0.1982,  1.2891, -0.2393, -0.6484, -0.4824],
        [ 0.0659,  0.9336, -0.0342, -0.6914, -0.4492],
        [ 0.1118,  1.2969, -0.3496, -0.6484, -0.8164],
        [-0.2617,  1.3438, -0.4551, -0.4980, -0.4453]], device='cuda:0',
       dtype=torch.bfloat16, grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None), SequenceClassifierOutput(loss=tensor(0.7427, device='cuda:0', grad_fn=<NllLossBackward0>), logits=tensor([[ 0.0083,  1.3203, -0.2949, -0.7852, -0.5938],
        [ 0.2246,  1.0469, -0.2295, -0.4688, -1.2578],
        [-0.2139,  1.2109, -0.3555, -0.5039, -0.4473],
        [ 0.0153,  1.1719, -0.3281, -0.6758, -0.3672],
        [-0.0698,  1.2031, -0.3398, -0.6133, -0.4824],
        [ 0.0479,  1.1641, -0.2734, -0.3496, -0.5859],
        [-0.0688,  1.1797, -0.0630, -0.7461, -0.8164],
        [ 0.1318,  1.2656, -0.2480, -0.8789, -0.3750],
        [-0.2988,  1.3047,  0.0300, -0.8086, -0.5078],
        [-0.0645,  0.8672, -0.4609, -0.5938, -0.2695],
        [ 0.2236,  1.0312, -0.4355, -0.7969, -0.5312],
        [-0.1216,  1.0547,  0.1572, -0.5625, -0.7305],
        [-0.1621,  1.3984, -0.2188, -0.9180, -0.4668],
        [ 0.0532,  1.0312, -0.1367, -0.5273, -0.4727],
        [-0.1992,  1.2188, -0.4609, -0.6289, -0.4238],
        [ 0.3418,  0.9453, -0.4785, -0.5977, -0.5469]], device='cuda:0',
       dtype=torch.bfloat16, grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None), SequenceClassifierOutput(loss=tensor(0.9944, device='cuda:0', grad_fn=<NllLossBackward0>), logits=tensor([[ 0.0270,  1.1250, -0.5938, -0.7812, -0.6680],
        [ 0.0520,  1.2344, -0.6172, -0.7305, -0.5742],
        [ 0.2148,  0.9531, -0.5039, -0.3086, -0.7148],
        [-0.2227,  1.1719, -0.3730, -0.6289, -0.1396],
        [ 0.6211,  0.9023, -0.7578, -0.9766,  0.2373],
        [ 0.0718,  1.3516, -0.5898, -0.6406, -0.3457],
        [-0.2637,  1.1016, -0.3789, -0.3887, -0.6172],
        [ 0.3945,  1.0156, -0.6172, -0.4980, -0.3125],
        [ 0.0933,  1.0234, -0.3301, -0.2158, -0.2656],
        [-0.1406,  1.4219, -0.3965, -0.8359, -0.5977],
        [ 0.0767,  1.0156, -0.5469, -0.3945, -0.4277],
        [ 0.0117,  1.3359, -0.7812, -0.5430, -0.5352],
        [ 0.1572,  0.9023,  0.1201, -0.4863, -0.4219],
        [ 0.1216,  1.7500, -0.3398, -0.9141, -0.1748],
        [ 0.0596,  1.2422, -0.4160, -0.6836, -0.3867],
        [ 0.3125,  0.9883, -0.1348, -0.7422, -0.8750]], device='cuda:0',
       dtype=torch.bfloat16, grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None), SequenceClassifierOutput(loss=tensor(0.8740, device='cuda:0', grad_fn=<NllLossBackward0>), logits=tensor([[ 1.6602e-01,  1.1484e+00, -2.9492e-01, -6.1328e-01, -6.6016e-01],
        [-1.4709e-02,  1.5156e+00, -5.2734e-01, -8.9844e-01, -6.6406e-01],
        [-2.4512e-01,  8.9453e-01, -4.8633e-01, -3.0859e-01, -8.0078e-01],
        [ 2.0410e-01,  1.1250e+00, -6.7578e-01, -4.6875e-01, -2.7148e-01],
        [ 2.6562e-01,  1.2266e+00, -1.8457e-01, -6.9531e-01, -5.1172e-01],
        [-4.4336e-01,  1.0938e+00, -1.2402e-01, -6.7188e-01, -4.7656e-01],
        [-1.3275e-03,  1.1328e+00, -4.6680e-01, -8.2031e-01, -7.2656e-01],
        [ 1.8311e-02,  1.1719e+00, -7.5000e-01, -6.1719e-01, -2.2461e-01],
        [-1.1133e-01,  8.8672e-01,  1.8652e-01, -3.7500e-01, -5.1172e-01],
        [-1.4258e-01,  1.4531e+00, -2.0605e-01, -1.0156e+00, -4.7852e-01],
        [-1.4062e-01,  1.0078e+00, -3.4570e-01, -3.3789e-01, -6.6016e-01],
        [-1.2695e-02,  1.2188e+00, -5.5469e-01, -4.8242e-01, -4.9414e-01],
        [ 1.2305e-01,  1.3906e+00, -6.3672e-01, -5.2344e-01, -2.9883e-01],
        [ 4.1016e-02,  8.3594e-01, -8.6719e-01, -1.0781e+00, -1.3672e-01],
        [ 9.2285e-02,  1.2969e+00, -5.9375e-01, -9.4141e-01, -5.4297e-01],
        [ 3.7994e-03,  1.2344e+00, -4.0430e-01, -5.3906e-01, -4.8047e-01]],
       device='cuda:0', dtype=torch.bfloat16, grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None), SequenceClassifierOutput(loss=tensor(0.8397, device='cuda:0', grad_fn=<NllLossBackward0>), logits=tensor([[ 0.1289,  1.0469, -0.2988, -0.7656, -0.4980],
        [ 0.1777,  1.1719, -0.5078, -1.0547, -0.6836],
        [-0.1123,  1.5234, -0.4082, -0.6445, -0.7383],
        [-0.1650,  1.3359, -0.4434, -0.7305, -0.5469],
        [ 0.1182,  1.1094, -0.1416, -0.7031, -0.6328],
        [ 0.1138,  1.2500, -0.4707, -0.6328, -0.4355],
        [-0.2695,  0.6367, -0.0603, -0.7773, -0.6680],
        [ 0.1719,  1.0391, -0.4023, -0.6094, -0.4375],
        [ 0.1973,  1.3594, -0.5703, -0.7266, -0.3789],
        [-0.1416,  0.8789, -0.3789, -0.6562, -0.5156],
        [-0.0216,  1.0625, -0.1426, -0.5742, -0.3750],
        [ 0.0981,  1.2734, -0.4082, -0.4844, -0.4062],
        [-0.3828,  0.6992, -0.2451, -0.9688, -1.0391],
        [-0.2754,  1.0625, -0.0134, -0.1055, -0.4844],
        [-0.3945,  1.2500, -0.2305, -0.7031, -0.3340],
        [-0.1523,  1.2734, -0.3418, -0.7578, -0.6250]], device='cuda:0',
       dtype=torch.bfloat16, grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None), SequenceClassifierOutput(loss=tensor(0.9268, device='cuda:0', grad_fn=<NllLossBackward0>), logits=tensor([[ 7.5684e-02,  6.6016e-01, -6.9922e-01, -6.2109e-01, -2.2852e-01],
        [-1.8652e-01,  1.0703e+00, -1.2451e-01, -7.1484e-01, -4.1992e-01],
        [ 3.0664e-01,  1.3359e+00, -6.1328e-01, -6.6797e-01, -7.8516e-01],
        [-1.7090e-01,  1.2109e+00, -3.3203e-01, -7.7734e-01, -6.2500e-01],
        [-4.2725e-02,  1.0938e+00, -3.4961e-01, -8.3984e-01, -6.5625e-01],
        [-1.1133e-01,  1.0234e+00, -4.2773e-01, -3.5938e-01, -5.5469e-01],
        [ 2.9883e-01,  1.2656e+00, -4.0039e-01, -5.4297e-01, -8.9062e-01],
        [-1.2085e-02,  1.1406e+00, -4.6484e-01, -1.0391e+00, -3.0469e-01],
        [ 2.6894e-04,  1.0469e+00, -4.5117e-01, -6.3281e-01, -4.5703e-01],
        [-1.0303e-01,  8.5938e-01, -1.2500e-01, -8.5547e-01, -7.2656e-01],
        [-2.3633e-01,  1.2656e+00, -2.8320e-01, -5.6250e-01, -5.1562e-01],
        [-1.3477e-01,  1.2656e+00, -7.1484e-01, -6.3672e-01, -6.2109e-01],
        [-3.5938e-01,  1.4297e+00, -4.6289e-01, -5.3516e-01, -7.3047e-01],
        [-2.6172e-01,  1.0703e+00, -1.7871e-01, -1.0156e+00, -6.7969e-01],
        [ 1.0889e-01,  1.2188e+00, -2.4023e-01, -7.7734e-01, -3.3789e-01],
        [ 1.4844e-01,  9.4141e-01, -3.0273e-01, -7.9688e-01, -7.5000e-01]],
       device='cuda:0', dtype=torch.bfloat16, grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None), SequenceClassifierOutput(loss=tensor(0.9453, device='cuda:0', grad_fn=<NllLossBackward0>), logits=tensor([[ 6.7188e-01,  2.5586e-01, -6.9141e-01, -4.5654e-02,  5.5859e-01],
        [-9.9609e-02,  1.2344e+00, -3.0469e-01, -9.0625e-01, -6.3672e-01],
        [ 3.2349e-03,  1.2344e+00, -4.2188e-01, -4.4141e-01, -2.4023e-01],
        [-8.4961e-02,  7.8125e-01, -6.4453e-01, -7.4609e-01, -7.9688e-01],
        [-6.5918e-02,  1.4141e+00, -4.6094e-01, -8.3984e-01, -6.7578e-01],
        [-5.0293e-02,  1.1016e+00, -5.0293e-02, -5.1172e-01, -7.5000e-01],
        [-3.0273e-02,  1.4844e+00, -5.0781e-01, -8.5156e-01, -3.0469e-01],
        [ 2.5586e-01,  1.3203e+00, -2.0703e-01, -4.5898e-01, -8.7500e-01],
        [ 1.1826e-04,  1.3125e+00, -3.6719e-01, -7.9297e-01, -7.6562e-01],
        [-7.0312e-02,  9.2188e-01, -4.6680e-01, -6.4844e-01, -6.6406e-01],
        [ 1.0400e-01,  1.2812e+00, -5.1172e-01, -6.6797e-01, -4.0430e-01],
        [-1.4282e-02,  1.3203e+00, -3.6719e-01, -1.0156e+00, -3.1982e-02],
        [ 4.7363e-02,  1.2109e+00, -4.7266e-01, -4.8633e-01, -5.3516e-01],
        [-4.1602e-01,  6.0938e-01, -4.4727e-01, -8.4375e-01, -6.1719e-01],
        [ 3.0518e-02,  1.2734e+00, -3.8477e-01, -2.8906e-01, -2.3730e-01],
        [ 2.4023e-01,  1.0469e+00, -5.1953e-01, -6.5625e-01, -1.6992e-01]],
       device='cuda:0', dtype=torch.bfloat16, grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None), SequenceClassifierOutput(loss=tensor(1.0730, device='cuda:0', grad_fn=<NllLossBackward0>), logits=tensor([[ 0.1475,  1.1406, -0.6172, -0.4844, -0.5664],
        [-0.0981,  1.2109, -0.8008, -0.6367, -0.3496],
        [-0.0791,  1.2891, -0.3438, -0.8555, -0.7969],
        [-0.1172,  0.9453, -0.1895, -0.8398, -0.9141],
        [ 0.1855,  1.3438, -0.4375, -0.6836, -0.6250],
        [-0.0515,  1.3906, -0.4160, -0.2793, -0.4336],
        [ 0.1006,  1.3438, -0.5586, -0.7070, -0.8008],
        [ 0.2871,  1.2656, -0.4082, -0.5625, -0.3730],
        [-0.0962,  1.1406, -0.5273, -0.5547, -0.5703],
        [-0.4219,  1.1172, -0.6445, -0.5391, -0.5547],
        [ 0.4297,  0.2930, -0.6719, -0.4727,  0.4277],
        [ 0.1475,  0.9766, -0.5781, -0.3965, -0.5625],
        [-0.4531,  1.2969,  0.0269, -0.7891, -0.6367],
        [-0.2910,  1.1172, -0.3203, -0.7852, -0.6250],
        [ 0.1147,  1.2422, -0.5898, -0.5117, -0.7148],
        [-0.1299,  1.3047, -0.5586, -1.0781, -0.4902]], device='cuda:0',
       dtype=torch.bfloat16, grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None), SequenceClassifierOutput(loss=tensor(0.7719, device='cuda:0', grad_fn=<NllLossBackward0>), logits=tensor([[-0.3711,  1.1797, -0.5625, -0.8320, -0.5898],
        [ 0.0094,  0.8125, -0.7031, -0.5195, -0.8750],
        [ 0.1021,  1.3984, -0.3789, -0.9883, -0.3398],
        [-0.2490,  1.0625, -0.4277, -0.5195, -0.5586],
        [ 0.1108,  1.3281, -0.5469, -1.0078, -0.4805],
        [ 0.1982,  0.9688,  0.0796, -0.3027, -0.5781],
        [-0.1270,  1.2812, -0.2295, -0.8242, -0.6250],
        [ 0.1777,  0.7148, -0.0869, -0.6055, -0.3359],
        [-0.2139,  1.3281, -0.0337, -0.2715, -0.3672],
        [ 0.1104,  0.9844, -0.5547, -0.9375, -0.6250],
        [-0.0771,  1.3203, -0.4824, -1.0156, -0.4512],
        [-0.5195,  1.4141, -0.4043, -0.8750, -0.4395],
        [ 0.1177,  1.0703, -0.0791, -0.8516, -0.7383],
        [ 0.0187,  0.9922, -0.2119, -0.8438, -0.8398],
        [ 0.0197,  0.9219, -0.4121, -0.7617, -0.7969],
        [-0.1768,  1.1250, -0.0933, -0.4102, -0.5625]], device='cuda:0',
       dtype=torch.bfloat16, grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None), SequenceClassifierOutput(loss=tensor(0.8464, device='cuda:0', grad_fn=<NllLossBackward0>), logits=tensor([[-0.0500,  0.6797, -0.3047, -0.6758, -0.7266],
        [-0.0238,  1.2188, -0.2676, -0.4941, -0.5195],
        [ 0.2100,  1.0625, -0.4551, -0.7031, -0.6055],
        [-0.0123,  1.2031, -0.1069, -0.7148, -1.0703],
        [ 0.1426,  1.1406, -0.4570, -0.9180, -0.1787],
        [ 0.0791,  1.0859, -0.5625, -0.7383, -0.7500],
        [ 0.0449,  0.8945, -0.4355, -0.8125, -0.3281],
        [-0.0684,  1.1719, -0.7031, -0.8047, -0.3711],
        [-0.0064,  1.1562, -0.0596, -0.6523, -0.3965],
        [ 0.1064,  1.2266, -0.4082, -0.8047, -0.6055],
        [ 0.1377,  1.2422, -0.4395, -0.6484, -0.4199],
        [-0.1543,  1.0859, -0.3750, -0.6484, -0.4434],
        [ 0.0454,  1.3438, -0.6758, -1.0078, -0.5234],
        [ 0.1050,  1.2578, -0.7109, -0.9219, -0.5742],
        [ 0.2598,  0.7266, -0.4102, -0.6055, -0.5664],
        [ 0.0147,  0.8711, -0.6758, -0.5273, -0.4277]], device='cuda:0',
       dtype=torch.bfloat16, grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None), SequenceClassifierOutput(loss=tensor(0.7793, device='cuda:0', grad_fn=<NllLossBackward0>), logits=tensor([[-0.1729,  1.0234, -0.0947, -0.7500, -0.5742],
        [-0.1533,  1.0781, -0.3535, -0.5664, -0.8672],
        [-0.0820,  1.0156, -0.1016, -0.3516, -0.5312],
        [ 0.2715,  1.2812, -0.5547, -0.5508, -0.2109],
        [ 0.0403,  1.0469, -0.4688, -0.6758, -0.4668],
        [-0.0635,  1.0156, -0.7461, -0.8711, -0.5039],
        [ 0.0581,  0.9883, -0.6172, -0.5742, -0.4512],
        [-0.2598,  1.3359, -0.5977, -0.7422, -0.3125],
        [-0.2285,  1.0859, -0.5742, -0.8555, -0.6406],
        [ 0.0060,  1.0859, -0.3711, -0.4414, -0.6328],
        [ 0.0698,  1.3125, -0.4844, -0.4551, -0.2520],
        [-0.2695,  1.1719, -0.0981, -0.3438, -0.2598],
        [-0.2793,  0.8906, -0.3438, -0.4707, -0.5039],
        [-0.1611,  1.1953, -0.3711, -0.6328, -0.5859],
        [-0.2344,  0.8281, -0.2812, -0.8008, -0.6641],
        [ 0.1719,  0.8203, -0.1777, -0.5312, -0.4648]], device='cuda:0',
       dtype=torch.bfloat16, grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None), SequenceClassifierOutput(loss=tensor(0.7972, device='cuda:0', grad_fn=<NllLossBackward0>), logits=tensor([[ 0.0908,  1.3828, -0.6250, -0.7695, -0.5039],
        [-0.0703,  1.2656, -0.8945, -0.9922, -0.3086],
        [-0.2812,  1.4453, -0.5312, -0.9648, -0.7930],
        [-0.4492,  1.1641, -0.7227, -0.5898, -0.5391],
        [ 0.2754,  1.3438, -0.4414, -0.9805, -0.4141],
        [-0.3301,  1.3516, -0.5664, -0.8242, -0.4590],
        [-0.1245,  1.0156, -0.0618, -0.6758, -0.8398],
        [-0.0972,  1.4688, -0.4395, -0.7500, -0.5898],
        [-0.0173,  1.1797, -0.2031, -0.6875, -0.5977],
        [ 0.0070,  1.4141, -0.4336, -0.5312, -0.7305],
        [ 0.0496,  1.2969, -0.3340, -0.8672, -0.5430],
        [-0.1475,  1.0312, -0.2988, -0.6016, -0.5898],
        [ 0.0601,  1.5391, -0.4395, -0.5508, -0.9219],
        [-0.1367,  1.1875, -0.6406, -0.7422, -0.4512],
        [ 0.0488,  1.7109, -0.2754, -0.6445, -0.5234],
        [-0.2949,  0.9883, -0.4043, -0.4453, -0.7969]], device='cuda:0',
       dtype=torch.bfloat16, grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None), SequenceClassifierOutput(loss=tensor(0.7490, device='cuda:0', grad_fn=<NllLossBackward0>), logits=tensor([[ 0.2158,  1.1172, -0.4707, -0.7617, -0.4258],
        [-0.1982,  1.2891, -0.7227, -0.8047, -0.5898],
        [-0.0535,  1.2266, -0.3809, -0.9062, -0.6016],
        [ 0.1777,  1.1328, -0.5742, -0.7148, -0.6406],
        [-0.0115,  1.1406, -0.1787, -0.8594, -0.6445],
        [-0.3535,  0.9414, -0.2715, -0.3242, -0.4570],
        [ 0.5938,  1.5156, -0.2441, -0.7773, -0.3906],
        [-0.0938,  1.2266, -0.0459, -0.8164, -0.5898],
        [-0.0928,  1.0859, -0.4688, -0.5508, -0.3945],
        [ 0.1455,  1.4062, -0.5195, -0.7070, -0.7461],
        [ 0.0437,  1.0781, -0.4375, -0.8945, -0.6406],
        [ 0.3945,  0.9883, -0.0132, -0.6133, -0.4238],
        [ 0.1426,  1.3516, -0.1729, -0.5078, -0.6289],
        [-0.1641,  1.0391, -0.5234, -0.2988, -0.2432],
        [ 0.3262,  1.2734, -0.6406, -0.6875, -0.6680],
        [ 0.5156,  0.2119, -0.9414, -0.3398,  0.3887]], device='cuda:0',
       dtype=torch.bfloat16, grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None), SequenceClassifierOutput(loss=tensor(0.7830, device='cuda:0', grad_fn=<NllLossBackward0>), logits=tensor([[ 9.8633e-02,  1.4531e+00, -5.6641e-01, -1.0000e+00, -4.2188e-01],
        [ 7.5073e-03,  1.1328e+00, -2.6367e-01, -8.7109e-01, -2.7344e-01],
        [ 1.5527e-01,  1.3984e+00, -2.6758e-01, -8.3984e-01, -3.6523e-01],
        [ 1.2207e-01,  1.1406e+00, -5.8203e-01, -8.1250e-01, -5.6641e-01],
        [ 1.0107e-01,  9.6484e-01, -3.8086e-01, -3.9648e-01, -3.6719e-01],
        [ 1.1816e-01,  1.0156e+00, -7.6562e-01, -7.6953e-01, -6.0156e-01],
        [-7.9590e-02,  9.6094e-01, -3.7109e-01, -5.5078e-01, -2.6562e-01],
        [-2.1875e-01,  1.3750e+00, -5.1562e-01, -5.6641e-01, -2.4219e-01],
        [-2.5940e-03,  1.1562e+00, -4.4922e-01, -7.5781e-01, -4.7852e-01],
        [-5.3223e-02,  1.2344e+00, -7.4609e-01, -7.2266e-01, -4.5312e-01],
        [ 1.3657e-03,  1.1719e+00, -2.0312e-01, -8.3594e-01, -5.8984e-01],
        [ 7.7515e-03,  1.3047e+00, -4.9805e-01, -7.3438e-01, -3.9648e-01],
        [ 9.6680e-02,  1.1875e+00, -5.1953e-01, -6.1328e-01, -3.7305e-01],
        [-2.8711e-01,  8.5156e-01, -2.0703e-01, -6.8359e-01, -6.7969e-01],
        [ 7.9102e-02,  1.4219e+00, -5.7422e-01, -1.0234e+00, -8.0469e-01],
        [-2.1875e-01,  1.1719e+00, -4.3164e-01, -7.0703e-01, -3.9844e-01]],
       device='cuda:0', dtype=torch.bfloat16, grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None), SequenceClassifierOutput(loss=tensor(0.7859, device='cuda:0', grad_fn=<NllLossBackward0>), logits=tensor([[-0.0625,  1.2422, -0.1992, -0.5156, -0.3828],
        [ 0.3184,  1.2031, -0.2656, -0.5938, -0.5508],
        [ 0.1309,  1.5703, -0.4824, -0.7852, -0.4473],
        [-0.1680,  1.0938, -0.2832, -0.9492, -0.6719],
        [-0.0132,  1.3125, -0.5039, -0.6992, -0.4668],
        [-0.0908,  1.3438, -0.5156, -0.6250, -0.3379],
        [-0.1787,  1.6641, -0.4531, -0.9531, -0.4062],
        [ 0.0317,  1.4922, -0.6016, -0.6758, -0.4297],
        [-0.1035,  0.8828, -0.4531, -0.6133, -0.0098],
        [-0.1846,  1.2109, -0.3516, -0.7344, -0.5039],
        [-0.0664,  1.2969, -0.4453, -0.8242, -0.4473],
        [-0.2637,  0.9961, -0.4980, -0.5898, -0.4258],
        [-0.0503,  1.2891, -0.5977, -0.7344, -0.6250],
        [ 0.0869,  1.1250, -0.3672, -0.9531, -0.6016],
        [-0.3301,  1.0234, -0.3340, -1.1562, -0.6016],
        [ 0.3027,  1.3359, -0.5312, -0.8359, -0.2656]], device='cuda:0',
       dtype=torch.bfloat16, grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None), SequenceClassifierOutput(loss=tensor(1.0511, device='cuda:0', grad_fn=<NllLossBackward0>), logits=tensor([[-0.0432,  1.1172, -0.5508, -1.2344, -0.3223],
        [ 0.0840,  0.9922, -0.4531, -0.8125, -0.6172],
        [-0.1182,  1.2344, -0.0630, -0.9453, -0.6250],
        [ 0.0515,  1.0703, -0.1670, -0.6133, -0.3574],
        [ 0.2773,  1.2031, -0.6133, -0.8867, -0.3477],
        [-0.2402,  1.3594, -0.4473, -0.5469, -0.8477],
        [-0.1562,  1.4922, -0.4453, -0.9297, -0.6680],
        [-0.0327,  1.0547, -0.3770, -0.5547, -0.5234],
        [ 0.0762,  1.4453, -0.1699, -0.4707, -0.3301],
        [ 0.3281,  1.0312, -0.3945, -0.6484, -0.5938],
        [ 0.1953,  1.0781, -0.5859, -0.7852, -0.5039],
        [ 0.1504,  0.8320, -0.2168, -0.7266, -0.3047],
        [-0.2207,  1.1328, -0.2139, -0.5039, -0.3574],
        [-0.1245,  1.0469, -0.4902, -1.0000, -0.6094],
        [ 0.0228,  1.0234, -0.4219, -0.6289, -0.5078],
        [-0.3203,  1.3203, -0.5508, -0.2500, -0.4570]], device='cuda:0',
       dtype=torch.bfloat16, grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None), SequenceClassifierOutput(loss=tensor(0.9822, device='cuda:0', grad_fn=<NllLossBackward0>), logits=tensor([[ 2.1875e-01,  1.2500e+00, -1.0742e-01, -6.9141e-01, -5.1562e-01],
        [ 1.4648e-01,  1.2031e+00, -4.3164e-01, -3.3398e-01, -3.1836e-01],
        [-1.3574e-01,  1.2812e+00, -4.7461e-01, -6.9141e-01, -5.0391e-01],
        [ 7.1777e-02,  9.3750e-01, -4.2773e-01, -7.0312e-01, -6.0547e-01],
        [-1.1523e-01,  1.2734e+00, -2.0801e-01, -4.3359e-01, -3.4180e-01],
        [-3.9551e-02,  1.2969e+00, -6.0938e-01, -2.6953e-01, -5.2734e-01],
        [-1.3379e-01,  1.4609e+00, -6.3672e-01, -4.1406e-01, -5.4688e-01],
        [-2.6953e-01,  1.3750e+00, -3.6719e-01, -7.0703e-01, -1.5820e-01],
        [ 1.1230e-01,  1.4062e+00, -6.2891e-01, -5.9766e-01, -3.3984e-01],
        [ 2.3926e-01,  8.1641e-01, -5.7422e-01, -6.7969e-01, -5.4688e-01],
        [ 1.0681e-04,  1.2812e+00, -4.5703e-01, -7.4219e-01, -3.8281e-01],
        [ 3.4424e-02,  1.1172e+00, -5.4688e-01, -5.8203e-01, -6.4844e-01],
        [-9.7656e-02,  1.4062e+00, -4.0430e-01, -1.0234e+00, -4.0039e-01],
        [-9.7656e-02,  1.1719e+00, -2.5781e-01, -6.0156e-01, -4.1797e-01],
        [ 3.4570e-01,  1.0938e+00, -8.7109e-01, -7.3047e-01, -5.4688e-02],
        [ 3.0884e-02,  1.4531e+00, -3.5156e-01, -7.2266e-01, -6.9141e-01]],
       device='cuda:0', dtype=torch.bfloat16, grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None), SequenceClassifierOutput(loss=tensor(0.8289, device='cuda:0', grad_fn=<NllLossBackward0>), logits=tensor([[-0.2451,  1.4766, -0.3809, -0.8789, -0.5859],
        [-0.3105,  0.7031, -0.2891, -0.7656, -0.2490],
        [-0.1602,  1.4688, -0.0762, -0.8203, -0.7617],
        [ 0.0148,  1.4922, -0.3457, -0.6719, -0.4258],
        [-0.0674,  1.6172, -0.3613, -0.8047, -0.4941],
        [-0.0649,  1.1172, -0.4219, -0.7266, -0.4707],
        [-0.1455,  1.4922, -0.0486, -0.8477, -0.5312],
        [-0.1064,  1.0938, -0.2480, -0.8203, -0.5586],
        [-0.2461,  0.7109, -0.0698, -0.6875, -0.8164],
        [-0.1240,  1.2734, -0.4961, -0.9141, -0.6797],
        [-0.0269,  1.1328, -0.3047, -0.8555, -0.6562],
        [ 0.1621,  1.0234, -0.2031, -0.8125, -0.5430],
        [-0.0496,  1.4062, -0.4805, -0.3613, -0.6172],
        [ 0.1748,  1.4375, -0.1250, -0.9609, -0.4707],
        [ 0.0237,  1.2031, -0.3438, -0.8789, -0.5938],
        [ 0.3320,  1.4297, -0.3926, -0.4922, -0.8281]], device='cuda:0',
       dtype=torch.bfloat16, grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None), SequenceClassifierOutput(loss=tensor(0.8557, device='cuda:0', grad_fn=<NllLossBackward0>), logits=tensor([[-0.4062,  1.1484, -0.4023, -0.2031, -0.5312],
        [-0.4883,  1.1719, -0.3535, -0.8086, -1.0156],
        [-0.0732,  1.2969, -0.5312, -0.7695, -0.6758],
        [ 0.2256,  1.2969, -0.7578, -0.8906, -0.5078],
        [-0.3145,  1.2422, -0.0513, -0.4551, -0.8281],
        [ 0.0771,  1.3359, -0.0317, -0.6953, -0.1206],
        [-0.4102,  0.9062, -0.3145, -0.6328, -0.7305],
        [-0.1201,  1.2266, -0.5938, -0.6875, -0.5859],
        [ 0.1196,  1.2188, -0.3359, -1.0234, -0.5352],
        [-0.1816,  0.8711, -0.3398, -0.6680, -1.1641],
        [-0.2178,  0.9180, -0.3691, -0.6562, -0.8789],
        [ 0.1196,  1.2031, -0.6562, -0.8516, -0.5391],
        [-0.2129,  1.2578, -0.5352, -0.6719, -0.3223],
        [ 0.0718,  1.1328, -0.3848, -0.4375, -0.6484],
        [ 0.0349,  1.3438, -0.7969, -0.1079, -0.4512],
        [ 0.0525,  1.5078,  0.0025, -0.7812, -0.2012]], device='cuda:0',
       dtype=torch.bfloat16, grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None), SequenceClassifierOutput(loss=tensor(0.8857, device='cuda:0', grad_fn=<NllLossBackward0>), logits=tensor([[ 0.1426,  1.2656, -0.2695, -0.8711, -0.6133],
        [ 0.2617,  1.0938, -0.3672, -0.7344, -0.6875],
        [-0.0718,  1.1094, -0.7734, -0.5820, -0.6328],
        [-0.4238,  1.1016, -0.2773, -0.9883, -0.6914],
        [-0.0703,  1.0391, -0.2285, -0.6133, -0.6523],
        [-0.1299,  1.1016, -0.5156, -0.5938, -0.5273],
        [-0.0618,  1.2188, -0.5781, -0.8008, -0.5820],
        [ 0.0048,  1.2656, -0.3652, -0.7578, -0.4863],
        [-0.1172,  1.2578, -0.2871, -0.9258, -0.8750],
        [-0.0354,  1.3047, -0.2090, -0.6953, -0.7539],
        [-0.0703,  1.2734, -0.4922, -0.6758, -0.9258],
        [ 0.1069,  1.3125, -0.4082, -0.6914, -0.2969],
        [ 0.0366,  1.2344, -0.6211, -0.8359, -0.3008],
        [ 0.0908,  1.2344, -0.4121, -0.8789, -0.5273],
        [ 0.2383,  1.3516, -0.5117, -0.6250, -0.1016],
        [ 0.1108,  1.3594, -0.7578, -0.5469, -0.1904]], device='cuda:0',
       dtype=torch.bfloat16, grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None), SequenceClassifierOutput(loss=tensor(1.0520, device='cuda:0', grad_fn=<NllLossBackward0>), logits=tensor([[ 0.0981,  0.5938, -0.7031, -0.4160, -0.4688],
        [-0.0142,  0.8945, -0.2129, -0.8477, -0.4297],
        [ 0.0947,  1.0703, -0.5547, -0.5273, -0.3750],
        [-0.1260,  1.2656, -0.5859, -0.6562, -0.6484],
        [ 0.1621,  1.5469, -0.7266, -1.0547, -0.7148],
        [-0.2002,  1.2578, -0.3223, -0.5898, -0.4805],
        [ 0.1982,  0.9922, -0.4668, -0.8164, -0.6484],
        [ 0.1201,  1.0859, -0.1060, -0.8008, -0.8125],
        [ 0.0408,  1.2266, -0.5156, -0.8242, -0.4473],
        [ 0.0767,  1.3203, -0.5469, -0.4336, -0.4336],
        [-0.0591,  1.4766, -0.2988, -0.6289, -0.6836],
        [-0.0303,  1.2344, -0.2490, -0.8555, -0.7773],
        [ 0.0879,  0.9336, -0.6211, -0.4766, -0.3496],
        [ 0.1465,  1.3906, -0.4941, -0.9023, -0.6680],
        [ 0.1406,  1.3359, -0.2930, -0.8203, -0.6328],
        [ 0.0825,  1.3828, -0.8438, -0.5430, -0.3848]], device='cuda:0',
       dtype=torch.bfloat16, grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None), SequenceClassifierOutput(loss=tensor(0.8469, device='cuda:0', grad_fn=<NllLossBackward0>), logits=tensor([[-0.2080,  1.0156, -0.0437, -0.7930, -0.9766],
        [ 0.0172,  1.1094, -0.4980, -0.5156, -0.4980],
        [ 0.0332,  1.1797, -0.5312, -0.7148,  0.0240],
        [-0.3125,  1.1406, -0.3047, -0.8008, -0.6484],
        [-0.1494,  0.9570, -0.4863, -0.5664, -0.5273],
        [-0.2451,  0.9609, -0.4316, -0.9258, -0.7383],
        [ 0.0280,  1.1797, -0.5156, -0.5078, -0.6406],
        [-0.1426,  1.6641, -0.8086, -0.6250, -0.4160],
        [ 0.0693,  1.4297, -0.4004, -0.8945, -0.2812],
        [-0.0903,  0.9297, -0.3828, -0.3984, -0.4863],
        [-0.0942,  1.1719, -0.5898, -0.9414, -0.8086],
        [ 0.0806,  0.8672, -0.0432, -0.8008, -0.9727],
        [ 0.0532,  1.1250, -0.1079, -0.5508, -0.2871],
        [-0.3125,  1.4922, -0.2500, -0.8047, -0.7344],
        [ 0.2070,  0.9219, -0.6367, -0.5508, -0.5703],
        [ 0.3379,  0.9102, -0.4746, -0.6094, -0.6367]], device='cuda:0',
       dtype=torch.bfloat16, grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None), SequenceClassifierOutput(loss=tensor(0.9847, device='cuda:0', grad_fn=<NllLossBackward0>), logits=tensor([[ 0.2793,  1.2109, -0.4863, -0.5859, -0.3828],
        [ 0.1074,  1.3281, -0.5898, -0.8789, -0.6055],
        [ 0.2676,  1.0078, -0.4727, -0.8281, -0.3242],
        [ 0.2334,  0.9883, -0.5156, -0.7656, -0.9609],
        [-0.5898,  1.4141, -0.3281, -0.2578, -0.3379],
        [ 0.1167,  0.6953, -0.4102, -0.4258, -0.3145],
        [-0.1777,  1.4766, -0.2461, -0.6953, -0.7305],
        [-0.1221,  1.3984, -0.4492, -0.8906, -0.4629],
        [-0.2295,  1.2500, -0.3926, -0.6055, -0.5000],
        [-0.4336,  0.8398, -0.3594, -0.5742, -0.8594],
        [ 0.0623,  1.2969, -0.4297, -0.8945, -0.4062],
        [ 0.2598,  1.3750, -0.3008, -0.7461, -0.4922],
        [ 0.0201,  1.2969, -0.5312, -0.7266, -0.5742],
        [-0.1533,  1.2422, -0.1299, -0.7969, -0.4805],
        [-0.0447,  1.5781, -0.3184, -0.8438, -0.3398],
        [-0.2012,  1.3359, -0.3066, -0.6289, -0.8086]], device='cuda:0',
       dtype=torch.bfloat16, grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None), SequenceClassifierOutput(loss=tensor(0.6118, device='cuda:0', grad_fn=<NllLossBackward0>), logits=tensor([[-0.0193,  0.7305, -0.3125, -0.8047, -0.7656],
        [-0.1406,  1.2812, -0.6562, -0.6836, -0.5742],
        [ 0.2256,  1.4297, -0.4883, -0.7500, -0.6953],
        [-0.2246,  1.3828, -0.2969, -0.5977, -0.5273],
        [-0.1211,  1.1953, -0.0291, -0.4531, -1.0391],
        [-0.2061,  1.4688, -0.7461, -0.9766, -0.6289],
        [-0.3379,  1.0938, -0.4746, -0.6172, -0.1729],
        [-0.0129,  1.0234, -0.5898, -0.7773, -0.5586],
        [-0.0928,  1.1953, -0.3301, -0.7266, -0.3340],
        [-0.1030,  1.2266, -0.5312, -0.8516, -0.3223],
        [ 0.1543,  1.5938, -0.6289, -0.7109, -0.3711],
        [-0.1943,  1.3594,  0.0027, -0.6484, -0.7539],
        [-0.1748,  1.2969, -0.5547, -0.6289, -0.4551],
        [ 0.2080,  1.3984, -0.6953, -0.8047, -0.6406],
        [-0.3203,  1.4062, -0.7031, -0.6602, -0.6406],
        [ 0.0303,  1.1875, -0.3906, -0.6641, -0.3066]], device='cuda:0',
       dtype=torch.bfloat16, grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None), SequenceClassifierOutput(loss=tensor(0.6415, device='cuda:0', grad_fn=<NllLossBackward0>), logits=tensor([[-0.1807,  1.1797, -0.5820, -0.5391, -0.0918],
        [ 0.0262,  1.3672, -0.1035, -0.8086, -0.5078],
        [-0.1523,  1.0703, -0.6133, -0.6680, -0.4492],
        [ 0.1260,  1.4375, -0.4219, -0.9648, -0.5156],
        [-0.0649,  1.4609, -0.3516, -0.8438, -0.7109],
        [ 0.0469,  1.5312, -0.3789, -0.6562, -0.3262],
        [ 0.1182,  1.1641, -0.4355, -0.4629, -0.6016],
        [ 0.0107,  1.0625, -0.4668, -1.0469, -0.9023],
        [ 0.0801,  1.0625, -0.4980, -0.6016, -0.4453],
        [-0.2520,  1.4219, -0.3027, -1.0938, -0.5430],
        [-0.1670,  1.4375, -0.2480, -0.5430, -0.5039],
        [-0.6641,  1.1797, -0.3105, -0.5430, -0.6758],
        [-0.1504,  1.1562, -0.3008, -0.6094, -0.4629],
        [ 0.2793,  1.4453, -0.4453, -0.4863, -0.6055],
        [ 0.0557,  1.2188, -0.4922, -0.5898, -0.6836],
        [ 0.0737,  0.9805, -0.3770, -0.9180, -0.7734]], device='cuda:0',
       dtype=torch.bfloat16, grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None), SequenceClassifierOutput(loss=tensor(0.7853, device='cuda:0', grad_fn=<NllLossBackward0>), logits=tensor([[ 0.0688,  1.2734, -0.6367, -0.8750, -0.2871],
        [ 0.1152,  1.4062, -0.6719, -0.8750, -0.2695],
        [-0.0728,  1.1641, -0.3926, -0.3789, -0.7344],
        [ 0.0874,  0.8047, -0.2969, -0.8477, -0.8281],
        [-0.0503,  1.4375, -0.3789, -0.9141, -0.7070],
        [ 0.2178,  1.1250, -0.4434, -0.6523, -0.4512],
        [ 0.0275,  1.4453, -0.5430, -1.1094, -0.3633],
        [ 0.1943,  1.2422, -0.4023, -0.7656, -0.4805],
        [-0.2354,  1.2500, -0.3340, -0.6602, -0.6289],
        [ 0.2812,  1.1250, -0.7031, -0.8242, -0.2656],
        [ 0.0258,  1.1250, -0.3438, -0.8086, -0.4609],
        [-0.1494,  1.5625, -0.2676, -1.1484, -0.2852],
        [-0.0796,  1.0859, -0.2363, -0.5156, -0.4707],
        [-0.0723,  1.3984,  0.0391, -0.3965, -0.6406],
        [ 0.3770,  1.2109, -0.6680, -0.7695, -0.6484],
        [-0.1406,  1.0469, -0.4434, -0.2871, -0.4180]], device='cuda:0',
       dtype=torch.bfloat16, grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None), SequenceClassifierOutput(loss=tensor(0.6362, device='cuda:0', grad_fn=<NllLossBackward0>), logits=tensor([[-0.0097,  1.4453, -0.4004, -0.3242, -0.3379],
        [ 0.2461,  0.9453, -0.6289, -0.3105, -0.4473],
        [-0.2832,  1.2812, -0.3242, -0.7891, -0.4902],
        [ 0.2168,  1.0781, -0.3027, -0.7852, -0.3125],
        [-0.2305,  1.0625, -0.3359, -1.0156, -0.6523],
        [ 0.0752,  1.1875, -0.3281, -0.3867, -0.4531],
        [-0.1963,  0.8438, -0.2949, -0.6250, -0.9375],
        [ 0.0251,  1.3438, -0.5625, -0.6328, -0.5703],
        [ 0.0221,  1.3047, -0.1846, -0.7461, -0.3223],
        [ 0.2910,  1.3125, -0.4102, -0.9766, -0.3164],
        [-0.0825,  1.4375, -0.3984, -0.8789, -0.7461],
        [-0.3340,  1.1875, -0.4824, -0.9219, -0.3477],
        [ 0.1787,  1.1875, -0.4980, -0.7969, -0.4746],
        [-0.1260,  1.4688, -0.3145, -0.7617, -0.6680],
        [ 0.0115,  1.5938, -0.6602, -0.7812, -0.3574],
        [ 0.3438,  1.4219, -0.5898, -0.5703, -0.4844]], device='cuda:0',
       dtype=torch.bfloat16, grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None), SequenceClassifierOutput(loss=tensor(1.1785, device='cuda:0', grad_fn=<NllLossBackward0>), logits=tensor([[ 0.0649,  0.7891, -0.5547, -0.8203, -0.7969],
        [ 0.0649,  1.3750, -0.4805, -0.6914, -0.2910],
        [-0.1855,  1.3516, -0.5977, -0.8047, -0.2012],
        [-0.0698,  1.3594, -0.3242, -0.7734, -0.2158],
        [ 0.1357,  1.1562, -0.4297, -0.9648, -0.4375],
        [-0.1934,  0.7578,  0.1069, -0.5703, -0.5859],
        [ 0.3965,  0.6367, -0.6250, -0.7109, -0.2109],
        [-0.0447,  1.2344, -0.2754, -0.5898, -0.3965],
        [ 0.0048,  1.3047, -0.3262, -0.7266, -0.5820],
        [ 0.1748,  0.7227, -0.2891, -0.3340, -0.5977],
        [ 0.0176,  1.4297, -0.5078, -0.7734, -0.8008],
        [ 0.2158,  1.2109, -0.3848, -0.8203, -0.6094],
        [ 0.0178,  1.3281, -0.4805, -0.5938, -0.4180],
        [ 0.2070,  1.4219, -0.5312, -0.6797, -0.5312],
        [-0.0540,  1.1562,  0.0552, -0.4863, -0.6992],
        [ 0.1172,  1.1641, -0.4141, -0.5703, -0.6250]], device='cuda:0',
       dtype=torch.bfloat16, grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None), SequenceClassifierOutput(loss=tensor(0.9425, device='cuda:0', grad_fn=<NllLossBackward0>), logits=tensor([[-0.3340,  1.2031, -0.6914, -0.7188, -0.6875],
        [ 0.0728,  1.5234, -0.2480, -1.1094, -0.7695],
        [-0.1299,  1.3672, -0.6133, -0.8672, -0.5977],
        [ 0.1953,  1.1875, -0.0830, -0.7969, -1.0312],
        [-0.1182,  0.8906, -0.2500, -0.7852, -0.6172],
        [ 0.0786,  0.9258, -0.6406, -0.5469, -0.7852],
        [-0.5117,  1.4219, -0.2559, -0.5938, -0.5938],
        [-0.1025,  1.1562, -0.4727, -0.8320, -0.9297],
        [ 0.0933,  1.7344, -0.6797, -0.8906, -0.5703],
        [ 0.1611,  0.9453, -0.7578, -0.8477, -0.4473],
        [-0.2012,  1.1406, -0.4180, -0.6836, -0.2676],
        [-0.0076,  1.1406, -0.5078, -0.8203, -0.4395],
        [-0.3535,  1.3828, -0.4355, -0.8477, -0.2793],
        [ 0.0366,  1.2500, -0.4414, -0.7891, -0.3965],
        [-0.1660,  1.4531, -0.3457, -0.6484, -0.9414],
        [ 0.1484,  1.0703, -0.3594, -0.6484, -0.2451]], device='cuda:0',
       dtype=torch.bfloat16, grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None), SequenceClassifierOutput(loss=tensor(1.0835, device='cuda:0', grad_fn=<NllLossBackward0>), logits=tensor([[-0.0354,  0.9102, -0.1689, -0.5586, -0.3086],
        [-0.1338,  1.1953,  0.0437, -0.5938, -0.4258],
        [-0.0713,  0.8516, -0.0396, -0.7070, -0.8125],
        [-0.1758,  1.3438, -0.4219, -0.5039, -0.3418],
        [ 0.2109,  1.0859, -0.5352, -0.8633, -0.4238],
        [-0.0486,  1.5000, -0.5625, -0.8789, -0.4395],
        [-0.0874,  0.9336, -0.1592, -0.2480, -0.4492],
        [ 0.0688,  1.6094, -0.3281, -1.0625, -0.6719],
        [ 0.1182,  1.2734, -0.5820, -0.6719, -0.6172],
        [ 0.1729,  1.4531, -0.1846, -0.8828, -0.4023],
        [ 0.0042,  1.0781,  0.0356, -0.6719, -0.5938],
        [ 0.0469,  1.3281, -0.3691, -0.7656, -0.4141],
        [-0.2988,  1.0547, -0.3633, -0.6133, -0.5117],
        [-0.1113,  1.2812, -0.5430, -1.0703, -0.5508],
        [ 0.1162,  1.1328, -0.8281, -1.0078, -0.3613],
        [ 0.0245,  1.4219, -0.4219, -0.6914, -0.4121]], device='cuda:0',
       dtype=torch.bfloat16, grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None), SequenceClassifierOutput(loss=tensor(0.9219, device='cuda:0', grad_fn=<NllLossBackward0>), logits=tensor([[-0.1631,  1.0859, -0.3438, -0.4355, -0.0226],
        [-0.0220,  1.5781, -0.2988, -1.1797, -0.7070],
        [-0.0403,  0.9453, -0.0156, -0.6953, -0.7930],
        [ 0.4238,  1.0781, -1.0547, -1.2578, -0.2520],
        [ 0.1533,  0.3965, -0.6484, -0.6758,  0.0474],
        [ 0.0591,  1.2812, -0.5898, -0.3926, -0.3711],
        [ 0.2578,  1.4062, -0.5781, -0.6953, -0.5312],
        [-0.0537,  1.2188, -0.2480, -0.6602, -0.7539],
        [-0.0267,  1.1172, -0.1475, -0.7969, -1.2969],
        [-0.1011,  1.3828, -0.3320, -0.7578, -0.2246],
        [ 0.1426,  0.7148, -0.6836, -0.4180, -0.4844],
        [-0.2754,  1.1250, -0.2334, -0.6016, -0.4238],
        [-0.0270,  1.3828, -0.4668, -0.9414, -0.3965],
        [-0.3164,  1.2266, -0.5742, -0.8750, -0.6719],
        [ 0.3223,  1.3984, -0.6953, -0.8281, -0.4102],
        [ 0.2275,  1.2969, -0.0386, -0.8984, -0.7383]], device='cuda:0',
       dtype=torch.bfloat16, grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None), SequenceClassifierOutput(loss=tensor(0.7623, device='cuda:0', grad_fn=<NllLossBackward0>), logits=tensor([[-0.2178,  1.1328, -0.3418, -0.6094, -0.3047],
        [ 0.0859,  1.4062, -0.4453, -0.8203, -0.5273],
        [-0.0396,  1.0938, -0.6172, -1.0938, -0.3867],
        [-0.0608,  1.1172, -0.5820, -0.6680, -0.7227],
        [ 0.1602,  1.1562, -0.4023, -0.5195, -0.4883],
        [ 0.1982,  1.2578, -0.5234, -0.8594, -0.5195],
        [-0.2295,  1.1562, -0.3945, -0.6016, -0.7227],
        [ 0.0986,  1.6172, -0.6875, -0.7461, -0.6016],
        [-0.3359,  1.4141,  0.0645, -1.1094, -0.5859],
        [-0.0889,  1.2891, -0.2422, -0.4180, -0.5391],
        [ 0.1040,  1.4219, -0.2061, -0.7930, -0.3730],
        [ 0.3164,  1.3047, -0.3750, -0.6445, -0.6133],
        [ 0.0265,  1.2422, -0.6250, -0.7383, -0.5234],
        [ 0.3848,  1.2422, -0.0947, -0.7617, -0.7969],
        [-0.1582,  1.6719, -0.5547, -0.8047, -0.4941],
        [-0.0771,  1.3672, -0.4570, -0.8828, -0.3652]], device='cuda:0',
       dtype=torch.bfloat16, grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None), SequenceClassifierOutput(loss=tensor(0.9772, device='cuda:0', grad_fn=<NllLossBackward0>), logits=tensor([[-0.1748,  1.1641, -0.4258, -0.6680, -0.4941],
        [-0.0306,  0.6953, -0.6680, -0.8672, -0.7891],
        [ 0.4141,  1.5703, -0.3965, -0.7695, -0.5000],
        [-0.0304,  1.2812, -0.5859, -0.6953, -0.3848],
        [-0.1826,  0.9883, -0.3711, -0.4551, -0.3047],
        [ 0.0908,  1.0547,  0.0562, -0.7891, -0.9180],
        [ 0.2676,  1.2031, -0.4141, -0.9609, -0.3984],
        [-0.0840,  1.2266, -0.3125, -0.6836, -0.4141],
        [-0.0708,  1.3359, -0.1553, -0.8164, -0.3594],
        [ 0.1416,  1.2891, -0.4355, -0.6875, -0.7188],
        [-0.0854,  1.2969, -0.3457, -0.5312, -0.3047],
        [-0.2891,  1.3594, -0.3047, -0.9609, -0.6562],
        [ 0.4219,  1.2422, -0.4727, -0.6680, -0.6133],
        [ 0.0540,  1.3125, -0.3066, -0.5469, -0.6680],
        [ 0.4453,  0.7422, -0.8711, -0.7461,  0.4746],
        [-0.2012,  0.8984, -0.6602, -0.9492, -0.6641]], device='cuda:0',
       dtype=torch.bfloat16, grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None), SequenceClassifierOutput(loss=tensor(0.6520, device='cuda:0', grad_fn=<NllLossBackward0>), logits=tensor([[ 0.0099,  1.3906, -0.4707, -0.8047, -0.5469],
        [ 0.1182,  1.3125, -0.0400, -0.8242, -0.6016],
        [ 0.1245,  1.2031, -0.5117, -1.1250, -0.4453],
        [ 0.1963,  1.5938, -0.5508, -0.8867, -0.3262],
        [ 0.2617,  1.4375, -0.6914, -1.2031, -0.6914],
        [ 0.2393,  1.1562, -0.5117, -0.8906, -0.5977],
        [-0.0811,  1.5156, -0.3906, -0.6992, -0.4707],
        [ 0.1807,  1.3516, -0.4512, -0.7188, -0.9414],
        [ 0.0569,  1.3281, -0.2041, -0.8477, -0.6367],
        [-0.0087,  1.3672, -0.6094, -0.8672, -0.3535],
        [ 0.2578,  1.5859, -0.5703, -0.7891, -0.4512],
        [ 0.0312,  1.2734, -0.3496, -1.0312, -0.6719],
        [ 0.2695,  0.8750, -0.8438, -0.7969, -0.9844],
        [ 0.0898,  1.2891, -0.2559, -0.8359, -0.7578],
        [ 0.1465,  1.6094, -0.5508, -0.9141, -0.5547],
        [ 0.0596,  1.4531, -0.4668, -0.6523, -0.3867]], device='cuda:0',
       dtype=torch.bfloat16, grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None), SequenceClassifierOutput(loss=tensor(0.8094, device='cuda:0', grad_fn=<NllLossBackward0>), logits=tensor([[ 0.1758,  1.5312, -0.4805, -0.7383, -0.5898],
        [-0.0713,  1.5312, -0.1953, -1.2578, -0.7070],
        [ 0.1279,  1.2266, -0.3477, -0.6875, -0.7734],
        [-0.1787,  1.4141, -0.3809, -0.6172, -0.4844],
        [ 0.0625,  1.1875, -0.2852, -0.7109, -0.2773],
        [-0.0786,  0.9141, -0.7188, -0.8945, -0.4727],
        [-0.0698,  1.3906, -0.5742, -0.7969, -0.3730],
        [ 0.2100,  1.1406, -0.1963, -0.7461, -0.9141],
        [-0.0027,  1.2344, -0.0679, -0.6992, -0.2969],
        [-0.0247,  1.3047, -0.1846, -0.3906, -0.5039],
        [ 0.3340,  1.3906, -0.3203, -0.9609, -0.8711],
        [-0.1138,  1.3125, -0.5508, -0.6172, -0.4336],
        [ 0.8008,  0.3379, -0.8984, -0.7734,  0.5938],
        [ 0.1621,  1.0625, -0.2637, -0.8555, -0.5469],
        [ 0.1357,  1.2656, -0.1338, -0.9023, -0.7930],
        [ 0.2373,  1.4453, -0.5469, -0.7070, -0.2539]], device='cuda:0',
       dtype=torch.bfloat16, grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None), SequenceClassifierOutput(loss=tensor(0.8595, device='cuda:0', grad_fn=<NllLossBackward0>), logits=tensor([[ 0.0061,  1.3359, -0.2695, -0.8047, -0.5703],
        [-0.3555,  1.4141, -0.2988, -0.8945, -0.4434],
        [-0.0820,  1.3984, -0.6211, -0.9570, -0.4883],
        [-0.0603,  1.3203, -0.2334, -1.1250, -0.3926],
        [ 0.0537,  1.2969, -0.7305, -0.7539, -0.3203],
        [ 0.2480,  1.4375, -0.4824, -0.6875, -0.3027],
        [ 0.0835,  0.9180, -0.4766, -0.6875, -0.1748],
        [ 0.1465,  1.3281, -0.7617, -0.7031, -0.6484],
        [-0.1436,  1.3281, -0.3574, -0.9570, -0.5586],
        [ 0.1641,  0.9648, -0.5117, -0.8086, -0.7461],
        [-0.0483,  1.4688, -0.5586, -0.9570, -0.3887],
        [-0.2832,  1.1484, -0.5234, -0.8750, -0.8477],
        [-0.0068,  1.2109, -0.2949, -0.8320, -0.4844],
        [-0.3262,  0.9023, -0.1309, -0.6914, -0.3730],
        [-0.3047,  1.2266, -0.6055, -0.6797, -0.5781],
        [ 0.0208,  1.3125, -0.3184, -0.6602, -0.4570]], device='cuda:0',
       dtype=torch.bfloat16, grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None), SequenceClassifierOutput(loss=tensor(1.1196, device='cuda:0', grad_fn=<NllLossBackward0>), logits=tensor([[-0.4512,  1.2422, -0.2432, -0.8008, -0.7617],
        [ 0.1079,  1.4531, -0.6875, -0.6914, -0.5586],
        [ 0.0102,  1.3828, -0.4922, -0.7148, -0.4492],
        [-0.0150,  1.3047, -0.3887, -0.4395, -0.5586],
        [-0.2031,  1.2891, -0.1396, -0.9570, -0.8945],
        [-0.0233,  1.2266, -0.8672, -0.5898, -0.4141],
        [-0.0248,  1.5391, -0.3672, -0.8359, -0.5195],
        [ 0.0400,  1.1641, -0.3242, -0.8477, -0.4043],
        [ 0.0167,  1.3672, -0.0469, -0.7578, -0.9609],
        [ 0.0625,  1.2812, -0.6562, -1.0703, -0.4785],
        [-0.1533,  1.2969, -0.6953, -0.7539, -0.2637],
        [ 0.4492,  1.1328, -0.4141, -0.6680, -0.4941],
        [-0.2188,  1.2109, -0.5352, -0.8984, -0.3691],
        [-0.0182,  1.2188, -0.1680, -0.7852, -0.4727],
        [ 0.1748,  1.1484, -0.2393, -0.2969, -0.6758],
        [-0.4453,  1.2266, -0.4824, -0.7109, -0.2393]], device='cuda:0',
       dtype=torch.bfloat16, grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None), SequenceClassifierOutput(loss=tensor(0.7393, device='cuda:0', grad_fn=<NllLossBackward0>), logits=tensor([[ 0.2002,  0.8594, -0.0752, -0.7188, -0.5352],
        [ 0.4746,  1.3359, -0.7109, -0.9102, -0.3340],
        [-0.0112,  1.2422, -0.4043, -0.9961, -0.5898],
        [ 0.2012,  1.3906, -0.6680, -0.7656, -0.4414],
        [-0.0933,  1.0469, -0.4414, -0.8672, -0.5938],
        [ 0.1328,  1.2109, -0.3457, -0.4414, -0.2793],
        [-0.2676,  1.1719, -0.4512, -0.4844, -0.3652],
        [-0.2852,  1.2656,  0.0557, -0.8594, -0.8047],
        [-0.0869,  1.5859, -0.4453, -0.7539, -0.6445],
        [-0.1445,  1.2500, -0.6484, -0.9297, -0.7656],
        [ 0.0549,  1.3516, -0.4648, -0.7344, -0.4922],
        [ 0.1523,  1.3750, -0.7461, -0.8750, -0.3848],
        [ 0.0562,  1.3203, -0.2236, -0.7422, -0.4238],
        [ 0.0854,  1.3281, -0.5312, -0.9219, -0.5352],
        [-0.0126,  1.0625, -0.1572, -0.8438, -0.2676],
        [ 0.3633,  0.0713, -0.9375, -0.0669,  0.4980]], device='cuda:0',
       dtype=torch.bfloat16, grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None), SequenceClassifierOutput(loss=tensor(0.9279, device='cuda:0', grad_fn=<NllLossBackward0>), logits=tensor([[ 3.1281e-03,  1.2422e+00, -7.8125e-01, -7.1875e-01, -3.0078e-01],
        [ 1.5430e-01,  1.6719e+00, -3.9258e-01, -1.0312e+00, -4.4922e-01],
        [ 3.4912e-02,  1.2500e+00, -5.1953e-01, -8.8672e-01, -2.8125e-01],
        [ 7.0801e-02,  1.6641e+00, -8.1250e-01, -7.6172e-01, -5.1562e-01],
        [ 6.4453e-02,  1.0859e+00, -3.2812e-01, -5.2344e-01, -2.8320e-01],
        [ 6.6376e-04,  1.2422e+00, -1.9922e-01, -6.4062e-01, -8.5938e-01],
        [ 2.3438e-01,  1.1797e+00, -5.5859e-01, -1.0156e+00, -5.1562e-01],
        [-1.2793e-01,  1.3281e+00, -3.8477e-01, -8.1641e-01, -6.0156e-01],
        [ 2.2852e-01,  1.2891e+00, -3.0469e-01, -5.3906e-01, -7.6172e-01],
        [-1.8262e-01,  9.4922e-01, -3.1445e-01, -4.2383e-01, -7.7344e-01],
        [ 1.5820e-01,  1.8047e+00, -3.8867e-01, -6.1719e-01, -3.6523e-01],
        [-4.5410e-02,  9.6484e-01, -6.4453e-01, -8.2812e-01, -5.7812e-01],
        [-1.4746e-01,  1.4766e+00, -5.4297e-01, -6.0156e-01, -5.4297e-01],
        [-9.0942e-03,  1.7266e+00, -5.8594e-01, -9.1016e-01, -4.3945e-01],
        [-1.1035e-01,  1.1797e+00, -4.5508e-01, -4.0430e-01, -3.4180e-01],
        [-2.6758e-01,  1.7500e+00, -4.6680e-01, -9.2188e-01, -7.1094e-01]],
       device='cuda:0', dtype=torch.bfloat16, grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None), SequenceClassifierOutput(loss=tensor(0.7238, device='cuda:0', grad_fn=<NllLossBackward0>), logits=tensor([[ 3.1738e-03,  1.2031e+00, -6.2891e-01, -7.4609e-01, -6.2109e-01],
        [-1.5332e-01,  1.5312e+00, -2.6172e-01, -7.2266e-01, -6.5234e-01],
        [-5.1172e-01,  1.2031e+00, -5.5859e-01, -7.8125e-01, -5.8984e-01],
        [ 2.8687e-02,  1.3359e+00, -2.8125e-01, -1.0625e+00, -3.4570e-01],
        [ 2.6562e-01,  1.3047e+00, -4.9805e-01, -8.9453e-01, -4.2969e-01],
        [-3.2227e-01,  1.5156e+00, -5.3906e-01, -5.9766e-01, -4.5898e-01],
        [-4.3678e-04,  1.2188e+00, -6.7578e-01, -3.6914e-01, -7.4219e-01],
        [-1.2598e-01,  1.2734e+00, -4.6875e-01, -8.1250e-01, -5.2734e-01],
        [-8.5449e-02,  1.2344e+00, -4.9609e-01, -5.9375e-01, -5.9375e-01],
        [-2.2852e-01,  9.9219e-01, -3.6328e-01, -5.1562e-01, -6.2500e-01],
        [-3.9453e-01,  1.2656e+00, -5.8594e-01, -8.5156e-01, -2.7148e-01],
        [ 1.9922e-01,  1.5625e+00, -7.0703e-01, -8.6328e-01, -2.2949e-01],
        [ 3.7598e-02,  1.4531e+00, -7.6953e-01, -6.5625e-01, -3.2812e-01],
        [-5.0781e-02,  8.7500e-01, -4.3359e-01, -6.3281e-01, -5.9375e-01],
        [-1.0010e-01,  1.2188e+00, -5.2734e-01, -8.5938e-01, -6.4453e-01],
        [-9.0820e-02,  1.1484e+00, -4.7070e-01, -7.5000e-01, -5.6641e-01]],
       device='cuda:0', dtype=torch.bfloat16, grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None), SequenceClassifierOutput(loss=tensor(0.7896, device='cuda:0', grad_fn=<NllLossBackward0>), logits=tensor([[ 0.2910,  1.5078, -0.6094, -0.8672, -0.3672],
        [ 0.0630,  0.6953, -0.4023, -0.8242, -0.7500],
        [-0.1436,  1.6172, -0.5820, -0.7148, -0.5273],
        [ 0.0830,  1.3594, -0.4258, -0.7305, -0.6562],
        [-0.0820,  1.2109, -0.6211, -0.8047, -0.3555],
        [ 0.2324,  1.3594, -0.4668, -0.6836, -0.3926],
        [-0.1426,  1.2891, -0.5898, -0.6914, -0.4414],
        [-0.1445,  1.4141, -0.2119, -1.0078, -0.5859],
        [-0.1436,  1.1719, -0.4141, -0.6133, -0.6250],
        [ 0.1602,  1.3828, -0.4902, -0.4980, -0.4062],
        [-0.0393,  1.4062, -0.5156, -0.9180, -0.4355],
        [-0.0444,  1.2578, -0.5703, -0.7578, -0.4062],
        [ 0.0186,  0.9336, -0.6523, -0.8828, -0.4688],
        [-0.0864,  1.6641, -0.3926, -0.7891, -0.6211],
        [-0.1582,  1.4219, -0.4277, -0.9336, -0.5078],
        [ 0.3008,  1.3438, -0.6094, -0.6328, -0.4258]], device='cuda:0',
       dtype=torch.bfloat16, grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None), SequenceClassifierOutput(loss=tensor(0.8093, device='cuda:0', grad_fn=<NllLossBackward0>), logits=tensor([[ 0.1318,  1.3828, -0.4277, -0.4473, -0.5898],
        [ 0.0713,  1.1094, -0.4609, -0.9531, -0.4121],
        [-0.3633,  1.0312, -0.3027, -0.7656, -0.5430],
        [-0.2129,  1.2031, -0.6250, -0.9258, -0.5391],
        [-0.1328,  1.2500, -0.0028, -0.5117, -0.4492],
        [ 0.1465,  1.2266, -0.8008, -0.8164, -0.4297],
        [-0.0593,  1.2734, -0.4395, -0.7109, -0.4199],
        [-0.4336,  1.2812, -0.4883, -1.1328, -0.4512],
        [ 0.0112,  0.9648, -0.6680, -0.8203, -0.6953],
        [ 0.2930,  1.3359, -0.5195, -0.8633, -0.7422],
        [ 0.0598,  1.4453, -0.5117, -0.7734, -0.4648],
        [ 0.1069,  1.1719, -0.1729, -0.8164, -0.8867],
        [ 0.0728,  0.8125,  0.1113, -0.5508, -1.0859],
        [ 0.2656,  1.0703, -0.3047, -0.5820, -0.3945],
        [ 0.1602,  1.2578, -0.4805, -0.9727, -0.3691],
        [-0.0444,  1.3828, -0.3164, -0.8984, -0.6758]], device='cuda:0',
       dtype=torch.bfloat16, grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None), SequenceClassifierOutput(loss=tensor(0.8925, device='cuda:0', grad_fn=<NllLossBackward0>), logits=tensor([[-0.0430,  1.2031,  0.1494, -0.9414, -0.3984],
        [ 0.1455,  1.5547, -0.6875, -0.8633, -0.4727],
        [-0.2305,  1.4219, -0.4648, -0.7695, -0.5625],
        [-0.0820,  1.3594, -0.4785, -0.9961, -0.7930],
        [-0.0537,  1.5234, -0.3555, -1.1719, -0.5156],
        [ 0.3633,  1.0703, -0.1768, -0.3848, -0.4219],
        [ 0.0369,  1.5859, -0.2227, -0.8828, -0.6992],
        [ 0.1357,  1.0938, -0.6875, -0.6172, -0.2148],
        [-0.2090,  1.3047, -0.3887, -0.6797, -0.8438],
        [-0.0020,  1.1641, -0.3281, -0.7344, -0.5000],
        [ 0.0315,  1.3672, -0.3828, -0.7578, -0.5352],
        [-0.0593,  1.6562, -0.5273, -0.9492, -0.4824],
        [ 0.1367,  1.4531, -0.0061, -0.5469, -0.6133],
        [-0.1196,  1.2891, -0.4512, -0.8359, -0.6562],
        [-0.0771,  1.0312, -0.5703, -0.1426, -0.5977],
        [ 0.2949,  1.6094, -0.4316, -0.9922, -0.4082]], device='cuda:0',
       dtype=torch.bfloat16, grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None), SequenceClassifierOutput(loss=tensor(0.8652, device='cuda:0', grad_fn=<NllLossBackward0>), logits=tensor([[ 0.2715,  1.5078, -0.7383, -0.8438, -0.4531],
        [-0.1953,  1.3281, -0.2109, -0.8672, -0.5078],
        [ 0.2480,  1.5078, -0.4082, -0.6094, -0.5039],
        [-0.1211,  1.6094, -0.6016, -0.7461, -0.3711],
        [ 0.3379,  0.8281, -0.6680, -0.7422,  0.7852],
        [-0.1172,  1.5234, -0.7148, -0.9062, -0.6133],
        [ 0.0449,  1.2266, -0.5352, -0.6992, -0.5273],
        [ 0.0084,  1.3906, -0.3184, -0.9570, -0.5312],
        [-0.0635,  1.0000, -0.6680, -0.8438, -0.1914],
        [ 0.1689,  1.1016, -0.4258, -0.7734, -0.5977],
        [-0.0776,  1.4062, -0.4004, -0.9062, -0.6836],
        [ 0.2773,  1.4844, -0.6641, -1.0000, -0.3848],
        [-0.0415,  1.3281, -0.5391, -0.6875, -0.3066],
        [-0.1836,  1.0234, -0.7148, -0.8945, -0.6992],
        [-0.1387,  1.3203, -0.5078, -0.8203, -0.4219],
        [ 0.1377,  1.1641, -0.3027, -0.9844, -0.5430]], device='cuda:0',
       dtype=torch.bfloat16, grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None), SequenceClassifierOutput(loss=tensor(0.7874, device='cuda:0', grad_fn=<NllLossBackward0>), logits=tensor([[ 0.3262,  1.5000, -0.6172, -0.5391, -0.4219],
        [ 0.1660,  1.4766, -0.4141, -0.7461, -0.5234],
        [ 0.0039,  1.1953, -0.2275, -1.0391, -0.8047],
        [-0.0649,  1.3594, -0.2148, -0.8711, -0.6836],
        [-0.2676,  1.2578, -0.1006, -1.2031, -0.7461],
        [ 0.0265,  1.1875, -0.6016, -0.6289, -0.4941],
        [-0.2793,  1.2500, -0.3730, -0.5977, -0.5039]], device='cuda:0',
       dtype=torch.bfloat16, grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)], [SequenceClassifierOutput(loss=tensor(2.1445, device='cuda:0', grad_fn=<NllLossBackward0>), logits=tensor([[ 0.0544,  1.4922, -0.7148, -0.8438, -0.4102],
        [ 0.2949,  0.8672, -0.7188, -1.0000, -0.3906],
        [-0.1523,  1.2188, -0.2119, -0.9258, -0.3242],
        [-0.1680,  1.2500, -0.2441, -0.7070, -0.9648],
        [-0.1021,  0.8828, -0.3652, -0.3906, -0.5625],
        [ 0.0500,  1.1562, -0.4102, -0.6602, -0.2373],
        [-0.1650,  1.0469, -0.7031, -0.5742, -0.4902],
        [ 0.1650,  0.9766, -0.9492, -0.9883, -0.7227],
        [-0.0172,  1.3594, -0.3027, -1.0000, -0.5625],
        [ 0.1768,  0.6797, -0.5938, -0.8242, -0.4102],
        [-0.3477,  0.6328, -0.1875, -0.6445, -0.1240],
        [-0.2930,  1.0469, -0.2295, -0.7383, -0.7617],
        [ 0.1177,  1.1406, -0.7969, -0.6523, -0.3086],
        [ 0.0952,  0.6172, -0.7070, -0.3887, -0.0055],
        [-0.1982,  0.9648, -0.6836, -0.6602, -0.9688],
        [-0.1553,  0.9922, -0.3652, -0.7695, -0.4961]], device='cuda:0',
       dtype=torch.bfloat16, grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None), SequenceClassifierOutput(loss=tensor(2.2075, device='cuda:0', grad_fn=<NllLossBackward0>), logits=tensor([[-0.1475,  1.4141, -0.7383, -0.6875, -0.3848],
        [-0.1465,  0.9297, -0.4785, -1.0391, -0.3262],
        [ 0.2275,  1.0312, -0.4414, -0.3750, -0.4941],
        [ 0.2158,  0.7578, -0.7734, -0.5859, -0.6719],
        [-0.1108,  1.3203, -0.5117, -0.6289, -0.5742],
        [ 0.1318,  1.1406, -0.5352, -0.7852, -0.7461],
        [ 0.1533,  1.2109, -0.7148, -0.7812, -0.6484],
        [-0.1738,  1.0000, -0.1934, -0.7969, -0.3789],
W0629 11:52:52.467000 139012379337344 torch/_dynamo/convert_frame.py:824]     result = inner_convert(
W0629 11:52:52.467000 139012379337344 torch/_dynamo/convert_frame.py:824]     result = inner_convert(
W0629 11:52:52.467000 139012379337344 torch/_dynamo/convert_frame.py:824]     result = inner_convert(
W0629 11:52:52.467000 139012379337344 torch/_dynamo/convert_frame.py:824]     result = inner_convert(
        [-0.3926,  1.3047,  0.0060, -0.6445, -0.8281],
        [-0.1992,  1.0156, -0.0688, -0.7852, -0.9805],
        [-0.2734,  1.3438, -0.6680, -0.8164, -0.3262],
        [-0.1660,  0.8359, -0.0432, -0.4922, -1.2500],
        [-0.1157,  1.1016, -0.6328, -0.5430, -0.3516],
        [-0.2578,  1.0469, -0.2256, -0.8672, -0.5039],
        [ 0.2715,  1.1094, -0.7578, -0.8867, -0.4395]], device='cuda:0',
       dtype=torch.bfloat16, grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None), SequenceClassifierOutput(loss=tensor(0.6782, device='cuda:0', grad_fn=<NllLossBackward0>), logits=tensor([[-0.0259,  0.9883, -0.3008, -0.8086, -0.8945],
        [ 0.1514,  1.3359, -0.3555, -0.5664, -0.6094],
        [-0.2441,  1.2422, -0.1226, -1.0312, -0.7969],
        [-0.0962,  1.1016, -0.3418, -0.4980, -1.0312],
        [ 0.2500,  1.4375, -0.3633, -0.6719, -0.4414],
        [-0.1904,  1.2969, -0.1504, -1.0391, -0.8750],
        [ 0.1787,  1.1719, -0.3789, -1.0781, -0.2734],
        [ 0.3086,  0.8203, -0.4980, -0.7070, -0.4453],
        [-0.1475,  1.3047, -0.2500, -0.8711, -0.7344],
        [ 0.0342,  1.3359, -0.4727, -0.7383, -0.5156],
        [-0.0278,  1.2109, -0.6719, -0.9531, -0.2490],
        [-0.1494,  1.2266, -0.3438, -1.1406, -0.5781],
        [-0.0175,  1.1875, -0.3594, -0.6562, -0.6016],
        [-0.3555,  1.3516, -0.6406, -0.4805, -0.5820],
        [-0.2197,  0.9492, -0.2207, -0.8086, -1.2188],
        [-0.2305,  0.9414, -0.3770, -0.7070, -0.5547]], device='cuda:0',
       dtype=torch.bfloat16, grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None), SequenceClassifierOutput(loss=tensor(0.7628, device='cuda:0', grad_fn=<NllLossBackward0>), logits=tensor([[ 0.0688,  1.3984, -0.6875, -0.7656, -0.5273],
        [ 0.0771,  1.0078, -0.6367, -0.4844, -0.2988],
        [ 0.1074,  1.2969, -0.4375, -0.7891, -0.8477],
        [-0.0108,  1.1172, -0.5273, -0.6445, -0.5664],
        [ 0.2695,  1.1562, -0.1650, -0.7383, -0.7773],
        [ 0.2676,  0.8320, -1.2109, -0.8828, -0.0364],
        [-0.1089,  1.2344, -0.1030, -0.7539, -0.9766],
        [-0.0194,  1.3906, -0.5703, -0.7617, -0.5312],
        [-0.1357,  1.3047, -0.6875, -0.9297, -0.4180],
        [-0.3340,  1.2500, -0.3594, -0.8711, -0.4609],
        [ 0.2295,  0.9375, -0.4551, -0.7969, -0.5508],
        [ 0.1245,  1.3281, -0.7617, -0.9180, -0.3906],
        [-0.5664,  1.3828, -0.4551, -0.8555, -0.5898],
        [ 0.2988,  1.1094, -0.2910, -0.8906, -0.5898],
        [ 0.3008,  1.3984, -0.9023, -0.8008, -0.6523],
        [-0.0608,  1.1172, -0.3281, -0.9375, -0.6484]], device='cuda:0',
       dtype=torch.bfloat16, grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None), SequenceClassifierOutput(loss=tensor(0.5588, device='cuda:0', grad_fn=<NllLossBackward0>), logits=tensor([[-0.1729,  1.3203, -0.1523, -1.0781, -0.7891],
        [-0.3477,  1.2031, -0.0679, -0.6719, -0.4238],
        [-0.2061,  1.2422, -0.5820, -1.1484, -0.6289],
        [-0.2041,  1.5625, -0.2559, -0.9141, -0.2178],
        [-0.0272,  0.9453, -0.6797, -0.7500, -0.7500],
        [ 0.0361,  1.1719, -0.6680, -0.9297, -0.2451],
        [ 0.0596,  1.4531, -0.6758, -0.8320, -0.3438],
        [-0.1016,  1.1016, -0.3105, -0.7695, -0.6875],
        [ 0.1748,  0.7188, -0.8008, -1.0312, -0.2539],
        [-0.0688,  1.7109, -0.5039, -0.9023, -0.4941],
        [ 0.2275,  1.1875, -0.5547, -0.6680, -0.6992],
        [ 0.2148,  1.0625, -0.3906, -0.7500, -0.4805],
        [-0.1963,  1.2500, -0.6406, -0.6719, -0.5156],
        [-0.2305,  1.4375, -0.5898, -1.0234, -0.2793],
        [ 0.2393,  1.6562, -0.3457, -0.8164, -0.7695],
        [-0.0781,  1.5938, -0.4492, -0.4316, -0.3164]], device='cuda:0',
       dtype=torch.bfloat16, grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None), SequenceClassifierOutput(loss=tensor(0.9971, device='cuda:0', grad_fn=<NllLossBackward0>), logits=tensor([[ 0.0286,  1.1172, -0.3965, -0.4570, -1.0078],
        [-0.0549,  1.1094,  0.0240, -0.6562, -0.7070],
        [ 0.0889,  1.4688, -0.3379, -0.6758, -0.2090],
        [-0.0327,  1.0625, -0.1445, -0.8164, -0.8008],
        [-0.0344,  1.0859, -0.2812, -0.7227, -0.8398],
        [-0.0204,  1.1484, -0.1426, -0.7344, -0.5820],
        [ 0.0430,  1.0000, -0.4023, -0.9414, -0.7539],
        [-0.0369,  1.4688, -0.6836, -0.7773, -0.5859],
        [ 0.1147,  1.1641, -0.4336, -1.0547, -0.5039],
        [ 0.0693,  1.6016, -0.6094, -1.0234, -0.6055],
        [-0.0747,  0.9219, -0.4199, -0.7578, -0.7852],
        [-0.3145,  1.4453, -0.5000, -0.7773, -0.7461],
        [ 0.2734,  1.0156, -0.5938, -0.7656, -0.5430],
        [-0.1177,  1.3438, -0.4336, -0.6680, -0.5859],
        [-0.4336,  1.4062, -0.6523, -0.8359, -0.6211],
        [ 0.2695,  1.5859, -0.1250, -1.1328, -0.3086]], device='cuda:0',
       dtype=torch.bfloat16, grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None), SequenceClassifierOutput(loss=tensor(0.8832, device='cuda:0', grad_fn=<NllLossBackward0>), logits=tensor([[ 0.1118,  1.2969, -0.4551, -0.2891, -0.8711],
        [-0.3613,  1.5391, -0.5547, -0.4316, -0.4453],
        [-0.1040,  0.9180, -0.5703, -0.6953, -0.8984],
        [-0.2617,  1.3594, -0.0801, -0.8164, -0.7227],
        [-0.3066,  1.3125, -0.0430, -0.7031, -0.6562],
        [-0.2734,  1.0703, -0.7305, -0.7422, -0.4863],
        [ 0.0264,  1.2734, -0.7578, -0.8555, -0.5742],
        [-0.1270,  1.3047, -0.3496, -0.5430, -0.7148],
        [-0.0030,  1.4453, -0.4492, -0.5703, -0.7734],
        [ 0.0674,  1.4141, -0.3848, -0.8203, -0.6172],
        [ 0.2578,  1.5625, -0.3594, -0.6992, -0.7266],
        [-0.1504,  0.9727, -0.3145, -0.6523, -0.6914],
        [ 0.5938,  0.3398, -0.8945, -0.0635,  0.1953],
        [ 0.2314,  1.4844, -0.2852, -0.7148, -0.4805],
        [-0.0124,  1.3672, -0.0776, -0.6758, -0.5312],
        [ 0.0903,  1.2344, -0.5547, -0.9688, -0.7617]], device='cuda:0',
       dtype=torch.bfloat16, grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None), SequenceClassifierOutput(loss=tensor(0.7350, device='cuda:0', grad_fn=<NllLossBackward0>), logits=tensor([[-0.0688,  1.6719, -0.4609, -1.1562, -0.4453],
        [-0.1177,  1.2344, -0.3691, -0.8516, -0.7070],
        [ 0.1680,  1.2500, -0.2393, -0.9297, -0.2500],
        [ 0.0967,  1.2891, -0.6055, -0.5000, -0.2695],
        [-0.0942,  1.3125, -0.1768, -0.8477, -0.8711],
        [ 0.0762,  1.5312, -0.3496, -0.9219, -0.4043],
        [ 0.1299,  1.2188, -0.5430, -0.6602, -0.5195],
        [-0.1094,  1.3359, -0.4121, -0.6719, -0.4844],
        [ 0.0938,  1.4141, -0.3555, -0.7461, -0.8320],
        [-0.1660,  1.2969, -0.7344, -0.8789, -0.3652],
        [-0.1001,  1.2656, -0.6602, -1.1406, -0.5234],
        [ 0.0041,  1.1328, -0.6992, -0.8086, -0.3340],
        [-0.0223,  1.3594, -0.2969, -0.5938, -0.4570],
        [-0.1157,  1.0938, -0.2832, -0.5156, -0.5195],
        [ 0.0718,  1.3125, -0.5391, -0.4766, -0.3711],
        [ 0.0084,  1.5234, -0.3203, -0.6328, -0.8828]], device='cuda:0',
       dtype=torch.bfloat16, grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None), SequenceClassifierOutput(loss=tensor(0.5533, device='cuda:0', grad_fn=<NllLossBackward0>), logits=tensor([[ 0.3066,  1.4297, -0.5156, -0.8281, -0.2715],
        [ 0.1582,  1.5391, -0.3965, -0.8242, -0.9531],
        [ 0.1133,  1.3984, -0.4277, -0.9648, -0.8594],
        [ 0.1011,  0.7969, -0.6602, -0.8203, -0.8867],
        [ 0.1523,  1.3906, -0.6211, -0.8086, -0.4434],
        [ 0.3398,  1.2656, -0.1738, -0.9219, -0.7109],
        [ 0.0544,  1.2969, -0.5859, -0.6016, -0.3184],
        [-0.2637,  1.3203, -0.5547, -0.4707, -0.5078],
        [-0.0483,  1.8359, -0.3594, -0.8555, -0.4941],
        [-0.2676,  1.2344, -0.3633, -0.8125, -0.4688],
        [-0.0569,  1.2578, -0.5234, -0.4766, -0.8359],
        [ 0.2129,  1.1406, -0.3906, -0.7188, -0.3691],
        [-0.0767,  1.2969, -0.6055, -0.9297, -0.5547],
        [ 0.0488,  1.4219, -0.3652, -0.6797, -0.5000],
        [-0.0332,  1.1719, -0.3887, -0.7617, -0.4512],
        [-0.0036,  1.3672, -0.2871, -0.7383, -0.6523]], device='cuda:0',
       dtype=torch.bfloat16, grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None), SequenceClassifierOutput(loss=tensor(0.9277, device='cuda:0', grad_fn=<NllLossBackward0>), logits=tensor([[-0.1060,  1.1641,  0.0952, -0.6641, -1.1562],
        [-0.3457,  0.9805, -0.5117, -0.6055, -0.8359],
        [-0.0029,  0.8555, -0.3926, -0.7656, -0.9531],
        [ 0.2451,  1.3594, -0.3984, -1.0625, -0.5469],
        [-0.0918,  0.9219, -0.0835, -0.8438, -0.6680],
        [-0.2266,  1.2500, -0.4492, -0.6602, -0.3379],
        [-0.0292,  1.5234, -0.5938, -1.0156, -0.3809],
        [ 0.4141,  0.4980, -0.9492, -0.5273,  0.4395],
        [ 0.0957,  1.8359, -0.4512, -0.7305, -0.4785],
        [-0.1836,  0.8359, -0.3457, -0.3242, -0.6328],
        [-0.1660,  1.3125, -0.4297, -0.7539, -0.4121],
        [ 0.3691,  1.2500, -0.5703, -0.4980, -0.2715],
        [-0.1328,  1.4922, -0.6953, -1.1094, -0.6758],
        [ 0.1562,  1.0000, -0.4473, -0.4297, -0.3477],
        [ 0.1895,  1.5703, -0.7422, -0.6875, -0.3770],
        [ 0.1260,  1.4453, -0.3691, -0.9609, -0.4121]], device='cuda:0',
       dtype=torch.bfloat16, grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None), SequenceClassifierOutput(loss=tensor(0.7124, device='cuda:0', grad_fn=<NllLossBackward0>), logits=tensor([[-0.1885,  0.7812, -0.5820, -0.4570, -1.0078],
        [-0.1504,  1.0703, -0.1216, -0.5469, -0.1934],
        [ 0.2285,  1.1875, -0.3555, -0.7461, -0.8164],
        [ 0.3242,  1.3438, -0.7695, -0.7305, -0.6094],
        [-0.0223,  1.3203, -0.7031, -0.7383, -0.3984],
        [ 0.4219,  1.4219, -0.6914, -0.9922, -0.3242],
        [-0.2109,  1.2578, -0.8281, -0.7812, -0.4160],
        [-0.1279,  1.4375, -0.4902, -1.1094, -0.6133],
        [ 0.3965,  1.4453, -0.5859, -0.8477, -0.1377],
        [ 0.1240,  1.3750, -0.2246, -0.7695, -0.7070],
        [-0.0574,  1.4609, -0.6523, -0.7734, -0.6797],
        [-0.3047,  1.3203, -0.6328, -0.8398, -0.6289],
        [-0.1240,  1.3516, -0.3730, -0.5195, -0.6055],
        [-0.2002,  1.0703, -0.5898, -0.7695, -0.3887],
        [-0.2334,  1.0859, -0.4570, -1.1172, -0.5742],
        [-0.0396,  1.4297, -0.8086, -0.7305, -0.3066]], device='cuda:0',
       dtype=torch.bfloat16, grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None), SequenceClassifierOutput(loss=tensor(0.6372, device='cuda:0', grad_fn=<NllLossBackward0>), logits=tensor([[ 0.1465,  1.1484, -0.4434, -0.8789, -0.8711],
        [ 0.0193,  1.3438, -0.2676, -0.7617, -0.9844],
        [ 0.2656,  1.4375, -0.5859, -1.0000, -0.5156],
        [ 0.0874,  1.0156, -0.1396, -0.5898, -0.4102],
        [-0.1318,  1.4688, -0.5156, -0.6445, -0.2637],
        [ 0.0447,  1.6328, -0.4102, -0.9531, -0.5898],
        [ 0.0181,  1.0859, -0.3574, -0.8164, -0.7930],
        [-0.0154,  1.0938, -0.4141, -0.4961, -0.6914],
        [ 0.3340,  1.3281, -0.5547, -0.6914, -0.5391],
        [ 0.1738,  1.6172, -0.6211, -0.8203, -0.2852],
        [ 0.1865,  1.1016, -0.3398, -0.6797, -0.6914],
        [ 0.4629,  1.7188, -0.5352, -0.7656, -0.8516],
        [ 0.1338,  1.4062, -0.3555, -0.7539, -0.4004],
        [ 0.0986,  1.3672, -0.6484, -0.7891, -0.3945],
        [ 0.3008,  1.4531, -0.2734, -0.9297, -0.3887],
        [ 0.1396,  1.0312, -0.2988, -1.1328, -0.6836]], device='cuda:0',
       dtype=torch.bfloat16, grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None), SequenceClassifierOutput(loss=tensor(0.7087, device='cuda:0', grad_fn=<NllLossBackward0>), logits=tensor([[-0.1475,  1.2344, -0.6836, -0.4004, -0.3945],
        [-0.1172,  1.3438, -0.5078, -0.4902, -0.5430],
        [-0.0688,  1.3516, -0.8359, -0.8750, -0.5820],
        [-0.1855,  1.0781, -0.3477, -0.8320, -0.5742],
        [ 0.1709,  1.3516, -0.8125, -0.7656, -0.4219],
        [ 0.2188,  1.4688, -0.6250, -0.9453, -0.4629],
        [-0.0610,  1.4219, -0.3555, -0.9375, -0.2031],
        [-0.2168,  1.1797, -0.2793, -0.8125, -0.6094],
        [-0.0374,  1.2656, -0.8672, -0.7500, -0.4395],
        [ 0.1670,  1.3984, -0.1367, -0.7422, -0.4609],
        [ 0.2041,  0.8828, -0.2451, -0.4941, -0.7070],
        [-0.1904,  1.1875, -0.7422, -0.6680, -0.4141],
        [-0.0021,  1.1641, -0.5078, -0.5000, -0.4043],
        [-0.0461,  1.2656, -0.2178, -0.4961, -0.6328],
        [-0.0942,  1.3984, -0.6641, -0.6914, -0.6484],
        [-0.0022,  1.2969, -0.2100, -1.0703, -0.8281]], device='cuda:0',
       dtype=torch.bfloat16, grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None), SequenceClassifierOutput(loss=tensor(0.5754, device='cuda:0', grad_fn=<NllLossBackward0>), logits=tensor([[ 0.1982,  1.6797, -0.4121, -0.8477, -0.7344],
        [-0.0493,  1.0078, -0.5273, -0.5312, -0.4512],
        [-0.3047,  1.0938, -0.6836, -1.0938, -0.4453],
        [ 0.2559,  1.2656, -0.8086, -1.0547, -0.6055],
        [-0.1924,  1.3984, -0.5586, -0.6836, -0.4707],
        [ 0.1582,  1.2500, -0.6406, -0.9141, -0.3027],
        [-0.0378,  1.1094, -0.3691, -0.8281, -0.9688],
        [-0.1001,  1.6562, -0.3965, -0.8164, -0.6328],
        [-0.0698,  1.3203, -0.3184, -0.5391, -0.7070],
        [ 0.2217,  1.3281, -0.2754, -0.4688, -0.4512],
        [-0.0757,  1.2500, -0.5273, -0.9727, -0.5078],
        [ 0.1182,  1.6719, -0.6445, -1.0391, -0.5586],
        [ 0.1660,  1.7422, -0.5430, -1.0625, -0.5312],
        [ 0.0135,  1.4766, -0.4902, -1.0234, -0.6367],
        [ 0.2598,  1.5000, -0.7656, -0.6133, -0.4023],
        [-0.0405,  1.5859, -0.2969, -0.8516, -0.4629]], device='cuda:0',
       dtype=torch.bfloat16, grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None), SequenceClassifierOutput(loss=tensor(0.8639, device='cuda:0', grad_fn=<NllLossBackward0>), logits=tensor([[ 0.0874,  1.2188, -0.3418, -1.0938, -1.1406],
        [-0.2275,  1.2500, -0.6641, -0.8711, -0.8047],
        [-0.2012,  1.3672, -0.2324, -1.2656, -0.6016],
        [ 0.0703,  1.2734, -0.2324, -0.7656, -0.9766],
        [-0.0047,  1.0156, -0.7031, -0.7969, -0.4297],
        [ 0.0496,  1.1562, -0.2559, -0.5703, -0.8516],
        [-0.2871,  1.2891, -0.8828, -1.0000, -0.3516],
        [-0.1855,  1.2422, -0.4102, -0.4707, -0.4336],
        [ 0.4883,  1.2500, -0.5781, -0.7070, -0.4727],
        [-0.0791,  1.1875, -0.3770, -0.5391, -0.3340],
        [ 0.0850,  1.5625, -0.5078, -0.9648, -0.4883],
        [-0.0952,  1.6719, -0.6133, -0.9141, -0.3965],
        [-0.0099,  1.3047, -0.5117, -0.6602, -0.6602],
        [-0.0214,  1.3750, -0.7188, -0.8789, -0.4824],
        [-0.2119,  1.0156, -0.5273, -0.2734, -0.7031],
        [ 0.0913,  1.4375, -0.6719, -0.8555, -0.7930]], device='cuda:0',
       dtype=torch.bfloat16, grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None), SequenceClassifierOutput(loss=tensor(0.7352, device='cuda:0', grad_fn=<NllLossBackward0>), logits=tensor([[-3.2617e-01,  1.3359e+00, -7.6172e-01, -6.4062e-01, -6.8750e-01],
        [ 2.8801e-04,  1.5469e+00, -7.1484e-01, -8.2422e-01, -2.0605e-01],
        [-1.5234e-01,  1.2422e+00, -5.5078e-01, -8.5938e-01, -7.6172e-01],
        [-2.5586e-01,  1.3047e+00, -4.3945e-01, -6.5234e-01, -5.9375e-01],
        [ 8.8867e-02,  1.0938e+00, -5.0781e-01, -3.6719e-01, -2.9688e-01],
        [ 1.5527e-01,  1.7500e+00, -6.0938e-01, -6.2109e-01, -6.1328e-01],
        [-1.6699e-01,  1.0312e+00, -1.6992e-01, -6.3281e-01, -4.1602e-01],
        [ 2.2705e-02,  1.1250e+00, -4.7656e-01, -9.6484e-01, -4.6484e-01],
        [-3.4375e-01,  1.1953e+00, -3.5742e-01, -7.3828e-01, -3.3984e-01],
        [ 9.4238e-02,  1.3125e+00, -7.9688e-01, -1.1094e+00, -3.5352e-01],
        [ 2.5781e-01,  1.1016e+00, -5.4297e-01, -8.2812e-01, -4.0039e-01],
        [ 1.3867e-01,  1.3828e+00, -5.4688e-01, -1.1172e+00, -4.2578e-01],
        [ 1.9336e-01,  1.4062e+00, -5.5469e-01, -5.4688e-01, -3.3203e-01],
        [-1.4355e-01,  1.1953e+00, -5.2734e-01, -6.4453e-01, -6.0547e-01],
        [ 6.2988e-02,  1.3906e+00, -6.3672e-01, -9.2578e-01, -4.8047e-01],
        [ 4.2480e-02,  1.5859e+00, -7.4219e-02, -6.8750e-01, -4.3945e-01]],
       device='cuda:0', dtype=torch.bfloat16, grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None), SequenceClassifierOutput(loss=tensor(0.8810, device='cuda:0', grad_fn=<NllLossBackward0>), logits=tensor([[ 0.1885,  1.0547, -0.4395, -0.9688, -0.7227],
        [ 0.1689,  1.5000, -0.3691, -0.8438, -0.7812],
        [ 0.2422,  1.2422, -0.3418, -0.5039, -0.4785],
        [ 0.0447,  1.5391, -0.6250, -0.8711, -0.3828],
        [-0.0442,  1.5391, -0.4023, -0.9375, -0.3809],
        [ 0.1206,  0.9727, -0.2930, -0.7344, -0.7070],
        [-0.0767,  1.1094, -0.5195, -0.8906, -0.7500],
        [ 0.0493,  1.1484, -0.3223, -0.7461, -0.4707],
        [-0.1963,  1.2188, -0.4570, -0.6836, -0.7852],
        [-0.1592,  1.0781, -0.5781, -0.4648, -0.4648],
        [ 0.0417,  0.9375, -0.9141, -0.9805, -0.4395],
        [-0.0649,  1.5078, -0.5039, -0.8477, -0.7617],
        [ 0.2637,  1.0156, -0.8281, -0.7539, -0.7188],
        [ 0.1196,  0.9727, -0.5508, -0.5156, -0.6445],
        [-0.1641,  1.5391, -0.5664, -1.1016, -0.6602],
        [-0.2500,  1.3672, -0.4492, -0.7500, -0.4922]], device='cuda:0',
       dtype=torch.bfloat16, grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None), SequenceClassifierOutput(loss=tensor(0.8883, device='cuda:0', grad_fn=<NllLossBackward0>), logits=tensor([[ 0.0864,  1.5156, -0.4395, -0.7227, -0.5000],
        [ 0.0110,  1.2422, -0.6289, -0.8906, -0.6875],
        [-0.1182,  1.2969, -0.3730, -0.4668, -0.5938],
        [-0.0422,  1.3672, -0.4766, -0.8320, -0.5312],
        [ 0.0869,  1.2344, -0.5469, -0.7422, -0.4121],
        [ 0.0096,  1.3672, -0.2949, -0.9883, -0.5234],
        [ 0.0094,  1.5312, -0.6250, -0.9727, -0.3926],
        [-0.3906,  1.3438, -0.6914, -0.2520, -0.4648],
        [-0.0361,  1.3984, -0.6602, -0.6719, -0.3223],
        [-0.1279,  1.3125, -0.8906, -0.7930, -0.3145],
        [ 0.1758,  0.9766, -0.2871, -0.9531, -0.8086],
        [-0.1094,  1.3828, -0.8008, -0.6914, -0.5039],
        [-0.0947,  1.4297, -0.3242, -0.6836, -0.3496],
        [ 0.3066,  1.2031, -0.3047, -0.7500, -0.5156],
        [-0.0786,  1.7422, -0.5977, -0.9727, -0.3691],
        [ 0.2539,  0.9258, -0.3477, -0.5039, -0.2520]], device='cuda:0',
       dtype=torch.bfloat16, grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None), SequenceClassifierOutput(loss=tensor(0.6923, device='cuda:0', grad_fn=<NllLossBackward0>), logits=tensor([[ 0.0396,  1.3906, -0.4590, -0.5781, -0.5898],
        [-0.1260,  1.5078, -0.4941, -1.0234, -0.0781],
        [-0.0417,  1.5391, -0.6758, -0.8555, -0.5625],
        [-0.2041,  0.8984, -0.2988, -1.0000, -0.6328],
        [ 0.0513,  1.2578, -0.4629, -0.4980, -0.4023],
        [ 0.2988,  1.0391, -0.7461, -0.5195, -0.1475],
        [ 0.1289,  1.4766, -0.4551, -0.5859, -0.8047],
        [ 0.0898,  1.3984, -0.6289, -1.0781, -0.3945],
        [ 0.2471,  1.1250, -0.5859, -0.5039, -0.2246],
        [-0.3418,  1.4609, -0.4062, -0.7539, -0.7109],
        [-0.1807,  0.9766, -0.4434, -0.6328, -0.5117],
        [-0.0947,  1.5938, -0.4141, -0.8086, -0.4004],
        [-0.2051,  1.3672, -0.2832, -0.7812, -0.9766],
        [-0.0330,  1.2031, -0.6758, -0.4902, -0.3652],
        [ 0.0835,  1.2422, -0.5977, -0.8672, -0.6016],
        [ 0.1562,  1.2500, -0.5547, -0.6328, -0.4766]], device='cuda:0',
       dtype=torch.bfloat16, grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None), SequenceClassifierOutput(loss=tensor(0.9218, device='cuda:0', grad_fn=<NllLossBackward0>), logits=tensor([[-0.1787,  1.3828, -0.3965, -0.7500, -0.8281],
        [-0.0898,  1.5703, -0.3770, -0.7852, -0.3477],
        [ 0.0166,  1.5078, -0.1328, -0.8477, -0.4316],
        [-0.0938,  1.3203, -0.0170, -0.4844, -0.3262],
        [ 0.1201,  1.7578, -0.3301, -1.0391, -0.8008],
        [ 0.3848,  1.3203, -0.3750, -0.7422, -0.6289],
        [ 0.0869,  1.6406, -0.4043, -0.9883, -0.3984],
        [ 0.0457,  1.0391, -0.4941, -0.7969, -0.6641],
        [ 0.1924,  1.4688, -0.5898, -0.8438, -0.5039],
        [ 0.0396,  1.4141, -0.8555, -1.0078, -0.5703],
        [ 0.6250,  1.1797, -0.4062, -0.6523, -0.4023],
        [-0.0366,  1.4219, -0.5938, -0.6758, -0.3086],
        [ 0.2520,  0.9023, -0.6758, -0.3691, -0.5742],
        [-0.0801,  1.0938, -0.6875, -0.7031, -0.5234],
        [ 0.0053,  1.5625, -0.6680, -0.6680, -0.4727],
        [ 0.1177,  1.1328, -0.4277, -0.8945, -0.9883]], device='cuda:0',
       dtype=torch.bfloat16, grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None), SequenceClassifierOutput(loss=tensor(0.8658, device='cuda:0', grad_fn=<NllLossBackward0>), logits=tensor([[-0.0645,  1.6562, -0.3594, -0.6914, -0.6016],
        [ 0.1787,  1.4062, -0.3906, -0.7227, -0.5312],
        [ 0.1094,  1.4062, -0.5742, -1.0703, -0.3789],
        [-0.2930,  1.5938, -0.2969, -0.8516, -0.4648],
        [ 0.2559,  1.4922, -0.3730, -0.6211, -0.3555],
        [-0.1641,  1.1250, -0.7656, -0.5430, -0.6484],
        [ 0.1465,  1.7188, -0.6523, -1.0859, -0.5547],
        [ 0.0481,  1.1797, -0.6211, -0.7227, -0.3672],
        [-0.2676,  1.5391, -0.4941, -0.7852, -0.4727],
        [ 0.2930,  1.6484, -0.3418, -0.9102, -0.7109],
        [ 0.0192,  1.1875, -0.6445, -0.7227, -0.3340],
        [ 0.0645,  1.4141, -0.4355, -0.7578, -0.5859],
        [ 0.1494,  0.9609, -0.3066, -0.7305, -0.5312],
        [-0.1660,  1.1094, -0.6367, -0.9219, -0.5352],
        [ 0.2344,  1.3672, -0.7266, -0.7461, -0.3477],
        [-0.1201,  1.6406, -0.4980, -0.7852, -0.5000]], device='cuda:0',
       dtype=torch.bfloat16, grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None), SequenceClassifierOutput(loss=tensor(0.6324, device='cuda:0', grad_fn=<NllLossBackward0>), logits=tensor([[ 0.1357,  1.4766, -0.7734, -0.8672, -0.5664],
        [-0.2754,  1.2109, -0.5273, -0.9922, -0.5391],
        [ 0.0045,  1.2344, -0.0352, -0.6836, -1.0078],
        [-0.2266,  1.4688, -0.4570, -0.8086, -0.3281],
        [ 0.5352,  0.7383, -0.6641, -0.4688,  0.4727],
        [ 0.0200,  1.3281, -0.3613, -0.7031, -0.7227],
        [ 0.0623,  1.7031, -0.6055, -0.7812, -0.4141],
        [ 0.0640,  1.0703, -0.4395, -0.8711, -0.5859],
        [ 0.3047,  1.3750, -0.6758, -0.8203, -0.6016],
        [-0.0820,  1.5312, -0.4941, -0.8281, -0.6328],
        [-0.2930,  1.1406, -0.1982, -0.8438, -1.2266],
        [-0.0952,  1.2969, -0.7266, -0.6758, -0.5195],
        [ 0.2070,  1.0312, -0.8398, -0.5352, -0.6289],
        [ 0.1123,  1.7188, -0.6562, -0.8047, -0.2852],
        [ 0.4082,  1.2734, -0.8320, -0.6016, -0.4316],
        [ 0.0649,  1.4844, -0.5703, -0.7188, -0.4590]], device='cuda:0',
       dtype=torch.bfloat16, grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None), SequenceClassifierOutput(loss=tensor(0.8694, device='cuda:0', grad_fn=<NllLossBackward0>), logits=tensor([[-0.2715,  1.3906, -0.3906, -0.7227, -0.6484],
        [ 0.0654,  1.0391, -0.3633, -0.7344, -0.4844],
        [ 0.0128,  1.2734, -0.4414, -0.6953, -0.2949],
        [-0.0461,  1.3828, -0.2676, -0.9219, -0.9258],
        [ 0.1240,  1.4844, -0.9375, -0.8828, -0.6289],
        [-0.2832,  1.2969, -0.5820, -0.4316, -0.8750],
        [ 0.0193,  1.1016, -0.6562, -0.8164, -0.2354],
        [ 0.1904,  1.3047, -0.7969, -0.6250, -1.0234],
        [ 0.1982,  1.0859, -0.4785, -0.7773, -0.5273],
        [ 0.1797,  1.3750, -0.3848, -1.1875, -0.5117],
        [ 0.0040,  1.3828, -0.6289, -0.4492, -0.3047],
        [ 0.0166,  1.3047, -0.4336, -1.0312, -0.3730],
        [ 0.2236,  1.6250, -0.3438, -0.6211, -0.5586],
        [-0.1582,  1.5000, -0.2832, -0.8477, -0.7500],
        [-0.0398,  1.5234, -0.4375, -0.6719, -0.4766],
        [-0.0728,  1.5938, -0.5547, -0.5938, -0.4824]], device='cuda:0',
       dtype=torch.bfloat16, grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None), SequenceClassifierOutput(loss=tensor(1.1519, device='cuda:0', grad_fn=<NllLossBackward0>), logits=tensor([[-0.1030,  1.2812, -0.3281, -0.7266, -0.5742],
        [-0.0957,  0.9453, -0.2695, -0.7148, -0.8984],
        [ 0.2188,  1.3984, -0.6289, -0.8555, -0.3047],
        [ 0.2754,  1.2422,  0.1196, -0.5156, -0.6641],
        [ 0.1318,  1.3281, -0.5078, -0.5703, -0.4258],
        [-0.1357,  1.3281, -0.7148, -0.6367, -0.7031],
        [-0.3691,  1.5078, -0.3965, -0.7266, -0.6758],
        [-0.2090,  1.2344, -0.4492, -0.8906, -0.7188],
        [ 0.2305,  1.2578, -0.2539, -0.4922, -0.6289],
        [ 0.0080,  1.0391, -0.4414, -0.5234, -0.3125],
        [ 0.0522,  1.3672, -0.5586, -0.7344, -0.5273],
        [-0.0674,  1.4141, -0.6211, -0.8398, -0.7305],
        [ 0.0190,  1.5781, -0.3027, -0.7891, -0.5586],
        [-0.1709,  1.3750, -0.4688, -0.6250, -0.5547],
        [ 0.0184,  1.4609, -0.6406, -0.8008, -0.4395],
        [-0.2021,  1.2500, -0.8125, -0.9414, -0.8555]], device='cuda:0',
       dtype=torch.bfloat16, grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None), SequenceClassifierOutput(loss=tensor(0.9910, device='cuda:0', grad_fn=<NllLossBackward0>), logits=tensor([[ 0.1602,  1.2812, -0.6719, -0.9336, -0.3477],
        [ 0.0413,  1.1484, -0.6641, -0.5469, -0.2871],
        [ 0.1689,  1.6406, -0.6016, -0.8633, -0.6953],
        [ 0.2432,  1.3125, -0.5625, -1.2578, -0.6133],
        [ 0.1011,  1.5781, -0.3633, -1.1328, -0.7188],
        [ 0.0659,  1.7031, -0.7148, -0.8398, -0.6484],
        [ 0.2812,  1.5547, -0.6484, -0.9023, -0.5898],
        [-0.0713,  1.4297, -0.2295, -1.0547, -0.7227],
        [ 0.1177,  1.1016, -0.2852, -0.5000, -0.4863],
        [ 0.0525,  1.3828, -0.5391, -0.9922, -0.3711],
        [ 0.3652,  1.2344, -0.3887, -0.8242, -0.7500],
        [-0.0432,  1.0938, -0.2012, -0.9375, -1.0547],
        [ 0.0219,  1.3281, -0.4609, -0.7383, -0.6133],
        [ 0.1592,  1.4141, -0.3164, -0.6719, -0.6094],
        [ 0.0260,  1.1797, -0.3770, -0.8359, -0.7148],
        [-0.1079,  1.2422, -0.6484, -0.6289, -0.3848]], device='cuda:0',
       dtype=torch.bfloat16, grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None), SequenceClassifierOutput(loss=tensor(0.6915, device='cuda:0', grad_fn=<NllLossBackward0>), logits=tensor([[ 0.2656,  1.4219, -0.5469, -1.1641, -0.4844],
        [-0.2197,  1.1328, -0.1914, -0.9180, -1.0547],
        [-0.1216,  1.4141, -0.4277, -0.6641, -0.5117],
        [ 0.3828,  0.9961, -0.6211, -0.9023, -0.4688],
        [-0.2148,  1.1875, -0.5156, -1.0703, -0.6328],
        [ 0.0437,  1.3438, -0.2637, -0.8672, -0.4883],
        [ 0.4238,  1.1562, -0.7500, -0.9531, -0.3145],
        [ 0.2676,  1.4297, -0.7148, -0.9023, -0.2773],
        [ 0.1118,  1.5391, -0.6289, -0.8828, -0.4727],
        [ 0.0161,  1.0625, -0.6602, -0.8359, -0.6445],
        [ 0.2383,  1.3594, -0.4297, -1.1094, -0.7461],
        [-0.2383,  1.1875, -0.0562, -0.7695, -0.6641],
        [ 0.0542,  1.4922, -0.6055, -1.0469, -0.4980],
        [ 0.3848,  1.4531, -0.2793, -0.4961, -0.4531],
        [-0.1826,  1.1953, -0.5625, -0.8398, -0.4453],
        [ 0.2090,  1.0469, -1.1172, -0.9180, -0.3770]], device='cuda:0',
       dtype=torch.bfloat16, grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None), SequenceClassifierOutput(loss=tensor(0.9186, device='cuda:0', grad_fn=<NllLossBackward0>), logits=tensor([[ 0.0991,  1.6797, -0.7617, -0.9219, -0.5156],
        [-0.0057,  1.3281, -0.5039, -0.5391, -0.4590],
        [ 0.0908,  1.2656, -0.3281, -0.5742, -0.3867],
        [ 0.0815,  1.2891, -0.5547, -0.7422, -0.5938],
        [ 0.2695,  1.3125, -0.9531, -1.1406, -0.0732],
        [ 0.1631,  1.6562, -0.6836, -0.6992, -0.6406],
        [-0.0625,  1.1328, -0.7148, -0.7852, -0.7305],
        [-0.0781,  1.2969, -0.5156, -0.9102, -0.7109],
        [ 0.0320,  1.0547, -0.3789, -0.7266, -0.4805],
        [ 0.1494,  1.1016, -0.7344, -0.9023, -1.0234],
        [-0.2061,  1.4531, -0.7148, -0.3691, -0.6562],
        [ 0.1050,  1.2812, -0.6680, -0.7188, -0.7227],
        [-0.1270,  0.9961, -0.0918, -0.6211, -0.7305],
        [ 0.2266,  1.4609, -0.5742, -0.7383, -0.4980],
        [-0.1992,  1.5391, -0.4824, -1.2188, -0.4531],
        [ 0.0135,  1.3047, -0.5977, -0.7422, -0.7500]], device='cuda:0',
       dtype=torch.bfloat16, grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None), SequenceClassifierOutput(loss=tensor(0.7634, device='cuda:0', grad_fn=<NllLossBackward0>), logits=tensor([[-0.0156,  1.2656, -0.5156, -1.1484, -0.6172],
        [ 0.2471,  1.5938, -0.6250, -0.9414, -0.6094],
        [ 0.1055,  1.1484, -0.5273, -0.6602, -0.5039],
        [-0.1221,  1.6875, -0.6055, -0.7227, -0.3945],
        [ 0.2275,  1.7578, -0.4238, -0.6484, -0.2480],
        [-0.3320,  1.2578, -0.5938, -0.5234, -0.2480],
        [ 0.3574,  1.3125, -0.4688, -0.8086, -0.5508],
        [ 0.2490,  1.2891, -1.0312, -0.8711, -0.2832],
        [ 0.0019,  1.1172, -0.1475, -0.2910, -0.3496],
        [ 0.3301,  1.2812, -0.4082, -0.9062, -0.5469],
        [-0.1631,  1.1328, -0.7617, -0.9648, -0.7969],
        [ 0.0143,  1.7656, -0.5781, -0.8945, -0.5820],
        [-0.1865,  1.2734, -0.7812, -0.6875, -0.7188],
        [ 0.2559,  1.0859, -0.9297, -0.9414, -0.4219],
        [ 0.2891,  1.6562, -0.5391, -0.8242, -0.4219],
        [ 0.1196,  1.5469, -0.6406, -0.7344, -0.4395]], device='cuda:0',
       dtype=torch.bfloat16, grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None), SequenceClassifierOutput(loss=tensor(0.7335, device='cuda:0', grad_fn=<NllLossBackward0>), logits=tensor([[-0.2227,  1.1250, -0.3613, -0.8555, -0.7344],
        [ 0.0549,  1.2344, -0.4199, -1.0391, -0.4492],
        [-0.2656,  1.5234, -0.4160, -0.9258, -0.1260],
        [ 0.0146,  1.6719, -0.7500, -0.8242, -0.4453],
        [ 0.1270,  1.2969, -0.4355, -1.0469, -0.7422],
        [ 0.3730,  1.2500, -0.7422, -0.5938, -0.5781],
        [-0.1328,  0.8594, -0.2197, -0.7383, -0.8438],
        [ 0.1777,  1.2031, -0.2383, -1.1875, -0.3730],
        [ 0.1221,  1.6250, -0.6523, -0.9414, -0.5273],
        [ 0.2334,  1.2734, -0.5117, -0.7734, -0.3496],
        [-0.2148,  1.4141, -0.4258, -0.7344, -0.4473],
        [-0.1924,  1.5938, -0.7109, -0.5898, -0.7383],
        [-0.3965,  1.2031, -0.0461, -0.8672, -0.9883],
        [-0.2217,  1.0781, -0.5547, -0.0781, -0.8242],
        [-0.2031,  1.2344, -0.2812, -0.6016, -0.4727],
        [-0.1836,  1.5391, -0.6484, -0.8242, -0.6758]], device='cuda:0',
       dtype=torch.bfloat16, grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None), SequenceClassifierOutput(loss=tensor(0.8859, device='cuda:0', grad_fn=<NllLossBackward0>), logits=tensor([[ 0.2949,  0.8984, -0.3438, -0.8945, -0.2314],
        [-0.1406,  1.3438, -0.3340, -0.8906, -0.5781],
        [ 0.2041,  1.5234, -0.3691, -1.0234, -0.6758],
        [-0.0967,  1.3750, -0.5898, -0.9570, -0.6211],
        [-0.1758,  1.4141, -0.4297, -0.9258, -0.6133],
        [ 0.0347,  1.3281, -0.5938, -0.4785, -0.4180],
        [ 0.1167,  1.2734, -0.5312, -0.6719, -0.7617],
        [-0.2041,  1.2266, -0.8672, -0.7500, -0.5547],
        [-0.0786,  1.4531, -0.2734, -0.9375, -0.5117],
        [-0.0811,  1.2812, -0.5625, -0.5586, -0.3652],
        [-0.0986,  1.5703, -0.7383, -0.6328, -0.0776],
        [-0.0128,  1.5547, -0.7500, -0.7500,  0.0067],
        [-0.1982,  1.2734, -0.7500, -0.3672, -0.4590],
        [ 0.1826,  0.8203, -0.2334, -0.9805, -0.6445],
        [ 0.4434,  1.3047, -0.6211, -0.6992, -0.8281],
        [ 0.1797,  1.5078, -0.6016, -0.7578, -0.7305]], device='cuda:0',
       dtype=torch.bfloat16, grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None), SequenceClassifierOutput(loss=tensor(0.9042, device='cuda:0', grad_fn=<NllLossBackward0>), logits=tensor([[ 0.3418,  0.5352, -0.7773, -0.3887,  0.6055],
        [ 0.1084,  1.1406, -0.4141, -0.6172, -0.5469],
        [ 0.0344,  1.3984, -0.5820, -0.7461, -0.1147],
        [-0.2734,  1.0938, -0.7070, -0.6172, -1.0312],
        [-0.2100,  1.4766, -0.7461, -0.9102, -0.6602],
        [ 0.1963,  1.2188, -0.1777, -0.4629, -0.4707],
        [ 0.1289,  1.9531, -0.7031, -0.9414, -0.5625],
        [ 0.2617,  1.2188, -0.4180, -0.7812, -0.7109],
        [ 0.2695,  1.3906, -0.6133, -0.8789, -0.4883],
        [ 0.2773,  0.6602, -1.1953, -0.8516, -0.0159],
        [ 0.0320,  1.2422, -0.5664, -0.7578, -0.3711],
        [ 0.0295,  1.2734, -0.6172, -0.9023, -0.2676],
        [ 0.2539,  1.3516, -0.4980, -0.8516, -0.6289],
        [-0.1172,  0.8359, -0.4531, -0.6523, -0.4004],
        [-0.2871,  1.0156, -0.3496, -0.5430, -0.5312],
        [ 0.4570,  1.2266, -0.4902, -0.6992, -0.3730]], device='cuda:0',
       dtype=torch.bfloat16, grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None), SequenceClassifierOutput(loss=tensor(0.9447, device='cuda:0', grad_fn=<NllLossBackward0>), logits=tensor([[-0.0991,  1.2188, -0.3711, -0.6328, -0.1982],
        [-0.0118,  1.4609, -0.4746, -0.9648, -0.5273],
        [ 0.1885,  1.0781, -0.6016, -0.8984, -0.7266],
        [ 0.1689,  1.1172, -0.6484, -0.7266, -0.7188],
        [ 0.1133,  1.4844, -0.5508, -0.8828, -0.6602],
        [ 0.0977,  1.3906, -0.3359, -0.6719, -0.8594],
        [ 0.1670,  1.3516, -0.5781, -1.0078, -0.6289],
        [ 0.0476,  1.5000, -0.5859, -0.8281, -0.6992],
        [-0.1572,  1.3203, -0.2676, -0.7891, -0.7188],
        [ 0.0381,  1.4844, -0.7109, -0.8438, -0.5977],
        [ 0.3496,  1.0703, -0.7656, -0.6719, -0.3047],
        [-0.0220,  1.1406, -0.6680, -0.5039, -0.4746],
        [-0.1279,  1.1406, -0.1157, -0.8359, -0.4082],
        [-0.3926,  1.6797, -0.5273, -0.7539, -0.5195],
        [ 0.1099,  1.5625, -0.6797, -0.7656, -0.4863],
        [ 0.0879,  1.3047, -0.5977, -0.9883, -0.3203]], device='cuda:0',
       dtype=torch.bfloat16, grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None), SequenceClassifierOutput(loss=tensor(0.6573, device='cuda:0', grad_fn=<NllLossBackward0>), logits=tensor([[ 1.2283e-03,  1.3906e+00, -6.2109e-01, -5.2734e-01, -3.0859e-01],
        [-1.6602e-01,  1.1953e+00, -4.1992e-01, -9.6875e-01, -7.8906e-01],
        [-1.8555e-01,  1.6328e+00, -7.7734e-01, -6.6797e-01, -5.6641e-01],
        [-2.5195e-01,  1.4297e+00, -6.3672e-01, -9.8438e-01, -6.4844e-01],
        [ 2.1118e-02,  1.4141e+00, -7.5000e-01, -8.7500e-01, -4.1016e-01],
        [ 1.9897e-02,  1.1328e+00, -2.3340e-01, -5.9375e-01, -3.8281e-01],
        [-2.4707e-01,  1.3594e+00, -6.2500e-01, -9.2578e-01, -3.8867e-01],
        [ 3.6719e-01,  1.0547e+00, -5.1172e-01, -4.3359e-01, -4.6484e-01],
        [ 4.6875e-02,  1.4688e+00, -3.6914e-01, -5.5859e-01, -6.3281e-01],
        [-6.1768e-02,  1.2812e+00, -3.1836e-01, -7.3438e-01, -6.5234e-01],
        [-7.2266e-02,  1.2266e+00, -6.4062e-01, -1.0781e+00, -5.1562e-01],
        [-2.1289e-01,  1.7188e+00, -6.4062e-01, -1.0234e+00, -7.1875e-01],
        [-1.6406e-01,  1.1797e+00, -1.5039e-01, -1.2188e+00, -8.8672e-01],
        [ 1.1084e-01,  1.3672e+00, -4.8828e-01, -1.2031e+00, -7.5781e-01],
        [-2.3340e-01,  1.1875e+00, -5.5859e-01, -6.0156e-01, -5.7422e-01],
        [ 8.0566e-02,  1.0547e+00, -2.8516e-01, -5.3125e-01, -5.8203e-01]],
       device='cuda:0', dtype=torch.bfloat16, grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None), SequenceClassifierOutput(loss=tensor(0.7422, device='cuda:0', grad_fn=<NllLossBackward0>), logits=tensor([[ 1.4551e-01,  1.3828e+00, -6.4844e-01, -8.7109e-01, -4.3164e-01],
        [ 2.5391e-01,  1.3516e+00, -4.7461e-01, -6.7969e-01, -7.6562e-01],
        [ 3.7305e-01,  1.3828e+00, -3.7695e-01, -6.9531e-01, -6.5234e-01],
        [-3.2227e-02,  1.2656e+00, -4.2188e-01, -9.1797e-01, -5.3516e-01],
        [-6.1768e-02,  9.3359e-01, -8.6328e-01, -7.8125e-01, -3.6133e-01],
        [-4.0527e-02,  1.4531e+00, -3.4570e-01, -1.2656e+00, -9.1016e-01],
        [-1.9409e-02,  9.2188e-01, -5.0391e-01, -7.1875e-01, -3.3398e-01],
        [-2.1680e-01,  1.3594e+00, -3.7109e-01, -5.3516e-01, -5.0781e-01],
        [-8.9722e-03,  1.2891e+00, -4.1992e-01, -9.7266e-01, -4.4922e-01],
        [-6.6895e-02,  1.1172e+00, -3.1836e-01, -7.5781e-01, -9.0234e-01],
        [ 2.6562e-01,  1.4688e+00, -4.8828e-01, -4.8047e-01, -5.7031e-01],
        [-1.1635e-04,  1.6250e+00, -5.6250e-01, -5.0781e-01, -4.4336e-01],
        [ 3.2227e-02,  1.4375e+00, -6.7188e-01, -8.9844e-01, -3.9258e-01],
        [ 2.2888e-03,  1.4844e+00, -4.1016e-01, -9.4141e-01, -8.2812e-01],
        [ 1.3086e-01,  1.5000e+00, -4.3555e-01, -6.5625e-01, -8.0469e-01],
        [ 1.3867e-01,  9.8828e-01, -7.6172e-01, -8.9453e-01, -6.1328e-01]],
       device='cuda:0', dtype=torch.bfloat16, grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None), SequenceClassifierOutput(loss=tensor(0.7156, device='cuda:0', grad_fn=<NllLossBackward0>), logits=tensor([[-0.0200,  0.9570, -0.5508, -0.7422, -0.6953],
        [-0.2598,  1.0938, -0.2617, -0.6953, -0.7930],
        [-0.3945,  1.1719, -0.4629, -0.6016, -0.4961],
        [ 0.1123,  1.1641, -0.7383, -0.5781, -0.3359],
        [-0.2412,  1.1328, -0.3105, -0.6758, -1.1484],
        [-0.0874,  1.3906, -0.4238, -0.9961, -0.6484],
        [ 0.1011,  1.4375, -0.6836, -0.9805, -0.5820],
        [-0.2471,  1.5000, -0.7344, -0.8359, -0.4453],
        [-0.1279,  1.2891, -0.5898, -0.8164, -0.8945],
        [ 0.2578,  1.5859, -0.5586, -0.6250, -0.3047],
        [ 0.0913,  1.4141, -0.5352, -0.6328, -0.4355],
        [ 0.0095,  1.0859, -0.3340, -0.4336, -0.6328],
        [-0.1523,  1.0469, -0.5859, -0.7852, -0.5938],
        [ 0.3164,  1.1328, -0.6211, -0.8359, -0.5391],
        [-0.0684,  1.0078, -0.4551, -1.1172, -0.5195],
        [-0.0425,  1.0859, -0.2119, -0.4121, -0.6523]], device='cuda:0',
       dtype=torch.bfloat16, grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None), SequenceClassifierOutput(loss=tensor(0.7806, device='cuda:0', grad_fn=<NllLossBackward0>), logits=tensor([[-0.0908,  1.1094, -0.8203, -0.9414, -0.4473],
        [ 0.1406,  1.3906, -1.0000, -1.1406, -0.3594],
        [-0.3242,  1.5234, -0.2695, -1.0234, -0.6797],
        [-0.1680,  1.0234, -0.4004, -0.5703, -0.7930],
        [ 0.2373,  1.4453, -0.7500, -0.7773, -0.6289],
        [-0.0835,  1.3828, -0.4297, -0.8477, -0.6055],
        [-0.2676,  1.3047, -0.1055, -0.5859, -1.0391],
        [-0.1123,  1.3672, -0.4199, -0.9883, -0.4297],
        [-0.0923,  1.3594, -0.4961, -0.8398, -0.7812],
        [-0.0698,  1.4297, -0.6367, -1.0156, -0.2324],
        [ 0.0903,  1.0625, -0.4102, -1.0625, -0.6328],
        [ 0.2168,  1.3516, -0.5039, -0.8164, -0.7695],
        [-0.0723,  1.5547, -0.6758, -0.7617, -0.4336],
        [-0.0087,  1.2812, -0.4941, -0.8398, -0.6172],
        [-0.1562,  1.7500, -0.6836, -0.9258, -0.6680],
        [-0.2217,  0.9766, -0.4551, -0.9609, -0.5273]], device='cuda:0',
       dtype=torch.bfloat16, grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None), SequenceClassifierOutput(loss=tensor(0.6768, device='cuda:0', grad_fn=<NllLossBackward0>), logits=tensor([[ 5.5420e-02,  1.2578e+00, -4.0820e-01, -7.8906e-01, -3.8672e-01],
        [ 2.4170e-02,  1.2734e+00, -6.0938e-01, -9.7266e-01, -5.1953e-01],
        [-2.2168e-01,  1.3594e+00, -6.3672e-01, -6.3281e-01, -3.7891e-01],
        [ 9.2773e-02,  1.1484e+00, -7.2266e-01, -8.3203e-01, -6.3672e-01],
        [-5.7678e-03,  1.0391e+00, -4.1748e-02, -8.3594e-01, -4.1406e-01],
        [-7.4707e-02,  1.2891e+00, -2.2168e-01, -6.6406e-01, -8.9062e-01],
        [ 2.2852e-01,  1.2812e+00, -4.7852e-01, -8.5938e-01, -6.6016e-01],
        [ 2.1973e-01,  1.2109e+00, -3.4570e-01, -1.0703e+00, -7.6562e-01],
        [ 8.2520e-02,  9.6875e-01, -3.9062e-01, -6.7188e-01, -5.2734e-01],
        [ 2.4719e-03,  1.4062e+00, -5.8984e-01, -8.1641e-01, -5.9766e-01],
        [ 3.4570e-01,  1.1875e+00, -6.9141e-01, -7.2656e-01, -5.6641e-01],
        [-1.9629e-01,  1.2969e+00, -2.9883e-01, -8.1250e-01, -8.2031e-01],
        [ 3.3594e-01,  1.5312e+00, -3.9844e-01, -8.7109e-01, -6.2109e-01],
        [-1.5259e-03,  1.4531e+00, -8.4375e-01, -3.3594e-01, -4.4531e-01],
        [ 3.0859e-01,  1.4844e+00, -4.3945e-01, -9.3750e-01, -6.3672e-01],
        [ 4.9609e-01,  4.8828e-01, -1.0859e+00, -7.4219e-01,  2.5195e-01]],
       device='cuda:0', dtype=torch.bfloat16, grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None), SequenceClassifierOutput(loss=tensor(0.6608, device='cuda:0', grad_fn=<NllLossBackward0>), logits=tensor([[ 0.3359,  1.3750, -0.5820, -1.0000, -0.8281],
        [-0.0317,  1.7031, -0.2656, -0.4746, -0.6094],
        [-0.0608,  1.4062, -0.6055, -0.9609, -0.3223],
        [-0.0527,  1.5078, -0.4316, -1.1484, -0.6992],
        [ 0.1797,  1.3906, -0.4531, -0.6211, -0.5547],
        [ 0.1846,  1.0078, -0.8711, -0.5625, -0.6562],
        [ 0.2559,  1.2656, -0.5430, -0.5273, -0.5195],
        [ 0.1040,  1.2656, -0.5430, -0.8398, -0.5664],
        [ 0.1123,  1.2812, -1.1328, -0.7578, -0.2988],
        [ 0.2422,  1.1484, -0.5625, -0.8633, -0.5078],
        [ 0.0664,  1.5078, -0.4668, -1.0625, -0.9141],
        [ 0.0569,  1.1719, -0.7227, -1.0234, -0.3242],
        [ 0.0435,  1.2969, -0.4453, -0.8945, -0.5156],
        [-0.0053,  1.1953, -0.8438, -0.7930, -0.4043],
        [-0.3945,  1.8125, -0.6445, -0.9766, -0.6719],
        [-0.2480,  1.3516, -0.4766, -0.7969, -0.6758]], device='cuda:0',
       dtype=torch.bfloat16, grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None), SequenceClassifierOutput(loss=tensor(0.7563, device='cuda:0', grad_fn=<NllLossBackward0>), logits=tensor([[ 0.2734,  1.4453, -0.2930, -1.0625, -0.2363],
        [ 0.2285,  1.1953, -0.3516, -0.7188, -0.6797],
        [ 0.0070,  1.7188, -0.7852, -0.9180, -0.5820],
        [-0.0050,  1.4453, -0.6367, -1.0156, -0.4238],
        [ 0.1631,  1.1641, -0.9531, -0.7188, -0.1138],
        [-0.0378,  1.7734, -0.3770, -0.8750, -0.6094],
        [-0.2637,  1.4141, -0.2363, -0.6172, -0.4629],
        [ 0.0048,  1.4141, -0.4727, -0.9531, -0.3633],
        [ 0.1885,  0.8633, -0.7656, -0.7812,  0.4668],
        [-0.0840,  1.2969, -0.4746, -0.9062, -0.8398],
        [ 0.2637,  1.1484, -0.5625, -0.9922, -0.6445],
        [-0.1846,  1.3047, -0.3652, -1.1250, -0.6719],
        [ 0.1396,  1.2188, -0.5430, -0.5742, -0.8164],
        [-0.0630,  1.4062, -0.4688, -0.6328, -0.6211],
        [-0.1699,  1.4141, -0.2490, -1.0781, -0.7812],
        [ 0.0208,  1.4453, -0.2773, -1.0156, -0.3223]], device='cuda:0',
       dtype=torch.bfloat16, grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None), SequenceClassifierOutput(loss=tensor(0.9436, device='cuda:0', grad_fn=<NllLossBackward0>), logits=tensor([[ 0.0439,  1.1953, -0.7188, -1.1875, -0.5625],
        [ 0.1904,  1.1094, -0.7344, -0.9219, -0.6172],
        [-0.2002,  1.5781, -0.4355, -0.7500, -0.6211],
        [ 0.1025,  1.2266, -0.4414, -0.5977, -0.5469],
        [-0.0420,  1.3906, -0.6953, -0.7695, -0.6133],
        [ 0.0840,  1.3438, -0.4121, -0.7969, -0.9297],
        [-0.1406,  1.1953, -0.5547, -0.7500, -0.7109],
        [ 0.1611,  1.3594, -0.5859, -0.8203, -0.7188],
        [-0.1074,  1.8203, -0.6172, -0.8828, -0.5273],
        [ 0.1582,  1.5938, -0.7578, -0.9102, -0.7070],
        [ 0.0420,  1.1953, -0.5625, -1.5000, -0.3711],
        [ 0.1748,  0.8828, -0.3301, -0.4688, -0.6797],
        [-0.1973,  1.3281, -0.5195, -0.3730, -0.5859],
        [-0.0757,  1.5312, -0.5742, -0.7500, -0.8672],
        [-0.1846,  1.0156, -0.7344, -0.4531, -0.1455],
        [-0.0684,  1.2422, -0.7578, -0.7148, -0.5156]], device='cuda:0',
       dtype=torch.bfloat16, grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None), SequenceClassifierOutput(loss=tensor(0.9618, device='cuda:0', grad_fn=<NllLossBackward0>), logits=tensor([[ 0.0996,  1.3828, -0.6602, -0.8711, -0.6055],
        [-0.5234,  1.0859, -0.4590, -0.6836, -0.5859],
        [-0.0342,  1.1875, -0.7422, -0.6602, -0.4766],
        [-0.0659,  1.0156, -0.6602, -0.9141, -0.2285],
        [-0.1377,  1.5469,  0.0635, -0.7969, -0.5195],
        [-0.0645,  1.5078, -0.3008, -0.5430, -0.3711],
        [ 0.0593,  1.6562, -0.4824, -0.8672, -0.5898],
        [ 0.1455,  1.1797, -0.4551, -0.5898, -0.4082],
        [-0.0084,  1.5391, -0.6602, -0.7734, -0.5859],
        [ 0.0815,  1.4453, -0.1748, -0.7188, -0.6602],
        [ 0.1475,  1.0938, -0.7500, -0.7070, -0.3242],
        [ 0.1582,  1.4297, -0.6758, -0.7930, -0.5352],
        [-0.1865,  1.7578, -0.6172, -0.9609, -0.1934],
        [-0.2295,  1.4609, -0.4141, -0.6484, -0.5898],
        [ 0.0894,  1.1328, -0.4570, -0.5039, -0.4961],
        [ 0.1436,  1.4922, -0.3555, -0.7656, -0.6016]], device='cuda:0',
       dtype=torch.bfloat16, grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None), SequenceClassifierOutput(loss=tensor(0.7830, device='cuda:0', grad_fn=<NllLossBackward0>), logits=tensor([[ 0.0977,  1.2891, -0.7852, -0.4863, -0.2324],
        [-0.4219,  1.2578, -0.4199, -0.7891, -0.7461],
        [-0.0062,  1.3516, -0.4414, -0.6953, -0.3398],
        [-0.1279,  1.5078, -0.5117, -0.7852, -0.3926],
        [ 0.1699,  1.6719, -0.3496, -1.0312, -0.6211],
        [ 0.1865,  1.1328, -0.3574, -0.5898, -0.3398],
        [-0.1030,  1.3438, -0.6445, -1.1562, -0.6719],
        [ 0.0284,  1.3594, -0.4941, -0.9492, -0.5508],
        [ 0.0105,  0.9453, -0.4492, -0.8750, -0.7266],
        [-0.0981,  1.2188, -0.5430, -0.7891, -0.5039],
        [-0.1260,  1.3672, -0.3359, -0.9297, -1.0078],
        [ 0.0918,  1.2109, -0.1475, -0.6875, -0.7500],
        [-0.0090,  1.2891, -0.6953, -0.6133, -0.3555],
        [-0.0148,  1.4062, -0.3906, -0.6797, -0.3770],
        [ 0.0527,  1.3047, -0.6484, -0.8320, -0.8828],
        [ 0.2012,  1.5156, -0.7344, -0.8047, -0.5391]], device='cuda:0',
       dtype=torch.bfloat16, grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None), SequenceClassifierOutput(loss=tensor(0.7396, device='cuda:0', grad_fn=<NllLossBackward0>), logits=tensor([[-0.3086,  1.2891, -0.6914, -0.7188, -0.7500],
        [ 0.0708,  1.0234, -0.6680, -1.0000, -0.6172],
        [ 0.0265,  1.5000, -0.7930, -0.7578, -0.6133],
        [-0.1201,  1.4219, -0.4668, -1.2344, -0.4824],
        [ 0.1328,  1.3516, -0.4102, -0.8750, -0.5859],
        [-0.1328,  1.5391, -0.2227, -0.7461, -0.4688],
        [ 0.1216,  1.5312, -0.6406, -0.6719, -0.8164],
        [-0.1113,  1.4531, -0.4121, -0.7070, -0.6406],
        [ 0.0461,  1.1797, -0.5195, -1.1641, -0.6445],
        [-0.0864,  0.8984, -0.5312, -0.8945, -0.9648],
        [ 0.1367,  1.2656, -0.4062, -0.6133, -0.7539],
        [ 0.1934,  1.5547, -0.9297, -1.0078, -0.6328],
        [-0.1050,  1.3828, -0.6797, -0.8984, -0.5273],
        [ 0.1187,  1.2266, -0.6406, -0.8438, -0.6445],
        [ 0.0077,  1.2344, -0.6914, -0.6602, -0.3867],
        [-0.2061,  1.3047, -0.3047, -0.8164, -0.4961]], device='cuda:0',
       dtype=torch.bfloat16, grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None), SequenceClassifierOutput(loss=tensor(0.7874, device='cuda:0', grad_fn=<NllLossBackward0>), logits=tensor([[ 0.2373,  1.5000, -0.6875, -0.9648, -0.4629],
        [-0.1465,  1.2031, -0.2891, -0.8477, -0.4727],
        [-0.1216,  1.5547, -0.5117, -0.8594, -0.7852],
        [-0.2578,  1.5000, -0.5508, -0.6484, -0.6172],
        [-0.0124,  1.4453, -0.3574, -0.6445, -0.6562],
        [-0.2559,  1.4766, -0.5742, -0.7344, -0.2949],
        [-0.0190,  1.2656, -0.7695, -0.8359, -0.5508],
        [-0.0913,  1.3281, -0.3711, -0.7031, -0.6484],
        [-0.0593,  1.3438, -0.3867, -0.6250, -0.8711],
        [ 0.0205,  1.2500, -0.2559, -0.7070, -0.3945],
        [ 0.1011,  1.0156, -0.4121, -0.7969, -0.5820],
        [ 0.2393,  1.4141, -0.4434, -0.7344, -0.4980],
        [-0.2559,  1.3438, -0.0435, -0.6836, -0.5195],
        [ 0.0708,  1.4219, -0.3965, -0.8711, -0.4668],
        [-0.1196,  1.3672, -0.4746, -0.7578, -0.2295],
        [ 0.3359,  1.5547, -0.9766, -1.0000, -0.3477]], device='cuda:0',
       dtype=torch.bfloat16, grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None), SequenceClassifierOutput(loss=tensor(0.9604, device='cuda:0', grad_fn=<NllLossBackward0>), logits=tensor([[ 0.3359,  0.9219, -0.5859, -0.6367, -0.5312],
        [-0.0903,  1.2500, -0.4980, -0.8555, -0.7227],
        [ 0.2305,  1.1484, -1.0703, -0.6289, -0.0933],
        [ 0.1050,  1.3203, -0.5430, -0.9375, -0.3809],
        [ 0.0544,  1.2969, -0.7383, -0.7422, -0.5273],
        [ 0.1406,  1.3125, -0.3848, -0.6094, -0.6211],
        [-0.1270,  1.2266, -0.5586, -1.1641, -0.4844],
        [-0.2207,  1.4375, -0.8047, -0.9570, -0.5430],
        [ 0.1924,  1.3906, -0.5977, -0.5547, -0.5078],
        [ 0.1650,  1.3359, -0.8242, -0.7031, -0.5898],
        [ 0.0732,  1.5000, -0.5859, -0.9453, -0.5078],
        [ 0.0908,  1.4531, -0.4375, -0.9492, -0.8477],
        [-0.2422,  0.9570, -0.5156, -0.6641, -0.2852],
        [ 0.0209,  1.3984, -0.6172, -0.8984, -0.5312],
        [ 0.1309,  1.7734, -0.9219, -1.1172, -0.5273],
        [-0.0327,  1.4922, -0.8320, -0.8555, -0.7930]], device='cuda:0',
       dtype=torch.bfloat16, grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None), SequenceClassifierOutput(loss=tensor(0.7764, device='cuda:0', grad_fn=<NllLossBackward0>), logits=tensor([[ 0.1748,  1.0938, -0.4551, -0.9844, -0.9766],
        [-0.0108,  1.3125, -0.3164, -0.5508, -0.4141],
        [-0.0613,  1.4062, -0.8711, -0.6250, -0.5781],
        [-0.1494,  1.3984, -0.4453, -0.7148, -0.5117],
        [-0.0815,  1.3125, -0.2256, -0.5938, -0.4590],
        [-0.1738,  1.0625, -0.5078, -1.0391, -0.6875],
        [-0.1094,  1.2031, -0.8555, -0.7383, -0.4336],
        [-0.0403,  1.4141, -0.5781, -0.8828, -0.6445],
        [ 0.1250,  1.2656, -0.5859, -0.6484, -0.4219],
        [-0.3750,  1.4219, -0.1826, -0.3262, -0.6250],
        [ 0.0801,  1.2812, -0.3379, -0.9688, -0.7031],
        [-0.0320,  0.9062, -0.2197, -0.6680, -0.8516],
        [-0.1777,  1.1328, -0.5898, -0.8398, -0.4492],
        [-0.1328,  1.3906, -0.2871, -0.7461, -0.5859],
        [ 0.0120,  0.9609, -1.0312, -0.5195, -0.5625],
        [ 0.1963,  1.1094, -0.5664, -0.5469, -0.5078]], device='cuda:0',
       dtype=torch.bfloat16, grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None), SequenceClassifierOutput(loss=tensor(0.9382, device='cuda:0', grad_fn=<NllLossBackward0>), logits=tensor([[ 0.4492,  1.4219, -0.6055, -0.9102, -0.4863],
        [-0.2178,  1.4453, -0.5195, -0.9453, -0.3301],
        [ 0.1758,  1.2500, -0.4277, -1.0391, -0.6641],
        [ 0.0376,  1.6641, -0.5234, -0.7461, -0.8047],
        [-0.0339,  1.5547, -0.4961, -0.5039, -0.4316],
        [ 0.2021,  0.9844, -0.5352, -0.6719, -0.3145],
        [ 0.0276,  1.1719, -0.4297, -0.6406, -0.6641],
        [ 0.0332,  1.6484, -0.6875, -1.0938, -0.4629],
        [-0.3008,  1.1797, -0.3906, -0.8828, -0.6641],
        [-0.1914,  1.3828, -0.3379, -0.6055, -0.6016],
        [-0.1641,  1.7109, -0.5977, -0.9883, -0.6094],
        [ 0.2910,  1.0703, -0.2871, -0.9141, -0.4727],
        [ 0.1006,  1.3594, -0.6406, -1.0078, -0.4629],
        [ 0.0532,  1.3281, -0.2910, -0.7969, -0.5703],
        [ 0.1680,  1.5547, -0.8047, -0.7773, -0.5781],
        [-0.0028,  1.2891, -0.4180, -0.8281, -0.5000]], device='cuda:0',
       dtype=torch.bfloat16, grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None), SequenceClassifierOutput(loss=tensor(0.5566, device='cuda:0', grad_fn=<NllLossBackward0>), logits=tensor([[ 0.1128,  0.7812, -0.4648, -0.5508, -0.5820],
        [ 0.0491,  1.4297, -0.5352, -0.9570, -0.6836],
        [ 0.1387,  1.4688, -0.6445, -0.8984, -0.6641],
        [-0.1069,  1.2969, -0.2832, -0.8633, -0.5234],
        [ 0.1416,  1.2500, -0.3340, -0.5156, -1.2734],
        [-0.1260,  1.6719, -0.7891, -0.7969, -0.4043],
        [-0.0752,  1.1484, -0.5117, -0.8594, -0.6680],
        [ 0.2559,  1.4453, -0.2334, -0.5742, -0.5586],
        [-0.0503,  1.3984, -0.6328, -1.0000, -0.4668],
        [-0.0544,  1.4531, -0.6094, -0.7266, -0.6602],
        [ 0.4980,  1.2266, -0.7422, -0.7461, -0.6094],
        [-0.2793,  1.6016, -0.3809, -0.7812, -0.7383],
        [ 0.0869,  1.5938, -0.6484, -0.7930, -0.3301],
        [-0.0244,  1.5234, -0.5000, -0.9336, -0.5938],
        [-0.2832,  1.4375, -0.7266, -1.1406, -0.5859],
        [ 0.0025,  1.3047, -0.1924, -1.0625, -0.9023]], device='cuda:0',
       dtype=torch.bfloat16, grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None), SequenceClassifierOutput(loss=tensor(0.5858, device='cuda:0', grad_fn=<NllLossBackward0>), logits=tensor([[-0.5859,  1.4844, -0.4219, -0.6758, -0.1357],
        [-0.0889,  1.6484, -0.2910, -0.8750, -0.4277],
        [ 0.0859,  1.3281, -0.2578, -0.5586, -0.5234],
        [ 0.1895,  1.4609, -0.4043, -0.9258, -0.5664],
        [-0.2422,  1.6641, -0.5430, -0.9883, -0.4688],
        [ 0.0101,  1.5391, -0.6289, -0.8398, -0.3262],
        [ 0.0908,  1.5156, -0.5039, -0.6289, -0.6602],
        [ 0.0835,  1.3906, -0.8867, -0.7109, -0.5977],
        [-0.1660,  1.1406, -0.5664, -0.8867, -0.4922],
        [ 0.1377,  1.5312, -0.6289, -0.7695, -0.1729],
        [-0.1562,  1.1406, -0.5039, -0.4473, -0.2617],
        [-0.2412,  1.1406, -0.4727, -0.9375, -0.6875],
        [-0.0654,  1.5312, -0.6445, -0.8633, -0.4609],
        [ 0.0439,  1.4297, -0.8086, -0.8984, -0.4453],
        [-0.1099,  1.5469, -0.4551, -0.7070, -0.6055],
        [ 0.2217,  1.1016, -0.5742, -0.8359, -0.4023]], device='cuda:0',
       dtype=torch.bfloat16, grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None), SequenceClassifierOutput(loss=tensor(0.6995, device='cuda:0', grad_fn=<NllLossBackward0>), logits=tensor([[ 2.1191e-01,  1.5547e+00, -4.7266e-01, -7.9688e-01, -4.5703e-01],
        [ 4.0245e-04,  1.6484e+00, -7.6172e-01, -1.1406e+00, -5.7031e-01],
        [ 2.1191e-01,  1.4766e+00, -3.9453e-01, -7.1094e-01, -8.7891e-01],
        [ 2.1191e-01,  9.9219e-01, -4.1016e-01, -7.2656e-01, -8.9453e-01],
        [ 3.2227e-01,  1.3672e+00, -7.2266e-01, -7.3438e-01, -6.5625e-01],
        [ 2.9688e-01,  1.2031e+00, -5.4297e-01, -8.4766e-01, -3.6914e-01],
        [ 2.9688e-01,  1.5703e+00, -4.7070e-01, -7.9297e-01, -2.5195e-01],
        [-1.6211e-01,  1.4453e+00, -4.5312e-01, -1.0625e+00, -5.7031e-01],
        [-3.1445e-01,  1.5625e+00, -4.9023e-01, -9.8047e-01, -5.0781e-01],
        [-1.2207e-01,  1.4297e+00, -8.5156e-01, -1.0859e+00, -4.0625e-01],
        [-7.3242e-02,  1.6562e+00, -5.7812e-01, -8.2812e-01, -2.3730e-01],
        [ 1.0645e-01,  1.5547e+00, -7.8516e-01, -9.4141e-01, -3.7891e-01],
        [-7.1777e-02,  1.0234e+00, -2.3926e-01, -6.2500e-01, -7.4609e-01],
        [-1.4648e-01,  1.4297e+00, -1.5918e-01, -5.8984e-01, -5.1562e-01],
        [ 2.1387e-01,  1.2578e+00, -8.7891e-01, -7.4219e-01, -7.8516e-01],
        [-1.8750e-01,  1.1641e+00, -6.9141e-01, -6.2891e-01, -5.6250e-01]],
       device='cuda:0', dtype=torch.bfloat16, grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None), SequenceClassifierOutput(loss=tensor(0.6125, device='cuda:0', grad_fn=<NllLossBackward0>), logits=tensor([[-0.0486,  1.3125, -0.5781, -0.6445, -0.5234],
        [ 0.0459,  1.4219, -0.6016, -0.6016, -0.4473],
        [-0.0801,  1.2734, -0.3340, -1.1094, -0.3691],
        [ 0.2256,  1.1094, -0.6602, -0.5352, -0.3242],
        [-0.2217,  1.2109, -0.3555, -0.6953, -0.5625],
        [-0.2100,  1.1172, -0.3281, -0.5625, -0.6055],
        [ 0.0947,  1.1094, -0.2314, -1.0156, -0.9102],
        [-0.4902,  1.4766, -0.4316, -0.7891, -0.6211],
        [ 0.3594,  1.2344, -0.2559, -0.9766, -0.5078],
        [ 0.0500,  1.3359, -0.3496, -0.8359, -0.5352],
        [ 0.2168,  1.8203, -0.2471, -0.8477, -0.4961],
        [ 0.0898,  1.2656, -0.3496, -0.9062, -0.5820],
        [ 0.0737,  1.4844, -0.4863, -0.5273, -0.7930],
        [-0.3359,  1.3438, -0.7070, -0.7969, -0.5977],
        [ 0.1680,  1.8047, -0.5117, -0.9062, -0.6094],
        [-0.0282,  1.6484, -0.5430, -1.0781, -0.5117]], device='cuda:0',
       dtype=torch.bfloat16, grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None), SequenceClassifierOutput(loss=tensor(1.1055, device='cuda:0', grad_fn=<NllLossBackward0>), logits=tensor([[-0.0496,  1.3828, -0.7070, -1.0547, -0.6797],
        [ 0.1074,  0.7344, -0.7734, -0.6250, -0.3770],
        [ 0.0261,  1.5469, -0.7812, -0.7383, -0.6289],
        [-0.4082,  1.6406, -0.5430, -0.7500, -0.4121],
        [ 0.0162,  1.7031, -0.4609, -0.7539, -0.4902],
        [ 0.0718,  1.0312, -0.1758, -0.5898, -0.7656],
        [ 0.3652,  0.5352, -0.6055, -0.8281, -0.1240],
        [ 0.0054,  1.2109, -0.0859, -0.6953, -0.2930],
        [-0.0947,  1.3906, -0.4609, -0.6016, -0.6367],
        [-0.0444,  1.3359, -0.0977, -0.6445, -0.5039],
        [-0.0220,  1.4688, -0.5352, -0.9219, -0.6172],
        [ 0.0173,  1.5859, -0.5391, -0.7969, -0.5703],
        [ 0.2656,  1.6328, -0.5781, -0.8633, -0.6836],
        [ 0.2383,  1.6641, -0.3242, -1.0078, -0.5234],
        [-0.0840,  1.4766, -0.2734, -0.4180, -0.6133],
        [ 0.1553,  1.2422, -0.3594, -0.7188, -0.5742]], device='cuda:0',
       dtype=torch.bfloat16, grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None), SequenceClassifierOutput(loss=tensor(0.9048, device='cuda:0', grad_fn=<NllLossBackward0>), logits=tensor([[-0.0232,  1.3516, -0.6875, -0.6680, -0.4883],
        [-0.0742,  1.2031, -0.3086, -0.9180, -0.7188],
        [-0.3633,  1.4609, -0.6367, -0.8125, -0.5156],
        [-0.1416,  0.9375, -0.2109, -0.4316, -0.7344],
        [-0.0254,  1.0859, -0.2578, -0.9297, -0.7812],
        [ 0.1396,  0.8633, -0.6133, -0.7578, -0.5078],
        [ 0.1040,  1.1953, -0.3750, -0.8789, -0.5703],
        [ 0.1523,  1.3594, -0.2969, -0.7695, -0.6055],
        [ 0.2100,  1.7891, -0.4766, -1.0156, -0.5664],
        [ 0.3184,  1.1484, -0.6094, -0.9062, -0.7305],
        [-0.1455,  1.6719, -0.5781, -0.8633, -0.5234],
        [-0.2090,  1.1094, -0.5859, -0.7695, -0.6289],
        [-0.2031,  1.4141, -0.6094, -0.9688, -0.3730],
        [-0.0669,  1.6562, -0.6211, -0.8320, -0.5820],
        [-0.1426,  1.5312, -0.2793, -0.7383, -0.4551],
        [ 0.0713,  1.4766, -0.6172, -0.7617, -0.4141]], device='cuda:0',
       dtype=torch.bfloat16, grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None), SequenceClassifierOutput(loss=tensor(0.9904, device='cuda:0', grad_fn=<NllLossBackward0>), logits=tensor([[-0.0688,  1.2891, -0.4355, -0.5000, -0.2793],
        [ 0.1240,  1.1797, -0.1138, -0.9453, -0.6836],
        [-0.1621,  0.9609, -0.3633, -0.7578, -1.0391],
        [-0.2295,  1.4453, -0.5078, -0.5820, -0.5430],
        [ 0.2373,  1.2500, -0.7500, -0.8516, -0.5312],
        [ 0.1377,  1.4531, -0.5898, -1.1484, -0.6172],
        [ 0.2119,  0.9648, -0.2969, -0.5430, -0.6406],
        [ 0.3730,  1.5234, -0.5352, -0.6992, -0.7383],
        [-0.2949,  1.0859, -0.5508, -0.6602, -0.5195],
        [ 0.2500,  1.1641, -0.5938, -0.5469, -0.3984],
        [-0.1826,  1.1484, -0.5586, -0.8125, -0.6875],
        [-0.1089,  1.5469, -0.6445, -1.0156, -0.3516],
        [-0.2637,  1.1328, -0.7344, -0.3711, -0.4668],
        [ 0.3477,  1.5938, -0.5820, -0.9961, -0.2402],
        [-0.0410,  1.2812, -0.7344, -1.1094, -0.3887],
        [-0.1650,  1.5547, -0.6250, -0.7266, -0.2969]], device='cuda:0',
       dtype=torch.bfloat16, grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None), SequenceClassifierOutput(loss=tensor(0.8693, device='cuda:0', grad_fn=<NllLossBackward0>), logits=tensor([[-6.2988e-02,  1.0391e+00, -3.6328e-01, -6.9922e-01, -4.3555e-01],
        [-1.5137e-01,  1.3359e+00, -6.2891e-01, -1.0703e+00, -6.2109e-01],
        [-1.6602e-01,  1.1250e+00, -2.3828e-01, -5.6250e-01, -9.4531e-01],
        [ 2.1387e-01,  1.2891e+00, -1.0000e+00, -9.9609e-01, -5.5469e-01],
        [ 7.3047e-01,  3.6914e-01, -9.0625e-01, -1.1182e-01,  5.7031e-01],
        [ 2.5977e-01,  1.0625e+00, -7.3828e-01, -5.6250e-01, -6.6016e-01],
        [ 1.6016e-01,  1.5781e+00, -4.2969e-01, -1.2109e+00, -6.0938e-01],
        [-1.9287e-02,  1.1250e+00, -3.4375e-01, -7.1484e-01, -4.0820e-01],
        [-1.2256e-01,  1.4219e+00, -1.5527e-01, -7.4609e-01, -1.0391e+00],
        [ 5.9509e-03,  1.2734e+00, -6.6406e-01, -5.7812e-01, -5.1562e-01],
        [-6.8359e-02,  1.2969e+00, -4.7070e-01, -9.8438e-01, -5.7031e-01],
        [-4.5410e-02,  1.5234e+00, -4.0234e-01, -9.1406e-01, -3.9648e-01],
        [ 2.4805e-01,  1.2188e+00, -6.9922e-01, -1.0781e+00, -5.9375e-01],
        [-1.7188e-01,  1.1328e+00, -7.5391e-01, -1.0078e+00, -5.7812e-01],
        [-3.6001e-05,  1.2656e+00, -7.1094e-01, -8.7109e-01, -7.8906e-01],
        [ 6.2256e-02,  1.4766e+00, -5.1172e-01, -1.0234e+00, -7.4609e-01]],
       device='cuda:0', dtype=torch.bfloat16, grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None), SequenceClassifierOutput(loss=tensor(0.6647, device='cuda:0', grad_fn=<NllLossBackward0>), logits=tensor([[ 0.0332,  1.1875, -0.0525, -0.6719, -0.5273],
        [ 0.0605,  1.6719, -0.5977, -1.1953, -0.5742],
        [ 0.1377,  1.4766, -0.6250, -1.1719, -0.7617],
        [-0.0776,  1.2109, -0.6367, -0.8828, -0.5898],
        [-0.2490,  1.5703, -0.6133, -0.4512, -0.3438],
        [ 0.1074,  1.2188, -0.8242, -0.8320, -0.2656],
        [-0.0066,  1.1250, -0.5078, -0.8203, -0.4648],
        [-0.0289,  1.4453, -0.5781, -0.7852, -0.4688],
        [-0.1973,  1.4609, -0.2207, -0.8164, -0.5859],
        [-0.0084,  1.1953, -0.6328, -0.6758, -0.4805],
        [-0.1118,  1.7031, -0.1235, -1.0156, -0.4258],
        [-0.0791,  1.4766, -0.3984, -0.8125, -0.7969],
        [-0.2852,  1.3750, -0.8281, -0.7227, -0.4414],
        [ 0.3262,  1.5547, -0.4043, -0.9727, -0.8008],
        [ 0.0742,  1.6172, -0.7422, -0.5469, -0.2969],
        [ 0.1196,  1.2734, -0.6211, -1.2031, -0.6758]], device='cuda:0',
       dtype=torch.bfloat16, grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None), SequenceClassifierOutput(loss=tensor(0.9436, device='cuda:0', grad_fn=<NllLossBackward0>), logits=tensor([[-0.3574,  1.2109, -0.6328, -0.8086, -0.5586],
        [-0.0410,  0.7656, -0.7305, -0.9492, -0.7305],
        [ 0.2715,  1.5312, -0.6328, -1.1797, -0.4355],
        [-0.1187,  1.4375, -0.5898, -0.9336, -0.4785],
        [-0.1836,  1.5234, -0.3164, -0.7812, -0.2168],
        [ 0.1196,  1.3047, -0.0854, -0.4961, -0.9883],
        [-0.0037,  1.6328, -0.6523, -0.8945, -0.5000],
        [ 0.3652,  1.1797, -0.4609, -0.5430, -0.4395],
        [-0.1865,  1.6094, -0.3555, -0.5547, -0.1895],
        [-0.1396,  1.9297, -0.4883, -1.2266, -0.4082],
        [ 0.0776,  1.3516, -0.4863, -0.6328, -0.3613],
        [-0.2754,  1.2031, -0.5117, -0.8359, -0.4922],
        [ 0.0216,  1.1172, -0.5547, -1.0000, -0.8047],
        [ 0.0840,  1.2500, -0.4121, -0.8828, -0.5547],
        [ 0.3359,  0.8047, -0.9922, -1.0938,  0.0215],
        [-0.1426,  1.0312, -0.4746, -0.7500, -0.9258]], device='cuda:0',
       dtype=torch.bfloat16, grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None), SequenceClassifierOutput(loss=tensor(0.6620, device='cuda:0', grad_fn=<NllLossBackward0>), logits=tensor([[ 0.1885,  1.2969, -0.5586, -0.8203, -0.2695],
        [-0.0065,  1.3359, -0.5195, -0.7344, -0.4746],
        [ 0.2812,  1.1406, -0.8398, -0.8438, -0.3359],
        [-0.0596,  1.9453, -0.7383, -0.9961, -0.5820],
        [ 0.0381,  1.4688, -0.6445, -1.1719, -0.6797],
        [ 0.3730,  1.3203, -0.5703, -0.8711, -0.5898],
        [-0.1758,  1.7188, -0.7188, -0.8359, -0.5312],
        [ 0.0947,  1.5234, -0.2129, -0.6953, -0.8281],
        [-0.1484,  1.5000, -0.3926, -0.9453, -0.7109],
        [-0.1455,  1.4609, -0.4902, -0.5469, -0.3008],
        [ 0.3340,  1.4922, -0.7070, -0.7734, -0.3730],
        [ 0.0781,  1.1641, -0.4727, -0.6094, -0.3945],
        [-0.2480,  1.1016, -0.6289, -0.8164, -0.8516],
        [-0.0559,  1.2734, -0.4707, -0.8750, -0.4570],
        [ 0.0101,  1.6484, -0.5742, -0.9180, -0.7148],
        [ 0.1475,  1.7812, -0.6719, -0.5898, -0.5195]], device='cuda:0',
       dtype=torch.bfloat16, grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None), SequenceClassifierOutput(loss=tensor(0.7733, device='cuda:0', grad_fn=<NllLossBackward0>), logits=tensor([[-3.1445e-01,  1.6797e+00, -5.5859e-01, -8.7109e-01, -5.5469e-01],
        [ 1.3199e-03,  1.2422e+00, -4.4727e-01, -9.8438e-01, -7.4219e-01],
        [ 2.8320e-01,  1.5000e+00, -5.7422e-01, -8.9062e-01, -5.7812e-01],
        [-2.3145e-01,  1.4297e+00, -4.7070e-01, -6.6016e-01, -5.9375e-01],
        [-1.1865e-01,  1.4062e+00, -6.9922e-01, -9.8047e-01, -7.2656e-01],
        [ 1.7480e-01,  9.9219e-01, -1.0625e+00, -1.1406e+00,  5.4626e-03],
        [ 1.8262e-01,  1.4766e+00, -6.6797e-01, -9.4922e-01, -5.7422e-01],
        [ 1.3184e-01,  1.1250e+00, -4.2383e-01, -1.0859e+00, -6.4062e-01],
        [ 9.5703e-02,  1.4609e+00, -3.6133e-01, -7.5391e-01, -6.9141e-01],
        [ 1.2988e-01,  1.1797e+00, -2.9492e-01, -4.6094e-01, -5.3906e-01],
        [ 4.2480e-02,  1.5234e+00, -3.7695e-01, -5.3125e-01, -4.2383e-01],
        [ 6.0303e-02,  1.3594e+00, -5.5859e-01, -1.0859e+00, -3.2031e-01],
        [ 5.8984e-01,  5.7422e-01, -8.5547e-01, -9.9219e-01,  6.2988e-02],
        [-1.2158e-01,  1.1484e+00, -2.8711e-01, -5.9375e-01, -7.3828e-01],
        [-6.8848e-02,  1.4766e+00, -3.2227e-01, -1.0391e+00, -7.3438e-01],
        [-1.3867e-01,  1.4531e+00, -4.7461e-01, -8.9844e-01, -4.1602e-01]],
       device='cuda:0', dtype=torch.bfloat16, grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None), SequenceClassifierOutput(loss=tensor(0.8130, device='cuda:0', grad_fn=<NllLossBackward0>), logits=tensor([[ 0.1621,  1.5938, -0.7344, -0.6133, -0.4121],
        [-0.1138,  1.4375, -0.1885, -0.8516, -0.5547],
        [-0.1167,  1.6875, -0.4238, -0.9727, -0.7148],
        [ 0.0693,  1.0156, -0.5820, -0.9023, -0.2393],
        [ 0.0125,  1.1875, -0.8984, -0.8438, -0.2910],
        [ 0.1133,  1.4531, -0.4902, -1.0703, -0.4395],
        [ 0.0461,  1.1406, -0.5469, -1.0078, -0.6680],
        [-0.1025,  1.4766, -0.6484, -0.9805, -0.6445],
        [ 0.3789,  1.4609, -0.2334, -0.7031, -0.5664],
        [ 0.2432,  1.5000, -0.8750, -0.8750, -0.6719],
        [-0.0137,  1.4453, -0.7070, -0.9492, -0.2236],
        [ 0.1245,  1.2422, -0.2910, -0.9492, -0.4668],
        [ 0.0757,  1.1641, -0.6172, -0.6133, -0.6875],
        [ 0.3027,  1.4062, -0.3730, -0.5586, -0.3926],
        [-0.1748,  1.4688, -0.5938, -0.7578, -0.3750],
        [ 0.2656,  1.1875, -0.6367, -0.9453, -0.3867]], device='cuda:0',
       dtype=torch.bfloat16, grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None), SequenceClassifierOutput(loss=tensor(1.0841, device='cuda:0', grad_fn=<NllLossBackward0>), logits=tensor([[-0.2471,  1.0156, -0.6055, -0.8516, -0.7148],
        [ 0.0386,  1.8047, -0.9844, -0.9141, -0.6211],
        [ 0.0299,  1.3594, -0.6562, -1.1328, -0.4922],
        [-0.1836,  1.3438, -0.5469, -0.7070, -0.5273],
        [ 0.0366,  1.2422, -0.2695, -0.9375, -0.3906],
        [-0.1699,  1.5391, -0.7500, -0.5352, -0.5273],
        [-0.0271,  1.6797, -0.5039, -0.8320, -0.7578],
        [ 0.3184,  1.1797, -0.0859, -0.8789, -0.3789],
        [ 0.4980,  1.4609, -0.4668, -0.8164, -0.5430],
        [-0.3164,  1.4062, -0.3398, -0.8203, -0.6406],
        [ 0.2432,  1.3984, -0.6250, -1.0547, -0.4238],
        [ 0.0981,  1.2188, -0.4805, -0.8281, -0.4434],
        [-0.0034,  1.5625, -0.9258, -0.9492, -0.6133],
        [ 0.2969,  1.1406, -0.2500, -0.6875, -0.5352],
        [ 0.0315,  1.0391, -0.5938, -0.3359, -0.3750],
        [ 0.0791,  1.3672, -0.3652, -0.7656, -0.3867]], device='cuda:0',
       dtype=torch.bfloat16, grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None), SequenceClassifierOutput(loss=tensor(0.6379, device='cuda:0', grad_fn=<NllLossBackward0>), logits=tensor([[ 0.7188,  1.1484, -0.5195, -0.6211, -0.5195],
        [ 0.2021,  1.4531, -0.5156, -1.0234, -0.7305],
        [-0.2871,  1.5391, -0.5859, -0.8750, -0.5391],
        [-0.2090,  1.4141, -0.7227, -1.1875, -0.6562],
        [ 0.0796,  1.1719, -0.5547, -0.3945, -0.3887],
        [ 0.2871,  1.1016, -0.7070, -0.9883, -0.4023],
        [ 0.1924,  1.5000, -0.4414, -0.6289, -0.3750],
        [ 0.2021,  1.3750, -0.1177, -0.8828, -0.5078],
        [ 0.0481,  1.5938, -0.6602, -0.7500, -0.6719],
        [ 0.1904,  1.5234, -0.5469, -0.6641, -0.2832],
        [ 0.1514,  1.5547, -0.5469, -0.7773, -0.5898],
        [ 0.1147,  1.4531, -0.6680, -0.9805, -0.3145],
        [-0.2119,  1.3203, -0.5352, -0.9375, -0.9023],
        [ 0.2246,  1.6719, -0.6445, -0.9961, -0.3574],
        [ 0.1045,  1.3047, -0.4434, -1.0703, -0.1826],
        [ 0.2109,  0.5625, -0.7969, -0.1680,  0.7969]], device='cuda:0',
       dtype=torch.bfloat16, grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None), SequenceClassifierOutput(loss=tensor(0.8839, device='cuda:0', grad_fn=<NllLossBackward0>), logits=tensor([[ 0.1318,  1.6016, -0.3516, -0.5742, -0.4355],
        [ 0.2812,  1.7578, -0.6875, -0.9453, -0.7266],
        [-0.3105,  1.1953, -0.4141, -0.7617, -0.3496],
        [ 0.0654,  1.4688, -0.7539, -0.8867, -0.5625],
        [ 0.0171,  1.4141, -0.4258, -0.7578, -0.7930],
        [ 0.1216,  1.3750, -0.2344, -0.8125, -0.8516],
        [ 0.0986,  1.4609, -0.6992, -0.7656, -0.1865],
        [-0.0559,  1.4219, -0.5156, -0.8711, -0.6250],
        [ 0.2090,  1.5703, -0.3379, -0.7773, -0.5977],
        [ 0.1514,  0.7656, -0.5195, -0.6367, -0.4824],
        [-0.0713,  1.3594, -0.3652, -0.6133, -0.6367],
        [-0.0786,  1.1719, -0.3984, -1.1016, -0.6641],
        [ 0.1270,  1.2266, -0.4473, -1.1016, -0.6602],
        [-0.4590,  1.3359, -0.9180, -1.2109, -0.2539],
        [ 0.0908,  1.2344, -0.5781, -0.7461, -0.4043],
        [-0.1934,  1.5469, -0.4453, -0.9609, -0.3652]], device='cuda:0',
       dtype=torch.bfloat16, grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None), SequenceClassifierOutput(loss=tensor(0.7047, device='cuda:0', grad_fn=<NllLossBackward0>), logits=tensor([[ 0.4785,  1.0469, -1.0625, -0.7383, -0.0596],
        [-0.1299,  1.5234, -0.4590, -0.8984, -0.2002],
        [-0.2676,  1.3750, -0.5625, -0.8594, -0.4238],
        [ 0.0376,  1.1562, -0.4707, -1.0156, -0.6484],
        [ 0.1226,  1.3047, -1.1172, -0.9805, -0.5742],
        [-0.0845,  1.4922, -0.6094, -1.2109, -0.3711],
        [ 0.1963,  1.1719, -0.4238, -0.6992, -0.6094],
        [-0.0294,  1.1797, -0.5391, -0.9375, -0.4922],
        [-0.0645,  1.3281, -0.8828, -0.9453, -0.6875],
        [-0.1475,  1.0625, -0.4277, -0.9062, -0.9531],
        [ 0.1455,  1.1797, -0.4902, -0.6562, -0.4961],
        [-0.1030,  1.5156, -0.7891, -0.6836, -0.5352],
        [-0.0027,  1.4922, -0.4316, -1.1406, -0.6055],
        [-0.1689,  1.1172, -0.4941, -0.6836, -0.7305],
        [-0.0261,  1.3750, -0.5234, -1.1484, -0.5469],
        [ 0.2734,  1.4062, -0.5820, -0.5703, -0.5078]], device='cuda:0',
       dtype=torch.bfloat16, grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None), SequenceClassifierOutput(loss=tensor(0.7753, device='cuda:0', grad_fn=<NllLossBackward0>), logits=tensor([[-0.1641,  1.5312, -0.7578, -0.9922, -0.5703],
        [ 0.0530,  0.8789, -0.5781, -0.8359, -0.5156],
        [ 0.0762,  1.6719, -0.7305, -1.0078, -0.4395],
        [ 0.0151,  1.3906, -0.6641, -1.1172, -0.6797],
        [-0.1904,  1.3203, -0.9648, -0.8008, -0.5742],
        [ 0.0476,  1.3984, -0.3926, -0.5547, -0.5820],
        [-0.4043,  1.3203, -0.7695, -0.8398, -0.9727],
        [ 0.3438,  1.4219, -0.4473, -0.8125, -0.5039],
        [ 0.2773,  0.9297, -0.5977, -0.9375, -0.4844],
        [-0.1514,  1.2109, -0.5195, -0.9141, -0.1943],
        [ 0.1494,  1.7031, -0.6250, -0.6172, -0.2715],
        [-0.2578,  1.7891, -0.6094, -0.9648, -0.4004],
        [ 0.0342,  1.2812, -0.6523, -1.1562, -0.6875],
        [ 0.0981,  1.6016, -0.5508, -0.8164, -0.6172],
        [-0.2930,  1.1016, -0.5547, -0.9141, -0.5078],
        [ 0.1016,  1.2500, -0.7617, -0.7227, -0.7148]], device='cuda:0',
       dtype=torch.bfloat16, grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None), SequenceClassifierOutput(loss=tensor(0.7637, device='cuda:0', grad_fn=<NllLossBackward0>), logits=tensor([[-0.2734,  1.4062, -0.8984, -0.7500, -0.5430],
        [ 0.1836,  1.3359, -0.7852, -0.9727, -0.3320],
        [-0.1670,  1.2656, -0.7773, -0.7148, -0.5391],
        [-0.1348,  1.1094, -0.5820, -0.7031, -0.6289],
        [-0.0101,  1.5078, -0.1040, -0.9883, -0.5586],
        [ 0.1650,  1.3828, -0.7617, -1.1484, -0.6797],
        [-0.1582,  1.4922, -0.5977, -1.0000, -0.5703],
        [ 0.2676,  1.2812, -0.4941, -0.9336, -0.6562],
        [ 0.0400,  1.1172, -0.6445, -0.9531, -0.6758],
        [ 0.2637,  1.4141, -0.5586, -0.7305, -1.2109],
        [ 0.0991,  1.6406, -0.3789, -0.9727, -0.3340],
        [-0.0786,  1.1406, -0.3789, -0.9375, -0.7266],
        [ 0.1660,  1.0781, -0.2969, -0.8164, -0.8828],
        [-0.1299,  1.2109, -0.5547, -0.6875, -0.8477],
        [ 0.0229,  1.4688, -0.6641, -0.7969, -0.2676],
        [-0.2334,  1.3750, -0.6133, -0.9570, -0.5039]], device='cuda:0',
       dtype=torch.bfloat16, grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None), SequenceClassifierOutput(loss=tensor(0.8322, device='cuda:0', grad_fn=<NllLossBackward0>), logits=tensor([[ 0.0884,  1.4375, -0.4102, -0.8711, -0.6836],
        [-0.0820,  1.3984, -0.9961, -0.9609, -0.5703],
        [-0.0752,  1.5078, -0.4199, -0.4570, -0.5586],
        [ 0.1084,  1.3281, -0.4844, -0.9258, -0.2285],
        [ 0.2148,  1.7812, -0.5625, -0.8516, -0.5742],
        [ 0.1245,  1.3125, -0.2891, -0.6562, -0.5469],
        [-0.1729,  1.6719, -0.7930, -1.0938, -0.5156],
        [ 0.2461,  1.3438, -0.8086, -1.1094, -0.3164],
        [ 0.1846,  1.3828, -0.3086, -0.6914, -1.0312],
        [ 0.2402,  1.2578, -0.5508, -0.7695, -0.4395],
        [-0.0352,  1.2812, -0.4180, -0.6992, -0.4805],
        [ 0.2148,  1.3359, -0.4316, -1.1875, -0.4512],
        [ 0.2773,  1.3906, -0.3750, -0.7383, -0.6602],
        [-0.1611,  1.2812, -0.5625, -0.9141, -0.4648],
        [-0.0728,  1.2891, -0.7383, -0.5156, -0.7812],
        [ 0.0391,  1.4062, -0.5547, -1.1719, -0.6836]], device='cuda:0',
       dtype=torch.bfloat16, grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None), SequenceClassifierOutput(loss=tensor(0.8108, device='cuda:0', grad_fn=<NllLossBackward0>), logits=tensor([[-0.1406,  1.6094, -0.5547, -0.7305, -0.2656],
        [-0.1416,  1.1875, -0.4375, -1.1328, -0.3027],
        [ 0.1226,  1.3359, -0.5547, -0.8672, -0.6641],
        [-0.0771,  1.6406, -0.6055, -0.5078, -0.6523],
        [-0.0581,  1.2188, -0.2949, -0.9492, -0.2773],
        [ 0.0505,  1.6250, -0.8125, -0.9531, -0.6953],
        [-0.0371,  1.4219, -0.8984, -1.0703, -0.3516],
        [ 0.1406,  1.5469, -0.4922, -1.0078, -0.4746],
        [-0.1719,  1.4297, -0.3066, -1.1484, -0.2285],
        [ 0.1270,  1.2344, -0.4629, -0.9141, -0.7188],
        [ 0.0033,  1.3438, -0.7930, -1.0234, -0.6562],
        [ 0.4473,  1.4375, -0.7891, -0.9492, -0.3887],
        [-0.0698,  1.6797, -0.4785, -0.7695, -0.5156],
        [-0.1328,  1.2969, -0.6367, -0.8477, -0.5859],
        [-0.1650,  1.0547, -0.2363, -0.4219, -0.8398],
        [-0.1318,  1.4609, -0.5508, -0.9141, -0.2539]], device='cuda:0',
       dtype=torch.bfloat16, grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None), SequenceClassifierOutput(loss=tensor(0.7966, device='cuda:0', grad_fn=<NllLossBackward0>), logits=tensor([[-0.0481,  1.4766, -0.5352, -0.6719, -0.5352],
        [-0.1777,  1.5078, -0.6211, -0.9961, -0.7734],
        [-0.0454,  1.2578, -0.3633, -1.1562, -0.8281],
        [ 0.1680,  1.0938, -0.7148, -0.8516, -0.5898],
        [ 0.0630,  1.3125, -0.3613, -0.8711, -0.7070],
        [ 0.3320,  1.3438, -0.8281, -0.9180, -0.5820],
        [-0.0522,  1.2266, -0.6523, -0.8359, -0.6484]], device='cuda:0',
       dtype=torch.bfloat16, grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)], [SequenceClassifierOutput(loss=tensor(2.2373, device='cuda:0', grad_fn=<NllLossBackward0>), logits=tensor([[ 0.1162,  1.5703, -0.6992, -1.0312, -0.5742],
        [ 0.2930,  0.7734, -1.0938, -0.9688,  0.0181],
        [-0.1807,  1.3984, -0.7266, -0.8711, -0.6602],
        [ 0.0127,  1.5156, -0.5469, -0.6875, -0.8320],
        [ 0.0674,  1.0703, -0.6133, -0.7344, -0.2480],
        [-0.3555,  1.1016, -0.5547, -0.5352, -0.3457],
        [ 0.3477,  1.3828, -0.7344, -0.8633, -0.5273],
        [ 0.3340,  1.0625, -1.0234, -0.9844, -0.3496],
        [ 0.0510,  1.4766, -0.4648, -0.5195, -0.5781],
        [ 0.1406,  0.9570, -0.8281, -0.9883, -0.6211],
        [-0.0796,  0.8516, -0.4902, -0.4668, -0.3848],
        [ 0.0928,  1.3594, -0.3633, -0.5586, -0.4805],
        [ 0.2832,  1.3047, -0.6680, -0.9492, -0.3164],
        [-0.1445,  0.3438, -0.9531, -0.8086, -0.2402],
        [-0.0498,  1.2031, -0.6211, -0.5859, -0.8828],
        [ 0.1157,  1.2188, -0.6094, -0.4629, -0.2715]], device='cuda:0',
       dtype=torch.bfloat16, grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None), SequenceClassifierOutput(loss=tensor(2.2871, device='cuda:0', grad_fn=<NllLossBackward0>), logits=tensor([[ 0.1357,  1.5625, -0.6641, -0.6406, -0.4336],
        [-0.0330,  1.2422, -0.7461, -0.7695, -0.4492],
        [ 0.6953,  1.1875, -0.6758, -0.5391, -0.1641],
        [ 0.3848,  0.8789, -0.4922, -0.5977, -0.7109],
        [ 0.0238,  1.3125, -0.6328, -0.5156, -0.4609],
        [ 0.1729,  1.1641, -0.6250, -0.9375, -0.6953],
        [ 0.1089,  1.2969, -0.8008, -0.9062, -0.7266],
        [-0.0354,  0.8633, -0.2559, -0.8906, -0.3340],
        [ 0.3008,  1.3047, -0.5312, -1.1016, -0.5703],
        [ 0.0444,  1.2734, -0.5625, -0.8828, -0.8633],
        [-0.0723,  1.4922, -1.2266, -0.9727, -0.1895],
        [-0.1338,  1.2344, -0.5234, -0.7578, -0.4297],
        [ 0.5352,  0.5664, -0.8867, -1.0234,  0.0918],
        [ 0.0260,  1.2109, -0.8203, -0.8789, -0.5273],
        [ 0.0422,  1.0469,  0.0688, -0.2656, -0.2031],
        [ 0.3398,  1.1875, -0.7500, -0.9180, -0.6914]], device='cuda:0',
       dtype=torch.bfloat16, grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None), SequenceClassifierOutput(loss=tensor(2.0112, device='cuda:0', grad_fn=<NllLossBackward0>), logits=tensor([[ 0.1914,  0.8438, -0.3867, -0.5234, -0.3672],
        [ 0.0884,  0.8516, -0.7461, -0.5703, -0.4375],
        [ 0.1211,  0.7656, -0.5391, -0.7070, -0.4941],
        [-0.0557,  1.4844, -0.4277, -0.8945, -0.6875],
        [-0.0549,  1.1641, -0.7500, -0.5586, -0.0547],
        [ 0.2539,  1.0156, -0.4844, -0.7109, -0.6641],
        [-0.0854,  1.1406, -0.4824, -0.3809, -0.3848],
        [-0.1118,  1.0703, -0.1865, -0.2617, -0.4043],
        [-0.2832,  1.1719, -0.3965, -0.4785, -0.2930],
        [-0.4102,  1.2344, -0.5859, -0.7578, -0.7461],
        [ 0.4355,  1.0859, -0.8945, -0.9023, -0.6562],
        [ 0.1348,  0.9258, -0.8828, -0.5781, -0.3652],
        [ 0.1670,  0.9180, -0.6992, -0.5625, -0.5195],
        [ 0.2012,  1.1484, -1.0312, -0.8789, -0.2100],
        [-0.1094,  1.4297, -0.4551, -0.7812, -0.3418],
        [ 0.1680,  0.9180, -1.0391, -1.0469,  0.0996]], device='cuda:0',
       dtype=torch.bfloat16, grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None), SequenceClassifierOutput(loss=tensor(2.1968, device='cuda:0', grad_fn=<NllLossBackward0>), logits=tensor([[-7.5195e-02,  1.4453e+00, -5.5469e-01, -6.2891e-01, -4.8828e-01],
        [ 2.6611e-02,  1.3047e+00, -6.1328e-01, -9.3750e-01, -4.5703e-01],
        [ 2.3174e-04,  1.1406e+00, -5.5859e-01, -8.9844e-01, -4.8242e-01],
        [ 3.0859e-01,  8.9062e-01, -8.2422e-01, -7.3828e-01, -4.1602e-01],
        [ 7.7209e-03,  1.1875e+00, -5.5859e-01, -9.5703e-01, -4.2383e-01],
        [ 2.5391e-01,  1.3984e+00, -4.4336e-01, -8.1641e-01, -4.0234e-01],
        [-3.1494e-02,  1.4531e+00, -6.8750e-01, -8.5547e-01, -4.1406e-01],
        [ 2.7710e-02,  1.1094e+00, -3.7891e-01, -6.0547e-01, -3.4766e-01],
        [ 1.2354e-01,  1.0000e+00, -7.7734e-01, -9.8047e-01, -1.3770e-01],
        [ 3.7305e-01,  1.0703e+00, -4.0820e-01, -5.1953e-01, -2.4512e-01],
        [ 2.2070e-01,  1.5391e+00, -8.0859e-01, -5.3906e-01, -6.2500e-01],
        [-7.1777e-02,  9.5703e-01, -6.2500e-01, -9.8828e-01, -8.8672e-01],
        [ 3.6621e-02,  1.0625e+00, -9.5312e-01, -6.9531e-01, -6.2891e-01],
        [-1.7090e-01,  1.3984e+00, -9.8438e-01, -8.8672e-01, -2.6367e-01],
        [ 2.6758e-01,  9.1797e-01, -3.9453e-01, -6.8359e-01, -7.8125e-01],
        [ 2.0801e-01,  1.4531e+00, -4.1016e-01, -7.8125e-01, -5.7031e-01]],
       device='cuda:0', dtype=torch.bfloat16, grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None), SequenceClassifierOutput(loss=tensor(2.3999, device='cuda:0', grad_fn=<NllLossBackward0>), logits=tensor([[ 0.1465,  1.5938, -0.7773, -0.9297, -0.4688],
        [ 0.5117,  1.4297, -0.5938, -0.9219,  0.0603],
        [ 0.2324,  1.0469, -0.7109, -0.6484, -0.3691],
        [-0.0262,  1.5469, -0.6719, -0.8828, -0.4531],
        [-0.0192,  1.4141, -0.5859, -0.8945, -0.4492],
        [ 0.3359,  0.2969, -0.6055, -0.1572,  0.3223],
        [ 0.1147,  1.3750, -0.6875, -0.8789, -0.4141],
        [ 0.5078,  0.6562, -0.8359, -0.5977,  0.2051],
        [ 0.2988,  0.8789, -1.0547, -0.8359,  0.2217],
        [-0.0613,  1.2969, -0.5156, -0.6758, -0.5859],
        [ 0.0160,  1.3594, -0.4082, -0.8438, -0.6914],
        [-0.1201,  0.8477, -0.5117, -0.6914, -0.5742],
        [-0.1191,  1.5234, -0.6094, -0.8281, -0.5469],
        [ 0.1387,  1.3125, -0.5430, -0.6133, -0.4043],
        [ 0.3223,  1.1406, -1.0469, -0.8984, -0.1807],
        [ 0.2578,  1.1406, -0.5117, -0.5859, -0.4395]], device='cuda:0',
       dtype=torch.bfloat16, grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None), SequenceClassifierOutput(loss=tensor(1.3702, device='cuda:0', grad_fn=<NllLossBackward0>), logits=tensor([[ 0.2256,  0.7070, -0.7109, -0.6055, -0.4863],
        [-0.0231,  1.5625, -0.5938, -0.3105, -0.3008],
        [ 0.0742,  1.6172, -0.5938, -0.8711, -0.4043],
        [ 0.0095,  1.2188, -0.4551, -0.8359, -0.5547],
        [-0.0593,  1.5703, -0.5820, -0.6719, -0.4375],
        [ 0.2695,  1.1953, -0.5156, -0.7695, -0.5625],
        [ 0.1113,  1.2891, -0.1387, -0.6406, -0.7617],
        [-0.0742,  1.4141, -0.2197, -1.0391, -0.8008],
        [ 0.0322,  1.5469, -0.2119, -0.7070, -0.8398],
        [-0.0835,  1.1094, -0.2695, -0.8945, -0.6016],
        [ 0.2578,  1.6797, -0.7812, -0.9883, -0.5859],
        [-0.3125,  1.1328, -0.4160, -1.0469, -0.8281],
        [ 0.1660,  1.7344, -0.7148, -1.0312, -0.3789],
        [-0.2637,  1.2266, -0.3789, -0.9102, -1.0078],
        [-0.1768,  1.4766, -0.4141, -0.8281, -0.7734],
        [ 0.0576,  1.2812, -0.5938, -0.7500, -0.9727]], device='cuda:0',
       dtype=torch.bfloat16, grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None), SequenceClassifierOutput(loss=tensor(1.0680, device='cuda:0', grad_fn=<NllLossBackward0>), logits=tensor([[ 0.2734,  1.3516, -0.7656, -0.7891, -0.6445],
        [-0.0737,  1.0469, -0.5352, -0.9375, -0.9336],
        [ 0.0918,  0.9375, -0.3027, -0.8711, -0.9141],
        [ 0.0664,  1.4766, -0.4941, -0.8984, -0.7930],
        [ 0.0439,  1.5469, -0.5000, -0.9102, -0.6133],
        [-0.1865,  1.3750, -0.2949, -0.5195, -0.8359],
        [-0.1108,  1.2422, -0.5000, -0.6953, -0.5391],
        [-0.2832,  1.3750, -0.4961, -0.8711, -0.8008],
        [ 0.1030,  1.1953, -0.1797, -0.4766, -0.6172],
        [ 0.0481,  1.1875, -0.3809, -0.9414, -0.5977],
        [ 0.1621,  1.1875, -0.2656, -0.8047, -0.7852],
        [-0.4414,  0.9844, -0.6172, -1.0469, -0.7344],
        [-0.0557,  1.0156, -0.6094, -0.9961, -0.8633],
        [ 0.0977,  1.4453, -0.3281, -0.8555, -0.8047],
        [-0.2275,  1.2422, -0.7266, -1.1172, -0.5273],
        [-0.0265,  1.2109, -0.4180, -0.4082, -0.6445]], device='cuda:0',
       dtype=torch.bfloat16, grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None), SequenceClassifierOutput(loss=tensor(0.6207, device='cuda:0', grad_fn=<NllLossBackward0>), logits=tensor([[ 1.6309e-01,  1.4453e+00, -3.5938e-01, -5.2734e-01, -5.7422e-01],
        [ 6.1417e-04,  1.2266e+00, -3.7891e-01, -1.1797e+00, -9.6484e-01],
        [-6.1951e-03,  1.1328e+00, -2.3926e-01, -8.5938e-01, -9.0625e-01],
        [ 1.4453e-01,  1.4922e+00, -6.0547e-01, -6.0938e-01, -8.3203e-01],
        [-2.8687e-02,  1.3594e+00, -1.6895e-01, -5.2734e-01, -3.1445e-01],
        [ 2.3730e-01,  1.3984e+00, -4.0820e-01, -8.0469e-01, -5.5469e-01],
        [ 1.2500e-01,  1.2188e+00, -3.7109e-01, -8.3984e-01, -5.5469e-01],
        [-1.7456e-02,  9.7656e-01, -3.3594e-01, -9.3359e-01, -3.8867e-01],
        [-1.1621e-01,  1.5547e+00, -2.7539e-01, -8.4375e-01, -8.4766e-01],
        [ 1.6797e-01,  1.5234e+00, -1.3672e-01, -7.1875e-01, -7.9297e-01],
        [ 2.2754e-01,  1.2578e+00, -4.8828e-01, -1.0000e+00, -6.3281e-01],
        [ 4.6387e-02,  1.1328e+00, -4.4727e-01, -7.7344e-01, -6.0156e-01],
        [ 8.1543e-02,  1.1953e+00, -4.2773e-01, -7.8125e-01, -8.5156e-01],
        [ 8.8501e-03,  1.1953e+00, -5.1562e-01, -1.0312e+00, -7.9688e-01],
        [-3.6621e-02,  1.2266e+00, -5.9375e-01, -1.0234e+00, -7.1484e-01],
        [-2.1387e-01,  1.4531e+00, -5.0781e-01, -7.5000e-01, -8.4375e-01]],
       device='cuda:0', dtype=torch.bfloat16, grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None), SequenceClassifierOutput(loss=tensor(0.7323, device='cuda:0', grad_fn=<NllLossBackward0>), logits=tensor([[-0.2090,  1.2578, -0.4375, -0.8711, -0.8711],
        [ 0.0293,  1.6797, -0.1021, -1.0547, -0.7500],
        [-0.0664,  1.5312, -0.2031, -0.7930, -0.7891],
        [ 0.3789,  1.3047, -0.3750, -0.8828, -0.4922],
        [ 0.2949,  0.8633, -0.4121, -0.8789, -0.6445],
        [ 0.0850,  1.3203, -0.1172, -0.8047, -1.0391],
        [ 0.0309,  1.4141, -0.0723, -0.6367, -0.9023],
        [-0.0913,  1.3203, -0.3594, -1.0156, -0.7852],
        [-0.0991,  1.1719, -0.1357, -0.9023, -0.5898],
        [ 0.1147,  1.2031, -0.4375, -0.8828, -0.6289],
        [-0.3203,  1.3672, -0.5078, -0.9141, -0.5117],
        [ 0.4043,  1.1328, -0.3906, -0.6367, -0.7695],
        [-0.1299,  1.0469, -0.4277, -0.6562, -0.3867],
        [-0.0106,  1.1484, -0.4297, -0.7617, -0.4883],
        [ 0.1191,  0.8906, -0.6016, -0.9961, -0.2871],
        [ 0.0952,  1.2734, -1.0312, -1.0312, -0.5000]], device='cuda:0',
       dtype=torch.bfloat16, grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None), SequenceClassifierOutput(loss=tensor(0.7472, device='cuda:0', grad_fn=<NllLossBackward0>), logits=tensor([[-0.1475,  1.2422, -0.4355, -0.7461, -0.8008],
        [-0.0151,  1.2344, -0.3887, -0.6445, -0.7070],
        [ 0.2520,  1.0234, -0.6289, -0.7461, -0.5586],
        [ 0.0618,  1.2734, -0.6367, -0.8438, -0.4668],
        [ 0.2451,  1.1953, -0.5117, -1.1328, -0.5117],
        [ 0.0315,  1.1953, -0.3555, -0.6914, -0.8438],
        [ 0.1797,  1.4453, -0.7227, -0.9531, -0.5078],
        [-0.2461,  1.5625, -0.8398, -0.7266, -0.7461],
        [ 0.0398,  0.9883, -0.5234, -0.6445, -0.9336],
        [-0.0366,  1.3984, -0.3594, -0.8867, -0.7734],
        [ 0.0962,  1.2031, -0.2988, -0.9805, -0.7500],
        [-0.2773,  1.1250, -0.0393, -0.5273, -0.6719],
        [ 0.1631,  1.0625, -0.4551, -1.1172, -1.0234],
        [-0.0518,  1.4922, -0.4141, -0.9805, -0.3184],
        [ 0.0564,  1.1719, -0.8828, -1.0156, -0.6797],
        [-0.0679,  1.2500, -0.5156, -0.7383, -0.6328]], device='cuda:0',
       dtype=torch.bfloat16, grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None), SequenceClassifierOutput(loss=tensor(0.8945, device='cuda:0', grad_fn=<NllLossBackward0>), logits=tensor([[-4.5166e-02,  9.6875e-01, -6.3672e-01, -1.0391e+00, -7.3047e-01],
        [-1.0547e-01,  1.1328e+00, -2.8906e-01, -6.2109e-01, -1.1406e+00],
        [-1.0107e-01,  1.3984e+00, -5.1953e-01, -1.0781e+00, -1.0078e+00],
        [ 3.2471e-02,  1.4297e+00, -3.8281e-01, -1.1328e+00, -9.7266e-01],
        [ 6.8283e-04,  1.4766e+00, -2.3730e-01, -8.0469e-01, -6.5625e-01],
        [-1.3379e-01,  1.4141e+00, -2.2168e-01, -1.0469e+00, -1.1406e+00],
        [-3.0884e-02,  1.0781e+00, -4.8438e-01, -8.2812e-01, -8.4375e-01],
        [ 7.1777e-02,  1.3594e+00, -4.7852e-01, -9.2969e-01, -5.7031e-01],
        [ 1.0986e-01,  9.5312e-01, -7.7734e-01, -9.3359e-01, -3.4766e-01],
        [-2.4609e-01,  1.1875e+00, -2.2266e-01, -8.0078e-01, -6.6797e-01],
        [-3.7354e-02,  1.2656e+00, -2.8320e-01, -9.4531e-01, -4.7656e-01],
        [ 3.1250e-02,  1.2188e+00,  4.8828e-03, -1.1641e+00, -7.5391e-01],
        [-1.6406e-01,  1.2344e+00, -3.5352e-01, -7.1875e-01, -5.7031e-01],
        [-2.2363e-01,  1.4453e+00, -5.4688e-01, -8.7109e-01, -1.0312e+00],
        [ 7.5378e-03,  1.1172e+00, -1.6699e-01, -7.2656e-01, -6.3281e-01],
        [ 7.2266e-02,  1.1875e+00, -8.1641e-01, -1.3828e+00, -5.2734e-01]],
       device='cuda:0', dtype=torch.bfloat16, grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None), SequenceClassifierOutput(loss=tensor(0.9449, device='cuda:0', grad_fn=<NllLossBackward0>), logits=tensor([[-0.0037,  1.2031, -0.8867, -1.0312, -0.6758],
        [ 0.0344,  1.4766, -0.4395, -1.0859, -0.8125],
        [ 0.1416,  1.2734, -0.5586, -1.1562, -0.5430],
        [-0.4102,  1.1250, -0.5859, -0.9648, -0.8633],
        [-0.2598,  1.1641, -0.6289, -1.0547, -0.3789],
        [ 0.1445,  1.3047, -0.1709, -0.8633, -0.9336],
        [ 0.0137,  0.9531, -0.5820, -1.1953, -0.4727],
        [-0.1216,  1.3438, -0.1270, -0.7070, -0.7109],
        [ 0.0708,  1.0938, -0.5195, -0.7227, -0.6875],
        [ 0.0214,  1.4531, -0.2637, -0.7773, -0.8281],
        [-0.2891,  1.3125, -0.0117, -0.8359, -0.8828],
        [ 0.0134,  1.0625, -0.3398, -0.6328, -0.5469],
        [ 0.2695,  1.2656, -0.6172, -0.9766, -0.5039],
        [-0.1089,  1.5000, -0.2236, -0.4941, -0.5430],
        [ 0.2275,  1.4453, -0.4629, -0.8516, -0.8047],
        [ 0.0664,  1.0703, -0.1924, -0.8320, -0.7188]], device='cuda:0',
       dtype=torch.bfloat16, grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None), SequenceClassifierOutput(loss=tensor(0.7823, device='cuda:0', grad_fn=<NllLossBackward0>), logits=tensor([[-0.3145,  0.9375, -0.5156, -0.7891, -0.4590],
        [-0.0286,  1.2188, -0.4004, -0.8828, -0.6641],
        [ 0.1807,  1.0469, -0.2285, -1.0156, -0.8945],
        [-0.0566,  1.3516, -0.3652, -0.6094, -0.4570],
        [-0.0275,  1.4609, -0.3574, -0.6914, -0.5938],
        [-0.1211,  1.5469, -0.4688, -1.0547, -0.6133],
        [ 0.1875,  1.3828, -0.5156, -0.7852, -0.5938],
        [-0.1157,  1.0000, -0.4453, -0.7969, -0.8984],
        [ 0.2930,  1.2578, -0.3965, -0.8789, -0.6289],
        [ 0.3457,  1.1406, -0.4590, -1.0703, -0.4941],
        [-0.2852,  1.2266, -0.5352, -0.9102, -0.5273],
        [-0.0034,  0.7617, -0.6367, -0.9180, -0.2832],
        [-0.1504,  1.1172, -0.1572, -0.8594, -0.7266],
        [-0.0618,  1.2812, -0.5781, -1.2734, -0.5586],
        [-0.3789,  1.2578, -0.2197, -1.0703, -1.1094],
        [-0.0908,  1.5938, -0.1973, -0.8984, -0.8281]], device='cuda:0',
       dtype=torch.bfloat16, grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None), SequenceClassifierOutput(loss=tensor(0.7434, device='cuda:0', grad_fn=<NllLossBackward0>), logits=tensor([[-0.0991,  1.3906, -0.7695, -1.0469, -0.4609],
        [ 0.1123,  1.1172, -0.3770, -0.8594, -0.6172],
        [ 0.0344,  1.3125, -0.3652, -0.5273, -0.7188],
        [-0.3457,  1.2891, -0.4766, -1.0547, -0.7266],
        [ 0.0085,  1.6484, -0.3184, -0.6445, -0.7852],
        [ 0.0850,  1.2344, -0.5508, -0.5508, -0.8711],
        [-0.1406,  1.0625, -0.7539, -0.7539, -0.2910],
        [ 0.1123,  1.0859, -0.6328, -0.8281, -0.5547],
        [ 0.2305,  1.0156, -0.7891, -1.0781, -0.6367],
        [ 0.3418,  1.1719, -0.8047, -0.6328, -0.3164],
        [-0.1807,  1.4766, -0.3281, -1.0469, -0.8047],
        [-0.2061,  1.4766, -0.8867, -0.8008, -0.5312],
        [-0.1206,  1.1250, -0.4531, -0.8633, -0.3301],
        [ 0.4668,  0.8359, -0.6719, -0.5977,  0.3926],
        [-0.2500,  1.4141, -0.5586, -0.9062, -0.6719],
        [-0.2188,  1.1172, -0.4805, -0.6914, -0.5039]], device='cuda:0',
       dtype=torch.bfloat16, grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None), SequenceClassifierOutput(loss=tensor(0.7172, device='cuda:0', grad_fn=<NllLossBackward0>), logits=tensor([[-0.0153,  1.1250, -0.4043, -0.9414, -0.8555],
        [-0.1836,  1.2188, -0.2178, -0.7969, -0.8203],
        [ 0.0305,  1.0781, -0.2695, -0.8047, -0.6875],
        [-0.1758,  1.4375, -0.4883, -1.1562, -0.6328],
        [-0.2871,  1.3594, -0.2930, -0.6328, -0.3145],
        [-0.0693,  1.1328, -0.3594, -0.9180, -0.9219],
        [ 0.0549,  1.1016, -0.2695, -0.7891, -0.7266],
        [-0.1309,  1.1172, -0.1602, -0.9883, -0.8281],
        [ 0.1338,  1.1953, -0.5000, -1.1406, -0.8398],
        [-0.0072,  1.2891, -0.4453, -0.8867, -0.6406],
        [-0.0172,  1.2109, -0.3613, -1.0000, -0.4863],
        [ 0.1504,  1.3750, -0.0981, -0.7266, -0.7773],
        [ 0.1040,  1.2188, -0.7266, -1.1797, -0.8633],
        [-0.0552,  1.0547, -0.1533, -0.9609, -0.3047],
        [-0.1523,  1.2500, -0.4219, -0.8555, -0.9375],
        [ 0.1318,  1.3828, -0.4336, -1.1016, -0.6250]], device='cuda:0',
       dtype=torch.bfloat16, grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None), SequenceClassifierOutput(loss=tensor(0.7584, device='cuda:0', grad_fn=<NllLossBackward0>), logits=tensor([[-0.1631,  1.2344, -0.4355, -0.3848, -0.3594],
        [ 0.0030,  1.4375, -0.5625, -0.6445, -0.2500],
        [-0.2480,  1.1719, -0.8359, -1.0078, -0.7461],
        [-0.1050,  1.0625, -0.2373, -0.4531, -1.0391],
        [ 0.2656,  1.1953, -0.3320, -0.9727, -1.0078],
        [ 0.0364,  0.9844, -0.5742, -0.9727, -0.2295],
        [-0.0245,  0.7188, -0.5039, -0.8398, -1.1328],
        [-0.1006,  1.2891, -0.7812, -0.2988, -0.4980],
        [-0.0869,  1.2031, -0.1455, -0.8828, -0.8984],
        [ 0.3750,  1.5781, -0.7305, -0.9102, -0.7383],
        [ 0.1001,  1.4609, -0.5898, -0.8242, -0.9805],
        [ 0.0110,  1.3516, -0.4883, -1.0078, -0.6562],
        [-0.0449,  1.3984, -0.2871, -0.9180, -0.5352],
        [ 0.0679,  1.2812, -0.2334, -1.0391, -0.8633],
        [-0.2148,  1.1406, -0.5820, -0.7227, -0.5195],
        [-0.2832,  1.2891, -0.3145, -0.6367, -0.7656]], device='cuda:0',
       dtype=torch.bfloat16, grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None), SequenceClassifierOutput(loss=tensor(0.8593, device='cuda:0', grad_fn=<NllLossBackward0>), logits=tensor([[ 0.0205,  1.3594, -0.2871, -0.9062, -0.5586],
        [-0.0260,  1.0781, -0.3398, -0.9766, -0.8750],
        [ 0.0618,  1.4297, -0.3691, -1.0391, -0.6133],
        [ 0.0271,  1.4531, -0.2100, -1.1250, -0.7852],
        [ 0.3672,  1.3594, -0.8359, -1.0312, -0.6367],
        [ 0.7227,  0.9844, -0.4316, -0.8516, -0.9375],
        [-0.1416,  1.2891, -0.5195, -0.7500, -0.6016],
        [ 0.2969,  1.5312, -0.4160, -0.9062, -0.3809],
        [ 0.0530,  1.1562, -0.8750, -0.9531, -0.5312],
        [ 0.1201,  1.3828, -0.1377, -0.9453, -0.9297],
        [ 0.0437,  0.9570, -0.7383, -1.0469, -0.8672],
        [-0.4023,  1.4531, -0.9141, -0.7773, -0.7188],
        [-0.0576,  1.2891, -0.4980, -0.7188, -0.8164],
        [ 0.2041,  1.4141, -0.9492, -1.1328, -0.5117],
        [-0.0403,  1.0859, -0.2051, -0.8438, -0.5547],
        [ 0.4414,  1.3828, -0.8906, -0.7227, -0.4668]], device='cuda:0',
       dtype=torch.bfloat16, grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None), SequenceClassifierOutput(loss=tensor(0.6504, device='cuda:0', grad_fn=<NllLossBackward0>), logits=tensor([[ 0.1035,  1.3672, -0.4883, -0.6875, -0.6641],
        [ 0.0513,  1.1719, -0.3398, -0.8672, -0.9688],
        [-0.0376,  1.2266, -0.4961, -1.1328, -0.6875],
        [-0.2676,  1.0391, -0.3867, -0.8477, -1.0703],
        [ 0.1025,  1.2812, -0.6406, -0.8516, -0.5742],
        [ 0.2539,  1.0938, -0.4648, -1.0859, -0.4336],
        [ 0.1060,  1.0625, -0.4219, -1.0078, -0.2041],
        [-0.0723,  0.9922, -0.5430, -0.4961, -0.4199],
        [-0.2080,  1.3203, -0.3633, -1.0469, -0.8164],
        [ 0.1060,  1.5234, -0.6914, -1.0703, -0.4492],
        [-0.0620,  1.2734, -0.8164, -0.6445, -0.4355],
        [-0.3164,  1.2266, -0.5156, -0.7812, -0.5039],
        [-0.0493,  1.3438, -0.4609, -0.9766, -0.3281],
        [-0.0102,  1.2422, -0.8125, -0.7031, -0.1680],
        [-0.0024,  1.0156, -0.7188, -0.9336, -0.5664],
        [-0.1196,  1.4531, -0.4746, -0.7773, -0.6680]], device='cuda:0',
       dtype=torch.bfloat16, grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None), SequenceClassifierOutput(loss=tensor(0.6995, device='cuda:0', grad_fn=<NllLossBackward0>), logits=tensor([[-0.0222,  1.2812, -0.6602, -0.8984, -0.5703],
        [ 0.2021,  1.1641, -0.8008, -0.9766, -0.5391],
        [-0.1172,  1.2891, -0.2441, -1.0156, -0.6055],
        [ 0.1602,  1.4219, -0.8867, -0.4453, -0.4766],
        [-0.2422,  1.4844, -0.5664, -0.8789, -0.4785],
        [ 0.1582,  1.2031, -0.8828, -0.6641, -0.3516],
        [ 0.1250,  1.0078, -0.2578, -0.7500, -0.6875],
        [-0.0659,  1.6250, -0.8242, -0.8477, -0.4141],
        [-0.0601,  1.2422, -0.4121, -0.7031, -0.5391],
        [ 0.0088,  1.1094, -0.6094, -0.9180, -0.5586],
        [ 0.1309,  1.0156, -0.2119, -0.6914, -0.4062],
        [ 0.1562,  1.3828, -0.6797, -0.8242, -0.3535],
        [-0.3574,  1.5234, -0.5703, -0.8945, -0.3887],
        [-0.0481,  1.3125, -0.6211, -0.8281, -0.6484],
        [ 0.2334,  1.5000, -1.0391, -0.7656, -0.8438],
        [-0.0825,  0.9844, -0.7344, -0.7891, -0.5938]], device='cuda:0',
       dtype=torch.bfloat16, grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None), SequenceClassifierOutput(loss=tensor(0.5192, device='cuda:0', grad_fn=<NllLossBackward0>), logits=tensor([[ 0.1084,  1.2031, -0.3691, -1.0000, -0.9492],
        [ 0.1045,  1.1641, -0.7227, -0.7070, -0.3672],
        [-0.0942,  1.1328, -0.6797, -0.8242, -0.6641],
        [ 0.1079,  1.4141, -0.3711, -1.0938, -0.6953],
        [-0.0791,  1.5859, -0.5391, -0.5859, -0.5352],
        [ 0.2656,  1.5703, -0.4941, -1.0938, -0.4961],
        [ 0.1650,  1.5781, -0.6602, -0.8828, -0.3125],
        [ 0.1113,  1.3359, -0.2451, -0.8242, -0.7148],
        [ 0.0223,  0.8789, -0.7891, -0.8516,  0.2598],
        [ 0.2119,  1.6875, -0.6797, -0.9062, -0.3340],
        [ 0.1943,  1.3828, -0.7422, -0.7461, -0.6836],
        [-0.2812,  1.3047, -0.1484, -0.8750, -0.6797],
        [-0.4277,  1.4609, -0.2539, -0.7969, -0.5078],
        [-0.0850,  1.5469, -0.4883, -1.0000, -0.5781],
        [-0.0160,  1.7344, -0.4238, -0.9297, -0.2354],
        [-0.3008,  1.2422, -0.4277, -0.5352, -0.6016]], device='cuda:0',
       dtype=torch.bfloat16, grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None), SequenceClassifierOutput(loss=tensor(0.9312, device='cuda:0', grad_fn=<NllLossBackward0>), logits=tensor([[ 0.1787,  1.3203, -0.3223, -0.5859, -0.6367],
        [-0.0391,  1.4922, -0.3730, -0.6758, -0.8125],
        [ 0.1934,  1.5859, -0.7852, -0.7305, -0.4395],
        [ 0.1924,  1.0625, -0.2617, -1.0078, -0.9414],
        [ 0.1562,  0.9297, -0.2305, -1.0547, -0.6367],
        [ 0.1030,  1.3438, -0.6016, -0.9766, -0.6016],
        [ 0.1553,  1.1172, -0.5078, -0.7969, -0.5781],
        [ 0.0613,  1.4609, -0.6523, -0.6250, -0.5469],
        [ 0.0601,  1.5234, -0.6328, -1.0234, -0.4043],
        [ 0.0303,  1.5312, -0.6172, -0.8984, -0.7031],
        [ 0.2520,  0.9375, -0.3496, -0.9414, -0.3984],
        [ 0.1748,  1.2734, -0.5391, -0.8672, -0.5391],
        [-0.0520,  1.0781, -0.4531, -0.9219, -0.3574],
        [-0.0378,  1.2266, -0.6211, -0.8242, -0.6211],
        [-0.1953,  1.3672, -0.7070, -0.9570, -0.9414],
        [ 0.0791,  1.5156, -0.5156, -0.8867, -0.3613]], device='cuda:0',
       dtype=torch.bfloat16, grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None), SequenceClassifierOutput(loss=tensor(0.8350, device='cuda:0', grad_fn=<NllLossBackward0>), logits=tensor([[ 0.1123,  1.5859, -0.6641, -0.6875, -0.7031],
        [-0.0378,  1.3828, -0.6953, -0.6484, -0.3281],
        [ 0.1553,  1.2109, -0.4844, -0.8438, -1.0781],
        [-0.0198,  1.1094, -0.4824, -0.7891, -0.4492],
        [ 0.2188,  1.4453, -0.3047, -0.6328, -0.5078],
        [-0.0228,  1.1172, -0.5859, -0.7383, -0.4512],
        [ 0.1641,  1.3359, -0.6680, -0.8047, -0.4043],
        [ 0.3418,  1.4062, -0.4785, -0.5781, -0.7422],
        [-0.2148,  1.7891, -1.0078, -0.8320, -0.5742],
        [ 0.1396,  1.4219, -0.2422, -0.9883, -0.6172],
        [ 0.0388,  1.4141, -0.5078, -0.8789, -0.5625],
        [ 0.1387,  1.2891, -0.8242, -0.7930, -0.5508],
        [ 0.1533,  0.9141, -1.1562, -0.5273,  0.2207],
        [ 0.1104,  1.6172, -0.6875, -0.6719, -0.2461],
        [-0.1914,  1.1719, -0.4746, -0.6836, -0.4766],
        [ 0.0894,  1.6797, -0.5039, -1.1250, -0.5508]], device='cuda:0',
       dtype=torch.bfloat16, grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None), SequenceClassifierOutput(loss=tensor(0.6992, device='cuda:0', grad_fn=<NllLossBackward0>), logits=tensor([[ 1.3867e-01,  1.4453e+00, -5.8203e-01, -1.0312e+00, -4.1992e-01],
        [-2.1289e-01,  1.3359e+00, -2.7930e-01, -6.0938e-01, -6.6406e-01],
        [ 5.2734e-02,  1.2969e+00, -5.2734e-01, -9.2578e-01, -2.8320e-01],
        [ 6.3965e-02,  1.3047e+00, -8.4375e-01, -9.0234e-01, -3.9844e-01],
        [ 1.0742e-01,  1.5938e+00, -5.1562e-01, -1.0312e+00, -9.5312e-01],
        [ 3.1445e-01,  1.4922e+00, -7.4609e-01, -1.0078e+00, -4.8828e-01],
        [-1.0449e-01,  1.3828e+00, -4.9609e-01, -8.3203e-01, -4.9023e-01],
        [-8.7891e-02,  1.4141e+00, -4.4727e-01, -7.8906e-01, -4.5703e-01],
        [ 3.7305e-01,  1.5391e+00, -4.1016e-02, -8.9844e-01, -5.2344e-01],
        [ 6.8848e-02,  1.3516e+00, -6.1719e-01, -1.1484e+00, -5.3906e-01],
        [-2.7100e-02,  1.5156e+00, -4.4922e-01, -7.5391e-01, -5.0000e-01],
        [ 1.4551e-01,  1.5156e+00, -6.3672e-01, -7.6172e-01, -3.8672e-01],
        [-5.2185e-03,  1.3672e+00, -4.5117e-01, -4.9609e-01, -4.8828e-01],
        [ 1.4160e-01,  1.2734e+00, -1.5430e-01, -4.4727e-01, -1.9141e-01],
        [ 2.2949e-01,  1.6875e+00, -7.0703e-01, -7.1484e-01, -5.6250e-01],
        [-9.3079e-04,  1.6016e+00, -3.4961e-01, -6.9531e-01, -6.7969e-01]],
       device='cuda:0', dtype=torch.bfloat16, grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None), SequenceClassifierOutput(loss=tensor(0.4901, device='cuda:0', grad_fn=<NllLossBackward0>), logits=tensor([[ 0.2559,  1.6094, -0.5781, -0.7461, -0.3652],
        [ 0.0659,  1.5625, -0.1357, -1.1172, -0.8125],
        [-0.0204,  1.4297, -0.5078, -0.8398, -0.9023],
        [-0.0408,  1.4453, -0.4688, -0.7148, -1.0625],
        [ 0.1465,  1.6484, -0.7500, -1.0391, -0.5273],
        [ 0.3574,  1.5859, -0.3906, -0.9375, -1.0312],
        [-0.0464,  1.4688, -0.5703, -0.8359, -0.4863],
        [-0.0981,  1.3984, -0.7305, -0.8008, -0.4531],
        [ 0.0031,  1.4766, -0.4297, -0.9922, -0.5156],
        [-0.1758,  1.4219, -0.6562, -0.8125, -0.3184],
        [ 0.1162,  1.3125, -0.6875, -0.7305, -0.7969],
        [ 0.0233,  1.0234, -0.6602, -0.7266, -0.4570],
        [ 0.1387,  1.2891, -0.6094, -0.5938, -0.5742],
        [-0.0884,  1.5625, -0.6133, -1.0625, -0.8008],
        [ 0.0859,  1.3359, -0.4297, -0.9453, -0.7656],
        [ 0.0728,  1.2969, -0.1592, -0.7852, -0.6367]], device='cuda:0',
       dtype=torch.bfloat16, grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None), SequenceClassifierOutput(loss=tensor(0.8625, device='cuda:0', grad_fn=<NllLossBackward0>), logits=tensor([[-2.7734e-01,  1.5156e+00, -2.4707e-01, -7.9297e-01, -1.0469e+00],
        [-2.2656e-01,  1.3516e+00, -6.4062e-01, -6.4844e-01, -4.6094e-01],
        [ 2.8516e-01,  1.0938e+00, -3.9062e-01, -6.2500e-01, -9.8047e-01],
        [-1.2012e-01,  1.2969e+00, -6.9922e-01, -7.8125e-01, -7.3828e-01],
        [ 7.2266e-02,  1.0547e+00, -5.9375e-01, -7.0703e-01, -3.8477e-01],
        [-8.3496e-02,  1.1953e+00, -4.1211e-01, -8.1641e-01, -4.6680e-01],
        [ 1.8164e-01,  1.5391e+00, -8.5547e-01, -1.1328e+00, -1.9824e-01],
        [ 3.8086e-01,  7.0312e-01, -9.2188e-01, -6.3672e-01, -1.5488e-03],
        [ 1.4572e-03,  1.3125e+00, -9.2578e-01, -8.1250e-01, -5.3125e-01],
        [ 4.8340e-02,  1.0078e+00, -8.8281e-01, -6.3281e-01, -4.1406e-01],
        [ 1.7285e-01,  1.4141e+00, -7.5000e-01, -8.9062e-01, -4.4727e-01],
        [ 1.2598e-01,  1.1094e+00, -5.8984e-01, -1.0703e+00, -5.5469e-01],
        [ 1.6016e-01,  1.4062e+00, -7.3047e-01, -9.0625e-01, -4.6094e-01],
        [-6.6895e-02,  1.0625e+00, -6.7188e-01, -5.8594e-01, -3.9062e-01],
        [ 1.2061e-01,  1.4453e+00, -7.4609e-01, -1.0156e+00, -5.4688e-01],
        [-9.6680e-02,  1.0078e+00, -6.2109e-01, -6.4844e-01, -5.8203e-01]],
       device='cuda:0', dtype=torch.bfloat16, grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None), SequenceClassifierOutput(loss=tensor(0.7162, device='cuda:0', grad_fn=<NllLossBackward0>), logits=tensor([[-0.0452,  1.2109, -0.7227, -0.3301, -0.8125],
        [-0.1543,  1.3672, -0.4746, -0.8672, -0.4434],
        [ 0.0918,  1.2578, -0.5039, -0.6797, -0.8125],
        [ 0.4844,  1.0781, -0.7227, -0.7031, -0.5195],
        [ 0.0181,  1.6016, -0.7891, -0.6367, -0.3730],
        [ 0.0236,  1.6484, -0.3613, -0.7969, -0.6289],
        [ 0.0099,  1.3672, -0.8125, -0.8633, -0.4707],
        [ 0.1504,  1.1250, -0.5195, -0.7227, -0.6523],
        [-0.0796,  1.3984, -0.7070, -0.7422, -0.8398],
        [ 0.0850,  1.5312, -0.1641, -0.9688, -0.6055],
        [-0.0366,  1.3438, -0.8516, -0.8750, -0.4668],
        [-0.0452,  1.3281, -0.5703, -0.5430, -0.5898],
        [-0.1104,  1.6562, -0.3789, -0.6289, -0.6133],
        [-0.0679,  1.0859, -0.6836, -0.9062, -0.5781],
        [ 0.1426,  1.3359, -0.6133, -0.7656, -0.6289],
        [ 0.1348,  1.0859, -0.7305, -0.6055, -0.3535]], device='cuda:0',
       dtype=torch.bfloat16, grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None), SequenceClassifierOutput(loss=tensor(0.5541, device='cuda:0', grad_fn=<NllLossBackward0>), logits=tensor([[ 6.6895e-02,  1.1484e+00, -6.9922e-01, -8.3203e-01, -5.6641e-01],
        [ 1.5918e-01,  1.4062e+00, -5.8984e-01, -8.8672e-01, -1.3047e+00],
        [ 1.0205e-01,  1.7422e+00, -7.0312e-01, -7.8516e-01, -6.8359e-01],
        [ 5.1562e-01,  1.4062e+00, -5.3906e-01, -7.8125e-01, -5.7422e-01],
        [ 7.9102e-02,  1.4219e+00, -7.2656e-01, -9.1016e-01, -4.3359e-01],
        [-3.5547e-01,  1.6719e+00, -2.0312e-01, -7.2266e-01, -6.8359e-01],
        [-1.2878e-02,  1.7422e+00, -7.3047e-01, -1.0703e+00, -5.5469e-01],
        [ 5.8838e-02,  9.6094e-01, -6.7969e-01, -1.0156e+00, -7.3438e-01],
        [ 4.8242e-01,  1.4688e+00, -7.9297e-01, -6.6797e-01,  7.1289e-02],
        [ 4.3750e-01,  1.3750e+00, -5.7422e-01, -5.7812e-01, -4.7266e-01],
        [-1.4648e-02,  1.2734e+00, -6.2109e-01, -1.0156e+00, -3.9062e-01],
        [ 2.0996e-01,  1.3438e+00, -4.6680e-01, -9.8438e-01, -6.7578e-01],
        [-1.8997e-03,  1.6484e+00, -7.7344e-01, -8.9844e-01, -5.8594e-01],
        [ 1.9043e-02,  2.0156e+00, -4.6680e-01, -1.2266e+00, -5.1172e-01],
        [ 1.6016e-01,  1.3750e+00, -3.4766e-01, -6.2500e-01, -4.5312e-01],
        [-1.8359e-01,  1.2734e+00, -2.8711e-01, -7.2266e-01, -9.3750e-01]],
       device='cuda:0', dtype=torch.bfloat16, grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None), SequenceClassifierOutput(loss=tensor(0.6323, device='cuda:0', grad_fn=<NllLossBackward0>), logits=tensor([[-0.3086,  0.9688, -0.5273, -0.3652, -0.3633],
        [ 0.1807,  1.6953, -0.6289, -1.0625, -0.6367],
        [-0.2812,  1.5234, -0.7461, -0.7070, -0.7852],
        [ 0.0109,  1.1016, -0.2520, -0.7227, -0.6172],
        [-0.0228,  1.4609, -0.5625, -0.9844, -0.7031],
        [ 0.0664,  1.5312, -0.5430, -1.0469, -0.5156],
        [ 0.0547,  2.0469, -0.4629, -1.2500, -0.2617],
        [-0.0703,  0.9883, -0.2129, -0.6406, -0.5273],
        [-0.3008,  1.4688, -0.5742, -0.7266, -0.6094],
        [-0.0251,  1.3984, -0.1572, -1.0469, -0.2949],
        [ 0.3535,  1.2734, -0.2637, -0.4961, -0.5469],
        [-0.4688,  1.5703, -0.5625, -0.6719, -0.6797],
        [ 0.5586,  0.7812, -1.0312, -0.8008, -0.1187],
        [ 0.1318,  1.6172, -0.4551, -0.8086, -0.6758],
        [-0.2598,  1.5078, -1.0078, -0.9141, -0.4023],
        [-0.2031,  1.3281, -0.4688, -1.3672, -0.9336]], device='cuda:0',
       dtype=torch.bfloat16, grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None), SequenceClassifierOutput(loss=tensor(0.5602, device='cuda:0', grad_fn=<NllLossBackward0>), logits=tensor([[ 0.2354,  1.4062, -0.4453, -0.9375, -0.4160],
        [ 0.0708,  1.1719, -0.5273, -0.5586, -0.2852],
        [ 0.2021,  1.3125, -0.7617, -0.8945, -0.4863],
        [ 0.1084,  1.4219, -0.6758, -0.9414, -0.5977],
        [-0.1289,  1.7266, -0.6289, -0.9219, -0.4707],
        [ 0.0459,  1.6562, -0.5664, -0.9258, -0.4238],
        [-0.1104,  1.5469, -0.4492, -1.0703, -0.4434],
        [ 0.3359,  1.4297, -0.6758, -0.9023, -0.3613],
        [-0.1089,  1.4375, -0.4434, -0.7500, -0.5117],
        [ 0.0376,  1.3359, -0.5391, -0.9531, -0.5508],
        [-0.0491,  1.1875, -0.5547, -1.1641, -0.6016],
        [-0.1094,  1.6172, -0.6875, -0.9844, -0.3770],
        [ 0.0332,  1.6562, -0.6289, -1.1797, -0.4219],
        [ 0.0211,  1.4141, -0.8008, -0.8828, -0.5898],
        [-0.0449,  1.5625, -0.7266, -0.9258, -0.6875],
        [ 0.0231,  1.4141, -0.7578, -1.1719, -0.4160]], device='cuda:0',
       dtype=torch.bfloat16, grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None), SequenceClassifierOutput(loss=tensor(0.8789, device='cuda:0', grad_fn=<NllLossBackward0>), logits=tensor([[ 0.3984,  1.1328, -0.3750, -0.8203, -0.8555],
        [-0.1602,  1.4531, -0.9102, -0.9336, -0.8477],
        [-0.0649,  1.3906, -0.8125, -0.9258, -0.5312],
        [-0.1719,  1.1562,  0.0957, -0.8398, -0.8242],
        [ 0.4727,  1.0312, -0.5977, -1.1016, -0.5039],
        [ 0.0045,  1.3359, -0.1328, -0.6562, -0.7461],
        [-0.0435,  0.9688, -0.7266, -0.9805, -0.4238],
        [-0.0320,  1.3828, -0.5039, -0.6250, -0.8047],
        [ 0.4668,  1.3828, -0.8047, -0.7383, -0.5117],
        [-0.1523,  1.5391, -0.7617, -0.8555, -0.5781],
        [ 0.0703,  1.3359, -0.8711, -0.8438, -0.5703],
        [-0.1030,  1.6719, -0.6914, -0.8711, -0.5898],
        [ 0.3438,  1.5547, -0.6953, -1.0078, -0.5938],
        [ 0.2314,  1.4297, -0.6875, -1.0078, -0.4004],
        [ 0.1050,  0.9922, -0.7344, -0.3496, -0.3965],
        [ 0.2578,  1.2891, -0.7305, -1.0000, -0.3496]], device='cuda:0',
       dtype=torch.bfloat16, grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None), SequenceClassifierOutput(loss=tensor(0.6750, device='cuda:0', grad_fn=<NllLossBackward0>), logits=tensor([[-0.0942,  1.4688, -1.0547, -0.7188, -0.4355],
        [ 0.0518,  1.5156, -0.6133, -1.0391, -0.4590],
        [-0.1631,  1.4297, -0.6484, -0.8008, -0.6484],
        [-0.0757,  1.3750, -0.9375, -0.8633, -0.5195],
        [ 0.1128,  1.6250, -0.6562, -0.6680, -0.5938],
        [ 0.0212,  1.8828, -0.7930, -0.7266, -0.6484],
        [ 0.0962,  1.2734, -0.3242, -0.7656, -0.4551],
        [-0.1523,  1.5156, -0.7031, -0.8906, -0.7344],
        [-0.0723,  1.6172, -0.7539, -0.6758, -0.0669],
        [ 0.0569,  1.0547, -0.8945, -1.1406, -0.2676],
        [ 0.2021,  1.1641, -0.8164, -0.7500, -0.3594],
        [ 0.0918,  1.6250, -0.6523, -0.8984, -0.3164],
        [ 0.0781,  1.6562, -0.7930, -0.7461, -0.5859],
        [-0.1235,  1.3594, -0.4043, -0.7852, -0.6289],
        [-0.1943,  1.4844, -0.5352, -0.9648, -0.6680],
        [ 0.0430,  1.4688, -0.2041, -0.8438, -0.5859]], device='cuda:0',
       dtype=torch.bfloat16, grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None), SequenceClassifierOutput(loss=tensor(0.9003, device='cuda:0', grad_fn=<NllLossBackward0>), logits=tensor([[-0.0211,  1.4922, -0.4121, -0.9844, -0.9414],
        [ 0.2773,  1.7031, -0.7383, -1.0000, -0.6094],
        [ 0.1631,  1.1406, -0.4336, -0.3789, -0.4570],
        [-0.0918,  1.4375, -0.6875, -0.7578, -0.4297],
        [ 0.1206,  1.5312, -0.6094, -1.0547, -0.7539],
        [ 0.2812,  1.0078, -0.4727, -0.8086, -0.5664],
        [-0.0815,  1.3281, -0.4004, -1.0859, -0.6328],
        [ 0.0623,  1.3594, -0.4570, -0.5977, -0.5352],
        [-0.0864,  1.3281, -0.6484, -1.2656, -0.8867],
        [-0.1660,  1.1328, -0.5430, -0.7617, -0.6328],
        [ 0.2695,  1.3516, -1.1406, -0.7539, -0.1514],
        [ 0.1826,  1.3672, -0.4941, -0.5742, -0.5547],
        [ 0.1348,  1.3516, -0.6836, -0.8008, -0.6055],
        [ 0.2051,  1.1875, -0.6367, -0.5234, -0.5273],
        [ 0.0525,  1.2969, -0.4629, -0.6445, -0.6367],
        [ 0.1060,  1.5391, -0.5586, -0.8945, -0.6562]], device='cuda:0',
       dtype=torch.bfloat16, grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None), SequenceClassifierOutput(loss=tensor(0.8759, device='cuda:0', grad_fn=<NllLossBackward0>), logits=tensor([[ 0.1543,  1.6016, -0.5273, -0.8242, -0.7109],
        [-0.1123,  1.2031, -0.8164, -1.0078, -0.5469],
        [ 0.0991,  1.2656, -0.6289, -0.8008, -0.1602],
        [ 0.2275,  1.6094, -0.6445, -0.8164, -0.7539],
        [-0.2480,  1.5938, -0.3496, -0.8047, -0.5430],
        [-0.0471,  1.6016, -0.5039, -0.6797, -0.5391],
        [ 0.1211,  1.4531, -0.6016, -0.7344, -0.2793],
        [ 0.0645,  1.8828, -0.5742, -0.8398, -0.4102],
        [ 0.1167,  1.5859, -0.4238, -0.9648, -0.4668],
        [-0.0708,  1.4219, -0.8750, -1.0469, -0.3691],
        [-0.0496,  1.2188, -0.5938, -1.1406, -0.8281],
        [ 0.0386,  1.5391, -0.7109, -0.9297, -0.3867],
        [ 0.2471,  1.2422, -0.5312, -0.7227, -0.4766],
        [ 0.3281,  1.5000, -0.2461, -0.8203, -0.6523],
        [ 0.1152,  1.4844, -0.5195, -1.0078, -0.8281],
        [ 0.3711,  1.2266, -0.4805, -0.5234, -0.4922]], device='cuda:0',
       dtype=torch.bfloat16, grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None), SequenceClassifierOutput(loss=tensor(0.6183, device='cuda:0', grad_fn=<NllLossBackward0>), logits=tensor([[ 0.1299,  1.5234, -0.6953, -0.8047, -0.8203],
        [ 0.0449,  1.5469, -0.5781, -0.8320, -0.2539],
        [ 0.2109,  1.8906, -0.8203, -1.1250, -0.5391],
        [-0.0061,  1.3828, -0.5156, -1.0000, -0.6719],
        [-0.3438,  1.0703, -0.4727, -1.0000, -0.9180],
        [ 0.1279,  1.0312, -0.8516, -0.8164, -0.0021],
        [ 0.0830,  1.6484, -0.5703, -0.8047, -0.1553],
        [ 0.2119,  1.6016, -0.6953, -0.9023, -0.4492],
        [ 0.1777,  1.0312, -0.9648, -0.4238, -0.4473],
        [-0.1748,  1.2656, -0.7461, -0.7500, -0.6055],
        [-0.4805,  1.3906, -0.4883, -0.5312, -0.6758],
        [ 0.2432,  1.3438, -0.6406, -0.6836, -0.6953],
        [ 0.0713,  1.5547, -0.5508, -0.9023, -0.8203],
        [-0.0500,  1.2969, -0.8359, -0.5312, -0.4277],
        [ 0.1055,  1.7891, -0.5469, -1.0547, -0.4414],
        [ 0.0244,  1.1719, -0.6016, -1.0078, -0.5742]], device='cuda:0',
       dtype=torch.bfloat16, grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None), SequenceClassifierOutput(loss=tensor(0.8488, device='cuda:0', grad_fn=<NllLossBackward0>), logits=tensor([[ 0.1592,  1.7734, -0.5742, -0.8438, -0.6172],
        [-0.4473,  1.6094, -0.4199, -0.6992, -0.5898],
        [ 0.2539,  1.4375, -0.8594, -1.0391, -0.6055],
        [ 0.1914,  1.2266, -0.5625, -0.7227, -0.5312],
        [-0.0581,  1.4688, -0.6758, -0.8320, -0.5625],
        [ 0.3633,  1.7422, -0.6719, -0.9766, -0.5742],
        [ 0.0518,  1.2812, -0.5312, -1.2891, -0.4141],
        [-0.0374,  1.4766, -0.2012, -0.9336, -0.5820],
        [ 0.0688,  1.6094, -0.6836, -0.7109, -0.4297],
        [-0.3770,  1.3750, -1.2578, -1.0547, -0.5742],
        [ 0.1992,  1.2109, -0.2852, -0.7617, -0.5039],
        [-0.1133,  1.3594, -0.7656, -0.8516, -0.3789],
        [-0.0131,  1.2344, -0.3379, -0.5547, -0.3145],
        [ 0.3457,  1.1875, -0.9062, -0.8945, -0.6641],
        [-0.1455,  1.6641, -0.5430, -1.0703, -0.4883],
        [-0.2012,  1.1797, -0.2793, -1.1797, -0.9883]], device='cuda:0',
       dtype=torch.bfloat16, grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None), SequenceClassifierOutput(loss=tensor(0.8552, device='cuda:0', grad_fn=<NllLossBackward0>), logits=tensor([[ 5.4016e-03,  1.4062e+00, -4.7070e-01, -8.3594e-01, -3.5352e-01],
        [ 5.6250e-01,  9.7656e-01, -7.1094e-01, -8.8672e-01, -3.9062e-01],
        [-2.2754e-01,  1.5312e+00, -3.1836e-01, -9.6875e-01, -4.7070e-01],
        [-2.7344e-01,  1.5781e+00, -5.4297e-01, -1.0859e+00, -5.1562e-01],
        [-1.9922e-01,  1.5859e+00, -5.2734e-01, -5.7031e-01, -4.7070e-01],
        [ 6.2988e-02,  1.3281e+00, -5.1953e-01, -9.4922e-01, -6.7969e-01],
        [ 2.5391e-01,  1.7891e+00, -9.6484e-01, -1.0391e+00, -3.3008e-01],
        [ 2.0020e-01,  1.5938e+00, -6.7969e-01, -6.8359e-01, -3.3789e-01],
        [-8.6914e-02,  1.4453e+00, -7.5000e-01, -8.3203e-01, -7.6953e-01],
        [ 8.1543e-02,  1.4297e+00, -5.0781e-01, -9.3750e-01, -6.4062e-01],
        [ 1.4893e-02,  1.4688e+00, -3.5156e-01, -7.1484e-01, -3.7305e-01],
        [-4.6875e-02,  1.2812e+00, -4.9023e-01, -9.5312e-01, -5.0781e-01],
        [-3.9453e-01,  1.3594e+00, -2.4121e-01, -7.4219e-01, -7.1875e-01],
        [-7.7637e-02,  1.1172e+00, -7.4219e-01, -5.4688e-01, -4.1602e-01],
        [ 8.1055e-02,  1.0391e+00, -1.0312e+00, -7.3438e-01, -1.5918e-01],
        [ 6.1035e-04,  1.5703e+00, -5.6250e-01, -5.4688e-01, -2.7539e-01]],
       device='cuda:0', dtype=torch.bfloat16, grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None), SequenceClassifierOutput(loss=tensor(0.6154, device='cuda:0', grad_fn=<NllLossBackward0>), logits=tensor([[ 0.2656,  1.7500, -0.5625, -0.9180, -0.7031],
        [-0.2852,  1.4766, -0.6406, -0.8477, -0.5781],
        [-0.0219,  1.2891,  0.0167, -0.7500, -0.8398],
        [-0.2539,  1.3125, -0.5547, -1.1328, -0.3164],
        [ 0.6836,  0.4961, -0.8633,  0.0781,  0.4414],
        [-0.0118,  1.3750, -0.4844, -1.0703, -0.4707],
        [ 0.2080,  1.5859, -0.4961, -0.7891, -0.6133],
        [-0.0265,  1.2500, -0.1631, -0.5820, -0.4023],
        [-0.0859,  1.5000, -0.7383, -0.9453, -0.5547],
        [-0.2305,  1.6250, -0.4277, -0.9492, -0.5469],
        [-0.0674,  1.2734, -0.4180, -0.3633, -1.1328],
        [ 0.2754,  1.5078, -0.4590, -1.2422, -0.7383],
        [ 0.0938,  1.4922, -0.6406, -0.6367, -0.4980],
        [ 0.2715,  1.7109, -0.5469, -1.0312, -0.3789],
        [ 0.1602,  1.6328, -0.9961, -0.5195, -0.7148],
        [ 0.1885,  1.4531, -0.8672, -0.6445, -0.2793]], device='cuda:0',
       dtype=torch.bfloat16, grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None), SequenceClassifierOutput(loss=tensor(0.8555, device='cuda:0', grad_fn=<NllLossBackward0>), logits=tensor([[-0.0688,  1.3516, -0.4531, -0.7578, -0.5234],
        [ 0.4004,  1.4297, -0.5859, -0.7344, -0.4766],
        [ 0.1523,  1.3906, -0.2422, -0.6992, -0.4336],
        [-0.1318,  0.9297, -0.3281, -0.7422, -0.8125],
        [ 0.0026,  1.4609, -0.6016, -0.7070, -0.4375],
        [ 0.0845,  1.1406, -0.8438, -0.3516, -0.6641],
        [ 0.3086,  1.2188, -0.6406, -1.0156, -0.5508],
        [ 0.3184,  1.3125, -0.7500, -0.8555, -0.8086],
        [ 0.4629,  1.3359, -0.7578, -0.9414, -0.5547],
        [ 0.0270,  1.3516, -0.7109, -0.9297, -0.2852],
        [ 0.2578,  1.3750, -0.8672, -0.5078, -0.2715],
        [ 0.2148,  1.7109, -0.5430, -1.0547, -0.8359],
        [ 0.2812,  1.5938, -0.6641, -0.8672, -0.5703],
        [ 0.3145,  1.4219, -0.5117, -0.7812, -0.3320],
        [ 0.1572,  1.5234, -0.4121, -0.7539, -0.4473],
        [-0.2598,  1.4453, -0.6797, -0.7812, -0.5820]], device='cuda:0',
       dtype=torch.bfloat16, grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None), SequenceClassifierOutput(loss=tensor(1.1301, device='cuda:0', grad_fn=<NllLossBackward0>), logits=tensor([[ 0.0356,  1.5312, -0.5312, -0.6289, -0.6836],
        [ 0.3730,  0.9805, -0.5156, -0.6289, -0.7070],
        [ 0.1250,  1.4375, -0.8047, -0.8047, -0.2871],
        [ 0.1104,  1.3672, -0.2061, -0.7891, -0.7344],
        [ 0.1201,  1.4219, -0.7148, -0.6719, -0.5430],
        [ 0.3477,  1.2812, -0.8047, -0.8750, -0.5195],
        [-0.3379,  1.3203, -0.8828, -0.6172, -0.4531],
        [-0.0588,  1.3047, -0.6211, -0.9805, -0.4648],
        [ 0.2080,  1.5000, -0.5352, -0.8789, -0.6328],
        [-0.0432,  1.5078, -0.6445, -0.6406, -0.3848],
        [ 0.0300,  1.6641, -0.8008, -1.0625, -0.2559],
        [ 0.1865,  1.6172, -0.6250, -1.0547, -0.3574],
        [ 0.0332,  1.5859, -0.9258, -0.8750, -0.6055],
        [-0.0513,  1.0469, -0.4570, -0.7617, -0.7188],
        [ 0.0486,  1.7109, -0.5352, -0.7305, -0.2910],
        [-0.1094,  1.2812, -0.5703, -0.7422, -0.7031]], device='cuda:0',
       dtype=torch.bfloat16, grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None), SequenceClassifierOutput(loss=tensor(0.9167, device='cuda:0', grad_fn=<NllLossBackward0>), logits=tensor([[ 0.0630,  1.3750, -0.9219, -0.8945,  0.0197],
        [ 0.1309,  1.2891, -0.6328, -0.6289, -0.3594],
        [-0.0127,  1.4844, -0.5312, -0.7266, -0.5391],
        [ 0.0791,  1.4766, -0.6484, -1.1250, -0.7852],
        [ 0.3730,  1.4688, -0.5938, -1.0078, -0.4688],
        [ 0.0767,  1.6797, -0.8672, -0.8320, -0.5430],
        [ 0.3105,  1.7422, -0.9414, -1.0781, -0.6172],
        [-0.2461,  1.4062, -0.3652, -0.8555, -0.8867],
        [-0.0400,  1.1953, -0.6289, -0.6445, -0.6719],
        [-0.1621,  1.2578, -0.2812, -1.2578, -0.3145],
        [-0.0986,  1.8438, -0.4453, -1.0938, -0.5430],
        [ 0.0845,  0.9805, -0.2090, -0.8633, -0.8086],
        [ 0.0222,  1.6641, -0.7773, -0.9961, -0.7305],
        [-0.0103,  1.4766, -0.1895, -0.7617, -0.5859],
        [ 0.1240,  1.3594, -0.6562, -1.0234, -0.7461],
        [-0.2109,  1.2812, -0.9414, -0.9023, -0.6328]], device='cuda:0',
       dtype=torch.bfloat16, grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None), SequenceClassifierOutput(loss=tensor(0.6511, device='cuda:0', grad_fn=<NllLossBackward0>), logits=tensor([[ 0.2676,  1.4609, -0.6836, -0.8789, -0.7695],
        [-0.0199,  1.3281, -0.0562, -0.8789, -1.1641],
        [-0.5625,  1.4219, -0.6484, -0.6484, -0.2188],
        [-0.2168,  1.4062, -0.6875, -0.7031, -0.6445],
        [-0.0284,  1.5625, -0.3789, -0.7383, -0.2168],
        [-0.0036,  1.3984, -0.4844, -0.5781, -0.6602],
        [ 0.2354,  1.2031, -0.7617, -1.0234, -0.4746],
        [ 0.3203,  1.5078, -0.6133, -0.9648, -0.3340],
        [-0.3691,  1.5000, -0.5469, -1.0781, -0.4727],
        [-0.0369,  1.3516, -0.8047, -0.6992, -0.3379],
        [ 0.2188,  1.4844, -0.7422, -0.8359, -0.4961],
        [-0.0334,  1.1406, -0.2354, -0.3145, -0.7070],
        [ 0.3184,  1.6562, -0.7109, -0.9297, -0.4199],
        [-0.0298,  1.4453, -0.3887, -0.7344, -0.4629],
        [-0.2314,  1.2969, -0.4023, -0.9023, -0.6016],
        [-0.0330,  0.9023, -0.9375, -0.8398, -0.6328]], device='cuda:0',
       dtype=torch.bfloat16, grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None), SequenceClassifierOutput(loss=tensor(0.8892, device='cuda:0', grad_fn=<NllLossBackward0>), logits=tensor([[-0.0277,  1.5703, -0.5703, -1.1641, -0.4277],
        [-0.0043,  1.6562, -0.6758, -0.8789, -0.3477],
        [ 0.1260,  1.4844, -0.4258, -0.6211, -0.4570],
        [ 0.0579,  1.5391, -0.3418, -0.8359, -0.4316],
        [ 0.4277,  1.2109, -0.7695, -1.1172, -0.2793],
        [-0.0442,  1.8125, -0.7109, -0.4805, -0.4316],
        [-0.1426,  1.1406, -0.5703, -0.6289, -0.6836],
        [ 0.2021,  1.3516, -0.5430, -0.5742, -0.4336],
        [-0.1836,  1.2500, -0.4707, -0.5781, -0.5977],
        [ 0.0947,  1.1484, -0.7031, -1.1406, -0.7695],
        [ 0.2061,  1.4453, -0.8320, -0.5898, -0.4961],
        [ 0.2100,  1.2812, -0.6797, -0.8945, -0.6562],
        [-0.0066,  1.6172, -0.2246, -0.7422, -0.6211],
        [ 0.1914,  1.5781, -0.6836, -0.8828, -0.2695],
        [ 0.0339,  1.5234, -0.8750, -0.9922, -0.4512],
        [-0.1187,  1.3281, -0.6602, -0.8633, -0.5352]], device='cuda:0',
       dtype=torch.bfloat16, grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None), SequenceClassifierOutput(loss=tensor(0.7737, device='cuda:0', grad_fn=<NllLossBackward0>), logits=tensor([[ 0.1719,  1.5156, -0.4922, -0.8594, -0.5742],
        [ 0.2314,  1.5938, -0.8164, -1.3359, -0.6914],
        [ 0.1650,  1.2109, -0.6406, -0.9531, -0.4531],
        [-0.2402,  1.4375, -0.8164, -0.6680, -0.3652],
        [ 0.3125,  1.5859, -0.6250, -0.6875, -0.4453],
        [-0.4375,  1.6328, -0.4219, -0.6641, -0.3711],
        [ 0.2402,  1.2578, -0.6055, -0.6523, -0.6523],
        [ 0.1465,  1.6172, -0.9531, -0.5430, -0.3340],
        [ 0.0145,  1.2969, -0.2520, -0.5234, -0.3867],
        [ 0.2793,  1.4453, -0.5469, -0.9102, -0.5234],
        [ 0.1514,  1.1875, -0.4688, -0.3848, -0.5742],
        [ 0.0330,  1.7656, -0.8984, -0.8125, -0.4570],
        [ 0.2832,  1.5859, -0.7773, -0.8594, -0.5234],
        [-0.1299,  1.3984, -0.9141, -0.9219, -0.4570],
        [ 0.1406,  1.5078, -0.5625, -1.1719, -0.6797],
        [ 0.0737,  1.3438, -0.6133, -0.5352, -0.4902]], device='cuda:0',
       dtype=torch.bfloat16, grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None), SequenceClassifierOutput(loss=tensor(0.7406, device='cuda:0', grad_fn=<NllLossBackward0>), logits=tensor([[-0.2578,  1.2422, -0.5859, -0.7617, -0.5156],
        [ 0.1357,  1.1719, -0.5898, -0.8594, -0.5039],
        [ 0.0898,  1.8438, -0.8008, -1.0469, -0.4219],
        [ 0.2539,  1.6016, -0.4707, -0.5156, -0.6250],
        [ 0.2598,  1.3906, -0.3613, -1.0156, -0.8047],
        [ 0.1084,  1.3281, -0.7031, -0.5820, -0.3691],
        [-0.2734,  1.2891, -0.2266, -0.7422, -0.7500],
        [-0.1016,  1.4453, -0.7266, -0.5820, -0.4766],
        [ 0.2988,  1.6953, -0.9531, -0.7578, -0.6914],
        [ 0.0718,  1.3750, -0.6641, -1.3047, -0.5273],
        [-0.1562,  1.3672, -0.4277, -1.0391, -0.8867],
        [ 0.0294,  1.3750, -0.6094, -0.8242, -0.2910],
        [-0.0405,  1.4688, -0.3184, -0.9023, -1.0234],
        [-0.0513,  1.0703, -0.2754, -0.3965, -0.5664],
        [-0.2559,  1.1484, -0.4629, -0.8672, -0.5547],
        [-0.1494,  1.5547, -0.4980, -0.6797, -0.6211]], device='cuda:0',
       dtype=torch.bfloat16, grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None), SequenceClassifierOutput(loss=tensor(0.8258, device='cuda:0', grad_fn=<NllLossBackward0>), logits=tensor([[ 0.0928,  0.9531, -0.9023, -0.7539, -0.2451],
        [-0.1426,  1.6562, -0.2217, -1.0781, -0.6172],
        [ 0.1797,  1.5391, -0.5469, -1.0078, -0.8398],
        [ 0.2852,  1.5391, -0.5234, -0.8125, -0.6719],
        [-0.2715,  1.5000, -0.5312, -0.9414, -0.8867],
        [-0.1611,  1.3516, -0.6133, -0.4512, -0.5352],
        [ 0.3574,  1.2188, -0.6680, -0.6836, -0.7266],
        [ 0.2041,  1.2812, -0.8828, -1.1250, -0.3066],
        [-0.3301,  1.3594, -0.4062, -0.8906, -0.4570],
        [ 0.0574,  1.1719, -0.6016, -0.8047, -0.4121],
        [-0.0435,  1.3672, -0.8086, -0.5625, -0.4414],
        [-0.3926,  1.7109, -0.5000, -0.7422, -0.3965],
        [-0.3613,  1.6016, -0.7344, -0.5977, -0.8008],
        [ 0.0204,  1.1797, -0.3359, -1.1250, -0.5586],
        [ 0.0581,  1.3750, -0.5938, -1.0625, -0.6250],
        [ 0.6367,  1.3750, -0.6953, -1.0078, -0.7656]], device='cuda:0',
       dtype=torch.bfloat16, grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None), SequenceClassifierOutput(loss=tensor(0.8156, device='cuda:0', grad_fn=<NllLossBackward0>), logits=tensor([[ 0.3301,  0.2812, -0.9297, -0.5625,  0.0781],
        [ 0.2451,  1.0312, -0.9180, -0.6562, -0.4902],
        [-0.0022,  1.6016, -0.8633, -1.1406, -0.4727],
        [-0.1406,  1.2109, -0.6562, -0.9141, -0.8438],
        [ 0.0498,  1.5312, -0.9062, -0.5312, -0.6406],
        [-0.0986,  1.3984, -0.3652, -0.7773, -0.5586],
        [-0.0542,  1.6172, -0.8789, -0.9375, -0.5938],
        [-0.0864,  1.5469, -0.2422, -0.9727, -0.7383],
        [ 0.0388,  1.4609, -0.5625, -0.6797, -0.3867],
        [ 0.4727,  0.8906, -0.7891, -0.9297,  0.1182],
        [-0.1226,  1.3359, -0.6016, -0.6758, -0.5938],
        [-0.2363,  1.7344, -0.5977, -1.1250, -0.5156],
        [ 0.0109,  1.3984, -0.5391, -0.8359, -0.6367],
        [-0.3750,  1.1875, -0.5859, -0.7812, -0.3320],
        [-0.1797,  1.3281, -0.4609, -0.4941, -0.6719],
        [ 0.3203,  1.2812, -0.6797, -0.8164, -0.1152]], device='cuda:0',
       dtype=torch.bfloat16, grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None), SequenceClassifierOutput(loss=tensor(0.9404, device='cuda:0', grad_fn=<NllLossBackward0>), logits=tensor([[ 0.0952,  1.4141, -0.6641, -0.4219, -0.3301],
        [ 0.0400,  1.3672, -0.6562, -0.8398, -0.5938],
        [ 0.0107,  1.5469, -0.8125, -1.0547, -0.4629],
        [-0.0610,  1.5234, -0.4590, -0.9141, -0.7539],
        [ 0.1226,  1.3672, -0.5938, -0.6953, -0.5547],
        [-0.0679,  1.6953, -0.4238, -0.8477, -0.5352],
        [ 0.3184,  1.6172, -0.4023, -0.6367, -0.5938],
        [ 0.3887,  1.2500, -0.7578, -0.9688, -0.4844],
        [-0.1602,  1.2422, -0.2754, -0.7930, -0.7461],
        [-0.3633,  1.2578, -0.5508, -0.5430, -0.6914],
        [ 0.0188,  0.8086, -0.9219, -0.6992, -0.0381],
        [ 0.1699,  1.3516, -0.7539, -0.7422, -0.4434],
        [-0.3262,  1.4141, -0.3965, -0.8711, -0.3887],
        [ 0.0723,  1.5156, -0.5625, -0.9336, -0.7930],
        [-0.1270,  1.4766, -0.6172, -0.8750, -0.9102],
        [-0.0840,  1.4219, -0.7031, -0.8516, -0.7109]], device='cuda:0',
       dtype=torch.bfloat16, grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None), SequenceClassifierOutput(loss=tensor(0.6639, device='cuda:0', grad_fn=<NllLossBackward0>), logits=tensor([[-0.0393,  1.4766, -0.9336, -0.6484, -0.4629],
        [ 0.1426,  1.0625, -0.7930, -0.8281, -0.7852],
        [-0.2734,  1.6094, -0.4824, -0.9375, -0.6094],
        [-0.5781,  1.6953, -0.1582, -0.7188, -0.5430],
        [-0.0135,  1.6484, -0.6133, -0.8828, -0.4238],
        [ 0.1357,  1.2500, -0.6602, -0.5703, -0.3418],
        [ 0.1680,  1.7578, -0.1631, -0.6914, -0.4883],
        [-0.2266,  0.9805, -0.2393, -0.7188, -0.3047],
        [ 0.2910,  1.3594, -0.5898, -0.6094, -0.3516],
        [ 0.1021,  1.3047, -0.1494, -1.1016, -0.5859],
        [ 0.3125,  1.6172, -0.7695, -0.9688, -0.4785],
        [-0.0583,  1.5938, -0.8320, -1.1406, -0.2617],
        [-0.0591,  1.3672,  0.0698, -0.5820, -0.9609],
        [ 0.1523,  1.3281, -0.6719, -0.8672, -0.6797],
        [-0.1543,  1.3828, -0.6797, -0.8477, -0.4199],
        [-0.0537,  1.2656, -0.6367, -0.7031, -0.6836]], device='cuda:0',
       dtype=torch.bfloat16, grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None), SequenceClassifierOutput(loss=tensor(0.7734, device='cuda:0', grad_fn=<NllLossBackward0>), logits=tensor([[ 0.2432,  1.0781, -0.6992, -0.9023, -0.6602],
        [ 0.0042,  1.2734, -0.7617, -0.9961, -0.6641],
        [-0.0342,  1.2812, -0.4004, -0.8281, -0.6641],
        [ 0.1064,  1.6250, -0.3926, -0.8945, -0.4531],
        [-0.1807,  1.5469, -0.5664, -0.8984, -0.5117],
        [ 0.1079,  1.5781, -0.3887, -1.1641, -0.8516],
        [ 0.2949,  0.8984, -0.6914, -0.9023, -0.2871],
        [-0.2334,  1.2656, -0.6406, -0.6016, -0.5156],
        [ 0.0238,  1.3281, -0.3691, -1.1797, -0.6250],
        [ 0.0796,  1.4141, -0.5859, -0.9844, -0.5508],
        [ 0.2334,  1.3125, -0.7617, -0.6953, -0.3047],
        [-0.0215,  1.5703, -0.6211, -0.4863, -0.3770],
        [ 0.2852,  1.4531, -0.6055, -0.8828, -0.2305],
        [-0.1069,  1.7031, -0.7305, -0.7305, -0.6016],
        [ 0.3438,  1.3672, -0.6758, -1.0312, -0.6016],
        [-0.0588,  1.3672, -0.8633, -0.5078, -0.6211]], device='cuda:0',
       dtype=torch.bfloat16, grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None), SequenceClassifierOutput(loss=tensor(0.6436, device='cuda:0', grad_fn=<NllLossBackward0>), logits=tensor([[-0.1084,  1.1484, -0.6289, -1.0312, -0.8477],
        [-0.3555,  1.2422, -0.3711, -0.8438, -0.6641],
        [-0.0874,  1.2656, -0.4609, -0.5312, -0.4668],
        [ 0.3711,  1.5391, -0.4004, -0.6250, -0.2422],
        [-0.2070,  1.2266, -0.6250, -0.7305, -0.7500],
        [ 0.1040,  1.5312, -0.3867, -0.8516, -0.4902],
        [ 0.1650,  1.5078, -0.8984, -0.7383, -0.4570],
        [-0.2539,  1.5312, -0.5078, -0.6680, -0.2949],
        [-0.2754,  1.7500, -0.3340, -0.8516, -0.7500],
        [ 0.1133,  1.5859, -0.6641, -0.4492, -0.6992],
        [ 0.0236,  1.2266, -0.5898, -0.6367, -0.4805],
        [-0.0035,  1.2422, -0.6641, -0.5664, -0.4512],
        [ 0.1025,  1.2734, -0.5508, -0.6523, -0.2812],
        [-0.2676,  1.3047, -0.5273, -0.9922, -0.7266],
        [ 0.0077,  1.0859, -0.9492, -1.2031, -0.7148],
        [ 0.2988,  1.1797, -0.4902, -0.7500, -0.4590]], device='cuda:0',
       dtype=torch.bfloat16, grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None), SequenceClassifierOutput(loss=tensor(0.7257, device='cuda:0', grad_fn=<NllLossBackward0>), logits=tensor([[ 2.1191e-01,  1.1875e+00, -7.8516e-01, -8.1250e-01, -5.0391e-01],
        [-2.4414e-01,  1.7188e+00, -9.9219e-01, -9.4141e-01, -5.5859e-01],
        [ 1.0791e-01,  1.7188e+00, -6.1328e-01, -1.0234e+00, -3.9258e-01],
        [-5.0391e-01,  1.0859e+00, -5.7812e-01, -7.4609e-01, -5.3906e-01],
        [ 7.7637e-02,  1.6016e+00, -8.2812e-01, -1.0391e+00, -6.9531e-01],
        [ 2.9419e-02,  1.4766e+00, -7.5781e-01, -8.0469e-01, -2.5977e-01],
        [ 5.5176e-02,  1.3750e+00, -2.4805e-01, -7.8906e-01, -6.7969e-01],
        [ 1.0147e-03,  1.7422e+00, -7.1875e-01, -9.8438e-01, -6.9922e-01],
        [ 1.6016e-01,  1.4766e+00, -5.5469e-01, -8.2031e-01, -6.2500e-01],
        [-2.1094e-01,  1.4766e+00, -4.9805e-01, -8.3984e-01, -5.4297e-01],
        [ 3.8086e-02,  1.2031e+00, -5.1953e-01, -1.0391e+00, -5.2734e-01],
        [ 1.6968e-02,  1.2656e+00, -6.3281e-01, -9.3750e-01, -8.9062e-01],
        [-8.3008e-02,  1.7031e+00, -8.3984e-01, -5.9375e-01, -2.8906e-01],
        [-1.0400e-01,  1.1484e+00, -3.6133e-01, -7.2656e-01, -5.1953e-01],
        [-3.8281e-01,  1.7812e+00, -5.7031e-01, -1.0000e+00, -3.3398e-01],
        [-1.0547e-01,  1.2812e+00, -7.3828e-01, -8.1250e-01, -7.1875e-01]],
       device='cuda:0', dtype=torch.bfloat16, grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None), SequenceClassifierOutput(loss=tensor(0.6217, device='cuda:0', grad_fn=<NllLossBackward0>), logits=tensor([[ 0.0913,  1.0938, -0.7188, -1.0391, -0.5117],
        [-0.1689,  1.4453, -0.7305, -0.9375, -0.4883],
        [ 0.2295,  1.5391, -0.6836, -0.8320, -0.6328],
        [ 0.2715,  1.4297, -0.8867, -1.1484, -0.3945],
        [-0.2324,  1.3828, -0.6133, -0.9492, -0.4141],
        [-0.1797,  1.0859, -0.3730, -0.8867, -0.8398],
        [ 0.3809,  1.6172, -0.7500, -1.2422, -0.5078],
        [ 0.4082,  1.2812, -0.3164, -0.7227, -0.7422],
        [ 0.0374,  1.3359, -0.5742, -0.6484, -0.4395],
        [ 0.2080,  1.6484, -0.6289, -0.9688, -0.5312],
        [ 0.1089,  1.1875, -0.7227, -0.8359, -0.7031],
        [ 0.0583,  1.4141, -0.6562, -0.6836, -0.7031],
        [ 0.4492,  1.4688, -0.3105, -1.1016, -0.6484],
        [-0.0723,  1.3984, -0.8828, -0.6641, -0.5156],
        [ 0.3027,  1.6406, -0.5039, -0.8086, -0.3672],
        [ 0.8633,  0.7422, -0.9844, -0.6484,  0.1128]], device='cuda:0',
       dtype=torch.bfloat16, grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None), SequenceClassifierOutput(loss=tensor(0.6400, device='cuda:0', grad_fn=<NllLossBackward0>), logits=tensor([[ 0.3457,  1.4766, -0.7383, -0.9453, -0.4785],
        [-0.2275,  1.4688, -0.8438, -0.7617, -0.6289],
        [-0.3184,  1.6172, -0.7305, -0.9453, -0.7148],
        [ 0.1494,  1.6016, -0.8594, -1.0078, -0.2539],
        [ 0.2910,  1.6562, -0.7734, -0.7734, -0.3750],
        [ 0.1406,  1.1875, -0.7812, -0.7656, -0.7812],
        [ 0.2002,  1.1953, -0.6406, -0.6133, -0.6289],
        [ 0.0659,  1.5078, -0.7070, -0.9492, -0.5898],
        [ 0.0303,  1.4141, -0.8477, -0.7578, -0.2285],
        [ 0.2539,  1.5469, -0.9297, -1.0156, -0.5547],
        [ 0.1680,  1.3750, -0.6680, -0.9727, -0.6289],
        [ 0.1377,  1.3359, -0.6523, -0.6523, -0.6328],
        [-0.2119,  1.4922, -0.6016, -0.8086, -0.3320],
        [-0.1416,  1.2969, -0.6445, -1.1875, -0.5859],
        [ 0.0513,  1.6719, -0.6328, -0.9492, -0.6562],
        [ 0.3633,  0.8438, -0.7383, -0.9102, -0.4570]], device='cuda:0',
       dtype=torch.bfloat16, grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None), SequenceClassifierOutput(loss=tensor(0.7914, device='cuda:0', grad_fn=<NllLossBackward0>), logits=tensor([[ 0.1631,  1.6328, -0.7773, -0.7031, -0.7539],
        [ 0.2363,  1.2578, -0.5859, -0.5625, -0.2578],
        [ 0.2129,  2.0000, -0.8398, -0.7891, -0.5234],
        [ 0.0240,  1.2266, -0.6641, -0.6484, -0.7422],
        [-0.1143,  1.4766, -0.7578, -1.0859, -0.2715],
        [ 0.0869,  1.4609, -0.7617, -0.8789, -0.2637],
        [ 0.0347,  1.7969, -0.5352, -0.8828, -0.4531],
        [-0.0957,  1.6406, -0.6719, -0.8281, -0.3770],
        [ 0.1660,  1.1641, -0.7383, -0.6250, -0.1318],
        [ 0.3809,  1.1328, -0.6289, -0.6641, -0.6719],
        [ 0.0693,  1.2656, -0.8867, -1.0078, -0.3828],
        [ 0.1299,  1.7500, -0.4570, -0.6445, -0.7070],
        [ 0.2139,  1.2578, -0.6875, -0.6562, -0.6797],
        [-0.0195,  1.1719, -0.3203, -1.0469, -0.6719],
        [-0.1680,  1.0703, -0.5234, -0.7773, -0.6836],
        [-0.4004,  1.5703, -0.3789, -1.0078, -0.4492]], device='cuda:0',
       dtype=torch.bfloat16, grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None), SequenceClassifierOutput(loss=tensor(0.9064, device='cuda:0', grad_fn=<NllLossBackward0>), logits=tensor([[-0.0869,  1.4141, -0.6953, -0.9844, -0.7109],
        [-0.0801,  1.3594, -0.6445, -1.0234, -0.7188],
        [ 0.2656,  1.6172, -0.6445, -0.9141, -0.4609],
        [-0.0300,  1.1797, -0.7461, -0.7656, -0.4258],
        [ 0.0850,  1.6562, -0.6250, -0.5742, -0.3145],
        [-0.1973,  1.4922, -0.5703, -0.8320, -0.6875],
        [-0.0330,  1.5312, -0.7227, -0.7422, -0.5859],
        [ 0.2891,  1.3594, -0.4922, -0.6289, -0.6211],
        [ 0.3633,  1.6562, -0.8359, -0.6602, -0.3984],
        [ 0.1367,  1.6641, -0.7070, -0.9102, -0.6055],
        [ 0.0840,  1.4922, -0.5312, -0.8516, -0.5430],
        [ 0.3965,  0.9844, -0.7852, -0.7188, -0.4141],
        [-0.0141,  1.3125, -0.5781, -0.6055, -0.4727],
        [-0.0179,  1.4375, -0.7227, -0.9609, -0.5664],
        [ 0.0132,  1.1797, -0.4766, -0.3281, -0.4355],
        [ 0.0145,  1.4609, -0.6133, -0.6133, -0.3828]], device='cuda:0',
       dtype=torch.bfloat16, grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None), SequenceClassifierOutput(loss=tensor(0.9740, device='cuda:0', grad_fn=<NllLossBackward0>), logits=tensor([[ 1.4160e-01,  1.2109e+00, -5.5078e-01, -1.0391e+00, -9.3359e-01],
        [-5.0781e-02,  1.4297e+00, -6.0547e-01, -5.3125e-01, -6.7969e-01],
        [ 9.1797e-02,  1.3359e+00, -5.7031e-01, -8.5938e-01, -4.8242e-01],
        [-8.9844e-02,  1.2266e+00, -5.6641e-01, -1.1484e+00, -7.0703e-01],
        [-2.2070e-01,  1.4375e+00, -2.2559e-01, -8.2031e-01, -3.4570e-01],
        [-2.4512e-01,  1.4219e+00, -6.3672e-01, -9.1406e-01, -4.7461e-01],
        [-2.7466e-02,  1.4766e+00, -5.3906e-01, -1.0703e+00, -5.3906e-01],
        [ 1.0449e-01,  1.3906e+00, -4.0625e-01, -8.7109e-01, -3.3398e-01],
        [-6.0547e-02,  1.4844e+00, -8.5156e-01, -8.1250e-01, -6.1328e-01],
        [ 1.3046e-03,  1.1953e+00, -1.3965e-01, -6.6406e-01, -7.7734e-01],
        [-1.1572e-01,  1.6250e+00, -4.1992e-01, -7.7344e-01, -5.3516e-01],
        [-8.4473e-02,  1.1016e+00, -6.8750e-01, -9.8438e-01, -7.1875e-01],
        [-1.1182e-01,  1.7266e+00, -6.3672e-01, -1.1328e+00, -4.9609e-01],
        [-9.8145e-02,  1.3750e+00, -4.8438e-01, -8.5938e-01, -2.2852e-01],
        [ 7.2266e-02,  1.6953e+00, -7.7734e-01, -1.0156e+00, -6.1719e-01],
        [-4.3164e-01,  1.4375e+00, -5.8594e-01, -1.0156e+00, -7.3047e-01]],
       device='cuda:0', dtype=torch.bfloat16, grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None), SequenceClassifierOutput(loss=tensor(0.7008, device='cuda:0', grad_fn=<NllLossBackward0>), logits=tensor([[ 0.0505,  1.4688, -0.7773, -1.0391, -0.3789],
        [-0.2656,  1.3359, -0.6719, -0.7266, -0.5938],
        [-0.0659,  1.6641, -0.3203, -1.0312, -0.7188],
        [ 0.1680,  1.4375, -0.6328, -0.6914, -0.2334],
        [-0.0125,  1.7500, -0.6641, -0.9961, -0.5430],
        [ 0.0942,  1.1094, -0.3750, -0.7188, -0.0603],
        [ 0.0130,  1.5312, -0.7891, -1.1094, -0.3750],
        [ 0.1011,  1.5000, -0.8711, -1.1172, -0.2295],
        [-0.2246,  1.0234, -0.4883, -1.0469, -0.7383],
        [ 0.1318,  1.5625, -0.6562, -1.1719, -0.6758],
        [-0.1167,  1.4219, -0.3496, -0.6016, -0.5195],
        [ 0.3516,  1.4375, -0.5820, -1.0000, -0.5117],
        [ 0.1963,  1.5078, -0.7930, -0.5234, -0.5430],
        [ 0.0884,  1.6484, -0.7305, -0.9844, -0.2002],
        [-0.0100,  1.6328, -0.7148, -0.8867, -0.5664],
        [-0.0476,  1.7656, -0.4453, -0.9297, -0.7344]], device='cuda:0',
       dtype=torch.bfloat16, grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None), SequenceClassifierOutput(loss=tensor(0.7046, device='cuda:0', grad_fn=<NllLossBackward0>), logits=tensor([[-0.3262,  1.3438, -0.6758, -0.4590, -0.3926],
        [ 0.0095,  1.1641, -0.3848, -0.8516, -0.6719],
        [ 0.1152,  1.5000, -0.9531, -1.0156, -0.5547],
        [ 0.1279,  1.5859, -0.6953, -1.1797, -0.5742],
        [ 0.0118,  1.3828, -0.1504, -0.7734, -0.7305],
        [ 0.2910,  1.5469, -0.2949, -0.6992, -0.2598],
        [ 0.4004,  1.3438, -0.9609, -1.1016, -0.5117],
        [-0.3379,  1.7500, -0.5117, -0.7344, -0.4629],
        [ 0.0276,  1.1719, -0.6523, -0.8125, -0.8242],
        [-0.1992,  1.1719, -0.6953, -0.8359, -0.9023],
        [ 0.2148,  1.1016, -0.7734, -0.6172, -0.8828],
        [ 0.3281,  1.6719, -0.5508, -0.6250, -0.5664],
        [ 0.0549,  1.5078, -0.6094, -0.9297, -0.2383],
        [ 0.0113,  0.9961, -0.5898, -0.8789, -0.4141],
        [ 0.1191,  1.4062, -0.5547, -0.7617, -0.4180],
        [-0.2207,  1.3125, -0.3340, -0.7344, -0.6914]], device='cuda:0',
       dtype=torch.bfloat16, grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None), SequenceClassifierOutput(loss=tensor(0.7875, device='cuda:0', grad_fn=<NllLossBackward0>), logits=tensor([[ 0.1455,  1.6562, -0.7383, -0.8594, -0.4395],
        [ 0.1133,  1.2891, -0.4238, -0.7148, -0.4648],
        [-0.3301,  1.5859, -0.5781, -0.8906, -0.5547],
        [-0.4277,  1.2266, -0.3184, -1.2031, -0.7891],
        [ 0.0234,  1.2656, -0.4883, -0.8672, -0.4355],
        [ 0.0859,  1.1797, -0.8945, -0.6484, -0.6289],
        [-0.0742,  1.7188, -0.7500, -1.0781, -0.7148],
        [ 0.1416,  1.6406, -0.5234, -0.8789, -0.6094],
        [ 0.0684,  1.4688, -0.2676, -0.5391, -0.6562],
        [ 0.0339,  1.5000, -0.4941, -0.7695, -0.3730],
        [-0.2656,  1.4609, -0.3965, -0.5859, -0.6602],
        [-0.1768,  1.5156, -0.6016, -0.7617, -0.6602],
        [ 0.1895,  1.4062, -0.6055, -1.2969, -0.4531],
        [-0.0542,  1.5391, -0.6289, -1.0859, -0.5000],
        [-0.1572,  1.4922, -0.8398, -0.8711, -0.2227],
        [-0.1123,  1.4297, -0.6875, -1.3516, -0.6289]], device='cuda:0',
       dtype=torch.bfloat16, grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None), SequenceClassifierOutput(loss=tensor(0.9403, device='cuda:0', grad_fn=<NllLossBackward0>), logits=tensor([[ 0.1064,  0.9219, -0.6172, -0.6094, -0.4785],
        [-0.1455,  1.2344, -0.3535, -0.7812, -0.4707],
        [ 0.1582,  1.3438, -0.9180, -0.8867,  0.0679],
        [ 0.1387,  1.4375, -0.4922, -0.6914, -0.7539],
        [ 0.3594,  1.5625, -0.8672, -1.3047, -0.9727],
        [ 0.2949,  1.3203, -0.5859, -0.7617, -0.5742],
        [ 0.0771,  1.2109, -0.7031, -0.6836, -0.6211],
        [ 0.1089,  1.6875, -0.4570, -1.1797, -0.6172],
        [-0.0569,  1.4688, -0.7266, -0.7109, -0.4883],
        [ 0.0276,  1.8203, -0.5195, -1.0312, -0.6445],
        [-0.1719,  1.3750,  0.0747, -0.8008, -0.6641],
        [ 0.1396,  1.5234, -0.4570, -1.0938, -0.6445],
        [ 0.0291,  1.0859, -0.7383, -0.7734, -0.3574],
        [-0.0913,  1.3047, -0.7461, -0.9258, -0.4746],
        [ 0.3340,  1.5703, -0.7695, -1.0234, -0.4121],
        [ 0.1416,  1.6953, -0.6992, -1.1172, -0.4629]], device='cuda:0',
       dtype=torch.bfloat16, grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None), SequenceClassifierOutput(loss=tensor(0.6971, device='cuda:0', grad_fn=<NllLossBackward0>), logits=tensor([[-2.3730e-01,  1.5234e+00, -4.8828e-01, -8.9844e-01, -1.1250e+00],
        [ 6.1279e-02,  1.4922e+00, -5.7422e-01, -5.9375e-01, -6.4062e-01],
        [ 1.3379e-01,  1.6484e+00, -7.4609e-01, -1.0938e+00, -3.2227e-01],
        [-1.0156e-01,  1.8203e+00, -4.0234e-01, -9.3750e-01, -6.9141e-01],
        [-7.5684e-02,  1.3750e+00, -3.7305e-01, -5.5859e-01, -2.6953e-01],
        [-2.8534e-03,  1.2344e+00, -5.9766e-01, -1.1016e+00, -7.1094e-01],
        [-2.1094e-01,  1.3984e+00, -1.0000e+00, -8.6328e-01, -5.0391e-01],
        [-5.7602e-04,  1.6172e+00, -6.4453e-01, -6.0547e-01, -4.2969e-01],
        [-9.8877e-03,  1.5000e+00, -6.9922e-01, -1.1953e+00, -3.7109e-01],
        [-3.3789e-01,  1.3906e+00, -5.5859e-01, -6.1328e-01, -6.2500e-01],
        [ 1.6406e-01,  1.5078e+00, -3.6719e-01, -1.0469e+00, -1.1172e+00],
        [ 3.8867e-01,  1.0859e+00, -3.4570e-01, -1.0234e+00, -6.0938e-01],
        [-4.1016e-02,  1.1797e+00, -7.8516e-01, -6.9922e-01, -4.4336e-01],
        [ 5.0293e-02,  1.7344e+00, -6.3281e-01, -8.1250e-01, -5.9766e-01],
        [-1.0010e-01,  9.2578e-01, -8.4766e-01, -5.2344e-01, -6.7578e-01],
        [ 2.2070e-01,  1.1719e+00, -6.3672e-01, -7.1094e-01, -4.2773e-01]],
       device='cuda:0', dtype=torch.bfloat16, grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None), SequenceClassifierOutput(loss=tensor(0.8785, device='cuda:0', grad_fn=<NllLossBackward0>), logits=tensor([[ 0.3945,  1.1250, -0.8086, -0.8164, -0.0967],
        [ 0.2139,  1.5234, -0.6406, -0.9961, -0.5781],
        [ 0.1602,  1.3828, -0.6641, -0.9766, -0.7188],
        [ 0.3965,  1.1719, -0.6875, -0.7734, -0.6836],
        [-0.0195,  1.3438, -0.7227, -0.8242, -0.5664],
        [ 0.1152,  1.1016, -0.6445, -0.6758, -0.5156],
        [-0.1050,  1.6406, -0.5820, -1.0156, -0.4336],
        [-0.0967,  1.4219, -0.5195, -1.2188, -0.4395],
        [-0.1387,  1.4453, -0.6289, -0.9336, -0.7227],
        [-0.1162,  1.2344, -0.4941, -0.8945, -0.4258],
        [-0.1309,  1.7109, -0.8477, -0.9258, -0.3594],
        [ 0.0583,  1.0938, -0.4043, -0.8047, -0.4648],
        [-0.0090,  1.5391, -0.6250, -0.8125, -0.6406],
        [ 0.4258,  0.9375, -0.8398, -1.1328,  0.0854],
        [ 0.1758,  1.5391, -0.7344, -0.9219, -0.4160],
        [-0.1914,  1.3594, -0.5195, -0.7500, -0.5820]], device='cuda:0',
       dtype=torch.bfloat16, grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None), SequenceClassifierOutput(loss=tensor(0.5502, device='cuda:0', grad_fn=<NllLossBackward0>), logits=tensor([[ 0.0786,  0.9336, -0.7305, -0.6797, -0.7695],
        [-0.0178,  1.4375, -0.6914, -0.7383, -0.7852],
        [ 0.1572,  1.3672, -0.4277, -0.7461, -0.7344],
        [-0.2578,  1.2734, -0.6289, -0.6914, -0.5391],
        [ 0.1348,  1.3125, -0.5547, -0.4492, -1.0078],
        [ 0.0664,  1.4844, -0.9570, -0.8633, -0.3984],
        [-0.1865,  1.4062, -0.5156, -0.8086, -0.6055],
        [ 0.1377,  1.4375, -0.6094, -1.1328, -0.4180],
        [ 0.1494,  1.4688, -0.6641, -1.0391, -0.6367],
        [-0.0732,  1.6094, -0.5586, -0.7734, -0.5078],
        [ 0.4824,  1.6562, -0.6172, -0.7266, -0.5078],
        [-0.1216,  1.6562, -0.3691, -0.5547, -0.7383],
        [ 0.3066,  1.2969, -0.8203, -0.7539, -0.4551],
        [-0.0923,  1.5625, -0.5469, -0.8242, -0.7031],
        [-0.2520,  1.5312, -0.5234, -0.7695, -0.6758],
        [ 0.0334,  1.2422, -0.3164, -0.6562, -0.3516]], device='cuda:0',
       dtype=torch.bfloat16, grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None), SequenceClassifierOutput(loss=tensor(0.5568, device='cuda:0', grad_fn=<NllLossBackward0>), logits=tensor([[-0.1865,  1.6484, -0.4961, -0.9102, -0.5703],
        [-0.2070,  1.3828, -0.3516, -1.0625, -0.6250],
        [ 0.3281,  1.1016, -0.4238, -0.6523, -0.4570],
        [ 0.0649,  1.6875, -0.8711, -1.0156, -0.5195],
        [ 0.1260,  1.7734, -0.3320, -0.5859, -0.7227],
        [ 0.0571,  1.4531, -0.7070, -0.8750, -0.5430],
        [ 0.3945,  1.6250, -0.5391, -0.5078, -0.3828],
        [-0.2021,  1.4922, -0.8906, -0.8086, -0.5586],
        [ 0.2227,  1.3828, -0.9570, -0.5117, -0.7852],
        [ 0.1206,  1.5156, -0.9414, -1.0938, -0.5156],
        [-0.2773,  1.5625, -0.5234, -0.9375, -0.4141],
        [-0.0977,  1.4453, -0.6406, -0.9492, -0.6211],
        [-0.1777,  1.5312, -0.5312, -0.9023, -0.3730],
        [ 0.1240,  1.7344, -0.3789, -0.7773, -0.6094],
        [ 0.0039,  1.5703, -0.6797, -0.8398, -0.7070],
        [-0.1309,  1.4609, -0.5508, -0.9062, -0.7578]], device='cuda:0',
       dtype=torch.bfloat16, grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None), SequenceClassifierOutput(loss=tensor(0.6865, device='cuda:0', grad_fn=<NllLossBackward0>), logits=tensor([[ 0.1816,  1.4453, -0.5547, -1.0234, -0.2500],
        [ 0.0605,  1.7656, -0.7188, -0.9570, -0.6445],
        [ 0.3867,  0.8086, -0.8984, -0.5117, -0.3828],
        [-0.1738,  1.1484, -0.4238, -0.9062, -0.6797],
        [ 0.3301,  1.2969, -0.7266, -0.9531, -0.4062],
        [ 0.2197,  0.9375, -0.4668, -0.7500, -0.6055],
        [ 0.0452,  1.8125, -0.5391, -0.8320, -0.4863],
        [ 0.0256,  1.3047, -0.5977, -1.0391, -0.3164],
        [-0.2559,  1.4766, -0.6133, -0.9258, -0.5195],
        [ 0.3008,  1.7031, -0.7461, -0.8633, -0.6289],
        [-0.1245,  1.5078, -0.8594, -0.8008, -0.3477],
        [-0.1426,  1.3750, -0.8242, -0.8281, -0.3770],
        [ 0.1406,  1.1016, -0.4297, -0.8359, -0.6250],
        [ 0.2266,  1.6016, -0.1611, -0.8047, -0.6992],
        [ 0.1270,  1.3984, -0.6953, -0.6445, -0.6523],
        [-0.0908,  1.2734, -0.3066, -0.4492, -0.8398]], device='cuda:0',
       dtype=torch.bfloat16, grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None), SequenceClassifierOutput(loss=tensor(0.5649, device='cuda:0', grad_fn=<NllLossBackward0>), logits=tensor([[ 0.3008,  1.7109, -0.5977, -0.5938, -0.5664],
        [ 0.2354,  1.1328, -0.8203, -1.0078, -0.3164],
        [-0.1885,  1.4219, -0.5508, -0.7070, -0.6016],
        [ 0.2812,  1.2500, -0.5977, -0.8164, -0.3828],
        [-0.1895,  1.2188, -0.6523, -0.8164, -0.6523],
        [ 0.1206,  0.8984, -0.5820, -0.3320, -0.5703],
        [-0.1050,  1.6641, -0.4590, -0.8594, -0.6055],
        [ 0.0277,  1.6797, -0.5391, -0.8398, -0.6094],
        [ 0.1953,  1.3594, -0.4512, -0.9180, -0.5938],
        [ 0.1855,  1.2422, -0.5781, -0.9141, -0.5234],
        [ 0.2812,  1.5625, -0.5547, -0.9766, -0.6055],
        [-0.2236,  1.3594, -0.5234, -0.9492, -0.7266],
        [-0.2773,  1.3516, -0.3418, -0.7734, -0.6680],
        [-0.1826,  1.3516, -0.8867, -0.6250, -0.4316],
        [-0.0649,  1.4375, -0.7305, -0.9062, -0.3145],
        [ 0.1162,  1.7578, -0.6719, -0.7227, -0.5938]], device='cuda:0',
       dtype=torch.bfloat16, grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None), SequenceClassifierOutput(loss=tensor(1.0651, device='cuda:0', grad_fn=<NllLossBackward0>), logits=tensor([[-0.1992,  1.4141, -0.7188, -0.8867, -0.9453],
        [ 0.1748,  1.2891, -0.5625, -0.7930, -0.2168],
        [ 0.1216,  1.1094, -0.9805, -0.9961, -0.5391],
        [-0.2520,  1.6328, -0.6250, -0.4375, -0.4707],
        [ 0.1631,  1.7109, -0.2793, -0.9883, -0.3691],
        [-0.1504,  1.0391, -0.4043, -0.8555, -0.7031],
        [ 0.0483,  0.6719, -0.9023, -0.5430, -0.1445],
        [-0.2363,  1.1875, -0.3516, -0.9766, -0.6133],
        [-0.0226,  1.1562, -0.4062, -0.9297, -0.3496],
        [ 0.2500,  1.3125, -0.5938, -0.8242, -0.7188],
        [ 0.0211,  1.5703, -0.4941, -0.9609, -0.6211],
        [ 0.1309,  1.4844, -1.0156, -0.8789, -0.4883],
        [-0.1318,  1.3906, -0.8633, -0.6445, -0.5391],
        [-0.0232,  1.5234, -0.7188, -0.9258, -0.5742],
        [-0.3125,  1.3203, -0.3008, -0.5508, -0.9062],
        [ 0.2344,  1.5547, -0.8086, -0.6445, -0.5430]], device='cuda:0',
       dtype=torch.bfloat16, grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None), SequenceClassifierOutput(loss=tensor(0.8800, device='cuda:0', grad_fn=<NllLossBackward0>), logits=tensor([[ 0.1738,  1.2344, -0.5469, -0.7969, -0.8867],
        [ 0.1484,  1.4297, -0.4121, -0.8281, -0.7930],
        [ 0.0854,  1.4453, -0.7031, -0.9922, -0.5039],
        [ 0.0042,  1.2578, -0.4219, -1.0859, -0.7734],
        [ 0.1758,  1.2500, -0.6094, -1.2031, -0.9180],
        [-0.0850,  1.3750, -0.4844, -0.8203, -0.8203],
        [ 0.0645,  1.0547, -0.6719, -0.7734, -0.3594],
        [ 0.0537,  1.4297, -0.5078, -0.8320, -0.7461],
        [ 0.0027,  1.5547, -0.6914, -0.7266, -0.3516],
        [-0.1016,  1.2812, -0.6523, -0.6797, -0.4980],
        [ 0.2217,  1.5312, -0.6289, -0.7461, -0.5742],
        [-0.0076,  1.3359, -0.8789, -1.0547, -0.5508],
        [-0.2031,  1.3281, -0.6836, -0.9258, -0.3535],
        [ 0.2598,  1.8047, -0.7109, -0.9375, -0.4238],
        [ 0.0476,  1.9062, -0.6172, -0.8438, -0.5234],
        [-0.0190,  1.7109, -0.4414, -0.9961, -0.4355]], device='cuda:0',
       dtype=torch.bfloat16, grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None), SequenceClassifierOutput(loss=tensor(0.9812, device='cuda:0', grad_fn=<NllLossBackward0>), logits=tensor([[-0.3379,  1.2578, -0.1914, -0.7109, -0.6328],
        [-0.1084,  1.3359, -0.0542, -0.8672, -0.5664],
        [ 0.2490,  1.4062, -0.5781, -1.0859, -0.7578],
        [ 0.0879,  1.8438, -0.6406, -0.6562, -0.7969],
        [ 0.3438,  1.3672, -0.3848, -1.1406, -0.7695],
        [-0.2393,  1.5391, -0.5977, -0.7969, -0.3867],
        [ 0.1934,  0.9727, -0.8086, -0.7539, -0.2520],
        [-0.0134,  1.6719, -0.6406, -0.7852, -0.6797],
        [ 0.1758,  1.4531, -0.5234, -0.5938, -0.3789],
        [ 0.1963,  1.2188, -0.6328, -0.3301, -0.6367],
        [ 0.0986,  0.9531, -0.6133, -0.7500, -0.5391],
        [ 0.0157,  1.5703, -0.4551, -0.9727, -0.6172],
        [-0.3398,  1.2969, -0.7383, -0.4473, -0.2373],
        [-0.0221,  1.4531, -0.8711, -1.0234, -0.3555],
        [ 0.1973,  1.5938, -0.6836, -1.1875, -0.5898],
        [-0.0442,  1.6328, -0.6523, -0.8359, -0.5586]], device='cuda:0',
       dtype=torch.bfloat16, grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None), SequenceClassifierOutput(loss=tensor(0.8121, device='cuda:0', grad_fn=<NllLossBackward0>), logits=tensor([[ 0.0337,  1.3203, -0.4160, -0.4316, -0.1865],
        [ 0.0747,  1.4375, -0.6250, -0.9492, -0.3223],
        [ 0.1875,  1.4062, -0.4062, -0.9180, -1.1484],
        [ 0.2393,  1.2344, -1.0078, -1.0938, -0.3652],
        [ 0.4199,  0.5078, -0.9414, -0.6250,  0.3730],
        [ 0.3105,  1.3516, -0.7344, -0.9180, -0.5391],
        [ 0.1445,  1.7344, -0.5586, -1.2891, -0.7500],
        [-0.1162,  1.4766, -0.4590, -0.8438, -0.3262],
        [-0.1816,  1.1953, -0.2949, -0.7617, -1.3516],
        [-0.0669,  1.4219, -0.3711, -0.8047, -0.4668],
        [-0.0796,  1.3672, -0.6406, -0.7734, -0.2773],
        [-0.0918,  1.5391, -0.6680, -1.0156, -0.2539],
        [-0.0320,  1.4062, -0.8750, -0.8633, -0.6094],
        [-0.0898,  1.3203, -0.5820, -0.8750, -0.6133],
        [-0.1992,  1.5703, -0.6445, -0.8984, -0.8086],
        [ 0.1924,  1.5469, -0.7266, -1.1484, -0.9102]], device='cuda:0',
       dtype=torch.bfloat16, grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None), SequenceClassifierOutput(loss=tensor(0.6798, device='cuda:0', grad_fn=<NllLossBackward0>), logits=tensor([[-0.3516,  1.3125, -0.5234, -0.6367, -0.5352],
        [-0.0840,  1.6953, -0.7422, -0.7188, -0.6133],
        [ 0.3125,  1.3047, -0.7656, -0.8633, -0.4336],
        [ 0.0231,  1.3984, -0.5547, -0.5117, -0.3984],
        [-0.0162,  1.2734, -0.3945, -0.8711, -0.4629],
        [ 0.2314,  1.4297, -0.6758, -0.6836, -0.6172],
        [-0.0349,  1.3438, -0.5430, -0.6719, -0.6953],
        [-0.0923,  1.7656, -0.6445, -0.9336, -0.4883],
        [-0.3750,  1.3984, -0.6289, -0.8555, -0.6562],
        [ 0.0388,  1.5078, -0.5859, -1.0547, -0.6719],
        [ 0.2578,  1.7109, -0.5312, -0.9141, -0.3730],
        [ 0.3398,  1.3594, -0.5859, -0.7773, -0.3750],
        [ 0.0654,  1.3594, -0.7891, -0.9648, -0.4961],
        [ 0.0786,  1.8906, -0.3535, -0.9492, -0.5195],
        [ 0.2314,  1.6953, -0.5625, -0.7227, -0.4219],
        [-0.1206,  1.6172, -0.7969, -1.2812, -0.4766]], device='cuda:0',
       dtype=torch.bfloat16, grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None), SequenceClassifierOutput(loss=tensor(0.8905, device='cuda:0', grad_fn=<NllLossBackward0>), logits=tensor([[-5.1953e-01,  1.5547e+00, -6.4844e-01, -7.1484e-01, -5.0000e-01],
        [ 1.5234e-01,  1.2578e+00, -8.6328e-01, -8.2812e-01, -4.6680e-01],
        [ 2.1582e-01,  1.2734e+00, -1.0703e+00, -1.1094e+00, -5.9766e-01],
        [ 1.8921e-02,  1.3984e+00, -5.0781e-01, -8.9453e-01, -6.5625e-01],
        [-1.2891e-01,  1.3906e+00, -1.8652e-01, -7.1484e-01, -5.0000e-01],
        [ 8.4961e-02,  1.2578e+00, -4.6387e-02, -8.7891e-01, -1.0625e+00],
        [ 1.9629e-01,  1.4219e+00, -9.2578e-01, -8.9062e-01, -4.2773e-01],
        [-1.1572e-01,  1.2578e+00, -4.6094e-01, -6.6797e-01, -2.7148e-01],
        [-8.5449e-02,  1.3672e+00, -4.9609e-01, -5.8594e-01, -2.8906e-01],
        [ 1.0693e-01,  1.7500e+00, -5.5469e-01, -9.3359e-01, -7.0703e-01],
        [-1.5182e-03,  1.3516e+00, -7.7344e-01, -7.9688e-01, -4.6289e-01],
        [-3.8672e-01,  1.6094e+00, -4.4727e-01, -7.5781e-01, -5.7031e-01],
        [ 2.8711e-01,  1.1484e+00, -6.1328e-01, -8.1641e-01, -9.1797e-01],
        [ 1.8164e-01,  1.2812e+00, -6.0156e-01, -9.9609e-01, -6.8359e-01],
        [ 1.5723e-01,  1.2109e+00, -9.7656e-01, -1.1641e+00, -5.8838e-02],
        [ 1.0547e-01,  1.2031e+00, -7.5000e-01, -8.0469e-01, -7.5781e-01]],
       device='cuda:0', dtype=torch.bfloat16, grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None), SequenceClassifierOutput(loss=tensor(0.6039, device='cuda:0', grad_fn=<NllLossBackward0>), logits=tensor([[-0.1523,  1.4922, -0.7422, -1.2969, -0.5273],
        [ 0.1660,  1.3438, -0.7500, -0.8359, -0.3906],
        [-0.0067,  1.6328, -0.7734, -1.1797, -0.4746],
        [-0.0425,  1.9453, -0.8320, -1.1719, -0.5117],
        [ 0.4141,  1.5859, -0.8789, -1.0547, -0.4863],
        [-0.0630,  1.5391, -0.4883, -1.0234, -0.6367],
        [-0.0356,  1.7109, -0.7422, -0.9102, -0.6055],
        [-0.1719,  1.5781, -0.4180, -0.7656, -0.7344],
        [-0.1504,  1.5234, -0.2295, -0.8828, -0.6875],
        [-0.0859,  1.5938, -0.5664, -0.7891, -0.6367],
        [ 0.2109,  1.2266, -0.8164, -0.9102, -0.5195],
        [-0.0908,  1.2891, -0.4512, -0.7344, -0.8984],
        [-0.0223,  0.9688, -0.5156, -0.8672, -0.9961],
        [ 0.1914,  1.7734, -0.6445, -0.9883, -0.6328],
        [ 0.5352,  1.6797, -0.7656, -0.7461, -0.5625],
        [-0.1279,  1.5781, -0.9141, -0.9727, -0.4062]], device='cuda:0',
       dtype=torch.bfloat16, grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None), SequenceClassifierOutput(loss=tensor(0.7213, device='cuda:0', grad_fn=<NllLossBackward0>), logits=tensor([[-0.1069,  1.2969, -0.5391, -1.1016, -0.7344],
        [-0.1562,  1.4922, -0.4902, -0.8398, -0.5352],
        [-0.0206,  1.2031, -0.3926, -0.8438, -0.6250],
        [ 0.0134,  1.6484, -0.3223, -0.6211, -0.4766],
        [ 0.1377,  1.3594, -0.6406, -0.7930, -0.5898],
        [ 0.3750,  0.8867, -0.7773, -0.9492, -0.2256],
        [ 0.0986,  1.6641, -0.7734, -1.2500, -0.5195],
        [ 0.2256,  1.5078, -0.4121, -0.9180, -0.6758],
        [ 0.0957,  1.1875, -0.5781, -0.9844, -0.4629],
        [ 0.0884,  0.8945, -0.2305, -0.4727, -0.5586],
        [-0.0496,  1.4375, -0.3125, -1.0000, -0.3066],
        [-0.2148,  1.4922, -0.8633, -1.0469, -0.5195],
        [ 0.6562,  0.3555, -1.1484, -0.6445,  0.4199],
        [-0.1309,  1.2266, -0.5820, -0.7344, -0.5977],
        [ 0.1138,  1.6641, -0.3281, -0.7969, -0.6836],
        [-0.0103,  1.4844, -0.5273, -1.0781, -0.7969]], device='cuda:0',
       dtype=torch.bfloat16, grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None), SequenceClassifierOutput(loss=tensor(0.7565, device='cuda:0', grad_fn=<NllLossBackward0>), logits=tensor([[ 0.1641,  1.1406, -0.9492, -0.5859, -0.1455],
        [ 0.0139,  1.6406, -0.2363, -1.2891, -0.4238],
        [ 0.0238,  1.6250, -0.5820, -0.9688, -0.6953],
        [-0.0923,  1.2812, -0.3242, -0.9805, -0.4043],
        [ 0.0613,  1.3672, -0.8047, -0.9023, -0.4648],
        [-0.0369,  1.5547, -0.5859, -0.9609, -0.6875],
        [ 0.1562,  1.3594, -0.6758, -1.1719, -0.5586],
        [-0.0767,  1.4688, -0.6562, -1.2109, -0.5078],
        [ 0.0972,  1.7422, -0.2158, -1.1406, -0.4551],
        [ 0.1357,  1.1719, -0.3984, -0.8789, -0.7539],
        [ 0.0442,  1.5000, -0.6289, -0.9766, -0.5195],
        [ 0.3066,  1.2812, -0.6250, -0.9883, -0.4375],
        [-0.1768,  1.1406, -0.7148, -0.7695, -0.9883],
        [ 0.1504,  1.4922, -0.3262, -0.8984, -0.5703],
        [-0.2734,  1.6641, -0.4727, -1.2109, -0.4570],
        [ 0.0054,  1.2578, -0.5938, -0.9453, -0.3750]], device='cuda:0',
       dtype=torch.bfloat16, grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None), SequenceClassifierOutput(loss=tensor(1.1124, device='cuda:0', grad_fn=<NllLossBackward0>), logits=tensor([[-0.0197,  1.3984, -0.4824, -0.8398, -0.8945],
        [ 0.0176,  1.4531, -0.9375, -0.9648, -0.5039],
        [ 0.0226,  1.6016, -0.7109, -0.7305, -0.6094],
        [-0.1523,  1.4453, -0.5859, -0.8438, -0.3340],
        [-0.2041,  1.1562, -0.4609, -0.9727, -0.4707],
        [-0.1758,  1.5781, -0.5469, -0.6719, -0.3301],
        [-0.2275,  1.6172, -0.6406, -0.8945, -0.5547],
        [ 0.1562,  1.2422, -0.1924, -0.7930, -0.7188],
        [ 0.0613,  1.6328, -0.5078, -0.8086, -0.8789],
        [ 0.3145,  1.7188, -0.5938, -1.1953, -0.4277],
        [ 0.1250,  1.7344, -0.7734, -0.8711, -0.5547],
        [ 0.0525,  1.3203, -0.7656, -0.8125, -0.6484],
        [ 0.1270,  1.6094, -0.8672, -0.9805, -0.4883],
        [ 0.0120,  1.4453, -0.4355, -0.3438, -0.4629],
        [ 0.3281,  1.2891, -0.2178, -0.3398, -0.4688],
        [-0.2236,  1.4609, -0.5820, -0.7812, -0.2480]], device='cuda:0',
       dtype=torch.bfloat16, grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None), SequenceClassifierOutput(loss=tensor(0.6515, device='cuda:0', grad_fn=<NllLossBackward0>), logits=tensor([[ 0.3125,  1.1562, -0.3379, -0.8711, -0.4746],
        [ 0.1006,  1.5000, -0.6172, -1.0156, -0.5547],
        [-0.1445,  1.5391, -0.5859, -1.0156, -0.4668],
        [ 0.2393,  1.2969, -0.5195, -0.8398, -0.4551],
        [ 0.0442,  1.4609, -0.6602, -0.9922, -0.5820],
        [ 0.1631,  1.0469, -0.6836, -0.6953, -0.5508],
        [ 0.1050,  1.4453, -0.5625, -0.5938, -0.4688],
        [-0.0977,  1.6719, -0.3809, -0.8008, -0.5430],
        [ 0.1328,  1.5547, -0.9414, -1.0625, -0.5273],
        [-0.0051,  1.4766, -0.6016, -1.0312, -0.7578],
        [ 0.2002,  1.6719, -0.9297, -1.1094, -0.4512],
        [ 0.3730,  1.4531, -0.7891, -1.0078, -0.7891],
        [-0.0728,  1.4062, -0.4160, -1.0703, -0.6367],
        [-0.0991,  1.5312, -0.4902, -0.9531, -0.6953],
        [-0.0071,  1.3516, -0.6445, -1.0078, -0.3652],
        [ 0.1260,  0.4043, -1.1719, -0.6445,  0.1895]], device='cuda:0',
       dtype=torch.bfloat16, grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None), SequenceClassifierOutput(loss=tensor(0.8547, device='cuda:0', grad_fn=<NllLossBackward0>), logits=tensor([[-0.0320,  1.3516, -0.9492, -0.6758, -0.5938],
        [-0.1113,  1.8750, -0.8203, -0.9805, -0.5977],
        [ 0.0659,  1.1953, -0.4238, -0.8555, -0.2832],
        [ 0.4199,  1.4453, -0.7617, -0.9805, -0.3262],
        [ 0.1621,  1.6953, -0.3965, -0.7031, -0.7500],
        [ 0.1309,  1.6875, -0.5469, -0.9414, -0.6641],
        [ 0.1680,  1.8281, -0.5820, -0.8828, -0.4219],
        [ 0.0618,  1.4297, -0.4922, -0.7539, -0.5781],
        [ 0.0767,  1.6953, -0.4043, -0.7305, -0.5625],
        [ 0.1260,  0.8008, -0.6562, -0.3926, -0.6250],
        [-0.4277,  1.3984, -0.6562, -0.6523, -0.2754],
        [ 0.2373,  1.5391, -0.5586, -1.2344, -0.4199],
        [-0.0557,  1.2109, -0.6719, -0.8633, -0.6367],
        [-0.0361,  1.6953, -0.7031, -1.0000, -0.5664],
        [ 0.2393,  1.1953, -0.5508, -0.5859, -0.4805],
        [ 0.2891,  1.6406, -0.8672, -0.8945, -0.5820]], device='cuda:0',
       dtype=torch.bfloat16, grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None), SequenceClassifierOutput(loss=tensor(0.6168, device='cuda:0', grad_fn=<NllLossBackward0>), logits=tensor([[ 1.6309e-01,  1.4141e+00, -8.7109e-01, -9.0234e-01, -8.5449e-02],
        [-1.6113e-01,  1.7266e+00, -5.5469e-01, -6.8750e-01, -6.2109e-01],
        [-2.4121e-01,  1.4531e+00, -5.1562e-01, -6.6406e-01, -7.6953e-01],
        [-4.5166e-02,  1.5703e+00, -4.7266e-01, -1.0234e+00, -5.5469e-01],
        [-9.7656e-02,  1.6484e+00, -1.0078e+00, -1.0234e+00, -4.8828e-01],
        [ 9.1797e-02,  1.5391e+00, -2.3047e-01, -8.9062e-01, -4.0039e-01],
        [-1.5527e-01,  1.4766e+00, -6.6016e-01, -8.7891e-01, -7.8125e-01],
        [ 2.5879e-02,  1.3438e+00, -4.8047e-01, -8.1250e-01, -6.5234e-01],
        [ 3.0273e-01,  1.6562e+00, -8.4766e-01, -9.5703e-01, -4.5898e-01],
        [ 5.1025e-02,  1.3594e+00, -6.0938e-01, -8.6328e-01, -6.7188e-01],
        [ 1.4420e-03,  1.5312e+00, -7.4609e-01, -6.9141e-01, -4.2969e-01],
        [-1.2451e-02,  1.6406e+00, -6.4062e-01, -1.0156e+00, -2.2168e-01],
        [-1.5527e-01,  1.8516e+00, -5.5078e-01, -9.9219e-01, -6.4453e-01],
        [ 2.3926e-02,  1.2109e+00, -8.5547e-01, -8.6328e-01, -4.7070e-01],
        [ 9.6191e-02,  1.5234e+00, -8.1641e-01, -1.1016e+00, -6.4844e-01],
        [ 1.0400e-01,  1.1719e+00, -8.1250e-01, -1.0078e+00, -5.3906e-01]],
       device='cuda:0', dtype=torch.bfloat16, grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None), SequenceClassifierOutput(loss=tensor(0.7092, device='cuda:0', grad_fn=<NllLossBackward0>), logits=tensor([[ 0.1807,  1.4844, -0.7695, -0.8242, -0.5547],
        [ 0.0576,  1.3516, -0.5352, -0.8164, -1.0234],
        [-0.0162,  1.9062, -0.4453, -0.7852, -0.1416],
        [ 0.2520,  1.9922, -0.6797, -1.0469, -0.7461],
        [-0.0603,  1.4219, -0.5508, -0.8477, -0.5625],
        [ 0.1514,  1.5547, -0.6797, -0.5117, -0.4336],
        [-0.2188,  1.3750, -0.7539, -0.9609, -0.5977],
        [ 0.1836,  1.5000, -0.8008, -1.0156, -0.3223],
        [ 0.2520,  1.4609, -0.8711, -0.6562, -0.2158],
        [ 0.1338,  1.1953, -0.7812, -0.6602, -0.0850],
        [-0.0996,  1.7031, -0.6758, -1.1406, -0.5625],
        [-0.1992,  1.8047, -0.8828, -0.7500, -0.5469],
        [-0.2930,  1.1562, -0.7930, -1.1016, -0.6445],
        [ 0.0640,  1.8281, -0.8359, -1.0391, -0.5078],
        [-0.0752,  1.5703, -0.7031, -0.7305, -0.5352],
        [ 0.4453,  1.6250, -0.9102, -0.5703, -0.6250]], device='cuda:0',
       dtype=torch.bfloat16, grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None), SequenceClassifierOutput(loss=tensor(0.7220, device='cuda:0', grad_fn=<NllLossBackward0>), logits=tensor([[ 0.2695,  1.4531, -0.7695, -0.6797, -0.8242],
        [ 0.3828,  1.5156, -0.8711, -1.1562, -0.4805],
        [-0.2373,  1.1406, -0.6836, -0.7148, -0.7109],
        [ 0.0094,  1.2969, -0.7500, -0.8750, -0.6016],
        [-0.2969,  1.5703, -0.2080, -0.5156, -0.5352],
        [ 0.0928,  1.5703, -0.8203, -0.8164, -0.6016],
        [-0.2168,  1.1172, -0.6211, -0.8477, -0.3477],
        [-0.2695,  1.0938, -0.6133, -1.0391, -0.4863],
        [ 0.1201,  1.2422, -0.8398, -0.8438, -0.3887],
        [ 0.3555,  1.4922, -0.3633, -1.0391, -0.8984],
        [-0.0444,  1.7109, -0.6289, -1.0703, -0.3203],
        [ 0.2500,  1.3750, -0.3145, -0.9102, -0.8672],
        [-0.0139,  1.0469, -0.3027, -0.9570, -1.0469],
        [ 0.1338,  1.3906, -0.4102, -0.8203, -0.5625],
        [ 0.1738,  1.5781, -0.6367, -1.2656, -0.4082],
        [-0.0024,  1.4297, -0.5625, -0.8906, -0.6523]], device='cuda:0',
       dtype=torch.bfloat16, grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None), SequenceClassifierOutput(loss=tensor(0.8101, device='cuda:0', grad_fn=<NllLossBackward0>), logits=tensor([[ 0.0776,  1.5234, -0.2500, -0.9570, -0.7539],
        [-0.0361,  1.3125, -0.8594, -0.8672, -0.3555],
        [ 0.0933,  1.5000, -0.6875, -0.4355, -0.7109],
        [-0.1924,  1.8359, -0.7422, -0.9453, -0.4062],
        [-0.0270,  1.6016, -0.7031, -0.8984, -0.6211],
        [ 0.1318,  1.4688, -0.3906, -0.5469, -0.4512],
        [ 0.1177,  1.4766, -0.8125, -0.8242, -0.4590],
        [ 0.4648,  1.2500, -0.8086, -1.0156, -0.4590],
        [ 0.0469,  1.3984, -0.4785, -0.6523, -0.6992],
        [ 0.0854,  1.0469, -1.0625, -0.9727, -0.2354],
        [-0.0254,  1.4766, -0.5703, -0.7734, -0.4355],
        [-0.1934,  1.3984, -0.4785, -1.0625, -0.6641],
        [ 0.0273,  1.3672, -0.5742, -0.8555, -0.4922],
        [ 0.0698,  1.7812, -0.7031, -0.7188, -0.5586],
        [-0.0130,  1.4922, -0.7734, -0.8281, -0.4980],
        [ 0.0143,  1.7109, -0.7656, -1.0391, -0.5781]], device='cuda:0',
       dtype=torch.bfloat16, grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None), SequenceClassifierOutput(loss=tensor(0.7469, device='cuda:0', grad_fn=<NllLossBackward0>), logits=tensor([[-0.0913,  1.6406, -0.7305, -0.8945, -0.3691],
        [-0.1484,  1.5859, -0.5078, -0.8906, -0.5742],
        [-0.1357,  1.7266, -0.6484, -1.0469, -0.6445],
        [-0.0728,  1.6641, -0.7617, -0.9102, -0.3594],
        [ 0.1191,  1.0156, -0.7109, -0.9961, -0.0043],
        [-0.1475,  1.8125, -0.7227, -0.6562, -0.6172],
        [ 0.0781,  1.5391, -0.4844, -1.0781, -0.6406],
        [ 0.1963,  1.5859, -0.6211, -0.7188, -0.4688],
        [-0.2178,  1.4688, -0.5820, -0.8086, -0.3711],
        [-0.2891,  1.5703, -0.3379, -1.0156, -0.5898],
        [-0.0879,  1.3281, -0.5508, -0.8320, -0.6250],
        [ 0.0162,  1.5312, -0.5625, -0.9023, -0.5352],
        [-0.0269,  1.4922, -0.5664, -0.9883, -0.5742],
        [-0.1787,  1.6406, -0.6406, -0.9023, -0.8008],
        [-0.4531,  1.2578, -0.4258, -0.7852, -0.6680],
        [-0.1660,  1.5469, -0.8203, -1.0781, -0.6289]], device='cuda:0',
       dtype=torch.bfloat16, grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None), SequenceClassifierOutput(loss=tensor(0.7238, device='cuda:0', grad_fn=<NllLossBackward0>), logits=tensor([[ 0.1230,  1.7422, -0.5898, -0.8945, -0.5469],
        [-0.0317,  1.5391, -0.4980, -0.9766, -0.4395],
        [-0.0432,  1.3594, -0.5273, -0.9844, -0.5898],
        [ 0.3008,  1.8438, -0.3086, -0.7227, -0.6211],
        [-0.2637,  1.7109, -0.4941, -1.1250, -0.8828],
        [ 0.0884,  1.4141, -0.8164, -0.8203, -0.7070],
        [-0.1523,  1.4688, -0.7383, -0.7695, -0.6484]], device='cuda:0',
       dtype=torch.bfloat16, grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)], [SequenceClassifierOutput(loss=tensor(2.2559, device='cuda:0', grad_fn=<NllLossBackward0>), logits=tensor([[ 0.1157,  1.3984, -0.8164, -0.9336, -0.5273],
        [ 0.3867,  1.2422, -0.8047, -1.0000, -0.3555],
        [-0.2178,  1.4766, -0.8125, -0.8242, -0.6133],
        [ 0.0342,  1.1797, -0.7227, -0.8008, -0.8633],
        [ 0.1973,  1.0234, -0.8438, -0.5898, -0.1445],
        [ 0.1260,  1.1016, -0.6602, -0.5625, -0.6133],
        [ 0.2373,  1.4531, -0.7578, -0.9336, -0.6445],
        [ 0.4648,  0.9531, -0.6484, -1.0547, -0.5312],
        [ 0.0938,  1.3516, -0.5234, -0.7812, -0.5508],
        [-0.0413,  1.3750, -0.4258, -0.7734, -0.5000],
        [-0.0601,  0.6719, -0.6445, -0.5195, -0.0840],
        [-0.1611,  1.2812, -0.2373, -0.3320, -0.3203],
        [ 0.1226,  1.2422, -0.8281, -0.7344, -0.3047],
        [-0.1396,  0.9102, -0.3594, -0.5977, -0.9648],
        [ 0.0679,  1.1484, -0.8008, -0.5625, -0.8125],
        [ 0.2617,  1.1875, -0.7188, -0.5039, -0.5312]], device='cuda:0',
       dtype=torch.bfloat16, grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None), SequenceClassifierOutput(loss=tensor(2.3657, device='cuda:0', grad_fn=<NllLossBackward0>), logits=tensor([[ 0.0065,  1.5859, -0.4961, -0.7656, -0.8125],
        [ 0.0469,  1.6250, -0.4863, -0.7422, -0.2539],
        [ 0.3477,  1.4766, -0.6055, -0.4453, -0.5586],
        [-0.0056,  0.9570, -0.6992, -0.8281, -0.6172],
        [ 0.2119,  1.4062, -0.6016, -0.6172, -0.6523],
        [ 0.0850,  1.1406, -0.6289, -0.7188, -0.3691],
        [ 0.3242,  1.6719, -0.8008, -1.0469, -0.6523],
        [-0.1738,  1.0547, -0.4082, -0.8828, -0.6523],
        [-0.1328,  1.3281, -0.5703, -1.1406, -0.6641],
        [ 0.1177,  1.2969, -0.5547, -1.0547, -0.7656],
        [-0.3535,  1.4609, -0.4922, -0.8750, -0.5312],
        [-0.2480,  1.2734, -0.7070, -0.8555, -0.6836],
        [ 0.4824,  0.9688, -0.8633, -0.7422,  0.3164],
        [ 0.4141,  0.9609, -1.3828, -0.8594, -0.3242],
        [ 0.1064,  0.9492, -0.3809, -0.3906,  0.0249],
        [ 0.1069,  1.0234, -0.6914, -0.7227, -0.6680]], device='cuda:0',
       dtype=torch.bfloat16, grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None), SequenceClassifierOutput(loss=tensor(2.0073, device='cuda:0', grad_fn=<NllLossBackward0>), logits=tensor([[ 0.3262,  1.0312, -0.8359, -0.5469, -0.5195],
        [-0.1885,  0.7695, -0.4141, -0.5547, -0.5469],
        [ 0.2578,  0.9414, -0.8906, -1.0859,  0.0654],
        [-0.2334,  1.5312, -0.6562, -0.7266, -0.4785],
        [ 0.0334,  1.4141, -1.0469, -0.9570, -0.2490],
        [ 0.3789,  0.3340, -0.8672, -0.8398,  0.2266],
        [ 0.1826,  0.9453, -0.5234, -0.5312, -0.6758],
        [ 0.2949,  1.5859, -0.8164, -0.5234, -0.3828],
        [-0.2871,  0.9805, -0.8594, -0.2734, -0.2734],
        [ 0.0048,  1.4609, -0.3789, -0.7539, -0.7656],
        [ 0.2490,  1.5312, -0.9961, -0.7617, -0.6367],
        [ 0.2676,  0.7148, -1.2734, -0.3379,  0.2812],
        [-0.0767,  1.0469, -0.5234, -0.4023, -0.2734],
        [ 0.0034,  1.4219, -0.8984, -1.0000, -0.0464],
        [-0.1348,  1.3906, -0.7070, -0.8281, -0.4902],
        [ 0.2949,  0.9414, -0.9102, -1.2422,  0.2676]], device='cuda:0',
       dtype=torch.bfloat16, grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None), SequenceClassifierOutput(loss=tensor(2.3105, device='cuda:0', grad_fn=<NllLossBackward0>), logits=tensor([[ 0.2832,  1.6484, -0.7695, -1.0078, -0.3887],
        [ 0.0679,  1.5312, -0.5195, -0.7656, -0.5312],
        [ 0.1514,  1.1172, -0.1318, -0.4551, -0.2871],
        [ 0.2334,  1.1250, -0.8828, -0.7227, -0.3965],
        [ 0.0317,  0.9609, -1.2500, -0.8984, -0.4941],
        [ 0.4863,  1.5078, -0.8516, -0.9961, -0.3379],
        [ 0.0649,  1.3438, -0.8125, -0.9258, -0.2021],
        [ 0.1309,  1.1016, -0.2949, -0.6562, -0.2539],
        [ 0.2002,  1.1016, -0.9492, -0.9727, -0.2100],
        [ 0.4238,  1.3828, -0.5312, -0.5039, -0.4668],
        [ 0.2490,  1.5078, -0.9180, -1.0469, -0.8750],
        [-0.0859,  1.4922, -1.0938, -1.0547, -0.6094],
        [-0.1235,  1.3984, -0.5586, -0.7695, -0.3203],
        [-0.0552,  1.3828, -0.8672, -0.5859, -0.3555],
        [ 0.3711,  1.2188, -0.4434, -0.5391, -1.0391],
        [ 0.3945,  1.2969, -0.8516, -0.8164, -0.6484]], device='cuda:0',
       dtype=torch.bfloat16, grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None), SequenceClassifierOutput(loss=tensor(2.5293, device='cuda:0', grad_fn=<NllLossBackward0>), logits=tensor([[ 0.0894,  1.6406, -0.8164, -0.9297, -0.5430],
        [ 0.2832,  1.6094, -0.6055, -0.9688, -0.5977],
        [ 0.0591,  1.6328, -0.8633, -0.7031, -0.4180],
        [ 0.1436,  1.6641, -0.6914, -0.8203, -0.2432],
        [ 0.0034,  1.2500, -0.6680, -0.9766, -0.7422],
        [ 0.8633,  0.6250, -0.7539, -0.0466,  0.5195],
        [-0.0136,  1.4453, -0.5156, -0.9922, -0.7188],
        [ 0.5195,  0.6133, -1.0469, -0.6328,  0.2119],
        [ 0.4316,  1.1953, -1.0156, -0.9219, -0.2090],
        [ 0.0610,  1.5156, -0.7148, -0.8828, -0.2480],
        [ 0.0109,  1.2266, -0.8555, -0.9766, -0.8203],
        [-0.0128,  0.9922, -0.3613, -0.8633, -0.4824],
        [ 0.0366,  1.1875, -0.4395, -0.6289, -0.6172],
        [ 0.1099,  1.3203, -0.3633, -0.7695, -0.3711],
        [ 0.1279,  1.0312, -1.0000, -0.6797, -0.4551],
        [ 0.0466,  1.3828, -0.6914, -1.0547, -0.4980]], device='cuda:0',
       dtype=torch.bfloat16, grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None), SequenceClassifierOutput(loss=tensor(1.3447, device='cuda:0', grad_fn=<NllLossBackward0>), logits=tensor([[-0.1299,  1.3359, -0.7227, -0.6758, -0.7812],
        [ 0.0229,  1.6016, -0.3789, -0.8906, -0.2100],
        [ 0.2969,  1.6250, -0.6719, -0.9258, -0.4980],
        [-0.1621,  1.5469, -0.5078, -0.7188, -0.6094],
        [ 0.0256,  1.3438, -0.7656, -0.6992, -0.5508],
        [ 0.0669,  1.3047, -0.9492, -0.8945, -0.2988],
        [-0.0605,  1.1562, -0.3613, -0.7500, -0.6094],
        [-0.3613,  1.2812, -0.1719, -0.9023, -0.9570],
        [-0.0879,  1.6484, -0.3770, -1.0156, -1.0312],
        [ 0.0879,  1.4375, -1.0000, -0.8086, -0.7070],
        [ 0.0564,  1.5938, -0.5781, -0.7305, -1.0547],
        [-0.0522,  1.3203, -0.6914, -1.0547, -0.5078],
        [ 0.1523,  1.5078, -0.3711, -0.8867, -0.3477],
        [ 0.1699,  1.1797, -0.6562, -1.0625, -0.8828],
        [ 0.1855,  1.3672, -0.4473, -1.0391, -0.5664],
        [-0.0679,  1.3125, -0.5820, -1.1016, -0.5352]], device='cuda:0',
       dtype=torch.bfloat16, grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None), SequenceClassifierOutput(loss=tensor(1.0846, device='cuda:0', grad_fn=<NllLossBackward0>), logits=tensor([[ 0.3281,  1.4922, -1.1172, -0.7617, -0.1895],
        [-0.3105,  1.4609, -0.3301, -1.0000, -0.9297],
        [-0.0311,  1.1562, -0.5781, -1.0625, -0.4141],
        [ 0.0302,  1.5312, -0.5078, -1.1250, -0.9062],
        [ 0.4473,  1.1328, -0.6406, -0.9336, -0.7344],
        [-0.2363,  1.4688, -0.5039, -0.7344, -0.7539],
        [-0.1748,  1.6328, -0.4863, -0.6328, -0.5742],
        [-0.1445,  1.4609, -0.3086, -0.6016, -0.8633],
        [ 0.0669,  1.0938, -0.4688, -0.5195, -0.7266],
        [-0.0322,  1.5078, -0.3535, -0.9375, -0.5234],
        [ 0.1069,  1.3281, -0.5820, -0.7109, -0.7344],
        [-0.2754,  0.9375, -0.7539, -1.0078, -0.9258],
        [ 0.1299,  1.2422, -0.5547, -1.1016, -0.8164],
        [-0.0850,  1.5781, -0.3320, -0.7812, -0.4141],
        [-0.0623,  1.0938, -0.7109, -1.2031, -0.4551],
        [-0.1680,  1.3672, -0.2637, -0.6680, -0.5898]], device='cuda:0',
       dtype=torch.bfloat16, grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None), SequenceClassifierOutput(loss=tensor(0.5804, device='cuda:0', grad_fn=<NllLossBackward0>), logits=tensor([[-0.0645,  1.4219, -0.2676, -1.0469, -0.6836],
        [ 0.2119,  1.2812, -0.3594, -0.9531, -0.8359],
        [-0.1533,  1.4922, -0.4609, -0.9727, -0.6602],
        [ 0.0981,  1.8672, -0.6836, -0.4492, -0.7539],
        [ 0.0107,  1.5000, -0.3203, -0.6992, -0.6562],
        [ 0.2314,  1.2188, -0.6445, -0.9141, -0.4258],
        [-0.0854,  1.2656, -0.4414, -0.8281, -0.5742],
        [-0.3281,  1.2812, -0.1875, -1.0391, -0.5156],
        [ 0.1572,  1.4766, -0.6328, -0.9180, -0.6406],
        [-0.3340,  1.4531, -0.4219, -0.7969, -1.0625],
        [ 0.1777,  1.2734, -0.5898, -1.0703, -0.6406],
        [ 0.1504,  1.2344, -0.5898, -0.6641, -0.5391],
        [-0.2383,  1.7812, -0.5781, -0.8359, -1.0703],
        [ 0.0027,  1.1172, -0.6523, -0.6133, -0.8203],
        [-0.1826,  1.3203, -0.4277, -0.7578, -1.0625],
        [-0.0198,  1.2578, -0.7148, -0.7266, -0.6289]], device='cuda:0',
       dtype=torch.bfloat16, grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None), SequenceClassifierOutput(loss=tensor(0.6691, device='cuda:0', grad_fn=<NllLossBackward0>), logits=tensor([[-0.4062,  1.3359, -0.3789, -1.0859, -0.8828],
        [ 0.1357,  1.3125, -0.4668, -0.9648, -0.2461],
        [-0.0110,  1.7734, -0.1924, -0.8477, -0.2715],
        [-0.0371,  1.2812, -0.5742, -1.0078, -0.9961],
        [-0.0349,  0.9805, -0.3281, -0.7383, -0.8867],
        [ 0.0549,  1.3125, -0.4082, -1.0938, -0.9570],
        [ 0.0415,  1.5000, -0.2236, -0.9102, -0.6250],
        [ 0.0894,  1.3594, -0.5273, -0.8906, -0.8438],
        [-0.0175,  1.4062, -0.2148, -0.8945, -0.9297],
        [-0.0192,  1.1484, -0.7656, -1.0234, -0.8594],
        [-0.1367,  1.5938, -0.5703, -0.9336, -0.3984],
        [-0.0527,  1.4766, -0.4023, -0.4609, -0.9883],
        [-0.3379,  1.3750, -0.6094, -0.7734, -0.4727],
        [-0.1641,  1.3750, -0.1504, -0.7656, -0.6289],
        [ 0.0508,  1.2891, -0.4082, -0.8633, -0.5508],
        [ 0.0762,  1.4141, -1.0078, -1.2188, -0.3555]], device='cuda:0',
       dtype=torch.bfloat16, grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None), SequenceClassifierOutput(loss=tensor(0.7083, device='cuda:0', grad_fn=<NllLossBackward0>), logits=tensor([[-0.2236,  1.5703, -0.5352, -1.0000, -0.7539],
        [ 0.1050,  1.3125, -0.4707, -0.8789, -0.7461],
        [ 0.3672,  1.4141, -0.7422, -0.6523, -0.4707],
        [ 0.0718,  1.0781, -0.7969, -0.9102, -0.6797],
        [ 0.2598,  1.6250, -0.4375, -0.9219, -0.6641],
        [ 0.1221,  1.3828, -0.4844, -0.8867, -0.8320],
        [-0.0193,  1.5000, -0.6094, -0.9414, -0.5781],
        [-0.1416,  1.6250, -0.8125, -0.7656, -0.3594],
        [-0.0242,  1.2109, -0.4492, -0.7227, -0.8398],
        [ 0.1309,  1.0781, -0.6758, -0.6875, -1.0781],
        [-0.2695,  1.2266, -0.5430, -0.8125, -0.6445],
        [ 0.0107,  1.5469, -0.4043, -0.7344, -0.5859],
        [ 0.0742,  1.2344, -0.5820, -0.8945, -0.5391],
        [ 0.0437,  1.8047, -0.7500, -1.0547, -0.5000],
        [-0.1045,  1.3672, -0.9883, -0.6328, -0.5273],
        [ 0.0035,  1.3203, -0.4355, -1.0234, -0.6133]], device='cuda:0',
       dtype=torch.bfloat16, grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None), SequenceClassifierOutput(loss=tensor(0.9065, device='cuda:0', grad_fn=<NllLossBackward0>), logits=tensor([[-0.2695,  0.9844, -0.4434, -0.8555, -0.8008],
        [-0.1055,  1.0469, -0.5820, -0.8125, -0.7930],
        [ 0.1455,  1.1484, -0.7539, -0.9492, -1.1016],
        [ 0.2832,  1.4375, -0.2432, -0.9570, -0.6523],
        [-0.2139,  1.3984, -0.0159, -0.5586, -0.7578],
        [-0.0054,  1.2812, -0.3633, -1.2656, -0.8906],
        [ 0.1895,  1.1094, -0.5859, -0.9844, -0.8945],
        [-0.0058,  1.6484, -0.6133, -1.0000, -0.4336],
        [ 0.0981,  1.2891, -0.6094, -1.2891, -0.9141],
        [ 0.2080,  1.2812, -0.1260, -0.6953, -0.7109],
        [-0.1660,  1.3125, -0.4297, -0.8555, -0.9297],
        [-0.1992,  1.3438, -0.3750, -0.9961, -1.0156],
        [-0.1592,  1.5078, -0.5352, -0.9883, -0.7930],
        [-0.0791,  1.8125, -0.7852, -1.0234, -0.7695],
        [ 0.0986,  1.3125, -0.4980, -0.9414, -0.5078],
        [ 0.1387,  1.0859, -0.7383, -0.8047, -0.4102]], device='cuda:0',
       dtype=torch.bfloat16, grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None), SequenceClassifierOutput(loss=tensor(0.8813, device='cuda:0', grad_fn=<NllLossBackward0>), logits=tensor([[ 1.6016e-01,  1.2734e+00, -8.2812e-01, -9.2188e-01, -5.7422e-01],
        [ 2.4805e-01,  1.3359e+00, -8.2422e-01, -1.2500e+00, -4.3164e-01],
        [-3.4375e-01,  1.3125e+00, -7.9297e-01, -9.8047e-01, -6.4844e-01],
        [ 6.4453e-02,  8.5547e-01, -4.7266e-01, -1.1484e+00, -3.8477e-01],
        [-3.0518e-02,  1.0938e+00, -8.9062e-01, -1.0938e+00, -9.6680e-02],
        [ 1.1523e-01,  1.3359e+00, -5.4297e-01, -1.1797e+00, -7.4219e-01],
        [ 2.6758e-01,  1.0156e+00, -9.2969e-01, -1.1797e+00, -3.1250e-01],
        [-2.7930e-01,  1.1094e+00, -5.5859e-01, -7.5000e-01, -8.4766e-01],
        [ 3.0151e-02,  1.1094e+00, -5.6250e-01, -9.2578e-01, -7.7734e-01],
        [ 7.2266e-02,  1.6406e+00, -4.0234e-01, -9.4141e-01, -8.6719e-01],
        [-1.0107e-01,  1.3359e+00, -2.4121e-01, -1.1328e+00, -6.3281e-01],
        [ 3.1738e-02,  1.4062e+00, -4.2773e-01, -6.7188e-01, -7.6953e-01],
        [-3.8574e-02,  1.2734e+00, -5.5859e-01, -9.4531e-01, -6.6016e-01],
        [-8.9645e-04,  1.7109e+00, -2.5977e-01, -8.1641e-01, -6.6797e-01],
        [-1.8555e-01,  1.5859e+00, -5.1562e-01, -8.8672e-01, -4.7461e-01],
        [-2.2852e-01,  1.2031e+00, -5.5078e-01, -9.4531e-01, -6.6016e-01]],
       device='cuda:0', dtype=torch.bfloat16, grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None), SequenceClassifierOutput(loss=tensor(0.7507, device='cuda:0', grad_fn=<NllLossBackward0>), logits=tensor([[-0.1035,  1.3438, -0.4082, -0.6016, -0.6211],
        [-0.2275,  1.2344, -0.3672, -0.7500, -0.8633],
        [ 0.1533,  1.3594, -0.4668, -1.0078, -0.7422],
        [ 0.2539,  1.2188, -0.4102, -0.8828, -0.4609],
        [ 0.0371,  1.7891, -0.2598, -0.8203, -0.5469],
        [ 0.3828,  1.2891, -0.8633, -0.9766, -0.1650],
        [ 0.4473,  1.3203, -0.4727, -1.1016, -0.9414],
        [ 0.1377,  1.3438, -0.4180, -0.8633, -0.6250],
        [ 0.1396,  1.2266, -0.6562, -0.9648, -0.6602],
        [ 0.0977,  1.5000, -0.6836, -1.1953, -0.7070],
        [-0.0845,  1.1875, -0.3105, -0.7812, -0.6328],
        [ 0.1396,  1.0703, -0.5312, -0.8203, -0.1494],
        [ 0.2832,  1.1484, -0.3340, -0.8398, -0.3281],
        [ 0.1138,  1.1328, -0.6172, -1.2344, -0.7500],
        [-0.2363,  1.4062, -0.3828, -1.1484, -0.8555],
        [-0.3125,  1.9297, -0.3926, -1.1172, -0.7461]], device='cuda:0',
       dtype=torch.bfloat16, grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None), SequenceClassifierOutput(loss=tensor(0.7126, device='cuda:0', grad_fn=<NllLossBackward0>), logits=tensor([[-5.1758e-02,  1.6797e+00, -6.3281e-01, -8.6328e-01, -6.4453e-01],
        [-3.1494e-02,  1.0859e+00, -2.8516e-01, -1.0078e+00, -8.1250e-01],
        [ 1.7480e-01,  1.4453e+00, -4.6875e-01, -6.6016e-01, -7.6172e-01],
        [ 2.7100e-02,  1.5312e+00, -6.0938e-01, -1.2109e+00, -4.8828e-01],
        [-1.1063e-03,  1.5000e+00, -7.0703e-01, -7.1875e-01, -5.8984e-01],
        [ 2.0117e-01,  1.3203e+00, -5.7422e-01, -1.0000e+00, -4.4141e-01],
        [-5.4199e-02,  1.2188e+00,  2.3560e-02, -1.1016e+00, -6.7969e-01],
        [-8.9844e-02,  1.3594e+00, -5.8594e-01, -9.0625e-01, -4.6289e-01],
        [-2.6953e-01,  1.2266e+00, -5.3516e-01, -8.9844e-01, -6.6797e-01],
        [-1.4526e-02,  1.2500e+00, -5.6641e-01, -9.8438e-01, -5.7031e-01],
        [ 6.9824e-02,  1.6250e+00, -4.1016e-01, -9.8438e-01, -5.3125e-01],
        [-3.1738e-02,  1.6094e+00, -8.3594e-01, -1.3203e+00, -6.6406e-01],
        [-1.2207e-01,  1.4688e+00, -5.8594e-01, -8.2812e-01, -2.8516e-01],
        [ 5.5078e-01,  7.8125e-01, -8.6719e-01, -7.6953e-01,  2.1680e-01],
        [ 3.7109e-01,  1.4609e+00, -6.4844e-01, -8.3594e-01, -6.0938e-01],
        [-1.4062e-01,  1.2812e+00, -8.1543e-02, -6.8359e-01, -6.3281e-01]],
       device='cuda:0', dtype=torch.bfloat16, grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None), SequenceClassifierOutput(loss=tensor(0.6586, device='cuda:0', grad_fn=<NllLossBackward0>), logits=tensor([[ 0.0393,  1.2578, -0.2197, -0.8008, -0.9062],
        [-0.1572,  1.4844, -0.3438, -0.6250, -0.8672],
        [-0.2012,  1.5625, -0.5430, -0.9844, -0.6992],
        [-0.2422,  1.3516, -0.2656, -1.0547, -0.6836],
        [-0.1777,  1.2578, -0.2520, -0.9336, -0.4062],
        [ 0.2578,  1.5078, -0.4824, -0.9805, -0.9141],
        [-0.0732,  1.5000, -0.3691, -0.7695, -0.6641],
        [-0.0322,  1.4062, -0.4570, -1.0547, -0.7734],
        [ 0.1631,  1.2891, -0.5625, -1.0312, -0.8398],
        [ 0.1260,  1.2031, -0.3555, -1.0859, -0.5430],
        [ 0.0162,  1.3750, -0.5508, -1.0234, -0.4902],
        [ 0.1582,  1.4609, -0.3594, -0.8828, -0.5820],
        [ 0.0605,  1.3984, -0.4082, -1.1094, -1.1016],
        [ 0.1865,  1.3047, -0.3867, -1.0938, -0.6016],
        [-0.1553,  1.3672, -0.5547, -0.7227, -0.7578],
        [ 0.1504,  1.3750, -0.4297, -1.0859, -0.5156]], device='cuda:0',
       dtype=torch.bfloat16, grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None), SequenceClassifierOutput(loss=tensor(0.7349, device='cuda:0', grad_fn=<NllLossBackward0>), logits=tensor([[ 0.1030,  1.4062, -0.6172, -0.5703, -0.7227],
        [ 0.3242,  1.6953, -0.5430, -0.7148, -0.5859],
        [-0.0598,  1.5312, -0.7461, -1.0312, -0.8672],
        [-0.1079,  1.4688, -0.4160, -0.8594, -0.8359],
        [ 0.1504,  1.3438, -0.6172, -1.0234, -0.8633],
        [-0.0522,  0.9844, -0.4902, -0.9570, -0.2969],
        [-0.0859,  1.2109, -0.4766, -0.9844, -0.9727],
        [ 0.1211,  1.2812, -0.3711, -0.6172, -0.8008],
        [ 0.1846,  1.0312, -0.2383, -0.9531, -0.8125],
        [ 0.1523,  1.4531, -0.3789, -1.1250, -0.7070],
        [ 0.2012,  1.2656, -0.7148, -1.1094, -0.7148],
        [ 0.0515,  1.5391, -0.5234, -0.9570, -0.7461],
        [-0.0240,  1.4141, -0.4082, -0.6758, -0.7695],
        [ 0.0073,  1.4609, -0.3320, -0.8320, -1.1719],
        [-0.0181,  0.8906, -0.5430, -0.8789, -0.6172],
        [-0.3867,  1.6250, -0.2383, -0.7031, -0.8438]], device='cuda:0',
       dtype=torch.bfloat16, grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None), SequenceClassifierOutput(loss=tensor(0.8932, device='cuda:0', grad_fn=<NllLossBackward0>), logits=tensor([[ 0.1641,  1.1406, -0.5117, -1.1484, -0.6680],
        [ 0.2139,  1.4219, -0.2793, -0.8008, -0.4766],
        [-0.1904,  1.3359, -0.5312, -1.0469, -0.5859],
        [-0.0728,  1.5625, -0.3945, -1.0234, -0.8867],
        [ 0.5078,  0.9336, -0.9570, -0.9102,  0.1250],
        [ 0.3633,  1.0391, -0.4707, -1.1094, -0.6367],
        [ 0.0110,  1.3203, -0.4004, -1.0547, -0.7695],
        [ 0.0791,  1.5547, -0.5938, -0.8828, -0.7773],
        [ 0.0388,  1.6016, -0.7734, -1.0703, -0.3477],
        [-0.1777,  1.2422, -0.2910, -1.0547, -1.1328],
        [-0.1230,  1.3281, -0.5000, -0.8789, -0.8242],
        [-0.1855,  1.2812, -0.5781, -0.4824, -0.6953],
        [-0.3105,  1.2891, -0.4688, -0.9492, -1.0312],
        [-0.1904,  1.5156, -0.7227, -0.8594, -0.4805],
        [-0.1045,  1.2812, -0.5664, -0.7852, -0.6055],
        [ 0.3125,  1.6406, -0.6367, -1.1328, -0.4609]], device='cuda:0',
       dtype=torch.bfloat16, grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None), SequenceClassifierOutput(loss=tensor(0.5789, device='cuda:0', grad_fn=<NllLossBackward0>), logits=tensor([[ 0.0461,  1.2500, -0.7539, -1.2656, -0.5352],
        [-0.0806,  1.6406, -0.4180, -0.8516, -0.9141],
        [-0.0109,  1.4141, -0.5039, -0.9609, -0.8516],
        [ 0.1436,  1.3828, -0.6562, -0.9336, -0.8281],
        [ 0.3906,  1.3984, -0.8906, -1.0625, -0.3535],
        [-0.0635,  1.1953, -0.6016, -1.1953, -0.5156],
        [-0.0500,  1.6016, -0.5234, -0.7266, -0.4004],
        [-0.1147,  1.1719, -0.9219, -0.8711, -0.5391],
        [ 0.1177,  1.5703, -0.7656, -1.1641, -0.5430],
        [ 0.2949,  1.4375, -0.9102, -1.0234, -0.3555],
        [ 0.1074,  1.4141, -0.7852, -0.9062, -0.5430],
        [-0.3828,  1.1016, -0.6836, -1.0391, -0.5859],
        [ 0.0043,  1.3594, -0.8086, -0.8477, -0.3242],
        [-0.1328,  1.7500, -0.5859, -0.8086, -0.3164],
        [ 0.0359,  1.1016, -0.6328, -0.9141, -0.9688],
        [ 0.0396,  1.1172, -0.6133, -0.8398, -0.2949]], device='cuda:0',
       dtype=torch.bfloat16, grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None), SequenceClassifierOutput(loss=tensor(0.6453, device='cuda:0', grad_fn=<NllLossBackward0>), logits=tensor([[-1.7188e-01,  1.5938e+00, -6.9141e-01, -1.1953e+00, -6.9922e-01],
        [ 2.0215e-01,  1.3750e+00, -8.7891e-01, -7.8906e-01, -5.9375e-01],
        [-2.4707e-01,  1.5625e+00, -5.5078e-01, -8.5156e-01, -6.2891e-01],
        [ 1.6406e-01,  1.4844e+00, -6.1719e-01, -8.4375e-01, -6.1719e-01],
        [ 1.3733e-03,  1.5156e+00, -5.2734e-01, -7.7344e-01, -4.0820e-01],
        [ 7.9956e-03,  1.3594e+00, -5.7812e-01, -4.9609e-01, -6.1719e-01],
        [-5.9326e-02,  1.3750e+00, -4.3164e-01, -7.5000e-01, -7.7344e-01],
        [-1.0437e-02,  1.7188e+00, -6.4062e-01, -7.1875e-01, -6.1328e-01],
        [ 2.7148e-01,  1.7109e+00, -3.9453e-01, -6.6797e-01, -4.7461e-01],
        [-5.5078e-01,  1.3906e+00, -5.9766e-01, -1.1016e+00, -2.9688e-01],
        [ 2.2559e-01,  1.3281e+00, -5.5859e-01, -6.9531e-01, -4.0625e-01],
        [-1.5039e-01,  1.7266e+00, -6.3672e-01, -8.6719e-01, -6.0156e-01],
        [-4.2578e-01,  1.5938e+00, -6.0156e-01, -7.5000e-01, -5.3125e-01],
        [ 1.4355e-01,  1.2656e+00, -6.4844e-01, -8.2422e-01, -4.8242e-01],
        [ 1.0693e-01,  1.2969e+00, -8.4766e-01, -9.3359e-01, -6.2109e-01],
        [-1.2061e-01,  1.4375e+00, -5.1172e-01, -1.0078e+00, -8.2031e-01]],
       device='cuda:0', dtype=torch.bfloat16, grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None), SequenceClassifierOutput(loss=tensor(0.4941, device='cuda:0', grad_fn=<NllLossBackward0>), logits=tensor([[ 0.0840,  1.4453, -0.4395, -0.9648, -0.6797],
        [ 0.1689,  1.4375, -0.5000, -0.8242, -0.4590],
        [ 0.2480,  1.4062, -0.7344, -1.0156, -0.8555],
        [ 0.0425,  1.6484, -0.4980, -0.8281, -0.4609],
        [ 0.3379,  1.5078, -0.7734, -0.6914, -0.5742],
        [ 0.1689,  1.6641, -0.8398, -1.0156, -0.4121],
        [ 0.2227,  1.7031, -0.6289, -0.8008, -0.4160],
        [ 0.1143,  1.3750, -0.4121, -0.8203, -0.5195],
        [-0.0194,  0.7148, -1.0781, -1.0781, -0.3203],
        [ 0.1367,  1.7422, -0.6289, -0.8086, -0.7539],
        [ 0.1660,  1.2656, -0.5938, -0.5391, -0.3789],
        [ 0.0835,  1.1953, -0.7539, -0.9961, -0.4805],
        [-0.3203,  1.5703, -0.8320, -0.5547, -0.5039],
        [ 0.1025,  1.6406, -0.7383, -1.0547, -0.6562],
        [ 0.0466,  1.5781, -0.6484, -0.7422, -0.3984],
        [ 0.1118,  1.2969, -0.5742, -0.5938, -0.5664]], device='cuda:0',
       dtype=torch.bfloat16, grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None), SequenceClassifierOutput(loss=tensor(0.8766, device='cuda:0', grad_fn=<NllLossBackward0>), logits=tensor([[ 0.2539,  1.3516, -0.4414, -0.5938, -1.0156],
        [-0.0359,  1.1641, -0.1162, -0.9023, -0.9219],
        [ 0.2461,  1.5469, -0.8008, -0.7383, -0.3613],
        [ 0.2734,  1.4297, -0.3926, -0.8984, -0.9727],
        [-0.0486,  1.6250, -0.5039, -0.7773, -0.7812],
        [ 0.0918,  1.0391, -0.3066, -0.8359, -0.7188],
        [ 0.1855,  1.3125, -0.4980, -0.6875, -0.6367],
        [ 0.1094,  1.4922, -0.8203, -0.7578, -0.2637],
        [-0.0405,  1.5000, -0.6406, -0.8945, -0.4434],
        [ 0.2637,  1.7891, -0.6523, -0.6562, -0.5195],
        [ 0.1357,  1.3438, -0.5430, -0.9766, -0.9453],
        [-0.2402,  1.2188, -0.7422, -0.6289, -0.6641],
        [ 0.1680,  1.1875, -0.5195, -0.9414, -0.5781],
        [-0.0325,  1.6328, -0.4492, -0.8438, -0.5664],
        [ 0.0287,  1.6328, -0.7227, -0.6953, -0.3867],
        [ 0.2598,  2.0156, -0.7578, -1.1250, -0.3320]], device='cuda:0',
       dtype=torch.bfloat16, grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None), SequenceClassifierOutput(loss=tensor(0.7887, device='cuda:0', grad_fn=<NllLossBackward0>), logits=tensor([[ 0.1157,  1.7734, -1.0625, -1.0156, -0.7227],
        [ 0.3086,  1.6172, -0.6641, -0.4668, -0.6875],
        [-0.1299,  1.1016, -0.9062, -1.0156, -0.7383],
        [-0.2715,  1.4844, -0.4453, -0.7461, -0.5039],
        [-0.0267,  1.7266, -0.1621, -0.7773, -0.8086],
        [ 0.2812,  1.2578, -0.2305, -0.7891, -0.3691],
        [ 0.3320,  1.5781, -0.9375, -0.6953, -0.4043],
        [ 0.5820,  1.4453, -0.6289, -0.7109, -0.8906],
        [-0.0854,  1.6016, -0.8672, -0.9023, -0.6875],
        [ 0.1904,  1.5703, -0.3672, -0.7773, -0.8867],
        [ 0.1226,  1.6250, -0.8086, -0.9375, -0.5156],
        [-0.0859,  1.3750, -0.5703, -0.8242, -0.5625],
        [ 0.2715,  0.7578, -0.9805, -0.2988,  0.1992],
        [-0.2500,  1.2891, -0.6914, -0.3672, -0.5430],
        [ 0.0747,  1.4609, -0.4453, -1.0156, -0.7773],
        [ 0.1055,  1.5312, -0.8984, -1.0781, -0.4629]], device='cuda:0',
       dtype=torch.bfloat16, grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None), SequenceClassifierOutput(loss=tensor(0.6788, device='cuda:0', grad_fn=<NllLossBackward0>), logits=tensor([[ 0.1289,  1.5156, -0.6875, -0.8281, -0.5820],
        [ 0.1865,  1.5703, -0.6055, -0.7109, -0.9688],
        [ 0.1060,  1.3828, -0.5586, -0.8047, -0.5664],
        [ 0.2598,  1.4766, -0.9062, -0.9102, -0.4395],
        [ 0.1250,  1.2109, -0.5859, -0.9297, -0.5000],
        [ 0.1377,  1.5312, -0.5938, -1.0781, -0.5781],
        [-0.0140,  1.2656, -0.8555, -0.8281, -0.4941],
        [-0.4629,  1.4453, -0.9609, -1.1172, -0.7852],
        [-0.2070,  1.6797, -0.3145, -1.0547, -0.8164],
        [ 0.5000,  1.0938, -1.0391, -0.9219, -0.2852],
        [-0.1377,  1.7656, -0.9141, -1.0703, -0.4688],
        [ 0.0415,  1.4141, -0.6641, -0.8516, -0.3535],
        [-0.1729,  1.5547, -0.6133, -0.5078, -0.3633],
        [ 0.0540,  1.3906, -0.2637, -0.3770, -0.4551],
        [-0.0889,  1.6875, -1.0703, -0.6172, -0.5352],
        [-0.0102,  1.1094, -0.3359, -0.7188, -0.8789]], device='cuda:0',
       dtype=torch.bfloat16, grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None), SequenceClassifierOutput(loss=tensor(0.4357, device='cuda:0', grad_fn=<NllLossBackward0>), logits=tensor([[-0.2197,  1.6797, -0.9531, -1.0859, -0.6523],
        [ 0.0752,  1.2969, -0.7266, -0.8984, -1.0547],
        [ 0.1484,  1.5156, -0.6914, -0.8398, -0.6914],
        [-0.0430,  1.4609, -0.5938, -0.7578, -0.7969],
        [ 0.2275,  1.6641, -0.4707, -0.9141, -0.4375],
        [ 0.0815,  1.9688, -0.2344, -0.8398, -0.7969],
        [ 0.1143,  1.3125, -0.7930, -0.9297, -0.5469],
        [-0.0352,  1.3828, -0.6797, -0.8047, -0.5898],
        [ 0.2559,  1.7109, -0.7617, -0.9492, -0.4785],
        [-0.0474,  1.6094, -0.8398, -0.7852, -0.5586],
        [-0.0043,  1.3906, -0.7812, -0.8320, -0.5352],
        [-0.2129,  1.4141, -0.5938, -0.9531, -0.4629],
        [-0.3965,  1.6797, -0.6289, -0.9531, -0.6445],
        [-0.0864,  1.4766, -0.5977, -0.7578, -0.5312],
        [-0.3418,  1.4141, -0.3750, -0.8086, -0.3027],
        [-0.1387,  1.3516, -0.6406, -0.6328, -0.8164]], device='cuda:0',
       dtype=torch.bfloat16, grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None), SequenceClassifierOutput(loss=tensor(0.8174, device='cuda:0', grad_fn=<NllLossBackward0>), logits=tensor([[-0.0199,  1.3828, -0.3926, -0.9180, -0.9219],
        [-0.0820,  1.3203, -0.4180, -0.5508, -0.6680],
        [ 0.2266,  1.3516, -0.3887, -0.9844, -1.0703],
        [ 0.0557,  1.3516, -0.7891, -1.1172, -0.4355],
        [-0.2520,  1.3984, -0.3652, -0.6094, -0.4434],
        [ 0.0811,  1.5625, -0.8047, -0.9453, -0.5898],
        [-0.0583,  1.5000, -0.9023, -1.0938, -0.3848],
        [ 0.1079,  1.3750, -0.1099, -0.8477, -0.7227],
        [ 0.3789,  1.4062, -0.5547, -0.9492, -0.6523],
        [ 0.0119,  1.0625, -0.6875, -0.6680, -0.3320],
        [ 0.1934,  1.7500, -0.6562, -0.8477, -0.3770],
        [ 0.4648,  0.9961, -0.7500, -0.6172, -0.0610],
        [ 0.1846,  1.5156, -1.2109, -0.9336, -0.6836],
        [-0.0084,  1.0078, -0.6289, -0.3691, -0.4434],
        [ 0.1709,  1.9453, -0.8164, -0.7227, -0.4707],
        [-0.0825,  1.5781, -0.6250, -0.9336, -0.4414]], device='cuda:0',
       dtype=torch.bfloat16, grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None), SequenceClassifierOutput(loss=tensor(0.6611, device='cuda:0', grad_fn=<NllLossBackward0>), logits=tensor([[-0.0125,  1.1094, -0.9023, -0.6992, -0.8320],
        [-0.1709,  1.3047, -0.5391, -0.9023, -0.3887],
        [ 0.0203,  1.3906, -0.3652, -0.8438, -0.5547],
        [ 0.1777,  1.2344, -0.7344, -0.6055, -0.5391],
        [ 0.0938,  1.6328, -0.5234, -0.7109, -0.7109],
        [ 0.2266,  1.5859, -0.5547, -1.1016, -0.5391],
        [-0.1963,  1.3828, -0.6641, -0.7852, -0.5312],
        [-0.2461,  1.3750, -0.5352, -0.7734, -0.6680],
        [ 0.0361,  1.7891, -0.4922, -0.7266, -0.7734],
        [ 0.1045,  1.6250, -0.4648, -1.0312, -0.6328],
        [ 0.0198,  1.6172, -0.7539, -0.8672, -0.5391],
        [-0.1572,  1.5234, -0.2910, -0.6289, -0.3320],
        [ 0.1270,  1.5625, -0.5938, -1.1172, -0.5430],
        [-0.1885,  1.0234, -0.4648, -0.6758, -0.6406],
        [-0.1260,  1.4609, -0.4785, -0.6797, -0.7578],
        [ 0.1572,  1.2500, -0.6758, -0.8242, -0.5000]], device='cuda:0',
       dtype=torch.bfloat16, grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None), SequenceClassifierOutput(loss=tensor(0.5581, device='cuda:0', grad_fn=<NllLossBackward0>), logits=tensor([[ 0.0791,  1.3203, -0.5547, -0.4727, -0.7422],
        [ 0.1826,  1.4062, -0.5156, -1.1953, -1.0703],
        [-0.0884,  1.5703, -0.8750, -1.1328, -0.5234],
        [ 0.1582,  1.2266, -0.5312, -0.7773, -0.6016],
        [ 0.2471,  1.7109, -0.6641, -0.5938, -0.4336],
        [-0.2412,  1.6562, -0.1582, -0.7891, -0.6406],
        [ 0.3027,  1.4453, -0.7070, -1.0547, -0.3594],
        [-0.0806,  1.2109, -0.5703, -0.4980, -0.6055],
        [ 0.0066,  1.4922, -0.8594, -0.8633, -0.4043],
        [ 0.2490,  1.4688, -0.5703, -0.8086, -0.3828],
        [ 0.1963,  1.3594, -0.4766, -0.9336, -0.4629],
        [ 0.2129,  1.5078, -0.6914, -0.9570, -0.5234],
        [-0.0977,  1.6797, -0.8281, -0.9844, -0.7070],
        [ 0.2070,  1.6797, -0.3242, -1.0625, -0.6094],
        [ 0.0864,  1.2969, -0.4590, -0.8203, -0.6562],
        [-0.0170,  1.6328, -0.5039, -1.1172, -0.7148]], device='cuda:0',
       dtype=torch.bfloat16, grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None), SequenceClassifierOutput(loss=tensor(0.6669, device='cuda:0', grad_fn=<NllLossBackward0>), logits=tensor([[-0.3535,  1.2578, -0.5938, -0.6445, -0.6914],
        [-0.3105,  1.8047, -0.5977, -1.0078, -0.4297],
        [-0.2051,  1.7578, -0.6602, -0.9219, -0.7383],
        [ 0.1299,  1.2266, -0.5156, -0.8359, -0.6406],
        [ 0.0586,  1.6875, -0.5391, -1.0781, -0.6875],
        [-0.1826,  1.3750, -0.8203, -0.9102, -0.4824],
        [ 0.0297,  1.7969, -0.5977, -0.8086, -0.8945],
        [ 0.0042,  1.3750, -0.5039, -0.9336, -0.6250],
        [ 0.2383,  1.4766, -0.6211, -0.7578, -0.5547],
        [ 0.0708,  1.0469, -0.4844, -0.9062, -0.5938],
        [ 0.0703,  1.2812, -0.5039, -0.6914, -0.7812],
        [-0.3574,  1.4219, -0.4355, -0.6914, -0.5938],
        [ 0.3848,  1.2188, -0.7305, -0.7500, -0.6992],
        [ 0.0593,  1.4844, -0.5977, -0.8164, -0.7539],
        [-0.3652,  1.3125, -0.7227, -0.7539, -0.6445],
        [-0.4121,  1.3750, -0.1875, -1.2031, -0.9102]], device='cuda:0',
       dtype=torch.bfloat16, grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None), SequenceClassifierOutput(loss=tensor(0.5126, device='cuda:0', grad_fn=<NllLossBackward0>), logits=tensor([[-0.0322,  1.9219, -0.6641, -0.6484, -0.5742],
        [-0.2930,  1.2422, -0.6172, -0.4883, -0.4277],
        [-0.0610,  1.7109, -0.5625, -1.1016, -0.4141],
        [ 0.2520,  1.5625, -0.5547, -0.8672, -0.5117],
        [-0.1562,  2.1562, -0.7656, -0.7109, -0.2109],
        [-0.2988,  1.3125, -0.6133, -0.6562, -0.6211],
        [ 0.1855,  1.3672, -0.6406, -1.0156, -0.6914],
        [ 0.0229,  1.5938, -0.5547, -0.8438, -0.5352],
        [-0.2432,  1.5469, -0.5234, -0.6094, -0.5352],
        [ 0.0679,  1.3750, -0.8164, -0.9023, -0.5859],
        [-0.0364,  1.4062, -0.5742, -0.9062, -0.6562],
        [ 0.2461,  1.6016, -0.9297, -0.9766, -0.2891],
        [ 0.1367,  1.8828, -0.5000, -1.1562, -0.3926],
        [ 0.0835,  1.7266, -0.7539, -1.1484, -0.5820],
        [ 0.0544,  1.4141, -1.0469, -0.7031, -0.6289],
        [ 0.1475,  1.3828, -0.9297, -1.1953, -0.4629]], device='cuda:0',
       dtype=torch.bfloat16, grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None), SequenceClassifierOutput(loss=tensor(0.7721, device='cuda:0', grad_fn=<NllLossBackward0>), logits=tensor([[ 0.2891,  1.5000, -0.6367, -1.2891, -0.6641],
        [-0.0874,  1.3672, -0.6289, -0.9883, -0.4355],
        [-0.0625,  1.3750, -0.5156, -0.8633, -0.7578],
        [-0.1748,  1.3516, -0.3516, -0.7969, -1.1875],
        [ 0.3848,  1.0703, -0.9336, -0.9258,  0.5039],
        [-0.1885,  1.5391, -0.1533, -0.6445, -0.7539],
        [-0.0036,  1.3359, -0.6172, -0.9062, -0.7383],
        [ 0.0083,  1.3828, -0.8047, -0.5742, -0.2637],
        [ 0.2109,  1.3750, -0.7578, -0.9414, -0.6406],
        [ 0.1797,  1.4844, -0.3398, -0.7930, -0.5742],
        [ 0.0239,  1.7188, -0.9609, -0.7461, -0.3984],
        [ 0.2373,  1.8359, -0.9258, -0.9336, -0.3613],
        [-0.0549,  1.4453, -0.5859, -1.1250, -0.5977],
        [ 0.1904,  1.3047, -0.9883, -0.9180, -0.5156],
        [ 0.0398,  1.0859, -0.5508, -0.5000, -0.4375],
        [ 0.1523,  1.6328, -0.6211, -0.9141, -0.6250]], device='cuda:0',
       dtype=torch.bfloat16, grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None), SequenceClassifierOutput(loss=tensor(0.6522, device='cuda:0', grad_fn=<NllLossBackward0>), logits=tensor([[-0.0791,  1.4375, -1.1250, -0.9141, -0.3711],
        [ 0.0742,  1.5625, -0.7812, -0.9258, -0.5781],
        [-0.0403,  1.6250, -0.8516, -0.6133, -0.4902],
        [-0.0576,  1.7422, -0.7734, -0.7695, -0.5547],
        [-0.0991,  1.5234, -0.5742, -0.6133, -0.4863],
        [ 0.1006,  1.6562, -0.8281, -0.9805, -0.5664],
        [ 0.4570,  1.1328, -0.5703, -0.9062, -0.4902],
        [-0.1318,  1.5625, -0.7500, -0.7930, -0.4551],
        [-0.1963,  1.4844, -0.5469, -0.6992, -0.2832],
        [ 0.3066,  1.5859, -0.7422, -0.8398, -0.3223],
        [ 0.2852,  1.5625, -0.9023, -0.9258, -0.5625],
        [-0.0084,  1.8828, -0.3965, -0.9805, -0.2598],
        [-0.0918,  1.4219, -0.9805, -0.8242, -0.3047],
        [-0.1040,  1.5312, -0.5742, -0.7148, -0.9062],
        [ 0.1631,  1.5312, -0.7148, -0.8320, -0.7461],
        [ 0.0089,  1.5781, -0.6016, -0.9453, -0.5195]], device='cuda:0',
       dtype=torch.bfloat16, grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None), SequenceClassifierOutput(loss=tensor(0.8853, device='cuda:0', grad_fn=<NllLossBackward0>), logits=tensor([[ 0.1025,  1.3125, -0.5586, -0.9688, -0.9648],
        [ 0.1992,  1.6641, -0.6484, -1.1641, -0.6914],
        [ 0.0464,  1.4609, -0.5391, -0.7188, -0.6875],
        [-0.1836,  1.6172, -0.5508, -0.9453, -0.3867],
        [ 0.0383,  1.4609, -0.5938, -0.8555, -0.4668],
        [-0.4141,  1.2500, -0.5156, -0.6602, -0.7148],
        [ 0.2695,  1.5391, -0.2354, -0.8984, -0.5430],
        [-0.1445,  1.1406, -0.6250, -0.6875, -0.7930],
        [ 0.0284,  1.6797, -0.7227, -0.9180, -0.7539],
        [ 0.0136,  1.3438, -0.5547, -0.6719, -0.7266],
        [-0.0461,  1.3047, -0.7773, -0.7461, -0.6523],
        [ 0.0830,  1.0312, -0.5234, -0.7539, -0.2109],
        [ 0.1055,  1.5078, -0.7227, -1.0000, -0.3340],
        [ 0.3066,  1.1094, -0.5547, -0.6523, -0.2354],
        [ 0.1318,  1.7109, -0.5898, -0.8164, -0.6797],
        [-0.1245,  1.3203, -0.8594, -0.7852, -0.5039]], device='cuda:0',
       dtype=torch.bfloat16, grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None), SequenceClassifierOutput(loss=tensor(0.8319, device='cuda:0', grad_fn=<NllLossBackward0>), logits=tensor([[ 0.0481,  1.6875, -0.8516, -0.8984, -0.3477],
        [-0.0062,  1.4922, -0.8750, -1.0312, -0.5469],
        [ 0.0299,  1.3984, -0.3418, -0.7383, -0.3984],
        [ 0.1523,  1.5547, -0.8945, -0.8359, -0.3965],
        [ 0.0515,  1.8047, -0.5859, -0.9141, -0.3496],
        [-0.1357,  1.4531, -0.4980, -0.8555, -0.4395],
        [ 0.3027,  1.5156, -0.6719, -1.0078, -0.4375],
        [-0.0601,  1.2969, -0.6641, -0.6211, -0.3203],
        [ 0.3398,  1.7109, -0.6641, -1.0312, -0.3418],
        [-0.1475,  1.5938, -0.8945, -0.7188, -0.3203],
        [ 0.1099,  1.5469, -0.6953, -1.0000, -0.6797],
        [ 0.0479,  1.6641, -0.8555, -0.9023, -0.4219],
        [ 0.1992,  1.1953, -0.3398, -0.7383, -0.1138],
        [ 0.1797,  1.2812, -0.5664, -0.8359, -0.5273],
        [-0.1309,  1.9609, -0.5547, -0.8203, -0.4375],
        [ 0.2285,  1.3125, -0.4766, -0.6367, -0.8008]], device='cuda:0',
       dtype=torch.bfloat16, grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None), SequenceClassifierOutput(loss=tensor(0.5796, device='cuda:0', grad_fn=<NllLossBackward0>), logits=tensor([[-6.9336e-02,  1.7500e+00, -6.0938e-01, -6.7578e-01, -5.8594e-01],
        [ 1.7090e-01,  1.6953e+00, -4.5508e-01, -1.2031e+00, -5.1172e-01],
        [ 1.3184e-01,  1.8047e+00, -8.7500e-01, -1.2266e+00, -3.2422e-01],
        [ 5.1758e-02,  1.2109e+00, -3.7500e-01, -7.3828e-01, -7.5000e-01],
        [-2.4414e-02,  1.5156e+00, -4.1016e-01, -8.7891e-01, -6.2500e-01],
        [ 5.4199e-02,  1.0547e+00, -8.6719e-01, -6.8750e-01, -1.4844e-01],
        [ 2.2852e-01,  1.7422e+00, -7.4219e-01, -8.5547e-01, -3.5938e-01],
        [ 3.8867e-01,  1.7578e+00, -5.3516e-01, -8.7109e-01, -6.2500e-01],
        [ 2.5781e-01,  8.0859e-01, -9.0234e-01, -4.8242e-01, -8.0078e-01],
        [-2.5195e-01,  1.6172e+00, -5.4688e-01, -9.2969e-01, -7.3828e-01],
        [-3.4180e-02,  1.3594e+00, -5.8203e-01, -7.2266e-01, -6.2500e-01],
        [ 7.1777e-02,  1.7266e+00, -6.6016e-01, -9.8828e-01, -5.1172e-01],
        [ 6.3324e-04,  1.5391e+00, -5.7031e-01, -1.0078e+00, -6.6016e-01],
        [-1.8750e-01,  1.6250e+00, -1.0078e+00, -5.9375e-01, -7.6172e-01],
        [ 2.1240e-02,  1.5156e+00, -6.2891e-01, -7.8906e-01, -4.5312e-01],
        [ 2.4609e-01,  1.2812e+00, -9.4531e-01, -8.7500e-01, -5.4297e-01]],
       device='cuda:0', dtype=torch.bfloat16, grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None), SequenceClassifierOutput(loss=tensor(0.8370, device='cuda:0', grad_fn=<NllLossBackward0>), logits=tensor([[ 0.1387,  1.5781, -0.6523, -1.0469, -0.5352],
        [ 0.0540,  1.4844, -0.4434, -0.7969, -0.8711],
        [-0.0708,  1.6641, -0.6445, -1.0469, -0.9102],
        [-0.2490,  1.4062, -0.3457, -0.7930, -0.6055],
        [-0.1328,  1.6094, -0.6133, -0.7969, -0.6250],
        [ 0.3555,  1.6641, -0.6094, -0.8516, -0.7227],
        [ 0.0089,  1.6484, -0.6016, -0.9727, -0.5859],
        [ 0.0275,  1.2891, -0.3594, -0.8984, -0.6719],
        [ 0.1729,  1.6484, -0.7148, -0.7734, -0.6484],
        [-0.3730,  1.5547, -0.8086, -1.1484, -0.5000],
        [ 0.4043,  1.4219, -0.4102, -0.8359, -0.6445],
        [ 0.0762,  1.5938, -0.6250, -0.6523, -0.1992],
        [ 0.2432,  1.1328, -0.3984, -0.3848, -0.4316],
        [ 0.2354,  1.0859, -1.0703, -0.8516, -0.3477],
        [ 0.0869,  1.4609, -0.9648, -0.9922, -0.2637],
        [-0.0255,  1.0234, -0.6836, -0.6875, -1.0234]], device='cuda:0',
       dtype=torch.bfloat16, grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None), SequenceClassifierOutput(loss=tensor(0.8553, device='cuda:0', grad_fn=<NllLossBackward0>), logits=tensor([[-0.0942,  1.4141, -0.7422, -0.7930, -0.4863],
        [ 0.2520,  1.2734, -0.8164, -0.8945, -0.4023],
        [-0.0306,  1.7109, -0.8359, -0.9727, -0.5195],
        [-0.0310,  1.2812, -0.6055, -0.8125, -0.2412],
        [-0.1641,  1.4297,  0.0192, -0.8828, -0.6055],
        [-0.0613,  1.3750, -0.6680, -0.7812, -0.7930],
        [ 0.0284,  1.4453, -0.8438, -0.9688, -0.6523],
        [-0.0718,  1.3516, -0.3574, -0.5312, -0.5859],
        [-0.0635,  1.6094, -0.6680, -0.7930, -0.6055],
        [ 0.3008,  1.3359, -0.7109, -0.8906, -0.4395],
        [-0.1191,  1.4141, -0.5781, -0.8789, -0.5977],
        [ 0.0801,  1.5469, -0.6055, -0.7344, -0.5391],
        [ 0.0596,  1.3047, -0.4336, -0.5781, -0.3105],
        [-0.2656,  1.3125, -0.4746, -0.7969, -0.4512],
        [ 0.2598,  1.2031, -1.1562, -0.8711,  0.1504],
        [-0.0337,  1.7109, -0.5898, -0.7031, -0.5156]], device='cuda:0',
       dtype=torch.bfloat16, grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None), SequenceClassifierOutput(loss=tensor(0.6334, device='cuda:0', grad_fn=<NllLossBackward0>), logits=tensor([[ 0.1719,  1.6328, -0.8086, -0.8789, -0.5938],
        [-0.0938,  1.5547, -0.5312, -0.8320, -0.6016],
        [ 0.1816,  1.2188, -0.3242, -0.6719, -1.0312],
        [-0.1797,  1.5000, -1.0938, -1.0625, -0.4766],
        [ 0.2734,  0.8047, -0.8516, -0.2969,  0.6133],
        [-0.2119,  1.3125, -0.6914, -0.9297, -0.5391],
        [ 0.2168,  1.3438, -0.7734, -0.8945, -0.5234],
        [-0.0728,  1.3047, -0.5078, -0.6016, -0.5273],
        [ 0.2949,  1.7500, -0.7852, -1.0391, -0.4023],
        [-0.1133,  1.6484, -0.7266, -0.8047, -0.5781],
        [ 0.1992,  1.2891, -0.5156, -0.6680, -0.9023],
        [ 0.2793,  1.5469, -0.4238, -0.8438, -0.7344],
        [ 0.0179,  1.3516, -0.5195, -0.7578, -0.7148],
        [ 0.1924,  1.4922, -0.8164, -0.8789, -0.4180],
        [ 0.1582,  1.5625, -0.8320, -0.8125, -0.5312],
        [-0.1318,  1.4531, -0.6367, -0.5938, -0.2793]], device='cuda:0',
       dtype=torch.bfloat16, grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None), SequenceClassifierOutput(loss=tensor(0.8029, device='cuda:0', grad_fn=<NllLossBackward0>), logits=tensor([[ 5.6885e-02,  1.3125e+00, -5.3516e-01, -1.2188e+00, -5.7422e-01],
        [ 3.5547e-01,  1.3984e+00, -3.8672e-01, -6.8359e-01, -5.1953e-01],
        [ 1.9287e-02,  1.4688e+00, -8.5938e-01, -9.0234e-01, -6.7188e-01],
        [-1.4725e-03,  1.2344e+00, -6.7188e-01, -7.2266e-01, -7.8906e-01],
        [ 5.8838e-02,  1.5000e+00, -7.4219e-01, -7.5781e-01, -7.8516e-01],
        [-2.7734e-01,  1.5312e+00, -8.6328e-01, -5.0391e-01, -5.0391e-01],
        [ 2.5391e-01,  1.5547e+00, -2.1875e-01, -8.2031e-01, -3.2227e-01],
        [ 4.5117e-01,  1.4766e+00, -9.6484e-01, -7.5781e-01, -6.9141e-01],
        [ 2.2754e-01,  1.2734e+00, -8.3984e-01, -1.0703e+00, -5.5078e-01],
        [ 2.2656e-01,  1.5469e+00, -5.0781e-01, -9.3359e-01, -5.8594e-01],
        [ 2.4805e-01,  1.5625e+00, -7.0312e-01, -5.6250e-01, -2.4707e-01],
        [ 1.5332e-01,  1.5703e+00, -4.9023e-01, -7.1094e-01, -6.1328e-01],
        [ 1.6309e-01,  1.6328e+00, -4.1797e-01, -8.0078e-01, -6.6797e-01],
        [-2.0801e-01,  1.5859e+00, -6.3672e-01, -8.1641e-01, -6.8750e-01],
        [ 2.0312e-01,  1.3672e+00, -4.4727e-01, -5.4688e-01, -5.5469e-01],
        [-2.3340e-01,  1.9688e+00, -6.6797e-01, -1.0234e+00, -6.4844e-01]],
       device='cuda:0', dtype=torch.bfloat16, grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None), SequenceClassifierOutput(loss=tensor(1.1155, device='cuda:0', grad_fn=<NllLossBackward0>), logits=tensor([[ 0.0913,  1.2812, -0.4980, -0.7344, -0.8008],
        [ 0.2266,  1.4219, -0.4844, -0.7539, -0.3965],
        [ 0.2871,  1.6484, -0.6133, -0.7812, -0.4609],
        [ 0.0640,  1.6797, -0.2793, -0.7852, -0.8164],
        [ 0.0913,  1.3672, -1.1250, -0.8164, -0.0259],
        [ 0.2061,  1.3672, -1.1328, -1.1484, -0.6406],
        [-0.1982,  1.5469, -0.7969, -0.7461, -0.6094],
        [-0.0586,  1.5000, -0.4258, -1.0859, -0.8242],
        [ 0.4434,  1.4453, -0.6094, -1.0625, -0.5039],
        [-0.2812,  1.2422, -0.4707, -0.7969, -0.6562],
        [-0.0378,  1.4609, -0.5977, -0.8672, -0.5977],
        [ 0.0474,  1.3516, -0.6914, -0.8438, -0.3379],
        [-0.1045,  1.6797, -0.6289, -0.7617, -0.5508],
        [-0.2012,  1.0938, -0.5469, -0.5547, -0.7617],
        [-0.2520,  1.6016, -0.6172, -0.8672, -0.4766],
        [ 0.0698,  1.1094, -0.5312, -0.8516, -0.7109]], device='cuda:0',
       dtype=torch.bfloat16, grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None), SequenceClassifierOutput(loss=tensor(0.9456, device='cuda:0', grad_fn=<NllLossBackward0>), logits=tensor([[ 0.0405,  1.2344, -0.4492, -0.9844, -0.2080],
        [ 0.2354,  1.2109, -0.5039, -0.7812, -0.1055],
        [-0.0796,  1.7656, -0.9336, -1.0859, -0.3477],
        [ 0.2490,  1.5859, -0.6367, -1.1641, -0.4727],
        [-0.1084,  1.7891, -0.5469, -0.9922, -0.7930],
        [ 0.1729,  1.7656, -0.8945, -0.9688, -0.6094],
        [ 0.0659,  1.6719, -0.8398, -1.1016, -0.5977],
        [-0.0698,  1.6484, -0.6445, -1.1641, -0.8008],
        [ 0.1113,  1.4453, -0.3242, -0.7266, -0.6016],
        [ 0.0258,  1.3359, -0.6250, -1.0000, -0.2754],
        [ 0.0261,  1.4688, -0.8477, -1.0000, -0.7266],
        [ 0.1641,  1.3594, -0.2480, -0.6758, -0.9922],
        [-0.1631,  1.7109, -0.5781, -0.6953, -0.3887],
        [-0.0124,  1.3359, -0.4453, -1.0234, -0.6758],
        [-0.0640,  1.2812, -0.3438, -0.9023, -0.7969],
        [ 0.1445,  1.2344, -0.9766, -0.5781, -0.7422]], device='cuda:0',
       dtype=torch.bfloat16, grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None), SequenceClassifierOutput(loss=tensor(0.6140, device='cuda:0', grad_fn=<NllLossBackward0>), logits=tensor([[ 0.3398,  1.5938, -0.7266, -0.8281, -0.5273],
        [ 0.0771,  1.2266, -0.4551, -0.7500, -1.0312],
        [-0.1729,  1.5234, -0.7617, -0.8203, -0.6523],
        [ 0.2295,  1.5234, -0.8047, -0.8594, -0.5156],
        [ 0.0713,  1.3047, -0.4258, -0.8633, -0.3281],
        [-0.1855,  1.3906, -0.4531, -0.9883, -0.3848],
        [ 0.1904,  1.4297, -0.7891, -1.0938, -0.4473],
        [ 0.2754,  1.4531, -0.9102, -0.8008, -0.1289],
        [ 0.0231,  1.4219, -0.3906, -0.9492, -0.5664],
        [ 0.2139,  1.4766, -0.9023, -1.0469, -0.5469],
        [ 0.1201,  1.4531, -0.7383, -0.8555, -0.8594],
        [-0.0815,  1.5859, -0.2910, -0.7734, -0.6836],
        [ 0.3008,  1.8516, -0.8320, -0.8164, -0.5781],
        [-0.0457,  1.7109, -0.4434, -0.8398, -0.4004],
        [-0.1582,  1.4297, -0.5625, -1.2422, -0.4043],
        [ 0.2852,  1.0312, -1.0781, -0.8359, -0.2256]], device='cuda:0',
       dtype=torch.bfloat16, grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None), SequenceClassifierOutput(loss=tensor(0.8451, device='cuda:0', grad_fn=<NllLossBackward0>), logits=tensor([[ 0.0361,  1.6797, -0.8984, -0.8438, -0.6680],
        [-0.3730,  1.5312, -0.8750, -0.6992, -0.6250],
        [ 0.0593,  1.3828, -0.5117, -0.6484, -0.6641],
        [-0.0425,  1.5000, -0.2051, -0.6953, -0.4551],
        [ 0.3496,  1.4844, -0.8008, -1.0312, -0.5000],
        [ 0.0518,  1.7656, -0.6016, -0.8945, -0.8008],
        [ 0.0977,  1.5312, -0.7656, -0.6836, -0.6055],
        [-0.1758,  1.2266, -0.7539, -0.9336, -0.7500],
        [-0.0067,  1.1172, -0.3867, -0.7305, -0.4922],
        [ 0.0981,  1.3594, -0.9219, -1.2891, -0.5156],
        [-0.1787,  1.6172, -0.8359, -0.7031, -0.4062],
        [ 0.2275,  1.2578, -0.6719, -0.5625, -0.4414],
        [-0.2051,  1.7031, -0.3008, -0.3965, -0.6094],
        [-0.0156,  1.5938, -0.6992, -0.8086, -0.9570],
        [ 0.2305,  1.7344, -0.7773, -1.0469, -0.5078],
        [ 0.1299,  1.5391, -0.9023, -1.0703, -0.7344]], device='cuda:0',
       dtype=torch.bfloat16, grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None), SequenceClassifierOutput(loss=tensor(0.6997, device='cuda:0', grad_fn=<NllLossBackward0>), logits=tensor([[ 0.4805,  1.5703, -0.6836, -0.7148, -0.6680],
        [ 0.0977,  1.6953, -0.8359, -1.2500, -0.5625],
        [ 0.1299,  1.3906, -0.7305, -1.0391, -0.7148],
        [-0.0164,  1.9375, -0.8008, -0.6797, -0.6992],
        [ 0.2812,  1.5547, -0.6992, -0.8789, -0.2344],
        [-0.3145,  1.5391, -0.4609, -0.5508, -0.5898],
        [-0.1196,  1.5703, -0.7305, -0.7695, -0.5859],
        [ 0.2314,  1.4375, -0.8086, -1.0469, -0.3867],
        [ 0.2090,  1.5938, -0.3633, -0.5078, -0.7109],
        [ 0.0034,  1.8438, -0.6367, -0.9844, -0.6055],
        [-0.1816,  1.3594, -0.9180, -0.7344, -0.7500],
        [-0.0374,  1.5781, -0.8125, -0.9648, -0.6875],
        [-0.3047,  1.4922, -0.9336, -0.5664, -0.5898],
        [ 0.2969,  1.1250, -1.0781, -1.1016, -0.3262],
        [ 0.3398,  1.6875, -0.9062, -0.9141, -0.5039],
        [ 0.1924,  1.8125, -0.8867, -0.5938, -0.5000]], device='cuda:0',
       dtype=torch.bfloat16, grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None), SequenceClassifierOutput(loss=tensor(0.7324, device='cuda:0', grad_fn=<NllLossBackward0>), logits=tensor([[-0.5312,  1.7422, -0.6172, -0.7852, -0.4375],
        [ 0.0332,  1.5156, -0.5742, -0.9648, -0.3906],
        [ 0.0425,  1.6094, -0.2910, -0.7266, -0.7188],
        [-0.0605,  1.5938, -0.5469, -1.0078, -0.5820],
        [ 0.1050,  1.3359, -0.5039, -0.9688, -0.9766],
        [ 0.2695,  1.4844, -0.7266, -0.5156, -0.5664],
        [-0.1709,  1.2188, -0.5625, -0.8945, -0.5352],
        [-0.2305,  1.4375, -0.6250, -0.9102, -0.6211],
        [ 0.4375,  1.7422, -0.6211, -0.9453, -0.5938],
        [ 0.0630,  1.2344, -0.6211, -0.6953, -0.1543],
        [-0.2295,  1.6250, -0.5273, -0.9219, -0.6406],
        [ 0.0177,  1.5078, -0.7578, -0.9609, -0.4590],
        [-0.1953,  1.5078, -0.2910, -0.9922, -0.9766],
        [-0.3105,  1.3281, -0.5586, -0.6484, -0.5820],
        [-0.3105,  1.2266, -0.6523, -0.8711, -0.3477],
        [ 0.2217,  1.5469, -0.7344, -0.9961, -0.5859]], device='cuda:0',
       dtype=torch.bfloat16, grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None), SequenceClassifierOutput(loss=tensor(0.8372, device='cuda:0', grad_fn=<NllLossBackward0>), logits=tensor([[ 0.0271,  1.0469, -0.8750, -0.8398, -0.6094],
        [ 0.0518,  1.5781, -0.5625, -1.1094, -0.5078],
        [-0.1118,  1.5078, -0.7188, -1.0469, -0.7344],
        [ 0.1250,  1.8203, -0.7148, -1.1953, -0.6602],
        [-0.1123,  1.2344, -0.3398, -1.0625, -0.7070],
        [-0.1240,  1.4219, -0.4707, -0.4844, -0.4277],
        [ 0.3320,  1.4141, -0.7383, -1.0547, -0.7617],
        [-0.1973,  1.6172, -0.8945, -0.9609, -0.3555],
        [-0.0850,  1.4609, -0.7109, -0.7461, -0.5273],
        [-0.0525,  1.2891, -0.5664, -0.8398, -0.4863],
        [-0.1924,  1.8516, -0.7422, -1.0391, -0.6289],
        [-0.0542,  1.5156, -0.7539, -0.6211, -0.3574],
        [ 0.1235,  1.4141, -0.6016, -0.4941, -0.2930],
        [ 0.0112,  1.3516, -0.6484, -0.8906, -0.6445],
        [-0.0266,  1.4453, -0.9492, -0.8828, -0.7852],
        [ 0.0403,  1.4297, -0.8516, -0.8359, -0.7852]], device='cuda:0',
       dtype=torch.bfloat16, grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None), SequenceClassifierOutput(loss=tensor(0.7964, device='cuda:0', grad_fn=<NllLossBackward0>), logits=tensor([[ 0.4922,  0.8203, -0.6250, -0.0581,  0.3906],
        [ 0.2275,  1.2891, -0.8906, -1.0625, -0.5469],
        [ 0.3926,  1.6797, -0.8320, -0.9102, -0.4434],
        [ 0.0439,  1.3750, -0.5703, -0.9023, -0.8594],
        [ 0.2217,  1.6172, -0.7227, -0.8086, -0.8008],
        [ 0.0050,  1.0469, -0.3926, -0.7148, -0.7266],
        [ 0.0708,  1.7578, -0.6758, -0.8320, -0.2988],
        [ 0.4316,  1.6250, -0.4082, -0.9414, -1.0156],
        [ 0.2812,  1.4297, -0.6523, -1.0000, -0.7422],
        [ 0.3027,  1.0000, -1.2422, -0.9727,  0.2910],
        [ 0.0762,  1.4219, -0.8672, -0.7031, -0.6133],
        [-0.0732,  1.5156, -0.7930, -0.9102, -0.6680],
        [ 0.0330,  1.7656, -0.4863, -0.8008, -0.3730],
        [-0.1807,  1.0156, -0.8477, -0.9805, -0.2598],
        [-0.1084,  1.4844, -0.4180, -0.7422, -0.6016],
        [ 0.3574,  1.5000, -0.5391, -0.6250, -0.3066]], device='cuda:0',
       dtype=torch.bfloat16, grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None), SequenceClassifierOutput(loss=tensor(0.9380, device='cuda:0', grad_fn=<NllLossBackward0>), logits=tensor([[-0.0135,  1.3828, -0.4902, -0.5469, -0.7266],
        [ 0.3203,  1.3750, -0.9961, -0.7695, -0.2354],
        [-0.0449,  1.3438, -0.8906, -0.9688, -0.6992],
        [-0.0830,  1.2266, -0.4043, -0.8828, -0.3125],
        [ 0.0762,  1.7188, -0.4688, -0.8242, -0.4121],
        [-0.0088,  1.6172, -0.3828, -0.9023, -0.4805],
        [ 0.0938,  1.3438, -0.7070, -0.9883, -0.8008],
        [ 0.4473,  1.2734, -0.9102, -0.7852, -0.2412],
        [-0.0618,  1.2109, -0.5352, -0.7422, -0.7188],
        [-0.2910,  1.6875, -0.8125, -0.9492, -0.2168],
        [ 0.2393,  1.0312, -0.8398, -0.7266, -0.2324],
        [ 0.1074,  1.2422, -0.6172, -0.7500, -0.5742],
        [-0.1162,  1.5625, -0.6055, -0.7852, -0.6328],
        [-0.1318,  1.7109, -0.4199, -1.0781, -0.7109],
        [-0.0688,  1.5703, -0.8945, -0.8750, -0.6172],
        [ 0.1147,  1.4844, -0.8438, -1.1094, -0.6680]], device='cuda:0',
       dtype=torch.bfloat16, grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None), SequenceClassifierOutput(loss=tensor(0.6294, device='cuda:0', grad_fn=<NllLossBackward0>), logits=tensor([[-2.6367e-01,  1.5156e+00, -5.1953e-01, -7.6562e-01, -4.0820e-01],
        [ 6.5918e-02,  1.3203e+00, -6.8750e-01, -1.1016e+00, -6.8750e-01],
        [ 1.2988e-01,  1.6953e+00, -9.6094e-01, -9.7656e-01, -3.1250e-02],
        [-2.0117e-01,  1.7578e+00, -8.0859e-01, -5.8984e-01, -4.6094e-01],
        [-3.5938e-01,  1.4219e+00, -8.6719e-01, -7.4609e-01, -5.0781e-01],
        [-6.6406e-02,  1.3750e+00, -3.0078e-01, -5.6250e-01, -2.2852e-01],
        [ 5.3955e-02,  1.5938e+00, -3.9453e-01, -4.3164e-01, -4.2578e-01],
        [ 2.0020e-01,  1.0000e+00, -2.9883e-01, -6.5234e-01, -3.3789e-01],
        [ 1.2012e-01,  1.3750e+00, -5.8984e-01, -7.5391e-01, -4.4922e-01],
        [-1.2793e-01,  1.3438e+00, -6.4062e-01, -1.1797e+00, -3.8477e-01],
        [ 8.0566e-02,  1.6406e+00, -7.3438e-01, -8.9062e-01, -6.4453e-01],
        [-2.8320e-01,  1.9062e+00, -5.1172e-01, -9.5312e-01, -3.3594e-01],
        [ 1.1063e-03,  1.1641e+00, -2.4023e-01, -1.0859e+00, -9.2578e-01],
        [-5.0293e-02,  1.5469e+00, -4.9609e-01, -9.6094e-01, -9.4141e-01],
        [ 2.5879e-02,  1.6250e+00, -5.5469e-01, -7.8125e-01, -5.8594e-01],
        [ 3.1641e-01,  1.3203e+00, -2.6953e-01, -8.2422e-01, -3.9062e-01]],
       device='cuda:0', dtype=torch.bfloat16, grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None), SequenceClassifierOutput(loss=tensor(0.6628, device='cuda:0', grad_fn=<NllLossBackward0>), logits=tensor([[ 0.0048,  1.6016, -0.9219, -0.9375, -0.6016],
        [-0.2207,  1.5938, -0.6016, -0.8789, -0.8086],
        [ 0.4668,  1.3359, -0.5430, -0.4609, -0.7344],
        [-0.1875,  1.8203, -0.6094, -0.7148, -0.8359],
        [-0.0557,  1.3203, -0.7930, -1.1406, -0.6406],
        [ 0.0488,  1.3594, -0.5312, -1.2266, -1.0469],
        [ 0.2197,  1.2188, -0.6016, -1.0469, -0.5703],
        [-0.0967,  1.4922, -0.7656, -1.0391, -0.4941],
        [ 0.3027,  1.2188, -0.5312, -1.0625, -0.8047],
        [ 0.0640,  1.4609, -0.6133, -1.1484, -0.5820],
        [-0.0498,  1.5312, -0.6367, -0.7109, -0.7266],
        [-0.0591,  1.7031, -0.5391, -0.7695, -0.5430],
        [ 0.1338,  1.6641, -0.7578, -0.8945, -0.6523],
        [ 0.0217,  1.6719, -0.6562, -0.9492, -0.7695],
        [ 0.2832,  1.3203, -0.6406, -0.6953, -0.7422],
        [ 0.2871,  1.1953, -0.8086, -0.9023, -0.5352]], device='cuda:0',
       dtype=torch.bfloat16, grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None), SequenceClassifierOutput(loss=tensor(0.5986, device='cuda:0', grad_fn=<NllLossBackward0>), logits=tensor([[ 0.1709,  1.4453, -0.4570, -0.9258, -0.5039],
        [-0.4727,  1.3203, -0.6133, -0.6719, -0.9492],
        [ 0.0830,  1.3516, -0.6914, -0.7539, -0.7734],
        [ 0.2178,  1.7500, -0.8438, -0.7891, -0.4941],
        [ 0.1338,  1.6016, -0.5078, -1.0312, -0.4922],
        [ 0.1035,  1.5312, -0.6719, -0.8242, -0.4609],
        [ 0.1367,  1.5078, -0.8906, -0.8594, -0.2676],
        [-0.2148,  1.3359, -0.4531, -0.8242, -0.5859],
        [-0.0625,  1.5938, -0.7539, -0.7461, -0.7539],
        [ 0.0208,  1.6641, -0.8828, -0.7930, -0.2891],
        [ 0.2158,  1.3906, -0.7812, -0.6836, -0.3848],
        [ 0.1426,  1.4609, -0.2773, -0.4434, -0.6719],
        [-0.1191,  1.2031, -0.5977, -0.7461, -0.3984],
        [-0.2119,  1.4531, -0.6016, -0.9336, -0.6602],
        [-0.0332,  1.1094, -0.7031, -1.1484, -0.7383],
        [ 0.4668,  1.0000, -1.0312, -0.4375, -0.5664]], device='cuda:0',
       dtype=torch.bfloat16, grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None), SequenceClassifierOutput(loss=tensor(0.7384, device='cuda:0', grad_fn=<NllLossBackward0>), logits=tensor([[ 1.7090e-01,  1.5625e+00, -8.8281e-01, -1.0703e+00, -5.9375e-01],
        [-8.1543e-02,  1.2656e+00, -8.4766e-01, -8.7109e-01, -5.2734e-01],
        [ 6.4941e-02,  1.5234e+00, -6.5625e-01, -1.0078e+00, -7.6953e-01],
        [ 1.6895e-01,  1.1719e+00, -1.0547e+00, -4.1797e-01, -4.1406e-01],
        [ 4.8633e-01,  1.5781e+00, -8.9453e-01, -9.8438e-01, -4.8242e-01],
        [ 1.0059e-01,  1.5156e+00, -5.7812e-01, -1.0000e+00, -5.4297e-01],
        [-3.4027e-03,  1.4141e+00, -2.1289e-01, -9.2578e-01, -8.0078e-01],
        [-1.4551e-01,  1.6172e+00, -7.3828e-01, -8.7891e-01, -4.7461e-01],
        [ 2.7539e-01,  1.3125e+00, -6.6016e-01, -7.6953e-01, -6.5625e-01],
        [ 1.7242e-03,  1.7656e+00, -7.1484e-01, -8.4375e-01, -4.8242e-01],
        [ 3.1055e-01,  7.6953e-01, -9.8828e-01, -9.3359e-01, -3.4766e-01],
        [-7.4219e-02,  1.4297e+00, -6.4062e-01, -9.8438e-01, -1.1250e+00],
        [ 2.9297e-01,  1.7031e+00, -1.0781e+00, -6.2891e-01, -5.4297e-01],
        [-5.1025e-02,  1.5078e+00, -6.0156e-01, -7.4609e-01, -5.1172e-01],
        [ 4.4727e-01,  1.7891e+00, -8.6719e-01, -8.9453e-01, -4.8047e-01],
        [-2.8320e-02,  1.3828e+00, -6.7188e-01, -7.3047e-01, -6.8359e-01]],
       device='cuda:0', dtype=torch.bfloat16, grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None), SequenceClassifierOutput(loss=tensor(0.5615, device='cuda:0', grad_fn=<NllLossBackward0>), logits=tensor([[ 0.0071,  1.4688, -0.6797, -1.0547, -0.7148],
        [-0.0042,  1.5312, -0.8906, -0.7383, -0.6523],
        [-0.0374,  1.5781, -0.5547, -1.0078, -0.4668],
        [ 0.2412,  1.3359, -0.9531, -0.9258, -0.6328],
        [-0.1738,  1.6406, -0.5117, -1.0938, -0.3340],
        [-0.0703,  1.4375, -0.4668, -0.8867, -0.8672],
        [ 0.1128,  1.6016, -0.5977, -0.8320, -0.4492],
        [ 0.0967,  1.6016, -0.3906, -0.7773, -0.5391],
        [ 0.0479,  1.5234, -0.4336, -1.1562, -0.5977],
        [ 0.2422,  1.3594, -0.6719, -0.8242, -0.6055],
        [-0.3613,  1.6328, -0.5781, -0.9922, -0.4102],
        [-0.1846,  1.1250, -0.5625, -0.6484, -0.3379],
        [ 0.3496,  1.7422, -0.4336, -0.7891, -0.6836],
        [ 0.0143,  1.3516, -0.9141, -0.6250, -0.4531],
        [ 0.1455,  1.5469, -0.6719, -1.0859, -0.7383],
        [ 0.2852,  0.7188, -1.0547, -0.3438,  0.2871]], device='cuda:0',
       dtype=torch.bfloat16, grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None), SequenceClassifierOutput(loss=tensor(0.6472, device='cuda:0', grad_fn=<NllLossBackward0>), logits=tensor([[-0.0085,  1.6875, -0.8281, -1.1719, -0.5273],
        [-0.4570,  1.4297, -0.5898, -0.7422, -0.5352],
        [-0.0791,  1.6016, -0.9883, -0.9375, -0.5664],
        [ 0.0728,  1.6797, -0.7070, -0.6484, -0.2285],
        [ 0.2109,  1.9375, -0.7695, -1.0156, -0.7461],
        [ 0.1030,  1.1875, -0.9023, -0.8398, -0.8711],
        [ 0.0269,  1.3047, -0.8750, -0.8203, -0.4688],
        [ 0.2490,  1.2266, -0.6328, -0.8867, -0.4121],
        [-0.2754,  1.4922, -0.5508, -0.7852, -0.4219],
        [ 0.2949,  1.5391, -0.8789, -0.9570, -0.3770],
        [-0.0052,  1.5078, -0.8164, -0.8906, -0.3652],
        [ 0.1621,  1.4688, -0.6523, -0.7500, -0.6016],
        [ 0.2080,  1.5469, -0.5312, -0.8086, -0.3281],
        [-0.2178,  1.5703, -0.6445, -1.0078, -0.8594],
        [ 0.0879,  1.5938, -0.5781, -1.1484, -0.5000],
        [ 0.1396,  1.0938, -0.4512, -1.0469, -0.4668]], device='cuda:0',
       dtype=torch.bfloat16, grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None), SequenceClassifierOutput(loss=tensor(0.6681, device='cuda:0', grad_fn=<NllLossBackward0>), logits=tensor([[ 0.0435,  1.5625, -0.6133, -0.9688, -0.7539],
        [ 0.3555,  1.6016, -0.4844, -0.5039, -0.5273],
        [ 0.0962,  1.9453, -0.8711, -1.1406, -0.7461],
        [-0.1133,  1.6406, -0.6094, -0.8516, -0.5469],
        [-0.0060,  1.2812, -0.8789, -0.9258, -0.4609],
        [ 0.1602,  1.3672, -0.9102, -0.9219, -0.5273],
        [-0.2500,  1.7500, -0.7422, -1.0312, -0.7852],
        [ 0.1768,  1.9062, -0.8750, -1.2812, -0.3691],
        [ 0.0040,  1.1016, -0.3789, -0.6992, -0.2969],
        [-0.0415,  1.2109, -0.7344, -0.8086, -0.8633],
        [-0.0535,  1.7031, -0.6914, -1.1953, -0.8047],
        [-0.0093,  1.9297, -0.5898, -0.6523, -0.5625],
        [ 0.1118,  1.7344, -1.0078, -0.9062, -0.8164],
        [-0.0078,  1.6406, -0.5469, -1.0859, -0.6406],
        [-0.5117,  1.5391, -0.5938, -0.4512, -0.7891],
        [ 0.1270,  1.6406, -0.5469, -0.8555, -0.5938]], device='cuda:0',
       dtype=torch.bfloat16, grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None), SequenceClassifierOutput(loss=tensor(0.8804, device='cuda:0', grad_fn=<NllLossBackward0>), logits=tensor([[ 3.1250e-01,  1.2188e+00, -7.0703e-01, -1.2109e+00, -4.9609e-01],
        [-2.0117e-01,  1.4766e+00, -7.8516e-01, -8.0078e-01, -8.2422e-01],
        [ 2.0215e-01,  1.3828e+00, -9.3359e-01, -1.1094e+00, -7.3438e-01],
        [ 1.0938e-01,  1.2891e+00, -7.5000e-01, -6.0938e-01, -3.1836e-01],
        [-1.1169e-02,  1.8828e+00, -4.5703e-01, -7.1875e-01, -7.8906e-01],
        [-4.0625e-01,  1.7734e+00, -7.3047e-01, -8.5547e-01, -8.0469e-01],
        [ 2.5586e-01,  1.4609e+00, -6.9922e-01, -7.1094e-01, -7.1094e-01],
        [ 1.6406e-01,  1.3281e+00, -7.5781e-01, -7.6953e-01, -6.5234e-01],
        [-1.2207e-01,  1.7188e+00, -5.1953e-01, -7.0703e-01, -4.7461e-01],
        [-1.9455e-03,  1.8438e+00, -9.7656e-01, -7.9688e-01, -5.8203e-01],
        [-1.0376e-03,  1.5938e+00, -7.1094e-01, -1.1172e+00, -6.8359e-01],
        [ 2.2168e-01,  1.2344e+00, -3.6133e-01, -8.7500e-01, -5.6641e-01],
        [-2.1484e-01,  1.3125e+00, -5.7031e-01, -7.6562e-01, -5.9375e-01],
        [-2.0898e-01,  1.4844e+00, -4.5117e-01, -8.6719e-01, -6.1328e-01],
        [ 1.8945e-01,  1.3984e+00, -4.7070e-01, -6.6406e-01, -3.8086e-01],
        [ 1.2451e-01,  1.5781e+00, -8.0859e-01, -7.3828e-01, -2.4414e-01]],
       device='cuda:0', dtype=torch.bfloat16, grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None), SequenceClassifierOutput(loss=tensor(0.9746, device='cuda:0', grad_fn=<NllLossBackward0>), logits=tensor([[ 2.7344e-01,  1.6484e+00, -3.3789e-01, -8.6719e-01, -7.3828e-01],
        [ 1.4160e-01,  1.2969e+00, -1.0703e+00, -7.5781e-01, -4.9023e-01],
        [-2.3145e-01,  1.6172e+00, -6.6406e-01, -8.7891e-01, -3.0273e-01],
        [-1.5918e-01,  1.3750e+00, -5.7422e-01, -9.3750e-01, -5.1172e-01],
        [ 7.9346e-03,  1.2734e+00, -7.1484e-01, -6.9531e-01, -7.5000e-01],
        [ 5.8838e-02,  1.2188e+00, -4.3555e-01, -7.2656e-01, -6.0938e-01],
        [ 6.0059e-02,  1.7109e+00, -8.5547e-01, -8.9062e-01, -4.5117e-01],
        [ 6.2500e-02,  1.4375e+00, -8.1250e-01, -1.0625e+00, -5.8984e-01],
        [-9.3750e-02,  1.7422e+00, -5.5859e-01, -6.6406e-01, -5.9766e-01],
        [ 2.4023e-01,  1.0859e+00, -5.8203e-01, -7.4219e-01, -1.0000e+00],
        [-6.9046e-04,  1.1953e+00, -6.5625e-01, -6.0156e-01, -3.3789e-01],
        [-2.6733e-02,  1.5000e+00, -5.6641e-01, -8.3984e-01, -4.3750e-01],
        [ 1.6602e-01,  1.6172e+00, -7.5781e-01, -9.2969e-01, -5.9375e-01],
        [-1.9824e-01,  1.4141e+00, -3.7500e-01, -6.5625e-01, -3.1641e-01],
        [ 1.3574e-01,  1.5703e+00, -6.9922e-01, -5.8594e-01, -1.8848e-01],
        [-3.6865e-02,  1.2891e+00, -6.5625e-01, -1.0078e+00, -3.9258e-01]],
       device='cuda:0', dtype=torch.bfloat16, grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None), SequenceClassifierOutput(loss=tensor(0.6829, device='cuda:0', grad_fn=<NllLossBackward0>), logits=tensor([[-0.1060,  1.9766, -0.8789, -0.9141, -0.5391],
        [-0.3125,  1.4922, -0.5469, -1.0391, -0.6641],
        [-0.2227,  1.6953, -0.4863, -1.0312, -0.5039],
        [-0.0674,  1.5781, -0.8359, -0.8633, -0.2314],
        [ 0.0159,  2.0000, -0.7539, -0.9141, -0.6094],
        [ 0.3809,  0.9297, -0.3145, -0.6406, -0.0933],
        [-0.0388,  1.6953, -0.7305, -1.1953, -0.7539],
        [-0.2090,  1.4219, -0.5430, -1.1250, -0.6484],
        [-0.0894,  1.3750, -0.4844, -1.1484, -0.6016],
        [ 0.0747,  1.6172, -0.3086, -1.0078, -0.6250],
        [-0.1943,  1.0469, -0.4238, -0.7930, -0.7461],
        [ 0.0664,  1.6172, -0.6250, -0.8438, -1.0234],
        [ 0.0591,  1.4062, -0.6641, -0.6953, -0.3711],
        [-0.0923,  1.6484, -0.6602, -0.9414, -0.5625],
        [ 0.1904,  1.5000, -0.8555, -0.9922, -0.5273],
        [ 0.3691,  1.6719, -0.7461, -0.9180, -0.6406]], device='cuda:0',
       dtype=torch.bfloat16, grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None), SequenceClassifierOutput(loss=tensor(0.6603, device='cuda:0', grad_fn=<NllLossBackward0>), logits=tensor([[-0.2637,  1.5234, -0.9844, -0.5977, -0.6992],
        [ 0.2314,  1.4141, -0.7930, -1.0156, -0.7695],
        [ 0.2500,  1.8594, -1.0391, -1.0000, -0.3066],
        [ 0.2070,  1.1719, -0.9922, -0.9531, -0.5039],
        [-0.1191,  1.6484, -0.3379, -0.7969, -0.9102],
        [-0.1719,  1.7734, -0.3652, -0.4512, -0.3320],
        [ 0.2217,  1.1719, -1.1562, -0.7383, -0.6172],
        [-0.2256,  1.5312, -0.3418, -0.5781, -0.4316],
        [-0.0669,  1.4219, -0.6914, -1.0703, -0.8164],
        [-0.2656,  1.1875, -0.5938, -0.8906, -1.0234],
        [ 0.2461,  1.2109, -0.5898, -0.6172, -0.8867],
        [ 0.1543,  1.6016, -0.7578, -0.8828, -0.5664],
        [ 0.3438,  1.4531, -0.4883, -0.6016, -0.6992],
        [-0.0962,  1.3438, -0.5039, -0.8438, -0.5195],
        [-0.0542,  1.6562, -0.7031, -0.7773, -0.5859],
        [-0.0107,  1.5391, -0.4961, -1.0078, -0.6094]], device='cuda:0',
       dtype=torch.bfloat16, grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None), SequenceClassifierOutput(loss=tensor(0.7467, device='cuda:0', grad_fn=<NllLossBackward0>), logits=tensor([[-0.0500,  1.9141, -0.8750, -0.8242, -0.5586],
        [ 0.0167,  1.1719, -0.7969, -0.9062, -0.6016],
        [-0.1523,  1.7812, -0.7539, -0.9258, -0.4004],
        [-0.3145,  1.2266, -0.4648, -1.1641, -0.7500],
        [-0.1064,  1.4453, -0.8086, -0.9688, -0.6914],
        [ 0.0518,  1.3984, -0.8359, -0.7148, -0.5039],
        [-0.1084,  1.6328, -0.5352, -1.2422, -0.6641],
        [ 0.1172,  1.8672, -0.8750, -0.9922, -0.6016],
        [-0.0204,  1.7109, -0.4043, -0.6992, -0.5312],
        [-0.0045,  1.7734, -0.8711, -0.8477, -0.5078],
        [-0.3105,  1.1719, -0.5859, -0.9492, -0.5352],
        [ 0.0737,  1.1406, -0.6445, -0.9023, -0.5703],
        [ 0.0325,  1.4375, -0.5664, -1.1562, -0.4785],
        [-0.3398,  1.7188, -0.6445, -0.9570, -0.5586],
        [-0.0295,  1.7109, -0.7773, -0.9258, -0.6211],
        [-0.0093,  1.8203, -0.6055, -1.1406, -0.5898]], device='cuda:0',
       dtype=torch.bfloat16, grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None), SequenceClassifierOutput(loss=tensor(0.9274, device='cuda:0', grad_fn=<NllLossBackward0>), logits=tensor([[ 0.0674,  0.9922, -0.6758, -0.8984, -0.4785],
        [ 0.0586,  1.4922, -0.5195, -0.9648, -0.5352],
        [ 0.2852,  1.1484, -1.0938, -0.7461, -0.0713],
        [ 0.0476,  1.6406, -0.6719, -1.0625, -0.7656],
        [ 0.0479,  1.7422, -0.7656, -1.1719, -0.4629],
        [-0.0781,  1.3594, -0.3203, -0.8555, -0.6523],
        [-0.0571,  1.3359, -0.7930, -0.6133, -0.6289],
        [ 0.1079,  1.6484, -0.8828, -1.2734, -0.5039],
        [-0.0347,  1.5312, -0.5898, -0.9141, -0.6211],
        [-0.0094,  1.7969, -0.6055, -0.9414, -0.7266],
        [-0.1270,  1.5547, -0.5391, -1.1016, -0.5898],
        [ 0.1064,  1.3047, -0.5508, -0.8594, -0.5000],
        [ 0.1572,  1.3750, -0.6602, -0.7617, -0.3535],
        [ 0.2129,  1.2422, -0.7812, -1.0781, -0.6406],
        [ 0.0376,  1.7188, -0.8750, -1.0391, -0.7930],
        [ 0.0378,  1.8984, -0.7773, -0.8086, -0.4883]], device='cuda:0',
       dtype=torch.bfloat16, grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None), SequenceClassifierOutput(loss=tensor(0.6754, device='cuda:0', grad_fn=<NllLossBackward0>), logits=tensor([[-0.0679,  1.5000, -0.4434, -0.8086, -0.8008],
        [ 0.1816,  1.5547, -0.6797, -0.7852, -0.6133],
        [-0.0216,  1.7500, -0.8047, -0.9219, -0.7109],
        [-0.0530,  1.5469, -0.5742, -0.7891, -0.8242],
        [ 0.0369,  1.4141, -0.6250, -0.7500, -0.4629],
        [-0.0869,  1.2109, -0.6719, -0.9961, -0.6914],
        [-0.2949,  1.6797, -0.4824, -0.7266, -0.6328],
        [-0.0713,  1.5234, -0.6250, -0.7305, -0.6836],
        [ 0.0123,  1.7188, -0.4824, -1.0156, -0.5430],
        [-0.1455,  1.4062, -0.8164, -0.9414, -0.7266],
        [ 0.3438,  1.7109, -0.6094, -1.0156, -1.0391],
        [ 0.1523,  1.3984, -0.3184, -0.9062, -0.5781],
        [ 0.0879,  1.1641, -0.5898, -0.8477, -0.3711],
        [-0.0283,  1.5312, -0.6875, -0.7773, -0.6406],
        [ 0.0398,  1.2734, -0.8086, -0.7422, -0.5117],
        [ 0.0903,  1.2578, -0.8320, -0.6836, -0.4766]], device='cuda:0',
       dtype=torch.bfloat16, grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None), SequenceClassifierOutput(loss=tensor(0.8809, device='cuda:0', grad_fn=<NllLossBackward0>), logits=tensor([[ 0.2520,  1.5234, -0.7031, -1.0469, -0.6016],
        [ 0.1758,  1.5703, -0.5859, -1.2969, -0.4258],
        [ 0.3730,  1.2188, -1.0078, -0.8789, -0.2314],
        [ 0.1592,  1.7109, -0.5156, -0.8242, -0.6016],
        [ 0.0771,  1.7500, -0.7148, -0.6055, -0.4688],
        [-0.0957,  1.1328, -0.6055, -0.6680, -0.3750],
        [-0.0903,  1.5312, -0.6328, -0.8047, -0.5000],
        [ 0.1133,  1.7188, -0.6523, -1.2188, -0.5664],
        [-0.0630,  1.5703, -0.5039, -0.9727, -0.5625],
        [-0.3340,  1.5078, -0.2754, -0.6836, -0.5312],
        [-0.1162,  1.8906, -0.5977, -1.0625, -0.7422],
        [ 0.1245,  1.0312, -0.7344, -0.5273, -0.3848],
        [-0.1729,  1.6719, -0.6133, -0.7773, -0.6289],
        [ 0.1836,  1.3281, -0.6523, -0.9258, -0.1230],
        [-0.0190,  1.7656, -0.5703, -1.0391, -0.7227],
        [-0.1465,  1.6094, -0.3008, -0.5547, -0.7383]], device='cuda:0',
       dtype=torch.bfloat16, grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None), SequenceClassifierOutput(loss=tensor(0.5123, device='cuda:0', grad_fn=<NllLossBackward0>), logits=tensor([[ 0.4512,  0.9570, -0.7344, -0.9961, -0.6680],
        [-0.1934,  1.4453, -0.6055, -0.8906, -0.8984],
        [-0.1211,  1.3359, -0.5000, -0.9883, -0.6094],
        [ 0.0327,  1.4531, -0.7461, -0.8242, -0.6289],
        [ 0.0488,  1.3438, -0.6172, -0.6758, -0.8750],
        [ 0.0496,  1.5938, -0.7891, -0.9453, -0.3398],
        [-0.0972,  1.0938, -0.7500, -0.9414, -0.4746],
        [ 0.3555,  1.4141, -0.4355, -0.6367, -0.4492],
        [-0.0084,  1.7266, -0.8047, -0.8867, -0.4785],
        [ 0.0781,  1.3906, -0.8750, -1.0312, -0.6797],
        [ 0.0106,  1.4688, -0.6133, -1.1250, -0.4785],
        [-0.1689,  1.5391, -0.5625, -0.6641, -0.8516],
        [ 0.1504,  1.3828, -0.7109, -0.8594, -0.5977],
        [ 0.1216,  1.7812, -0.7383, -0.9414, -0.4922],
        [-0.0698,  1.6875, -0.5859, -0.9492, -0.7109],
        [-0.0255,  1.2188, -0.7227, -0.7578, -0.5820]], device='cuda:0',
       dtype=torch.bfloat16, grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None), SequenceClassifierOutput(loss=tensor(0.4972, device='cuda:0', grad_fn=<NllLossBackward0>), logits=tensor([[-0.3730,  1.2734, -0.8047, -0.6719, -0.5234],
        [ 0.1582,  1.9219, -0.3203, -0.9102, -0.6562],
        [ 0.1328,  1.3594, -0.7734, -0.5703, -0.5938],
        [-0.1147,  1.5859, -0.6758, -0.9492, -0.5391],
        [-0.2324,  1.6484, -0.5430, -0.5234, -0.5312],
        [ 0.0713,  1.5078, -0.8203, -0.3340, -0.3652],
        [ 0.4883,  1.6172, -0.7109, -0.7930, -0.3652],
        [ 0.2324,  1.3125, -0.9258, -1.0078, -0.7070],
        [ 0.2305,  1.5547, -0.7305, -0.7070, -0.4785],
        [-0.1807,  1.6641, -0.7539, -0.7930, -0.7422],
        [-0.2520,  1.5469, -0.5781, -0.9609, -0.5234],
        [ 0.0315,  1.3594, -0.7031, -0.6914, -0.3477],
        [-0.0811,  1.8750, -0.8320, -0.8984, -0.5156],
        [-0.0356,  1.8047, -0.7852, -0.8477, -0.4609],
        [ 0.1631,  1.6406, -0.5781, -0.8242, -0.5078],
        [-0.2139,  1.6641, -0.5742, -1.3203, -0.4199]], device='cuda:0',
       dtype=torch.bfloat16, grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None), SequenceClassifierOutput(loss=tensor(0.6538, device='cuda:0', grad_fn=<NllLossBackward0>), logits=tensor([[-0.2295,  1.5078, -0.6562, -0.7773, -0.4199],
        [ 0.0126,  1.5625, -0.7734, -1.1016, -0.6953],
        [ 0.1187,  1.3438, -0.7852, -0.7695, -0.8008],
        [ 0.2988,  1.1250, -0.6055, -0.9609, -0.7188],
        [-0.0287,  1.4219, -0.6016, -0.8320, -0.3926],
        [ 0.3223,  1.2969, -0.9258, -0.6602, -0.4531],
        [-0.2266,  1.7578, -0.2412, -0.6289, -0.7305],
        [ 0.1279,  1.7656, -0.8047, -0.7812, -0.5977],
        [-0.2832,  1.5078, -0.6875, -0.6484, -0.5859],
        [ 0.1816,  1.8672, -0.6523, -0.9570, -0.5469],
        [ 0.1426,  1.4922, -0.4219, -0.9531, -0.4199],
        [-0.1201,  1.7109, -0.7344, -1.0859, -0.4004],
        [ 0.0299,  1.2891, -0.5117, -0.8477, -0.4727],
        [-0.1011,  1.3359, -0.4707, -0.5781, -0.6836],
        [-0.0309,  1.5469, -0.6758, -1.1406, -0.5273],
        [-0.0403,  1.2188, -0.6602, -0.6406, -0.7695]], device='cuda:0',
       dtype=torch.bfloat16, grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None), SequenceClassifierOutput(loss=tensor(0.4987, device='cuda:0', grad_fn=<NllLossBackward0>), logits=tensor([[-1.3672e-01,  1.6484e+00, -3.1445e-01, -8.7891e-01, -4.7461e-01],
        [ 2.3340e-01,  1.0625e+00, -1.1172e+00, -7.2266e-01, -6.2500e-02],
        [ 5.5176e-02,  1.6484e+00, -6.6406e-01, -8.7109e-01, -5.8984e-01],
        [ 1.6895e-01,  1.3984e+00, -3.5352e-01, -9.1406e-01, -6.4062e-01],
        [-1.1328e-01,  1.4766e+00, -5.4688e-01, -9.1016e-01, -7.8125e-01],
        [ 5.0049e-03,  1.2109e+00, -7.2656e-01, -7.6562e-01, -6.4844e-01],
        [-1.6211e-01,  1.5000e+00, -4.0039e-01, -8.1250e-01, -8.1250e-01],
        [-3.9648e-01,  1.4844e+00, -8.3984e-01, -8.6719e-01, -7.6172e-01],
        [ 3.1738e-02,  1.5312e+00, -8.9062e-01, -7.5781e-01, -9.2188e-01],
        [ 1.5030e-03,  1.8906e+00, -7.3047e-01, -4.5312e-01, -4.0820e-01],
        [-3.6377e-02,  1.6094e+00, -3.6719e-01, -7.2656e-01, -7.7344e-01],
        [-1.5015e-02,  1.4062e+00, -4.5508e-01, -7.8125e-01, -5.0781e-01],
        [ 2.1484e-01,  1.5547e+00, -6.0938e-01, -9.0625e-01, -4.8438e-01],
        [-1.5918e-01,  1.3438e+00, -6.7969e-01, -7.2656e-01, -5.8594e-01],
        [ 4.9561e-02,  1.9688e+00, -8.1250e-01, -8.4766e-01, -3.5352e-01],
        [ 2.6172e-01,  1.8281e+00, -7.6953e-01, -7.8906e-01, -5.3125e-01]],
       device='cuda:0', dtype=torch.bfloat16, grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None), SequenceClassifierOutput(loss=tensor(1.0358, device='cuda:0', grad_fn=<NllLossBackward0>), logits=tensor([[ 2.1289e-01,  1.3125e+00, -4.8047e-01, -1.2188e+00, -8.3203e-01],
        [ 1.7773e-01,  1.2422e+00, -4.3750e-01, -8.5547e-01, -4.2773e-01],
        [ 1.1230e-01,  1.6328e+00, -9.2969e-01, -9.8047e-01, -1.4844e-01],
        [-6.4453e-02,  1.6016e+00, -5.8203e-01, -8.1641e-01, -3.4180e-01],
        [ 1.5723e-01,  1.7656e+00, -7.1094e-01, -1.1641e+00, -4.9805e-01],
        [-7.5684e-02,  1.4766e+00, -5.0781e-01, -8.1641e-01, -8.2031e-01],
        [ 6.2500e-01,  8.3594e-01, -1.1484e+00, -7.3047e-01,  1.0156e-01],
        [-1.1182e-01,  1.4219e+00, -3.6914e-01, -8.3203e-01, -4.5508e-01],
        [-2.0703e-01,  1.7500e+00, -7.6562e-01, -8.0078e-01, -4.9805e-01],
        [-1.6022e-03,  1.5781e+00, -5.3125e-01, -9.6484e-01, -4.2188e-01],
        [-2.7539e-01,  1.8203e+00, -5.1562e-01, -9.6484e-01, -5.9375e-01],
        [ 1.0400e-01,  1.7812e+00, -6.9531e-01, -6.1719e-01, -4.6094e-01],
        [-8.9844e-02,  1.5234e+00, -8.4766e-01, -6.4844e-01, -5.9375e-01],
        [ 1.3574e-01,  1.8672e+00, -6.5625e-01, -1.0547e+00, -7.3438e-01],
        [-1.1865e-01,  1.5312e+00, -3.1641e-01, -8.3594e-01, -7.8906e-01],
        [ 7.9590e-02,  1.3594e+00, -6.7578e-01, -5.5469e-01, -5.8594e-01]],
       device='cuda:0', dtype=torch.bfloat16, grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None), SequenceClassifierOutput(loss=tensor(0.8572, device='cuda:0', grad_fn=<NllLossBackward0>), logits=tensor([[ 0.1797,  1.6406, -0.4805, -0.9805, -0.3320],
        [ 0.1484,  1.3906, -0.7812, -0.9727, -0.3965],
        [-0.0265,  1.5625, -0.8086, -1.0781, -0.5859],
        [-0.0442,  1.2344, -0.3926, -0.6172, -0.9648],
        [ 0.1172,  1.3438, -0.8516, -1.1250, -0.9219],
        [ 0.3730,  1.4219, -0.5391, -0.7227, -0.5703],
        [-0.1138,  1.0078, -0.9766, -0.9805, -0.2773],
        [ 0.1836,  1.5625, -0.6641, -0.8984, -0.8398],
        [ 0.1377,  1.5781, -0.7930, -0.9336, -0.6641],
        [ 0.2012,  1.6016, -0.8750, -0.8008, -0.6094],
        [-0.1328,  1.5938, -0.5078, -1.1016, -0.6445],
        [ 0.0354,  0.8945, -0.7969, -0.8984, -0.5391],
        [-0.0371,  1.6328, -0.5039, -0.8828, -0.4512],
        [ 0.0053,  1.6797, -0.5703, -0.9414, -0.7422],
        [-0.2402,  1.7109, -0.6211, -0.6875, -0.5703],
        [-0.0359,  1.6250, -0.9023, -0.6523, -0.4688]], device='cuda:0',
       dtype=torch.bfloat16, grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None), SequenceClassifierOutput(loss=tensor(0.9415, device='cuda:0', grad_fn=<NllLossBackward0>), logits=tensor([[-0.0767,  1.5156, -0.6797, -0.7617, -0.1943],
        [ 0.0537,  1.4453, -0.6016, -0.9336, -0.4316],
        [-0.0393,  1.6484, -0.1318, -0.9375, -0.6602],
        [ 0.0947,  1.5000, -0.6367, -0.6562, -0.5859],
        [ 0.2432,  1.6328, -0.5000, -1.0469, -0.8398],
        [ 0.1875,  1.4297, -0.3027, -1.1719, -0.5156],
        [ 0.0618,  1.1016, -0.5273, -0.6484, -0.4102],
        [-0.0310,  1.6016, -0.4082, -1.0312, -0.5195],
        [-0.1167,  1.5938, -0.6016, -0.9023, -0.3574],
        [ 0.0991,  1.3594, -0.7109, -0.7656, -0.2949],
        [ 0.0747,  0.9531, -0.2637, -0.9219, -0.5703],
        [ 0.1982,  1.8281, -0.7109, -0.8555, -0.6328],
        [-0.3418,  1.5391, -0.6758, -0.7266, -0.6719],
        [ 0.2715,  1.5703, -1.0469, -1.1562, -0.3613],
        [ 0.0977,  1.1719, -0.8594, -1.2109, -0.4336],
        [ 0.2393,  1.6641, -0.8242, -1.1797, -0.5117]], device='cuda:0',
       dtype=torch.bfloat16, grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None), SequenceClassifierOutput(loss=tensor(0.8271, device='cuda:0', grad_fn=<NllLossBackward0>), logits=tensor([[-0.1191,  1.3906, -0.4336, -0.5938, -0.3691],
        [-0.2109,  1.6016, -0.5977, -0.8516, -0.6992],
        [-0.1079,  1.5625, -0.4668, -1.1172, -0.8594],
        [ 0.0052,  1.5781, -1.1406, -1.0156, -0.8281],
        [ 0.3164,  0.5508, -0.8516, -0.1533,  0.3438],
        [ 0.3027,  1.1328, -0.7812, -0.6562, -0.6914],
        [ 0.0378,  1.6328, -0.7461, -0.6836, -0.5703],
        [-0.2490,  1.4922, -0.1582, -0.8984, -0.7656],
        [ 0.1924,  1.5000, -0.4102, -0.9805, -0.9258],
        [-0.0198,  1.3984, -0.7188, -0.9531, -0.7227],
        [ 0.1089,  1.5078, -0.7422, -0.7031, -0.6055],
        [ 0.1973,  1.5078, -0.8359, -0.5625, -0.3594],
        [ 0.1235,  1.5938, -0.7500, -0.8359, -0.4648],
        [-0.1699,  1.1172, -0.5273, -0.8711, -0.6445],
        [ 0.1777,  1.5156, -0.9766, -0.9102, -0.6836],
        [ 0.0757,  1.8125, -0.7852, -0.8945, -0.6914]], device='cuda:0',
       dtype=torch.bfloat16, grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None), SequenceClassifierOutput(loss=tensor(0.6965, device='cuda:0', grad_fn=<NllLossBackward0>), logits=tensor([[-0.0272,  1.2734, -0.6328, -0.4941, -0.4199],
        [ 0.0267,  1.3672, -0.9062, -0.8633, -0.4727],
        [ 0.1807,  1.5938, -0.8203, -1.0391, -0.6172],
        [ 0.0669,  1.3906, -0.5508, -0.7188, -0.3848],
        [ 0.3262,  1.1562, -0.6406, -0.8047, -0.5352],
        [-0.0742,  1.4141, -0.6562, -0.8086, -0.8438],
        [ 0.1245,  1.5625, -0.6523, -0.9297, -0.4863],
        [-0.1650,  1.3047, -0.6719, -0.9219, -0.6797],
        [-0.3359,  1.4922, -0.3555, -0.8516, -0.2520],
        [ 0.1611,  1.3984, -0.7344, -0.7188, -0.7344],
        [-0.0261,  1.8828, -0.5312, -0.9883, -0.3555],
        [ 0.1865,  1.5391, -0.4941, -0.8984, -0.7070],
        [ 0.1079,  1.6328, -0.5508, -1.1484, -0.4258],
        [ 0.2471,  2.1406, -0.4941, -0.6914, -0.5781],
        [ 0.3164,  1.4844, -0.8867, -0.8516, -0.2754],
        [-0.0195,  1.5859, -0.7656, -0.8203, -0.3652]], device='cuda:0',
       dtype=torch.bfloat16, grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None), SequenceClassifierOutput(loss=tensor(0.8616, device='cuda:0', grad_fn=<NllLossBackward0>), logits=tensor([[-0.0684,  1.6250, -0.7266, -0.6211, -0.5273],
        [ 0.0537,  1.1328, -0.6992, -0.7930, -0.8477],
        [ 0.4004,  1.6250, -0.8984, -1.0703, -0.6914],
        [-0.2139,  1.6562, -0.7383, -0.8047, -0.6562],
        [ 0.1094,  1.4688, -0.5508, -0.9141, -0.4023],
        [ 0.1079,  1.4062, -0.3164, -0.9180, -1.0156],
        [-0.0605,  1.4766, -0.9883, -1.1250, -0.5234],
        [ 0.0063,  1.4219, -0.4863, -0.6836, -0.5547],
        [ 0.0996,  1.4844, -0.2734, -0.7812, -0.1504],
        [-0.3848,  1.6406, -0.3965, -0.8516, -0.6211],
        [ 0.0708,  1.5078, -0.7344, -0.6875, -0.6055],
        [-0.1797,  1.5625, -0.5430, -0.8203, -0.6172],
        [-0.0374,  0.9883, -0.0437, -1.0234, -0.5977],
        [ 0.1836,  1.5391, -0.5898, -1.2031, -0.7617],
        [ 0.8867,  0.9375, -1.1484, -1.0234, -0.0649],
        [ 0.1592,  1.2266, -0.4570, -0.9766, -0.8906]], device='cuda:0',
       dtype=torch.bfloat16, grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None), SequenceClassifierOutput(loss=tensor(0.5510, device='cuda:0', grad_fn=<NllLossBackward0>), logits=tensor([[ 0.2891,  1.6250, -0.6250, -1.0469, -0.5625],
        [-0.0217,  1.4297, -0.4648, -0.6992, -0.8906],
        [ 0.3828,  1.8047, -0.6914, -0.8633, -0.7461],
        [ 0.0684,  1.6562, -0.8867, -0.8555, -0.4961],
        [ 0.1367,  1.5391, -0.7891, -1.0078, -0.6602],
        [-0.0371,  1.6484, -0.7070, -1.4531, -0.9258],
        [ 0.1660,  1.7188, -0.7734, -1.0391, -0.4707],
        [ 0.1040,  1.5781, -0.6719, -0.8438, -0.7031],
        [-0.0908,  1.3906, -0.2891, -1.0156, -0.6016],
        [-0.0294,  1.7578, -0.7734, -1.0078, -0.5156],
        [ 0.3242,  1.8203, -0.6523, -1.0938, -0.7539],
        [ 0.0249,  1.4844, -0.5547, -0.7070, -0.7461],
        [ 0.1709,  1.1719, -1.1953, -1.1797, -0.6562],
        [ 0.5039,  1.4688, -0.5000, -1.1406, -0.6328],
        [ 0.2295,  1.4453, -1.1172, -1.0547, -0.4512],
        [ 0.1143,  1.6797, -0.9375, -1.0391, -0.5547]], device='cuda:0',
       dtype=torch.bfloat16, grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None), SequenceClassifierOutput(loss=tensor(0.7562, device='cuda:0', grad_fn=<NllLossBackward0>), logits=tensor([[ 3.7695e-01,  1.6250e+00, -6.0938e-01, -8.9062e-01, -4.6484e-01],
        [ 1.0645e-01,  1.6719e+00, -6.5234e-01, -1.3281e+00, -6.3281e-01],
        [ 2.9883e-01,  1.5781e+00, -5.1172e-01, -1.0156e+00, -6.2500e-01],
        [-3.0664e-01,  1.8281e+00, -3.5156e-01, -6.9141e-01, -6.3672e-01],
        [-5.2734e-02,  1.5312e+00, -7.6172e-01, -7.6172e-01, -3.2031e-01],
        [ 5.2002e-02,  1.5391e+00, -5.4688e-01, -8.4375e-01, -5.2734e-01],
        [ 2.9373e-04,  1.4062e+00, -5.5469e-01, -7.3828e-01, -2.9883e-01],
        [ 6.2500e-01,  1.3203e+00, -5.7422e-01, -9.0234e-01, -5.5859e-01],
        [ 7.0801e-02,  1.4688e+00, -4.3750e-01, -8.7109e-01, -4.1406e-01],
        [ 3.2616e-04,  1.3438e+00, -5.7422e-01, -5.3516e-01, -4.7266e-01],
        [ 9.5215e-02,  1.5391e+00, -1.8945e-01, -1.0547e+00, -6.5234e-01],
        [-1.1523e-01,  1.5391e+00, -8.6719e-01, -9.6094e-01, -3.5352e-01],
        [ 4.2383e-01,  5.7031e-01, -8.0469e-01, -3.8086e-01,  3.4180e-01],
        [ 2.0410e-01,  1.2734e+00, -2.5781e-01, -6.2891e-01, -6.2109e-01],
        [-2.7344e-01,  1.6719e+00, -5.1172e-01, -1.0859e+00, -7.8516e-01],
        [-1.7700e-02,  1.6719e+00, -4.1992e-01, -8.1250e-01, -9.2188e-01]],
       device='cuda:0', dtype=torch.bfloat16, grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None), SequenceClassifierOutput(loss=tensor(0.7759, device='cuda:0', grad_fn=<NllLossBackward0>), logits=tensor([[ 0.1504,  1.5938, -0.7031, -1.0547, -0.5117],
        [-0.3398,  1.3750, -0.3711, -0.8594, -0.2100],
        [-0.1416,  1.6641, -0.6914, -0.9414, -0.3828],
        [ 0.1602,  1.4062, -0.6602, -1.2656, -0.4121],
        [ 0.3047,  1.2656, -0.8906, -0.8711, -0.7344],
        [-0.0588,  1.7344, -0.4023, -1.0234, -0.6445],
        [ 0.2578,  1.1562, -0.7109, -0.9336, -0.4082],
        [ 0.1699,  1.6328, -0.9609, -0.6406, -0.5430],
        [ 0.1226,  1.6562, -0.1680, -0.7031, -0.6133],
        [ 0.2617,  1.4453, -0.6680, -1.0234, -0.7344],
        [ 0.0405,  1.8516, -0.5000, -1.1641, -0.2891],
        [ 0.1982,  1.4375, -0.5117, -1.1797, -0.6328],
        [ 0.0811,  1.2969, -0.6094, -1.0391, -0.6914],
        [ 0.1514,  1.3984, -0.4219, -0.8633, -0.6758],
        [-0.0593,  1.3359, -0.6328, -0.9453, -0.8086],
        [ 0.2188,  1.4219, -0.5586, -1.0781, -0.7930]], device='cuda:0',
       dtype=torch.bfloat16, grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None), SequenceClassifierOutput(loss=tensor(1.0898, device='cuda:0', grad_fn=<NllLossBackward0>), logits=tensor([[ 0.3750,  1.3125, -0.7383, -0.9219, -0.8672],
        [ 0.0815,  2.0625, -0.9570, -0.9727, -0.3945],
        [-0.0864,  1.4922, -0.7539, -0.9414, -0.4648],
        [-0.1094,  1.5547, -0.9922, -0.8164, -0.5859],
        [ 0.0801,  1.6562, -0.4395, -0.9648, -0.5664],
        [ 0.1387,  1.6328, -0.7812, -0.8125, -0.3086],
        [-0.0996,  1.8594, -0.6914, -0.8320, -0.4453],
        [ 0.2002,  1.5234, -0.6328, -0.7656, -0.4785],
        [ 0.0830,  1.5781, -0.2520, -0.9414, -0.7305],
        [-0.0177,  1.5000, -0.7422, -1.0000, -0.6836],
        [ 0.3691,  1.7031, -0.7305, -1.0312, -0.6523],
        [ 0.3984,  1.5391, -0.7578, -0.6562, -0.6836],
        [-0.2334,  1.7500, -0.5586, -0.9414, -0.3398],
        [ 0.2344,  1.1484, -0.4531, -0.5664, -0.6211],
        [-0.1689,  1.4453, -0.7500, -0.3789, -0.5352],
        [ 0.3691,  1.3750, -0.9375, -0.5938, -0.4004]], device='cuda:0',
       dtype=torch.bfloat16, grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None), SequenceClassifierOutput(loss=tensor(0.6370, device='cuda:0', grad_fn=<NllLossBackward0>), logits=tensor([[ 0.4160,  1.1641, -0.3555, -0.7031, -0.7695],
        [ 0.0674,  1.7969, -0.6758, -0.8086, -0.5781],
        [-0.0781,  1.5938, -0.5156, -0.9141, -0.3789],
        [ 0.2598,  1.2031, -1.0391, -0.9922, -0.0640],
        [ 0.1055,  1.3359, -0.6016, -0.8789, -0.6680],
        [ 0.2773,  1.1016, -0.7227, -0.7227, -0.3223],
        [-0.3633,  1.5781, -0.6484, -0.5547, -0.5742],
        [-0.0101,  1.4141, -0.3887, -1.0156, -0.8281],
        [ 0.2158,  1.3594, -0.7031, -0.9453, -0.2012],
        [ 0.2598,  1.5781, -0.7344, -0.6367, -0.5078],
        [ 0.1318,  1.7422, -0.6797, -0.9922, -0.4707],
        [-0.0366,  1.6484, -0.8203, -1.1094, -0.3125],
        [-0.1768,  1.4375, -0.7109, -0.7188, -0.8477],
        [-0.0417,  1.7812, -0.6367, -0.7383, -0.6562],
        [ 0.1152,  1.3438, -0.8125, -0.8242, -0.2539],
        [ 0.4609,  0.2246, -0.5820, -0.3262,  0.4219]], device='cuda:0',
       dtype=torch.bfloat16, grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None), SequenceClassifierOutput(loss=tensor(0.8583, device='cuda:0', grad_fn=<NllLossBackward0>), logits=tensor([[ 0.0981,  1.5078, -1.0781, -0.9336, -0.5586],
        [ 0.2500,  1.7266, -0.5586, -1.0391, -0.4629],
        [-0.2754,  1.7031, -0.6055, -0.7422, -0.6836],
        [ 0.4863,  1.8047, -0.8711, -0.8320, -0.3770],
        [ 0.0684,  1.5703, -0.4902, -0.9062, -0.4434],
        [ 0.3457,  1.5938, -0.6992, -0.8320, -0.8203],
        [ 0.0713,  1.8750, -0.7500, -0.7344, -0.3340],
        [ 0.4395,  1.4609, -0.5117, -0.9141, -0.3438],
        [-0.2832,  1.5234, -0.6562, -0.7773, -0.8125],
        [ 0.2021,  0.7891, -0.5938, -0.5859, -0.6367],
        [ 0.1426,  1.5938, -0.9102, -0.8047, -0.1475],
        [ 0.3105,  1.5156, -0.6133, -0.8242, -0.7227],
        [ 0.0586,  1.7734, -0.6797, -1.1172, -0.7266],
        [ 0.1992,  1.8203, -0.8906, -0.9570, -0.5430],
        [-0.2871,  1.3516, -0.3711, -0.6680, -0.6602],
        [ 0.0557,  1.7812, -0.7695, -0.6719, -0.5469]], device='cuda:0',
       dtype=torch.bfloat16, grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None), SequenceClassifierOutput(loss=tensor(0.6093, device='cuda:0', grad_fn=<NllLossBackward0>), logits=tensor([[ 0.3418,  1.3125, -1.0469, -0.9023, -0.0289],
        [-0.0417,  1.4844, -0.4316, -0.9414, -0.7227],
        [-0.3203,  1.5469, -0.7461, -0.9102, -0.6250],
        [ 0.2676,  1.4922, -0.4395, -0.6445, -0.4688],
        [ 0.0884,  1.6172, -0.8281, -0.9258, -0.3477],
        [-0.1011,  1.5703, -0.8555, -0.8906, -0.4395],
        [-0.2148,  1.3516, -0.6289, -0.7422, -0.6836],
        [ 0.0903,  1.5078, -0.6641, -1.1016, -0.5898],
        [ 0.3379,  1.5625, -0.6797, -0.9922, -0.6562],
        [-0.1230,  1.3594, -0.4766, -0.8555, -0.7617],
        [ 0.0659,  1.7969, -0.7188, -0.8203, -0.7227],
        [-0.2734,  1.6406, -0.6758, -1.0156, -0.3867],
        [ 0.0486,  1.8281, -0.8242, -0.9961, -0.7656],
        [-0.3066,  1.1406, -0.6172, -0.8398, -0.6250],
        [ 0.2363,  1.5859, -0.6953, -0.9219, -0.6094],
        [-0.0190,  1.5156, -0.8477, -0.5000, -0.5195]], device='cuda:0',
       dtype=torch.bfloat16, grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None), SequenceClassifierOutput(loss=tensor(0.6744, device='cuda:0', grad_fn=<NllLossBackward0>), logits=tensor([[ 0.2188,  1.6953, -0.7266, -0.8594, -0.6250],
        [ 0.3789,  1.2500, -0.5156, -0.8867, -0.8789],
        [-0.1338,  1.7031, -0.7031, -0.7266, -0.5586],
        [ 0.0574,  1.8047, -0.9531, -1.0469, -0.7070],
        [ 0.0369,  1.3281, -0.7500, -1.0625, -0.6094],
        [ 0.2656,  1.4922, -0.6250, -0.7461, -0.6172],
        [-0.0422,  1.4922, -0.9102, -0.7188, -0.8008],
        [ 0.0879,  1.3672, -0.9414, -1.1406, -0.7344],
        [ 0.1729,  1.1484, -0.7031, -0.6172, -0.6953],
        [ 0.2285,  1.1953, -0.9492, -0.9258, -0.0972],
        [ 0.0894,  1.6953, -0.8867, -1.1641, -0.7148],
        [-0.0522,  1.8672, -0.8633, -0.9180, -0.5508],
        [ 0.3105,  1.6250, -0.7227, -1.0234, -0.6055],
        [-0.1543,  2.0156, -0.6484, -1.0625, -0.6016],
        [-0.3262,  1.4609, -0.4160, -0.6172, -0.4922],
        [-0.0566,  1.7656, -0.7031, -0.5703, -0.5625]], device='cuda:0',
       dtype=torch.bfloat16, grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None), SequenceClassifierOutput(loss=tensor(0.7233, device='cuda:0', grad_fn=<NllLossBackward0>), logits=tensor([[ 0.0669,  1.4609, -0.7539, -0.4980, -0.7617],
        [ 0.1748,  1.6250, -0.9805, -1.2656, -0.4980],
        [-0.2109,  1.2656, -0.5469, -0.7891, -0.5195],
        [ 0.0996,  1.2344, -0.5430, -0.9258, -0.5508],
        [ 0.0791,  1.2344, -0.3047, -0.8398, -0.7734],
        [-0.1465,  2.1250, -0.8750, -0.5859, -0.6523],
        [-0.3418,  1.3906, -0.4629, -0.8203, -0.3691],
        [-0.0796,  1.1172, -0.4434, -1.1172, -0.4980],
        [ 0.1045,  1.3672, -0.6719, -0.9336, -0.6914],
        [ 0.0972,  1.5469, -0.9961, -1.1016, -1.0469],
        [-0.2344,  1.7734, -0.7344, -0.9922, -0.5508],
        [ 0.2422,  1.5156, -0.4707, -1.1172, -0.9570],
        [ 0.2754,  1.0781, -0.3906, -0.6758, -0.9805],
        [ 0.1377,  1.6562, -0.5977, -0.7305, -0.7031],
        [ 0.1787,  1.4688, -0.6094, -0.9883, -0.3516],
        [-0.1089,  1.6953, -0.5977, -0.9727, -0.7695]], device='cuda:0',
       dtype=torch.bfloat16, grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None), SequenceClassifierOutput(loss=tensor(0.7529, device='cuda:0', grad_fn=<NllLossBackward0>), logits=tensor([[ 0.1338,  1.0391, -0.0591, -0.7773, -0.5625],
        [ 0.0796,  1.4844, -0.8711, -0.7383, -0.4492],
        [-0.0208,  1.5078, -0.8086, -0.7109, -0.5898],
        [-0.1250,  1.2812, -0.8789, -0.6719, -0.5156],
        [ 0.3262,  1.4531, -0.6523, -1.1484, -0.6133],
        [-0.0859,  1.5859, -0.6211, -0.5898, -0.6484],
        [ 0.1270,  1.4453, -0.5742, -1.2031, -0.3145],
        [ 0.1367,  1.6797, -0.8086, -1.0938, -0.5312],
        [ 0.1465,  1.7812, -0.2812, -0.6406, -0.9570],
        [ 0.2930,  1.3281, -0.5938, -0.9570, -0.6172],
        [-0.0042,  1.3906, -0.5000, -0.9453, -0.3984],
        [-0.0334,  1.7344, -0.7227, -0.8867, -0.4844],
        [ 0.1416,  1.5703, -0.5938, -0.6797, -0.5000],
        [-0.3184,  1.5547, -0.7617, -0.8633, -0.4824],
        [-0.1191,  1.5547, -0.7109, -0.5664, -0.4570],
        [ 0.1035,  2.0312, -0.7109, -1.0391, -0.6250]], device='cuda:0',
       dtype=torch.bfloat16, grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None), SequenceClassifierOutput(loss=tensor(0.7916, device='cuda:0', grad_fn=<NllLossBackward0>), logits=tensor([[ 0.0371,  1.9062, -0.3281, -1.1016, -0.5977],
        [-0.1270,  1.4531, -0.5586, -1.1875, -0.5664],
        [ 0.1016,  1.6953, -0.7422, -0.8672, -0.3789],
        [ 0.0630,  1.6016, -0.8906, -1.0156, -0.6289],
        [ 0.1592,  0.6016, -0.9961, -0.5391,  0.7891],
        [-0.2178,  1.6641, -0.8633, -0.8711, -0.5703],
        [-0.0156,  1.3906, -0.6602, -0.9023, -0.7656],
        [ 0.0063,  1.3672, -0.7734, -0.8125, -0.1973],
        [-0.1016,  1.4375, -0.7812, -0.9297, -0.3516],
        [-0.0913,  1.6562, -0.6055, -1.2656, -0.4531],
        [ 0.1406,  1.6016, -0.6250, -0.7500, -0.6953],
        [ 0.0063,  1.7969, -0.6914, -0.9609, -0.5508],
        [ 0.0718,  1.7812, -0.6094, -1.0547, -0.8281],
        [ 0.0194,  1.5391, -0.9023, -0.8789, -0.8711],
        [-0.2500,  1.4062, -0.7031, -0.6328, -0.7148],
        [ 0.1021,  1.7734, -0.8438, -0.9961, -0.6133]], device='cuda:0',
       dtype=torch.bfloat16, grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None), SequenceClassifierOutput(loss=tensor(0.7263, device='cuda:0', grad_fn=<NllLossBackward0>), logits=tensor([[-0.1904,  1.5859, -0.7930, -0.7305, -0.5898],
        [-0.0986,  1.6094, -0.7500, -1.0391, -0.4844],
        [-0.0162,  1.6094, -0.3965, -1.0156, -0.9492],
        [ 0.1738,  1.5391, -0.6992, -0.8047, -0.5938],
        [-0.0811,  1.8359, -0.4941, -1.0312, -0.9492],
        [ 0.0311,  1.5547, -0.8242, -0.6055, -0.7070],
        [ 0.1709,  1.5469, -0.6836, -0.8984, -0.3887]], device='cuda:0',
       dtype=torch.bfloat16, grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)], [SequenceClassifierOutput(loss=tensor(2.2178, device='cuda:0', grad_fn=<NllLossBackward0>), logits=tensor([[ 0.0148,  1.5078, -0.7969, -0.6836, -0.4121],
        [ 0.3105,  1.1016, -1.1094, -1.0703, -0.1621],
        [-0.1006,  1.5234, -0.6758, -0.9180, -0.4941],
        [-0.2461,  1.2891, -0.8398, -0.7812, -0.6641],
        [-0.0177,  1.1953, -0.8672, -0.8047, -0.3242],
        [-0.0640,  1.1797, -0.6055, -0.8320, -0.5547],
        [ 0.0197,  1.2109, -1.1094, -1.0312, -0.2598],
        [ 0.4629,  1.1172, -1.0625, -0.9141, -0.4141],
        [ 0.2070,  1.2734, -0.6250, -0.8398, -0.6641],
        [ 0.1846,  1.0938, -1.0312, -0.6055, -0.5156],
        [-0.0201,  0.6680, -0.7422, -0.6406, -0.2100],
        [ 0.1504,  1.1797, -0.2354, -0.3809, -0.5391],
        [ 0.0058,  1.4688, -1.0312, -1.0547, -0.5078],
        [ 0.5391,  0.3340, -0.8867, -0.5508,  0.0204],
        [ 0.3457,  1.2344, -0.7539, -0.9492, -0.6094],
        [-0.1416,  1.5391, -0.5469, -0.5039, -0.3535]], device='cuda:0',
       dtype=torch.bfloat16, grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None), SequenceClassifierOutput(loss=tensor(2.3628, device='cuda:0', grad_fn=<NllLossBackward0>), logits=tensor([[ 0.2070,  1.4609, -0.9258, -0.6562, -0.5820],
        [-0.0669,  1.3359, -0.5664, -0.7734, -0.4297],
        [ 0.2227,  1.4297, -0.5195, -0.7539, -0.3320],
        [ 0.1230,  1.1094, -0.8047, -0.8359, -0.3145],
        [-0.0850,  1.7031, -0.7031, -0.4785, -0.5117],
        [ 0.4863,  0.7227, -0.6758, -0.6211, -0.5078],
        [ 0.0183,  1.4375, -0.8672, -1.2500, -0.2754],
        [-0.0649,  1.3281, -0.5039, -0.9219, -0.8320],
        [-0.0938,  1.5547, -0.6289, -0.7266, -0.6992],
        [ 0.1206,  1.3594, -0.6680, -0.8320, -0.6953],
        [ 0.1289,  1.3203, -0.9414, -0.8125, -0.5273],
        [-0.2412,  1.5078, -0.4277, -0.8750, -0.4980],
        [ 0.5859,  0.9023, -1.0625, -1.0859,  0.1001],
        [ 0.1406,  1.2656, -0.8125, -0.7930, -0.5273],
        [ 0.2051,  0.9297, -0.7539, -0.4707, -0.0151],
        [ 0.0972,  1.0547, -0.5625, -0.7656, -0.6289]], device='cuda:0',
       dtype=torch.bfloat16, grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None), SequenceClassifierOutput(loss=tensor(1.9932, device='cuda:0', grad_fn=<NllLossBackward0>), logits=tensor([[ 0.1865,  0.7578, -0.6445, -0.4688, -0.0476],
        [ 0.1475,  0.9062, -0.5508, -0.9258, -0.3770],
        [ 0.1289,  1.0234, -1.1875, -0.9336,  0.1846],
        [ 0.1943,  1.7578, -0.7656, -0.7031, -0.3945],
        [ 0.1533,  1.2578, -0.7969, -0.5664, -0.3887],
        [ 0.5078,  0.5586, -0.9883, -0.4023,  0.2441],
        [ 0.0109,  1.1172, -0.4961, -0.5508, -0.5781],
        [ 0.0757,  1.5156, -0.5234, -0.5312, -0.3301],
        [-0.1167,  0.9023, -0.7188, -0.5703, -0.2637],
        [-0.1064,  1.2500, -0.7891, -0.9805, -0.3066],
        [ 0.2070,  1.3438, -0.8516, -1.0156, -0.4121],
        [ 0.6172,  0.5547, -1.0078, -0.3105,  0.3379],
        [ 0.3027,  1.4531, -0.6328, -0.6367, -0.5039],
        [ 0.0977,  1.1953, -0.9258, -0.8203, -0.3438],
        [-0.1113,  1.7031, -0.3633, -0.6875, -0.6289],
        [ 0.0723,  0.7148, -1.0391, -0.7031,  0.2871]], device='cuda:0',
       dtype=torch.bfloat16, grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None), SequenceClassifierOutput(loss=tensor(2.2852, device='cuda:0', grad_fn=<NllLossBackward0>), logits=tensor([[ 0.3184,  1.7578, -0.9531, -0.9727, -0.6094],
        [ 0.2832,  1.3750, -0.7422, -1.0469, -0.3730],
        [ 0.1504,  1.1016, -0.4023, -0.7617, -0.7500],
        [ 0.2500,  1.2344, -0.8789, -1.1406, -0.2773],
        [ 0.0461,  1.3047, -0.7148, -1.1250, -0.5664],
        [ 0.2578,  1.4766, -0.6055, -1.0234, -0.4785],
        [ 0.2422,  1.3594, -0.7031, -0.9375, -0.3926],
        [ 0.3906,  1.0625, -0.5508, -0.7227, -0.3496],
        [ 0.7148,  1.0938, -1.1016, -0.9414,  0.0718],
        [ 0.2139,  1.3281, -0.8477, -0.8359, -0.5273],
        [-0.0378,  1.7188, -0.9023, -0.7930, -0.5039],
        [ 0.0449,  1.5000, -0.7188, -0.8555, -0.6406],
        [ 0.2393,  0.9766, -0.9766, -0.5039, -0.3809],
        [-0.0649,  1.5234, -0.8242, -1.0312, -0.5703],
        [ 0.3398,  0.9258, -0.0938, -0.8320, -0.8086],
        [ 0.5703,  1.3203, -1.0234, -0.9648, -0.4688]], device='cuda:0',
       dtype=torch.bfloat16, grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None), SequenceClassifierOutput(loss=tensor(2.5337, device='cuda:0', grad_fn=<NllLossBackward0>), logits=tensor([[-0.0019,  1.5234, -0.9727, -0.9727, -0.3379],
        [ 0.4102,  1.5000, -0.8086, -0.8008, -0.4570],
        [ 0.2617,  1.3984, -0.6367, -0.9102, -0.2598],
        [ 0.2402,  1.8516, -0.7773, -0.8359, -0.4453],
        [ 0.1650,  1.6406, -0.8164, -0.8789, -0.5938],
        [ 0.3516,  0.6641, -0.5781, -0.4141,  0.5586],
        [-0.2578,  1.6172, -0.8359, -0.8477, -0.4199],
        [ 0.4336,  1.3750, -1.1406, -0.8125, -0.0469],
        [ 0.1235,  1.1562, -0.9180, -1.0000, -0.4492],
        [-0.0107,  1.3672, -0.9727, -1.0469, -0.4941],
        [ 0.1846,  1.6094, -0.7734, -1.0625, -0.7539],
        [ 0.1040,  1.1484, -0.7656, -1.0625, -0.6484],
        [-0.4082,  1.6328, -0.7148, -0.5742, -0.7383],
        [ 0.1338,  1.3203, -0.8281, -0.6523, -0.4473],
        [ 0.0679,  1.1250, -1.0703, -0.9414, -0.3340],
        [ 0.0176,  1.3672, -0.4824, -0.5469, -0.5195]], device='cuda:0',
       dtype=torch.bfloat16, grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None), SequenceClassifierOutput(loss=tensor(1.3083, device='cuda:0', grad_fn=<NllLossBackward0>), logits=tensor([[-0.1514,  1.3438, -0.5898, -0.4922, -0.6094],
        [-0.0518,  1.4922, -0.4023, -0.9922, -1.0391],
        [ 0.2070,  1.4141, -0.8633, -0.7422, -0.5664],
        [ 0.1094,  1.3594, -0.8867, -0.8555, -0.4863],
        [ 0.2354,  1.2578, -0.8164, -0.6641, -0.4258],
        [-0.0255,  1.6641, -0.9375, -0.9883, -0.5820],
        [-0.2637,  1.4141, -0.6016, -0.6602, -1.0156],
        [ 0.0173,  1.8594, -0.5039, -1.0469, -0.7617],
        [ 0.0041,  1.2109, -0.3320, -0.7578, -0.9570],
        [ 0.1729,  1.4219, -0.7891, -1.1172, -0.3789],
        [ 0.3047,  1.8125, -0.7188, -0.9453, -0.7383],
        [-0.0835,  1.7578, -0.5820, -0.7852, -0.8203],
        [ 0.0042,  1.7500, -0.2334, -0.8125, -0.5312],
        [ 0.2471,  1.2578, -0.6875, -1.1094, -0.9648],
        [-0.2148,  1.3750, -0.6406, -0.7695, -0.8398],
        [ 0.1934,  1.4688, -0.5664, -0.8359, -0.8555]], device='cuda:0',
       dtype=torch.bfloat16, grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None), SequenceClassifierOutput(loss=tensor(1.1317, device='cuda:0', grad_fn=<NllLossBackward0>), logits=tensor([[ 0.0825,  1.3672, -1.0547, -0.9102, -0.3496],
        [-0.2578,  1.4453, -0.2480, -1.0625, -1.0469],
        [-0.0889,  1.1016, -0.6836, -0.9922, -0.3340],
        [ 0.4727,  1.6953, -0.6250, -0.8867, -0.7695],
        [ 0.1299,  1.0625, -0.7383, -1.1797, -0.4375],
        [ 0.0284,  1.3984, -0.4746, -0.6328, -0.8477],
        [-0.3184,  1.6328, -0.5078, -1.0469, -0.3770],
        [-0.3965,  1.2656, -0.5430, -1.0156, -0.8125],
        [ 0.4082,  1.4375, -0.3125, -0.4883, -0.7383],
        [ 0.1465,  1.1172, -0.4727, -0.9219, -0.4395],
        [ 0.4297,  1.1016, -0.6172, -0.9727, -0.7422],
        [ 0.0219,  0.6719, -0.8945, -1.0312, -0.8750],
        [ 0.1177,  1.2500, -0.6016, -0.9805, -0.8984],
        [-0.0140,  1.6172, -0.3379, -0.8359, -0.6328],
        [-0.1553,  1.3594, -0.6328, -1.3828, -0.6367],
        [-0.3555,  1.4688, -0.6641, -1.0000, -0.6289]], device='cuda:0',
       dtype=torch.bfloat16, grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None), SequenceClassifierOutput(loss=tensor(0.5933, device='cuda:0', grad_fn=<NllLossBackward0>), logits=tensor([[-0.0623,  1.5312, -0.1680, -0.8672, -0.6797],
        [ 0.2139,  1.3438, -0.3613, -0.9531, -0.9375],
        [ 0.0669,  1.5000, -0.7773, -1.2188, -0.6719],
        [-0.0430,  0.7930, -0.2695, -0.8789, -1.0469],
        [-0.0603,  1.5625, -0.3027, -0.4316, -0.7305],
        [-0.0094,  1.4375, -1.0156, -0.8320, -0.6328],
        [-0.0248,  1.7422, -0.0640, -1.0078, -0.7461],
        [-0.0483,  1.4375, -0.3652, -0.8203, -0.6875],
        [ 0.1963,  1.7109, -0.5586, -0.8555, -0.4707],
        [-0.0210,  1.6484, -0.3047, -0.9727, -0.9102],
        [ 0.0708,  1.2344, -0.7773, -0.9922, -0.7031],
        [-0.1318,  1.2891, -0.3809, -0.9141, -0.6172],
        [ 0.1953,  1.4844, -0.2734, -0.6016, -1.1641],
        [ 0.2295,  1.4531, -0.4844, -1.1016, -0.8281],
        [-0.1128,  1.4062, -0.3281, -0.9883, -0.7656],
        [-0.1221,  1.3594, -0.5742, -0.8281, -0.7031]], device='cuda:0',
       dtype=torch.bfloat16, grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None), SequenceClassifierOutput(loss=tensor(0.6206, device='cuda:0', grad_fn=<NllLossBackward0>), logits=tensor([[-0.0845,  1.6719, -0.7227, -0.9727, -0.8086],
        [ 0.0879,  1.2734, -0.3281, -0.8711, -0.7031],
        [-0.2910,  1.4766, -0.1025, -0.9688, -0.6133],
        [ 0.0552,  1.2500, -0.3848, -0.8594, -0.5273],
        [-0.3047,  1.4297, -0.4414, -1.0781, -0.7305],
        [-0.0227,  1.4062, -0.3848, -0.8750, -0.8203],
        [-0.1079,  1.2734, -0.2617, -0.9180, -0.7773],
        [-0.0659,  1.5781, -0.4375, -0.8945, -0.9023],
        [ 0.0214,  1.3672, -0.7188, -1.2109, -0.4492],
        [ 0.2217,  1.4219, -0.9531, -1.2578, -0.8438],
        [-0.1123,  1.5078, -0.6172, -0.7422, -0.5898],
        [ 0.4277,  1.1953, -0.6953, -0.6562, -0.7812],
        [ 0.0476,  1.4453, -0.1953, -0.8711, -0.7539],
        [-0.1650,  1.1875, -0.1973, -1.1016, -0.5312],
        [ 0.2930,  1.2812, -0.8867, -1.0703, -0.4062],
        [ 0.0889,  1.2656, -1.0703, -1.1406, -0.5195]], device='cuda:0',
       dtype=torch.bfloat16, grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None), SequenceClassifierOutput(loss=tensor(0.6807, device='cuda:0', grad_fn=<NllLossBackward0>), logits=tensor([[ 0.0908,  1.4297, -0.4609, -0.9531, -0.6641],
        [-0.1670,  1.4453, -0.4336, -1.1719, -0.8906],
        [ 0.5781,  1.4609, -0.7188, -0.5781, -0.6641],
        [ 0.2812,  1.1094, -0.7461, -1.1250, -0.3164],
        [ 0.3516,  1.3828, -0.5938, -0.8594, -0.3809],
        [ 0.1826,  1.4141, -0.6875, -1.0234, -0.7852],
        [ 0.1885,  1.6328, -0.5430, -0.9648, -0.6055],
        [-0.0317,  1.8828, -0.7266, -0.8242, -0.4375],
        [ 0.1924,  1.0312, -0.3828, -0.7734, -0.9648],
        [ 0.1328,  1.2422, -0.4082, -0.8672, -1.1484],
        [ 0.0718,  1.1953, -0.8984, -1.2109, -0.4277],
        [-0.1377,  1.3906, -0.3359, -0.6289, -0.8477],
        [ 0.0155,  1.5156, -0.6406, -0.9727, -0.6914],
        [ 0.0830,  1.3594, -0.8242, -0.6367, -0.4395],
        [-0.1128,  1.1016, -1.0469, -0.6836, -0.6914],
        [ 0.0410,  1.5938, -0.6836, -1.0469, -0.4746]], device='cuda:0',
       dtype=torch.bfloat16, grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None), SequenceClassifierOutput(loss=tensor(0.8475, device='cuda:0', grad_fn=<NllLossBackward0>), logits=tensor([[ 0.1069,  1.4609, -0.5625, -1.2188, -0.7227],
        [-0.1377,  1.3516, -0.6445, -0.7500, -0.6875],
        [-0.1216,  1.0547, -0.8047, -1.1094, -1.2812],
        [ 0.0884,  1.3906, -0.5625, -1.0156, -0.6875],
        [ 0.0762,  1.4531, -0.5859, -0.6328, -0.5703],
        [ 0.1670,  1.4062, -0.3652, -1.0547, -0.8398],
        [ 0.2090,  1.0781, -0.7188, -0.7617, -0.6328],
        [ 0.2031,  1.3594, -0.8008, -0.7266, -0.6211],
        [ 0.3887,  1.0781, -1.0000, -1.2969, -0.5547],
        [ 0.1797,  1.3594, -0.3418, -0.9297, -0.6289],
        [-0.1279,  1.5469, -0.5117, -0.8828, -0.6406],
        [-0.1494,  1.6484, -0.3203, -0.7539, -0.9023],
        [ 0.2910,  1.4609, -0.7344, -0.9297, -0.9609],
        [ 0.1865,  1.5312, -0.7930, -0.6992, -0.7773],
        [-0.0552,  1.4062, -0.6172, -0.9141, -0.3125],
        [-0.1982,  1.2734, -0.5312, -1.2031, -0.6641]], device='cuda:0',
       dtype=torch.bfloat16, grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None), SequenceClassifierOutput(loss=tensor(0.8309, device='cuda:0', grad_fn=<NllLossBackward0>), logits=tensor([[ 0.0486,  1.2188, -0.8438, -1.0469, -0.4355],
        [-0.0184,  1.3203, -0.6953, -1.0391, -0.5156],
        [-0.1582,  1.4766, -0.8555, -0.9844, -0.7930],
        [ 0.0908,  0.9297, -0.6719, -0.9648, -0.8945],
        [-0.0510,  0.9258, -0.8867, -1.0078, -0.7148],
        [ 0.0713,  1.5625, -0.3105, -0.8594, -0.9492],
        [ 0.2256,  1.1641, -0.8008, -0.9883, -0.3945],
        [-0.1992,  1.5000, -0.0874, -0.7266, -0.6250],
        [ 0.0554,  1.4922, -0.7305, -0.8750, -0.6211],
        [-0.1045,  1.4766, -0.5586, -1.0859, -0.6875],
        [ 0.0037,  1.5625, -0.2334, -1.0781, -0.5742],
        [ 0.3398,  1.2734, -0.3789, -0.9062, -0.6289],
        [-0.0859,  1.4609, -0.6055, -1.1562, -0.9648],
        [-0.0327,  1.6641, -0.2217, -0.9570, -0.7969],
        [-0.1729,  1.3750, -0.6992, -1.2734, -0.6328],
        [-0.1504,  1.5703, -0.3887, -1.0547, -0.8164]], device='cuda:0',
       dtype=torch.bfloat16, grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None), SequenceClassifierOutput(loss=tensor(0.7252, device='cuda:0', grad_fn=<NllLossBackward0>), logits=tensor([[-0.0330,  1.3672, -0.4414, -0.7500, -0.3359],
        [ 0.2275,  1.4453, -0.5000, -0.8359, -0.6445],
        [ 0.1118,  1.6328, -0.4297, -1.0156, -0.9453],
        [-0.0569,  1.4453, -0.7031, -0.6367, -0.5312],
        [-0.2559,  1.6250, -0.3945, -0.9336, -0.7812],
        [ 0.1807,  1.5781, -0.4648, -0.9219, -0.8203],
        [ 0.2188,  1.5391, -0.4160, -1.0625, -1.0625],
        [ 0.0374,  1.3125, -0.6680, -1.0703, -0.7852],
        [ 0.2559,  1.4844, -0.1768, -0.8672, -0.8789],
        [-0.0374,  1.4141, -1.0547, -1.2031, -0.6328],
        [-0.2168,  1.1172, -0.3008, -0.8281, -0.7266],
        [-0.0840,  1.1016, -0.4609, -0.6992, -0.1553],
        [-0.0806,  1.4609, -0.4199, -1.0625, -0.7422],
        [ 0.0732,  1.5078, -0.5000, -0.7422, -0.5859],
        [-0.1650,  1.7422, -0.4121, -0.9844, -1.0000],
        [ 0.0952,  1.5234, -0.2402, -1.0078, -0.6055]], device='cuda:0',
       dtype=torch.bfloat16, grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None), SequenceClassifierOutput(loss=tensor(0.7384, device='cuda:0', grad_fn=<NllLossBackward0>), logits=tensor([[-0.1631,  1.5938, -0.8320, -0.9922, -0.6328],
        [ 0.1436,  1.3281, -0.6172, -1.0078, -0.7969],
        [ 0.3477,  1.4375, -0.7344, -0.7578, -0.8320],
        [-0.3555,  1.4922, -0.6328, -1.2891, -0.7930],
        [ 0.3516,  1.5156, -0.4023, -0.7734, -0.9102],
        [ 0.1875,  0.9727, -0.6211, -0.8242, -0.4102],
        [-0.0957,  1.4297, -0.6445, -1.1406, -0.7812],
        [-0.1021,  1.5781, -0.6289, -1.1094, -0.7344],
        [ 0.2217,  1.3750, -0.7070, -0.8867, -0.8164],
        [ 0.5430,  1.0312, -1.1484, -0.9414, -0.2559],
        [ 0.1660,  1.8047, -0.3770, -0.8594, -1.1719],
        [-0.1787,  1.6328, -0.7266, -0.8398, -0.6523],
        [-0.2100,  1.9141, -0.4590, -0.8086, -0.4180],
        [ 0.4902,  0.8828, -0.8750, -0.8125,  0.2617],
        [ 0.0361,  1.4688, -0.4297, -0.7617, -0.4805],
        [ 0.2256,  1.3047, -0.3457, -0.6523, -0.8750]], device='cuda:0',
       dtype=torch.bfloat16, grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None), SequenceClassifierOutput(loss=tensor(0.6646, device='cuda:0', grad_fn=<NllLossBackward0>), logits=tensor([[-1.9150e-03,  1.4297e+00, -5.5859e-01, -8.9844e-01, -7.3438e-01],
        [-6.7871e-02,  1.6250e+00, -3.7891e-01, -6.7969e-01, -8.7500e-01],
        [-1.8066e-01,  1.5312e+00, -4.7461e-01, -4.4141e-01, -6.9141e-01],
        [ 1.5527e-01,  1.9766e+00, -6.2500e-01, -9.2578e-01, -7.4219e-01],
        [-2.6733e-02,  1.4219e+00, -4.5508e-01, -9.2578e-01, -5.8594e-01],
        [-4.0039e-02,  1.3047e+00, -2.2559e-01, -7.0312e-01, -5.7812e-01],
        [ 3.3984e-01,  1.5156e+00, -2.4414e-01, -7.5391e-01, -8.2031e-01],
        [ 1.1475e-02,  1.5000e+00, -5.0781e-01, -1.2266e+00, -9.6484e-01],
        [ 2.8516e-01,  1.3672e+00, -5.3516e-01, -9.6484e-01, -5.5078e-01],
        [ 2.0898e-01,  1.1094e+00, -6.6016e-01, -1.1484e+00, -6.4062e-01],
        [-1.8945e-01,  1.5859e+00, -3.2617e-01, -1.0078e+00, -4.9805e-01],
        [-2.0215e-01,  1.4922e+00, -4.8633e-01, -8.7500e-01, -7.5391e-01],
        [-2.2656e-01,  1.6641e+00, -3.4375e-01, -1.1719e+00, -8.4375e-01],
        [ 1.0254e-01,  1.0703e+00, -2.7148e-01, -1.0000e+00, -5.2344e-01],
        [-5.2246e-02,  1.2500e+00, -9.3359e-01, -9.3750e-01, -6.1328e-01],
        [-7.1289e-02,  1.6328e+00, -5.2734e-01, -8.2812e-01, -8.9453e-01]],
       device='cuda:0', dtype=torch.bfloat16, grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None), SequenceClassifierOutput(loss=tensor(0.6984, device='cuda:0', grad_fn=<NllLossBackward0>), logits=tensor([[ 0.1118,  1.0391, -0.4746, -0.3125, -0.5703],
        [ 0.2852,  1.6797, -0.4512, -0.7109, -0.5898],
        [-0.0400,  1.5938, -0.6289, -0.8789, -0.7773],
        [-0.0742,  1.3750, -0.5156, -0.9766, -0.8008],
        [ 0.2490,  1.4766, -0.6328, -1.1016, -1.0000],
        [ 0.4453,  0.7773, -0.8008, -1.0156,  0.0732],
        [ 0.2021,  1.5156, -0.6094, -1.1641, -1.0703],
        [-0.0057,  1.4453, -0.2520, -0.7500, -0.9336],
        [ 0.2031,  1.5000, -0.5781, -0.7852, -0.5273],
        [ 0.2451,  1.2422, -0.8477, -1.1172, -0.5859],
        [ 0.0933,  1.4219, -0.4961, -1.0703, -0.7344],
        [ 0.0024,  1.7031, -0.5234, -1.0781, -0.8047],
        [ 0.1064,  1.2422, -0.4863, -0.6133, -0.4746],
        [-0.0157,  1.5000, -0.1572, -0.8555, -1.0859],
        [-0.1079,  1.6328, -0.4707, -1.0547, -0.4141],
        [-0.3242,  1.6875, -0.4688, -0.6289, -0.5742]], device='cuda:0',
       dtype=torch.bfloat16, grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None), SequenceClassifierOutput(loss=tensor(0.8087, device='cuda:0', grad_fn=<NllLossBackward0>), logits=tensor([[ 0.0033,  1.3359, -0.6562, -1.0078, -0.7227],
        [ 0.1094,  1.5156, -0.6953, -1.0859, -0.6953],
        [ 0.1299,  1.3281, -0.7227, -0.9570, -0.5117],
        [ 0.0767,  1.3125, -0.6289, -1.0781, -0.7148],
        [ 0.4688,  1.0391, -0.9062, -1.2188,  0.1060],
        [ 0.1328,  1.7266, -0.5078, -0.8906, -0.9805],
        [ 0.1328,  1.6172, -0.5859, -1.0781, -0.6289],
        [ 0.2178,  1.3203, -0.3965, -0.9414, -0.7773],
        [ 0.1030,  1.4766, -0.9062, -0.9062, -0.4102],
        [-0.1069,  1.2031, -0.5078, -0.9883, -1.2109],
        [-0.0669,  1.5000, -0.5469, -0.9531, -0.8867],
        [-0.2383,  1.3672, -0.7539, -0.7891, -0.5586],
        [ 0.0598,  1.5156, -0.4531, -1.1172, -1.1875],
        [ 0.3574,  1.2969, -0.9688, -1.0547, -0.3789],
        [ 0.0486,  1.2188, -0.8398, -0.9414, -0.4473],
        [-0.1318,  1.2500, -0.8086, -1.0859, -0.4102]], device='cuda:0',
       dtype=torch.bfloat16, grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None), SequenceClassifierOutput(loss=tensor(0.5897, device='cuda:0', grad_fn=<NllLossBackward0>), logits=tensor([[ 0.0801,  1.3125, -0.6953, -0.7305, -0.7148],
        [ 0.1738,  1.4688, -0.3906, -0.7617, -0.7305],
        [-0.1191,  1.4609, -0.3555, -1.1328, -0.7383],
        [ 0.2451,  1.3281, -0.4668, -0.9102, -0.8047],
        [ 0.3418,  1.2578, -0.6602, -0.8516, -0.5547],
        [ 0.1973,  1.3125, -0.6914, -0.8906, -0.3574],
        [ 0.1162,  1.5078, -0.6875, -0.9375, -0.6367],
        [-0.2256,  1.2656, -0.3809, -1.0391, -0.5508],
        [ 0.0903,  1.4375, -0.7227, -1.0859, -0.6641],
        [ 0.1943,  1.2734, -0.7461, -0.9414, -0.4668],
        [ 0.1152,  1.4531, -0.8438, -0.8750, -0.6484],
        [-0.0303,  1.2734, -0.6602, -0.9297, -0.5156],
        [ 0.2793,  1.3828, -0.7734, -0.8750, -0.6406],
        [ 0.0408,  1.6250, -0.8867, -0.8477, -0.4922],
        [-0.1348,  1.0781, -0.6719, -0.8633, -0.6602],
        [-0.1465,  1.5547, -0.5859, -0.7773, -0.2871]], device='cuda:0',
       dtype=torch.bfloat16, grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None), SequenceClassifierOutput(loss=tensor(0.6537, device='cuda:0', grad_fn=<NllLossBackward0>), logits=tensor([[-0.0830,  1.5078, -0.8398, -1.0781, -0.7031],
        [ 0.0757,  1.2266, -0.7656, -0.9141, -0.7969],
        [-0.0615,  1.4453, -0.7344, -0.8633, -0.8242],
        [-0.0488,  1.5703, -0.5859, -0.9258, -0.7148],
        [-0.0284,  1.4844, -0.7070, -1.0156, -0.7227],
        [ 0.6211,  1.3438, -0.9922, -0.5859, -0.4375],
        [-0.0099,  1.4453, -0.5117, -0.6641, -0.8555],
        [ 0.0903,  1.6172, -0.6641, -0.7812, -0.5547],
        [ 0.0215,  1.6641, -0.7539, -0.7070, -0.3730],
        [ 0.1426,  1.1406, -0.7812, -1.0391, -0.4746],
        [ 0.3926,  0.8086, -0.5352, -0.9961, -0.6406],
        [ 0.1465,  1.4609, -0.9766, -1.3047, -0.5273],
        [-0.2305,  1.6016, -0.8438, -0.9531, -0.3379],
        [ 0.0025,  1.5859, -0.4844, -0.7422, -0.6250],
        [-0.1914,  1.6328, -0.9922, -0.9414, -0.4414],
        [ 0.3730,  1.5000, -0.6211, -0.7852, -0.5703]], device='cuda:0',
       dtype=torch.bfloat16, grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None), SequenceClassifierOutput(loss=tensor(0.4354, device='cuda:0', grad_fn=<NllLossBackward0>), logits=tensor([[ 1.4844e-01,  1.4844e+00, -4.6289e-01, -9.2578e-01, -7.7344e-01],
        [ 1.9043e-01,  1.4062e+00, -4.8047e-01, -9.9219e-01, -5.5078e-01],
        [-1.1749e-03,  1.4844e+00, -5.7031e-01, -9.0234e-01, -7.0703e-01],
        [ 1.7822e-02,  1.9766e+00, -5.5469e-01, -1.0312e+00, -5.5078e-01],
        [-2.4658e-02,  1.3984e+00, -9.1797e-01, -9.5312e-01, -7.4219e-01],
        [ 1.0352e-01,  1.6953e+00, -8.6328e-01, -1.1562e+00, -4.4727e-01],
        [ 3.4180e-01,  1.8438e+00, -1.0938e+00, -8.1250e-01, -3.7305e-01],
        [ 1.6895e-01,  1.6016e+00, -4.0430e-01, -9.9609e-01, -8.9453e-01],
        [ 2.4512e-01,  1.0312e+00, -9.0625e-01, -1.1406e+00, -1.6992e-01],
        [ 2.8442e-02,  1.9062e+00, -6.9922e-01, -7.3438e-01, -8.6328e-01],
        [-2.2827e-02,  1.3984e+00, -5.7812e-01, -8.6328e-01, -7.8516e-01],
        [ 1.6211e-01,  1.4219e+00, -4.2578e-01, -7.8906e-01, -6.0938e-01],
        [-1.2598e-01,  1.7578e+00, -8.4375e-01, -5.9375e-01, -1.8359e-01],
        [ 6.1768e-02,  1.6641e+00, -7.0703e-01, -8.3594e-01, -5.4688e-01],
        [ 3.1128e-02,  1.8750e+00, -7.4609e-01, -1.2266e+00, -2.6758e-01],
        [-8.6426e-02,  1.3750e+00, -5.7812e-01, -8.2422e-01, -3.3398e-01]],
       device='cuda:0', dtype=torch.bfloat16, grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None), SequenceClassifierOutput(loss=tensor(0.8998, device='cuda:0', grad_fn=<NllLossBackward0>), logits=tensor([[ 0.4473,  1.5078, -0.5820, -0.8359, -0.9180],
        [-0.0981,  1.4688, -0.2930, -0.6680, -0.9414],
        [ 0.3828,  1.4844, -0.9609, -0.8555, -0.4180],
        [ 0.1001,  1.4297, -0.3848, -0.8398, -0.9258],
        [ 0.0977,  1.6250, -0.4141, -0.9453, -0.3359],
        [ 0.2734,  1.3203, -0.5078, -1.0625, -0.8242],
        [ 0.3066,  1.0156, -0.9688, -1.2031, -0.5859],
        [-0.0737,  1.6406, -0.8828, -0.6875, -0.3965],
        [ 0.0383,  1.3828, -0.9336, -0.9219, -0.4883],
        [ 0.2598,  1.7812, -0.5938, -0.6523, -0.6133],
        [-0.2051,  1.1406, -0.5117, -0.9961, -0.5859],
        [ 0.1187,  1.5469, -0.6250, -0.7148, -0.7109],
        [ 0.3223,  1.1328, -0.5273, -0.8242, -0.5625],
        [ 0.0106,  1.3516, -0.6797, -0.7148, -0.5664],
        [-0.1206,  1.6484, -0.5781, -0.6406, -0.6758],
        [ 0.2158,  1.6562, -0.2520, -0.9570, -0.1357]], device='cuda:0',
       dtype=torch.bfloat16, grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None), SequenceClassifierOutput(loss=tensor(0.8008, device='cuda:0', grad_fn=<NllLossBackward0>), logits=tensor([[ 0.1035,  1.8750, -0.4219, -0.8242, -0.8438],
        [-0.1328,  1.7031, -0.7969, -0.7930, -0.4648],
        [ 0.3262,  1.5234, -0.7695, -0.9258, -0.6758],
        [ 0.1279,  1.4766, -0.4082, -0.5703, -0.4043],
        [ 0.1416,  1.5859, -0.4414, -0.9688, -0.5312],
        [-0.0325,  1.3984, -0.5859, -0.7578, -0.5898],
        [ 0.0806,  1.4922, -0.8555, -0.8555, -0.4199],
        [ 0.2354,  1.7031, -0.5234, -0.7539, -0.6562],
        [ 0.1738,  1.8281, -0.8242, -1.0469, -0.4824],
        [ 0.0474,  1.4688, -0.4805, -1.3125, -0.7930],
        [ 0.1709,  1.4922, -0.8359, -0.8359, -0.6172],
        [ 0.1582,  0.9375, -1.0000, -0.4336, -0.5039],
        [ 0.2910,  0.8516, -0.6641, -1.0312, -0.0854],
        [-0.0193,  1.7734, -0.7266, -0.6680, -0.4648],
        [-0.1455,  1.4531, -0.8672, -0.8594, -0.1377],
        [ 0.1250,  1.7734, -0.6367, -0.9258, -0.5625]], device='cuda:0',
       dtype=torch.bfloat16, grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None), SequenceClassifierOutput(loss=tensor(0.6127, device='cuda:0', grad_fn=<NllLossBackward0>), logits=tensor([[ 0.2910,  1.5938, -0.7891, -0.9570, -0.5430],
        [ 0.0160,  1.5156, -0.2910, -0.7266, -0.6367],
        [ 0.0952,  1.6094, -0.6016, -1.0859, -0.5898],
        [ 0.2070,  1.5703, -0.9258, -1.0312, -0.3516],
        [ 0.2373,  1.4609, -0.4688, -0.8750, -0.6953],
        [ 0.3438,  1.5781, -0.7812, -1.1484, -0.4941],
        [ 0.1904,  1.3281, -0.8633, -0.9805, -0.3711],
        [-0.0049,  1.5312, -0.4160, -0.8633, -0.4590],
        [-0.0708,  1.7188, -0.4297, -1.0078, -0.7930],
        [ 0.0825,  1.4688, -0.6719, -1.0078, -0.5508],
        [ 0.1904,  1.7109, -0.7305, -0.9531, -0.6641],
        [ 0.0830,  1.5781, -0.8047, -0.8828, -0.5039],
        [ 0.1387,  1.1484, -0.8672, -0.7422, -0.0869],
        [-0.1865,  1.5938, -0.7852, -0.6992, -0.4766],
        [ 0.1572,  1.8047, -0.9258, -0.8359, -0.5781],
        [-0.0269,  1.5625, -0.3848, -0.6641, -0.8398]], device='cuda:0',
       dtype=torch.bfloat16, grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None), SequenceClassifierOutput(loss=tensor(0.4443, device='cuda:0', grad_fn=<NllLossBackward0>), logits=tensor([[ 0.2295,  1.6797, -0.6641, -0.9492, -0.5000],
        [ 0.3008,  1.7031, -0.3984, -0.9922, -0.7734],
        [ 0.0659,  1.5312, -1.1797, -1.1250, -0.3691],
        [ 0.1455,  1.1875, -0.6562, -1.1016, -0.7188],
        [ 0.0469,  1.6406, -0.7148, -0.8164, -0.6484],
        [ 0.3340,  1.5938, -0.5625, -0.9102, -0.7734],
        [ 0.1729,  1.5547, -0.4668, -1.0703, -0.6367],
        [-0.0996,  1.6172, -0.6055, -0.8477, -0.5234],
        [ 0.1289,  1.8281, -0.5508, -0.9453, -0.6406],
        [ 0.0381,  1.1172, -0.9688, -1.0312, -0.4863],
        [ 0.1758,  1.3984, -0.8789, -0.7852, -0.6992],
        [ 0.1484,  1.4141, -0.5781, -1.0000, -0.4160],
        [ 0.2295,  1.7500, -0.3887, -0.6758, -0.5547],
        [ 0.0698,  1.6406, -0.9531, -0.7852, -0.5898],
        [-0.0042,  1.5625, -0.5859, -0.8047, -0.5391],
        [ 0.0928,  1.6875, -0.4453, -0.7812, -0.4883]], device='cuda:0',
       dtype=torch.bfloat16, grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None), SequenceClassifierOutput(loss=tensor(0.8313, device='cuda:0', grad_fn=<NllLossBackward0>), logits=tensor([[-3.9062e-02,  1.4688e+00, -2.9297e-01, -8.9844e-01, -8.8281e-01],
        [-3.8477e-01,  1.1250e+00, -5.0781e-01, -5.0781e-01, -6.0938e-01],
        [ 9.6191e-02,  1.3047e+00, -3.8477e-01, -9.1016e-01, -1.0469e+00],
        [ 9.5215e-02,  1.4375e+00, -7.5000e-01, -1.0781e+00, -7.4219e-01],
        [ 4.9805e-02,  1.4609e+00, -5.8594e-01, -1.0391e+00, -6.2109e-01],
        [-8.7402e-02,  1.6875e+00, -4.5508e-01, -9.3750e-01, -3.6914e-01],
        [ 7.1777e-02,  1.7656e+00, -6.7188e-01, -1.0547e+00, -4.4336e-01],
        [ 7.7344e-01,  9.1016e-01, -1.0781e+00, -7.0312e-01, -5.6885e-02],
        [ 2.0695e-04,  1.4062e+00, -5.8203e-01, -9.6484e-01, -7.6172e-01],
        [ 7.1777e-02,  1.0312e+00, -1.0859e+00, -6.0938e-01, -6.4062e-01],
        [ 2.1777e-01,  1.7578e+00, -9.0234e-01, -1.0391e+00, -7.5391e-01],
        [ 3.6328e-01,  1.4062e+00, -6.2891e-01, -4.9219e-01, -7.4219e-01],
        [ 5.9082e-02,  1.4922e+00, -7.2266e-01, -8.8281e-01, -4.1797e-01],
        [ 2.2070e-01,  1.4531e+00, -6.2109e-01, -8.1641e-01, -4.9414e-01],
        [-9.0820e-02,  1.8281e+00, -7.8125e-01, -9.7266e-01, -6.3281e-01],
        [-3.2812e-01,  1.3281e+00, -6.0156e-01, -7.5391e-01, -5.8203e-01]],
       device='cuda:0', dtype=torch.bfloat16, grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None), SequenceClassifierOutput(loss=tensor(0.6262, device='cuda:0', grad_fn=<NllLossBackward0>), logits=tensor([[-0.1396,  1.3516, -0.7656, -0.4004, -0.5781],
        [-0.2949,  1.5781, -0.3086, -0.8438, -0.5156],
        [ 0.6992,  1.5469, -0.5352, -0.7734, -0.6445],
        [ 0.4688,  1.1484, -0.6328, -0.5703, -0.4922],
        [ 0.2168,  1.3984, -0.8047, -0.9336, -0.3262],
        [ 0.0684,  1.6250, -0.7266, -0.8750, -0.4512],
        [-0.0206,  1.5703, -0.6875, -0.3770, -0.8086],
        [-0.0581,  1.4766, -0.8750, -1.1406, -0.5117],
        [ 0.1050,  1.5312, -0.6289, -0.6953, -0.7539],
        [ 0.2109,  1.5859, -0.3711, -0.9766, -0.3965],
        [ 0.0574,  1.6797, -0.9844, -0.8555, -0.3555],
        [ 0.0093,  1.3438, -0.8125, -0.5195, -0.7188],
        [ 0.2139,  1.6953, -0.6172, -0.6836, -0.5625],
        [-0.2256,  1.1094, -0.6875, -0.7344, -0.6055],
        [-0.1562,  1.7734, -0.3262, -1.1641, -0.7227],
        [ 0.1001,  1.3125, -0.8164, -0.8750, -0.4492]], device='cuda:0',
       dtype=torch.bfloat16, grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None), SequenceClassifierOutput(loss=tensor(0.5204, device='cuda:0', grad_fn=<NllLossBackward0>), logits=tensor([[-0.0320,  1.6250, -0.6016, -0.7812, -0.6758],
        [-0.0491,  1.5781, -0.5430, -1.3047, -0.9844],
        [ 0.2295,  1.5547, -0.8359, -1.0938, -0.2949],
        [ 0.3379,  1.3516, -0.3730, -0.8047, -0.2227],
        [ 0.0483,  1.6094, -0.7656, -0.8867, -0.4805],
        [-0.1289,  1.5391, -0.3281, -0.8984, -0.8008],
        [ 0.0178,  1.8281, -0.8906, -0.8203, -0.7305],
        [ 0.1011,  1.3750, -0.2930, -0.7383, -0.4980],
        [-0.1245,  1.4219, -0.7109, -0.8555, -0.2988],
        [ 0.5273,  1.6797, -0.6406, -0.7930, -0.4199],
        [-0.0591,  1.3672, -0.3320, -0.9883, -0.8125],
        [ 0.0571,  1.6250, -0.9219, -0.9062, -0.7188],
        [-0.2656,  1.9453, -0.6445, -0.9141, -0.8242],
        [ 0.2412,  1.5391, -0.6211, -0.8242, -0.5430],
        [ 0.1050,  1.7500, -0.3770, -0.9336, -0.5391],
        [ 0.0376,  1.2344, -0.5273, -1.0078, -0.9180]], device='cuda:0',
       dtype=torch.bfloat16, grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None), SequenceClassifierOutput(loss=tensor(0.6493, device='cuda:0', grad_fn=<NllLossBackward0>), logits=tensor([[-0.1436,  1.1172, -0.7578, -0.4062, -0.3965],
        [ 0.1084,  1.8203, -0.8555, -0.7578, -0.7383],
        [ 0.0830,  1.7109, -0.8789, -0.8047, -0.5352],
        [-0.0845,  0.8984, -0.8477, -0.8008, -0.6836],
        [ 0.0977,  1.6719, -0.9648, -1.2422, -0.5625],
        [ 0.3750,  1.5312, -0.7734, -1.1484, -0.3594],
        [ 0.1475,  1.8594, -0.8398, -0.7617, -0.5156],
        [-0.1206,  1.5391, -0.3105, -0.8711, -0.6133],
        [-0.2490,  1.4219, -0.8828, -1.0859, -0.7305],
        [ 0.0532,  1.3281, -0.6562, -0.9570, -0.9570],
        [ 0.0591,  1.3125, -0.4824, -1.0859, -0.6719],
        [-0.0466,  1.4531, -0.5664, -0.8242, -0.5039],
        [ 0.3574,  1.1719, -1.0781, -0.8047, -0.3027],
        [ 0.0918,  1.2422, -0.4531, -0.9141, -0.8047],
        [-0.2891,  1.4766, -0.6914, -0.5508, -0.9727],
        [ 0.1016,  1.2656, -0.4180, -1.2266, -0.8945]], device='cuda:0',
       dtype=torch.bfloat16, grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None), SequenceClassifierOutput(loss=tensor(0.4670, device='cuda:0', grad_fn=<NllLossBackward0>), logits=tensor([[ 1.4355e-01,  1.7578e+00, -8.0859e-01, -9.0625e-01, -5.2344e-01],
        [-5.6763e-03,  1.4922e+00, -5.5859e-01, -5.2344e-01, -3.2812e-01],
        [ 8.4473e-02,  1.5234e+00, -9.1406e-01, -1.2812e+00, -3.7500e-01],
        [ 2.6562e-01,  1.1406e+00, -6.0156e-01, -9.1016e-01, -5.8594e-01],
        [-4.7119e-02,  1.9609e+00, -7.0703e-01, -9.7656e-01, -5.8203e-01],
        [ 6.2180e-04,  1.5156e+00, -6.6797e-01, -8.0469e-01, -5.7031e-01],
        [ 2.6172e-01,  1.3594e+00, -5.9375e-01, -1.0938e+00, -5.3125e-01],
        [-2.6001e-02,  1.7734e+00, -5.9766e-01, -1.0078e+00, -5.6250e-01],
        [-6.7139e-03,  1.3516e+00, -6.0938e-01, -8.0469e-01, -5.9766e-01],
        [-5.6641e-02,  1.7266e+00, -7.6172e-01, -7.2266e-01, -5.3906e-01],
        [-2.5195e-01,  1.6797e+00, -5.0781e-01, -8.2812e-01, -8.2031e-01],
        [ 2.2583e-02,  1.5391e+00, -7.7344e-01, -9.4141e-01, -4.4727e-01],
        [-4.7852e-02,  1.8359e+00, -7.1875e-01, -1.1172e+00, -3.7305e-01],
        [ 1.2268e-02,  1.4531e+00, -8.2422e-01, -1.1406e+00, -5.8594e-01],
        [-2.2095e-02,  2.0312e+00, -1.0312e+00, -7.1484e-01, -6.8750e-01],
        [-1.8433e-02,  1.6406e+00, -9.2188e-01, -9.7266e-01, -7.3828e-01]],
       device='cuda:0', dtype=torch.bfloat16, grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None), SequenceClassifierOutput(loss=tensor(0.7802, device='cuda:0', grad_fn=<NllLossBackward0>), logits=tensor([[ 0.0510,  1.6172, -0.2227, -0.9844, -0.9023],
        [-0.2793,  1.5156, -0.8672, -0.5039, -0.9727],
        [ 0.1104,  1.3750, -0.5391, -0.9453, -0.6914],
        [ 0.1465,  1.4375, -0.4648, -1.3906, -0.7578],
        [ 0.4004,  0.9883, -1.0391, -0.5703,  0.5039],
        [ 0.1177,  1.5000, -0.3789, -0.8594, -0.8047],
        [-0.0732,  1.5547, -0.8477, -0.8047, -0.5898],
        [ 0.1865,  1.3750, -0.8164, -0.5078, -0.3320],
        [ 0.4766,  1.3594, -0.7539, -0.9219, -0.6641],
        [-0.2129,  1.4141, -0.4785, -0.9961, -0.6289],
        [ 0.1187,  1.6797, -0.6992, -0.8789, -0.3359],
        [ 0.1289,  1.8125, -1.1250, -1.1172, -0.1357],
        [-0.0527,  1.7656, -0.7305, -0.8555, -0.5898],
        [-0.0830,  1.5000, -1.1250, -1.0625, -0.6016],
        [-0.0679,  1.3906, -0.4434, -0.2490, -0.4785],
        [ 0.1914,  1.4766, -0.3887, -0.8359, -0.5039]], device='cuda:0',
       dtype=torch.bfloat16, grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None), SequenceClassifierOutput(loss=tensor(0.6226, device='cuda:0', grad_fn=<NllLossBackward0>), logits=tensor([[ 0.2559,  1.8672, -1.0000, -0.9531, -0.3008],
        [-0.1875,  1.6953, -0.8398, -0.9062, -0.4805],
        [-0.2109,  1.6016, -0.6797, -0.7812, -0.4473],
        [-0.1455,  1.6562, -0.8984, -0.6797, -0.4844],
        [ 0.2471,  1.7812, -0.7344, -0.5977, -0.4824],
        [ 0.0135,  2.0469, -0.9258, -0.6836, -0.5625],
        [-0.0032,  1.6562, -0.4062, -0.6758, -0.3828],
        [-0.2305,  1.7500, -0.6133, -0.8242, -0.5234],
        [-0.1826,  1.3047, -0.7109, -0.7695, -0.1699],
        [ 0.0776,  1.5781, -0.7773, -1.1250, -0.4883],
        [ 0.4648,  1.5234, -1.0000, -0.7148, -0.4062],
        [ 0.2070,  1.8438, -0.4707, -1.0156, -0.8047],
        [ 0.0923,  1.3828, -0.7109, -0.8555, -0.3984],
        [-0.0613,  1.3672, -0.9922, -1.0625, -0.5703],
        [ 0.1235,  1.5156, -0.5547, -0.7812, -0.6055],
        [ 0.2539,  1.6016, -0.4883, -0.8047, -0.4727]], device='cuda:0',
       dtype=torch.bfloat16, grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None), SequenceClassifierOutput(loss=tensor(0.8502, device='cuda:0', grad_fn=<NllLossBackward0>), logits=tensor([[ 0.0330,  1.3750, -0.4629, -0.9844, -0.7930],
        [ 0.0786,  1.8906, -0.7344, -0.8867, -0.4629],
        [ 0.5312,  1.3672, -0.8086, -0.8516, -0.6172],
        [-0.1914,  1.5625, -0.7305, -0.9219, -0.1875],
        [ 0.0854,  1.1484, -0.7617, -1.1094, -0.2988],
        [-0.2129,  1.1719, -0.6172, -0.8008, -0.7617],
        [ 0.1787,  1.6719, -0.3203, -1.1797, -0.5820],
        [-0.1484,  1.3828, -0.5156, -0.8945, -0.5117],
        [-0.0688,  1.2344, -0.5859, -0.7930, -0.9219],
        [ 0.0383,  1.1797, -0.8477, -0.6914, -0.4941],
        [ 0.1875,  0.9375, -0.9141, -0.7305, -0.2432],
        [ 0.1128,  1.6406, -0.5938, -0.8398, -0.7109],
        [-0.0703,  1.4531, -0.9023, -0.8125, -0.5312],
        [ 0.3496,  0.7695, -0.8750, -0.6719, -0.5352],
        [ 0.0302,  1.5000, -0.8633, -1.0156, -0.4219],
        [-0.0247,  1.7891, -0.7656, -0.6172, -0.4746]], device='cuda:0',
       dtype=torch.bfloat16, grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None), SequenceClassifierOutput(loss=tensor(0.7919, device='cuda:0', grad_fn=<NllLossBackward0>), logits=tensor([[ 0.0413,  1.6953, -0.8555, -0.6367, -0.1836],
        [ 0.0496,  1.6328, -0.7344, -1.1797, -0.5703],
        [ 0.1074,  1.5781, -0.6016, -1.0469, -0.4668],
        [ 0.1494,  1.5781, -0.8281, -0.7266, -0.5859],
        [ 0.0030,  1.4062, -0.4492, -0.7930, -0.4375],
        [ 0.3418,  1.7109, -0.6758, -0.8711, -0.3242],
        [ 0.2061,  1.9688, -0.7070, -0.9766, -0.5391],
        [ 0.0383,  1.5547, -0.6836, -0.7305, -0.4023],
        [ 0.0742,  1.6172, -1.0312, -1.1250, -0.7734],
        [-0.1748,  1.5781, -0.7695, -0.6445, -0.3906],
        [ 0.2266,  1.5391, -0.5195, -1.2266, -0.9844],
        [ 0.0166,  1.8438, -1.0078, -0.7852, -0.6094],
        [ 0.0264,  1.2344, -0.4922, -0.8672, -0.5469],
        [ 0.5547,  1.4219, -0.6641, -0.8867, -0.7500],
        [-0.1699,  1.6641, -0.6016, -0.8711, -0.5391],
        [ 0.0496,  1.1172, -0.6680, -0.7539, -0.4512]], device='cuda:0',
       dtype=torch.bfloat16, grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None), SequenceClassifierOutput(loss=tensor(0.5757, device='cuda:0', grad_fn=<NllLossBackward0>), logits=tensor([[-0.0023,  1.5703, -0.6211, -0.7930, -0.7812],
        [ 0.0903,  2.1094, -0.7031, -0.8594, -0.5586],
        [ 0.1367,  1.6484, -1.0625, -0.7227, -0.3711],
        [ 0.1816,  1.1797, -0.7656, -0.9609, -0.4219],
        [-0.2344,  1.2109, -0.6445, -1.0625, -0.5234],
        [ 0.1514,  0.9102, -1.0391, -0.9023, -0.2812],
        [ 0.0500,  1.6875, -0.7148, -0.7070, -0.5195],
        [-0.0413,  1.4297, -0.7500, -1.0625, -0.5391],
        [ 0.4258,  0.5938, -0.8477, -0.5508, -0.6328],
        [-0.2148,  1.5391, -0.7656, -1.0859, -0.6797],
        [-0.3750,  1.0938, -0.6680, -0.5352, -0.5273],
        [ 0.1089,  1.4453, -0.7148, -1.0547, -0.7070],
        [ 0.0320,  1.5938, -0.4609, -0.6758, -0.7227],
        [ 0.3359,  1.8047, -0.7344, -0.6641, -0.4180],
        [ 0.1245,  1.8906, -0.6406, -1.0000, -0.3691],
        [ 0.2344,  1.3828, -0.8984, -1.1172, -0.9375]], device='cuda:0',
       dtype=torch.bfloat16, grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None), SequenceClassifierOutput(loss=tensor(0.8562, device='cuda:0', grad_fn=<NllLossBackward0>), logits=tensor([[ 0.0150,  2.0625, -0.8281, -0.7695, -0.7773],
        [-0.1436,  1.8750, -0.8125, -0.8359, -0.6289],
        [ 0.2754,  1.6016, -0.4492, -1.0391, -0.5664],
        [ 0.0216,  1.0000, -0.5273, -0.7500, -0.7812],
        [ 0.2109,  1.5781, -0.7383, -0.9062, -0.6875],
        [ 0.2275,  1.6484, -0.7539, -0.8047, -0.7109],
        [-0.0869,  1.7656, -0.8789, -1.0156, -0.6602],
        [-0.1650,  1.3828, -0.2891, -0.7344, -0.7852],
        [ 0.3086,  1.6562, -0.7500, -0.5977, -0.5117],
        [ 0.0098,  1.6562, -1.1016, -0.7500, -0.2480],
        [ 0.3516,  1.5156, -0.5977, -0.9961, -0.6719],
        [-0.2480,  1.6328, -0.6914, -0.4805,  0.0947],
        [ 0.2773,  1.1328, -0.5977, -0.4863, -0.2266],
        [-0.1826,  1.3672, -0.6797, -0.7773, -0.7773],
        [-0.2207,  1.6797, -1.1094, -1.0547, -0.1738],
        [-0.2002,  1.1406, -0.6719, -0.8359, -0.7734]], device='cuda:0',
       dtype=torch.bfloat16, grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None), SequenceClassifierOutput(loss=tensor(0.7649, device='cuda:0', grad_fn=<NllLossBackward0>), logits=tensor([[-0.1143,  1.6094, -0.5586, -0.8984, -0.6016],
        [ 0.0947,  1.4922, -0.5625, -0.9727, -0.4199],
        [-0.0262,  1.6641, -0.5508, -0.8594, -0.5195],
        [-0.1855,  1.6250, -0.8164, -0.8047, -0.7812],
        [ 0.3770,  1.4922, -0.6523, -0.7969, -0.4473],
        [-0.2266,  1.2578, -0.5859, -0.7383, -0.9883],
        [ 0.1367,  1.9922, -0.7852, -0.8594, -0.5859],
        [ 0.1396,  1.5391, -0.4668, -0.8242, -0.7617],
        [ 0.0737,  1.7109, -0.8281, -1.0312, -0.5586],
        [ 0.1221,  1.6016, -0.7500, -0.8477, -0.3281],
        [ 0.0649,  1.7109, -0.8750, -1.0391, -0.5273],
        [-0.2871,  1.4844, -0.6211, -0.9414, -0.7461],
        [ 0.0972,  1.3906, -0.4941, -0.8789, -0.6914],
        [ 0.0391,  1.4297, -0.6133, -0.8633, -0.5664],
        [ 0.4258,  1.1172, -1.2500, -0.8984,  0.2539],
        [-0.0938,  1.6250, -0.5469, -0.7734, -0.7109]], device='cuda:0',
       dtype=torch.bfloat16, grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None), SequenceClassifierOutput(loss=tensor(0.5677, device='cuda:0', grad_fn=<NllLossBackward0>), logits=tensor([[ 0.4434,  1.6484, -0.9453, -0.9297, -0.6211],
        [-0.1680,  1.3438, -0.8633, -1.0703, -0.4941],
        [ 0.0693,  1.6094, -0.2402, -0.7344, -0.6211],
        [-0.0977,  1.5234, -0.7539, -1.1016, -0.8359],
        [ 0.4648,  0.7500, -1.0234, -0.8438, -0.2891],
        [-0.1357,  1.3125, -0.4199, -1.0391, -0.7891],
        [-0.0432,  1.9531, -0.9219, -0.6445, -0.7305],
        [ 0.0684,  1.5703, -0.4844, -0.8281, -0.5195],
        [ 0.3066,  1.6484, -1.0469, -0.9961, -0.5352],
        [-0.0242,  1.4219, -0.8359, -0.5898, -0.6875],
        [-0.2061,  1.3516, -0.4199, -0.8828, -0.7930],
        [ 0.1367,  1.4844, -0.4590, -1.1094, -0.3555],
        [-0.1348,  1.8047, -0.5000, -1.0078, -0.6914],
        [ 0.2559,  1.8359, -0.8203, -1.0391, -0.5781],
        [ 0.1055,  1.6797, -0.9492, -0.5352, -0.6367],
        [ 0.2373,  1.6719, -0.7461, -0.7773, -0.3574]], device='cuda:0',
       dtype=torch.bfloat16, grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None), SequenceClassifierOutput(loss=tensor(0.7803, device='cuda:0', grad_fn=<NllLossBackward0>), logits=tensor([[ 0.0113,  1.6953, -0.5938, -0.9414, -0.3574],
        [ 0.2109,  1.5781, -0.6602, -0.9531, -0.5547],
        [ 0.1768,  1.4375, -0.8047, -0.8125, -0.8359],
        [-0.1729,  1.2812, -0.6758, -0.8945, -0.8164],
        [ 0.2891,  1.7500, -0.8203, -0.8164, -0.4941],
        [-0.1167,  1.4922, -0.7969, -0.7383, -0.6758],
        [ 0.1826,  1.5703, -0.6445, -0.9570, -0.4980],
        [ 0.0498,  1.5391, -0.7383, -0.7891, -0.6328],
        [ 0.1543,  1.6484, -1.0391, -1.1719, -0.3672],
        [ 0.1533,  1.5938, -0.9688, -1.1250, -0.4004],
        [ 0.1777,  1.4375, -0.6875, -0.6484, -0.3457],
        [ 0.1514,  1.4766, -0.5547, -1.1328, -0.3750],
        [-0.0757,  1.6953, -0.5625, -0.8594, -0.5117],
        [ 0.1348,  1.3984, -0.7070, -0.6250, -0.8711],
        [ 0.2148,  1.8359, -0.6250, -1.3125, -0.5859],
        [-0.0952,  1.6953, -0.7773, -0.8281, -0.5508]], device='cuda:0',
       dtype=torch.bfloat16, grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None), SequenceClassifierOutput(loss=tensor(1.0450, device='cuda:0', grad_fn=<NllLossBackward0>), logits=tensor([[ 0.3281,  1.3906, -0.9102, -1.0078, -0.6289],
        [ 0.1523,  1.3359, -0.7734, -1.0234, -0.3398],
        [ 0.1768,  1.6250, -0.9258, -0.6328, -0.1152],
        [ 0.3008,  1.5234, -0.2969, -0.8398, -0.7148],
        [ 0.2256,  1.5859, -0.9609, -1.0312, -0.1367],
        [ 0.1816,  1.4453, -1.0391, -1.1641, -0.8398],
        [ 0.1152,  1.4062, -0.5391, -0.6953, -0.6758],
        [-0.3262,  1.3906, -0.6289, -0.5352, -0.7969],
        [-0.0299,  1.4141, -0.5625, -0.9414, -0.4512],
        [ 0.2012,  1.5078, -0.4961, -0.7773, -0.6602],
        [-0.0498,  1.5391, -0.7227, -0.8125, -0.6953],
        [ 0.1289,  1.7109, -1.0547, -1.0547, -0.4746],
        [-0.4434,  1.7344, -0.8789, -1.0312, -0.5078],
        [-0.1260,  1.5234, -0.6367, -0.9414, -0.8828],
        [ 0.0796,  1.6562, -0.6836, -0.7812, -0.4688],
        [ 0.0728,  1.3828, -0.4707, -0.7148, -0.5508]], device='cuda:0',
       dtype=torch.bfloat16, grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None), SequenceClassifierOutput(loss=tensor(0.9209, device='cuda:0', grad_fn=<NllLossBackward0>), logits=tensor([[ 0.1553,  1.5625, -0.9844, -1.2812, -0.3613],
        [-0.0613,  0.9180, -0.7500, -0.5664, -0.2012],
        [ 0.1318,  1.9531, -0.6875, -1.0078, -0.5781],
        [ 0.1436,  1.6719, -0.7930, -1.1094, -0.7227],
        [ 0.0889,  1.5938, -0.7344, -0.9883, -0.7188],
        [ 0.1426,  1.6875, -0.8516, -1.1016, -0.5156],
        [ 0.1953,  1.6406, -0.7969, -0.9727, -0.6289],
        [ 0.1030,  1.6328, -0.6289, -1.0234, -0.5781],
        [-0.0850,  1.5000, -0.5664, -0.9414, -0.6562],
        [-0.0294,  1.7422, -0.6016, -0.9961, -0.6055],
        [ 0.0830,  1.9219, -0.5977, -1.1406, -0.6523],
        [ 0.2520,  1.2188, -0.7070, -1.1719, -0.6250],
        [-0.0728,  1.5469, -0.7227, -0.9961, -0.8789],
        [ 0.0879,  1.2891, -0.2100, -0.6953, -0.5078],
        [-0.0708,  1.4062, -0.5156, -0.7891, -0.5859],
        [-0.1279,  1.7422, -0.7578, -0.8594, -0.4531]], device='cuda:0',
       dtype=torch.bfloat16, grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None), SequenceClassifierOutput(loss=tensor(0.5948, device='cuda:0', grad_fn=<NllLossBackward0>), logits=tensor([[-0.2295,  1.8672, -0.5703, -1.0078, -0.7188],
        [ 0.2520,  1.4219, -0.6094, -1.0547, -0.7031],
        [-0.0190,  1.5625, -0.5508, -0.7109, -0.3574],
        [ 0.0135,  1.4688, -0.8516, -0.9453, -0.6992],
        [ 0.1865,  1.1641, -0.6367, -0.4316, -0.5156],
        [ 0.0302,  1.6953, -0.6250, -0.7695, -0.5234],
        [-0.0143,  1.8438, -0.8633, -1.0781, -0.3633],
        [ 0.2148,  1.5156, -0.7266, -1.1094, -0.3008],
        [-0.2305,  1.9609, -0.2871, -0.8867, -0.7656],
        [ 0.0962,  1.5156, -0.9141, -0.8750, -0.7031],
        [ 0.2275,  1.4375, -0.6641, -0.9805, -0.2930],
        [-0.2275,  1.6328, -0.5078, -0.7109, -0.6328],
        [ 0.2500,  1.5000, -0.8672, -0.9219, -0.4805],
        [-0.0596,  1.5234, -0.2832, -0.7734, -0.6367],
        [ 0.0160,  1.1641, -0.7539, -0.8164, -0.6484],
        [ 0.2393,  1.3516, -0.8086, -0.9102, -0.5625]], device='cuda:0',
       dtype=torch.bfloat16, grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None), SequenceClassifierOutput(loss=tensor(0.8851, device='cuda:0', grad_fn=<NllLossBackward0>), logits=tensor([[-0.0913,  1.7266, -0.6875, -0.7695, -0.3691],
        [ 0.2178,  1.9062, -0.6133, -0.9414, -0.4941],
        [ 0.1787,  1.4297, -0.4453, -0.6836, -0.6484],
        [ 0.0688,  1.3359, -0.7422, -0.7031, -0.3477],
        [ 0.4238,  1.0156, -1.2891, -1.1797, -0.0967],
        [-0.0282,  1.8906, -0.6016, -1.0156, -0.7031],
        [-0.1797,  1.7578, -0.5312, -0.9141, -0.7188],
        [ 0.2617,  1.3672, -0.7812, -0.6250, -0.4883],
        [ 0.1885,  1.1094, -0.6758, -0.7500, -0.6484],
        [ 0.1250,  1.5391, -0.9844, -1.0625, -0.8047],
        [ 0.0208,  1.8516, -1.0625, -0.7734, -0.4180],
        [ 0.2354,  1.0781, -1.0859, -0.7266, -0.8750],
        [-0.1787,  1.7266, -0.5430, -0.6523, -0.4727],
        [ 0.0879,  1.6641, -0.8594, -0.9219, -0.3066],
        [ 0.0273,  1.6719, -0.7031, -0.9766, -0.6367],
        [-0.0184,  1.4453, -1.1250, -1.2109, -0.6836]], device='cuda:0',
       dtype=torch.bfloat16, grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None), SequenceClassifierOutput(loss=tensor(0.6930, device='cuda:0', grad_fn=<NllLossBackward0>), logits=tensor([[ 0.3965,  1.6250, -0.6445, -0.9141, -0.6680],
        [-0.0723,  1.6797, -0.7500, -1.0859, -0.4199],
        [ 0.1660,  1.3047, -1.0156, -0.7930, -0.7461],
        [ 0.0435,  1.5938, -0.6953, -0.9258, -0.6367],
        [ 0.3496,  1.4844, -0.9688, -0.9961, -0.0864],
        [-0.2480,  1.6172, -0.4590, -0.8750, -0.4512],
        [ 0.3828,  1.3047, -0.8945, -0.7695, -0.3926],
        [ 0.0342,  1.5938, -1.2969, -0.7695, -0.4941],
        [ 0.2852,  1.3672, -0.3203, -0.7344, -0.3340],
        [-0.0583,  1.7969, -0.6406, -0.8555, -0.4082],
        [-0.1543,  1.4141, -0.5469, -0.9336, -0.6523],
        [-0.1050,  1.7031, -0.7109, -1.1484, -0.5547],
        [ 0.1396,  1.6172, -0.6992, -1.0391, -0.5859],
        [ 0.2354,  1.3594, -0.6719, -1.1094, -0.5156],
        [ 0.1885,  1.6719, -0.6836, -1.2500, -0.7617],
        [-0.0967,  1.7344, -0.4746, -0.4746, -0.5742]], device='cuda:0',
       dtype=torch.bfloat16, grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None), SequenceClassifierOutput(loss=tensor(0.6658, device='cuda:0', grad_fn=<NllLossBackward0>), logits=tensor([[-0.0286,  1.5859, -0.5703, -0.8867, -0.5078],
        [ 0.3906,  1.4922, -0.8906, -0.9609, -0.4355],
        [-0.3633,  1.6484, -1.0156, -1.1953, -0.6719],
        [ 0.0464,  1.8984, -0.7656, -0.9531, -0.5547],
        [-0.0095,  1.4766, -0.4453, -0.8281, -0.7891],
        [ 0.3711,  1.5859, -1.0781, -0.7148, -0.3691],
        [-0.0107,  1.4922, -0.5000, -0.9570, -0.5977],
        [ 0.3379,  1.3984, -0.5078, -0.8125, -0.4492],
        [ 0.0608,  1.5391, -0.9844, -0.8672, -0.5391],
        [ 0.3184,  1.0859, -0.7109, -0.9883, -0.5273],
        [-0.1641,  1.3594, -0.6797, -0.6758, -0.6367],
        [ 0.0718,  1.8672, -0.9102, -0.6328, -0.4922],
        [-0.0981,  1.3750, -0.4453, -1.2188, -0.7930],
        [ 0.0571,  1.0781, -0.4609, -0.5352, -0.5938],
        [-0.1553,  1.4844, -0.4727, -0.7969, -0.5898],
        [-0.5586,  1.8203, -0.7070, -0.9180, -0.4824]], device='cuda:0',
       dtype=torch.bfloat16, grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None), SequenceClassifierOutput(loss=tensor(0.8058, device='cuda:0', grad_fn=<NllLossBackward0>), logits=tensor([[ 0.0698,  1.0312, -1.0000, -0.9922, -0.1406],
        [-0.1157,  1.7031, -0.2930, -0.8086, -0.8516],
        [-0.0776,  1.5469, -0.8398, -1.0938, -0.7812],
        [ 0.0471,  1.8047, -0.7656, -1.0781, -0.7109],
        [-0.1641,  1.5312, -0.4902, -1.1250, -0.9414],
        [ 0.0121,  1.1953, -0.8164, -0.6133, -0.4883],
        [ 0.3613,  1.2344, -0.8945, -1.0312, -0.7734],
        [ 0.0447,  1.5859, -0.9297, -0.9805, -0.6367],
        [-0.3320,  1.6719, -0.3281, -0.8320, -0.8633],
        [-0.0762,  1.2500, -0.4023, -0.5352, -0.6406],
        [-0.0698,  1.7656, -0.8984, -0.9688, -0.7227],
        [ 0.0286,  1.7500, -0.7539, -0.7070, -0.5078],
        [-0.0598,  1.7891, -0.7656, -0.9141, -0.5000],
        [-0.1221,  1.5781, -0.8594, -0.6953, -0.8438],
        [ 0.0280,  1.4844, -0.8008, -0.8320, -0.5547],
        [ 0.1079,  1.6172, -1.0000, -1.0156, -0.9688]], device='cuda:0',
       dtype=torch.bfloat16, grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None), SequenceClassifierOutput(loss=tensor(0.8137, device='cuda:0', grad_fn=<NllLossBackward0>), logits=tensor([[ 0.4434,  0.4082, -0.8164, -0.4824,  0.2109],
        [ 0.1943,  1.3125, -0.7656, -0.8047, -0.5625],
        [ 0.2070,  1.7422, -0.9531, -0.9336, -0.4766],
        [ 0.0408,  1.4531, -0.8203, -0.9102, -0.6992],
        [ 0.0952,  1.5469, -0.6367, -0.6094, -0.3809],
        [ 0.1973,  1.3516, -0.3770, -0.7461, -0.5859],
        [ 0.2578,  1.7969, -0.8789, -0.7891, -0.4746],
        [ 0.1099,  1.4844, -0.4238, -0.8047, -0.6055],
        [-0.0258,  1.7344, -0.4492, -0.9375, -0.7969],
        [ 0.4609,  1.1328, -0.9648, -0.6523,  0.2168],
        [ 0.2119,  1.4219, -0.7305, -0.6758, -0.7695],
        [ 0.1279,  1.5391, -0.8828, -0.9531, -0.5586],
        [ 0.3516,  1.4844, -0.5117, -0.6094, -0.5312],
        [ 0.3691,  1.0938, -0.9023, -0.7891, -0.6133],
        [-0.1250,  1.9062, -0.2598, -0.7617, -0.5273],
        [ 0.3027,  1.5625, -0.4355, -0.8320, -0.6836]], device='cuda:0',
       dtype=torch.bfloat16, grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None), SequenceClassifierOutput(loss=tensor(0.9027, device='cuda:0', grad_fn=<NllLossBackward0>), logits=tensor([[ 0.1357,  1.4297, -0.5898, -0.7422, -0.5234],
        [ 0.3828,  1.2734, -0.8242, -0.8867, -0.2109],
        [ 0.2559,  1.4609, -0.8594, -1.1484, -0.6367],
        [-0.0266,  1.4766, -0.5508, -0.9805, -0.5586],
        [ 0.1406,  1.5469, -0.6016, -1.0391, -0.4766],
        [ 0.0131,  1.3984, -0.1895, -0.8438, -0.8828],
        [ 0.1245,  1.6172, -0.5625, -0.6758, -0.6445],
        [ 0.0894,  1.4766, -0.6953, -1.0000, -0.6484],
        [ 0.1152,  1.4375, -0.5898, -0.7305, -0.5195],
        [-0.6406,  1.6797, -0.6680, -0.9258, -0.9805],
        [ 0.0825,  1.3750, -0.9492, -0.5469, -0.6445],
        [ 0.0669,  1.6250, -0.7617, -0.6406, -0.4023],
        [-0.4102,  1.5859, -0.3633, -1.0625, -0.7070],
        [-0.2266,  1.7031, -0.5898, -0.9023, -0.6953],
        [-0.1309,  1.5000, -0.6406, -0.9180, -0.5000],
        [-0.1455,  1.5781, -0.7344, -0.9648, -0.7891]], device='cuda:0',
       dtype=torch.bfloat16, grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None), SequenceClassifierOutput(loss=tensor(0.6407, device='cuda:0', grad_fn=<NllLossBackward0>), logits=tensor([[-0.0317,  1.4062, -0.7227, -0.6445, -0.4238],
        [-0.0918,  1.6328, -0.6914, -0.8164, -0.5859],
        [ 0.1206,  1.6562, -0.9492, -1.1250, -0.3105],
        [-0.4121,  1.4922, -0.8320, -0.9336, -0.6836],
        [-0.1113,  1.5938, -0.6758, -1.0469, -0.8633],
        [ 0.1226,  1.2266, -0.6914, -0.9219, -0.3750],
        [ 0.1357,  1.8281, -0.6328, -0.4375, -0.5781],
        [-0.0859,  1.1484, -0.2168, -0.6445, -0.4141],
        [-0.1172,  1.4219, -0.5273, -0.5938, -0.4082],
        [ 0.0393,  1.5156, -0.5195, -0.8320, -0.4062],
        [-0.2383,  1.6562, -0.7891, -1.0000, -0.5586],
        [-0.3027,  1.7422, -0.4531, -0.9141, -0.4004],
        [ 0.3887,  1.3750, -0.4570, -0.8398, -0.6484],
        [ 0.1396,  1.5547, -0.8789, -1.1797, -0.8359],
        [ 0.0444,  1.7656, -0.4316, -1.0156, -0.8984],
        [-0.0684,  1.4062, -0.7227, -0.7891, -0.5156]], device='cuda:0',
       dtype=torch.bfloat16, grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None), SequenceClassifierOutput(loss=tensor(0.6333, device='cuda:0', grad_fn=<NllLossBackward0>), logits=tensor([[ 0.1426,  1.7500, -0.6523, -1.0312, -0.8750],
        [ 0.2148,  1.6094, -0.6172, -0.6211, -0.4570],
        [ 0.4531,  1.0391, -0.6719, -0.7461, -0.5742],
        [ 0.0112,  1.9375, -0.4746, -1.0000, -0.7031],
        [ 0.0396,  1.6641, -0.8633, -1.0625, -0.5703],
        [ 0.1436,  1.6406, -0.5195, -1.0938, -0.6211],
        [-0.0165,  1.1797, -0.8594, -0.7930, -0.4941],
        [ 0.0889,  1.5078, -0.5469, -0.7188, -0.5312],
        [ 0.2490,  1.3047, -0.4316, -0.8203, -0.5469],
        [-0.1128,  1.2891, -0.4062, -0.8242, -0.6445],
        [-0.0510,  1.5547, -0.7539, -0.9414, -0.6016],
        [-0.0366,  1.8281, -0.7148, -0.8906, -0.4395],
        [-0.0447,  1.4766, -1.0938, -1.0078, -0.3027],
        [-0.1885,  1.9297, -0.8242, -0.8984, -0.6250],
        [ 0.1260,  1.6328, -0.3457, -0.6211, -0.6641],
        [ 0.3281,  1.1016, -0.7109, -0.7070, -0.7617]], device='cuda:0',
       dtype=torch.bfloat16, grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None), SequenceClassifierOutput(loss=tensor(0.5831, device='cuda:0', grad_fn=<NllLossBackward0>), logits=tensor([[ 0.0879,  1.7188, -0.4551, -0.9336, -0.7422],
        [-0.1484,  1.5234, -0.9062, -0.9961, -0.5547],
        [-0.1133,  1.5938, -0.5781, -0.8047, -0.9062],
        [ 0.1021,  1.5703, -0.6758, -0.7578, -0.6836],
        [ 0.1826,  1.5234, -0.6016, -1.1250, -0.4707],
        [ 0.1299,  1.5938, -0.7773, -0.9297, -0.5273],
        [ 0.2852,  1.7500, -0.8828, -0.8281, -0.9375],
        [-0.0574,  1.5156, -0.5664, -0.9141, -0.5391],
        [ 0.0255,  1.5078, -0.9570, -0.9805, -0.6797],
        [ 0.1855,  1.7109, -0.8555, -0.6016, -0.7500],
        [-0.0378,  1.5156, -0.6289, -0.7188, -0.7266],
        [-0.0198,  1.0469, -0.4453, -0.5195, -0.9414],
        [-0.1875,  1.4297, -0.6836, -0.7773, -0.5000],
        [-0.0306,  1.4141, -0.7539, -0.8984, -0.4941],
        [ 0.0386,  0.7930, -0.5234, -1.0000, -0.7539],
        [-0.0059,  1.1094, -0.7109, -0.6133, -0.5469]], device='cuda:0',
       dtype=torch.bfloat16, grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None), SequenceClassifierOutput(loss=tensor(0.7239, device='cuda:0', grad_fn=<NllLossBackward0>), logits=tensor([[ 0.0420,  1.4453, -0.9375, -0.9102, -0.4199],
        [-0.0864,  1.5312, -0.9375, -1.0703, -0.6250],
        [-0.0206,  1.7969, -0.7539, -0.9609, -0.4863],
        [-0.2852,  1.3984, -0.5859, -0.6992, -0.4961],
        [ 0.3594,  1.6328, -0.9297, -0.9297, -0.4961],
        [ 0.1494,  1.5156, -0.8867, -0.7773, -0.7852],
        [ 0.0664,  1.3047, -0.4004, -0.8242, -0.7852],
        [-0.2617,  1.7969, -0.9961, -0.8789, -0.5977],
        [ 0.1982,  1.4609, -0.7500, -0.7773, -0.5430],
        [-0.1201,  1.6016, -1.0938, -0.9141, -0.4375],
        [-0.0255,  0.9258, -0.7930, -0.7422, -0.4902],
        [ 0.0103,  1.7031, -0.8672, -0.9727, -0.7266],
        [-0.0859,  1.3672, -1.0234, -0.7734, -0.5664],
        [-0.0334,  1.4766, -0.3633, -0.6445, -0.4531],
        [-0.0116,  1.8047, -0.9766, -0.8945, -0.4531],
        [-0.3418,  1.4531, -0.4082, -0.8789, -0.5312]], device='cuda:0',
       dtype=torch.bfloat16, grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None), SequenceClassifierOutput(loss=tensor(0.5262, device='cuda:0', grad_fn=<NllLossBackward0>), logits=tensor([[ 0.0080,  1.2500, -0.8711, -1.1094, -0.4316],
        [-0.1748,  1.6484, -0.9922, -0.7617, -0.3086],
        [ 0.4160,  1.3672, -0.6875, -0.8867, -0.4980],
        [ 0.2402,  1.2734, -0.8008, -0.9180, -0.5352],
        [-0.0422,  1.6016, -0.6836, -0.7070, -0.3262],
        [-0.2852,  1.4766, -0.6289, -0.7578, -0.7891],
        [ 0.2520,  1.5469, -0.7344, -0.7930, -0.5898],
        [ 0.1689,  1.4688, -0.4336, -1.1641, -0.5742],
        [-0.1553,  1.8359, -0.4551, -0.8594, -0.5781],
        [ 0.0327,  1.5312, -0.5391, -0.7773, -1.1406],
        [-0.1719,  1.5859, -0.6172, -0.9102, -0.3691],
        [-0.1807,  1.4844, -0.1963, -1.1172, -0.8477],
        [ 0.1787,  1.6172, -0.2109, -0.8281, -0.6641],
        [ 0.0186,  1.5391, -0.6992, -0.9648, -0.5352],
        [ 0.0086,  1.6953, -0.7227, -1.1719, -0.8281],
        [ 0.4121,  1.1406, -1.0781, -0.9219,  0.1089]], device='cuda:0',
       dtype=torch.bfloat16, grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None), SequenceClassifierOutput(loss=tensor(0.5884, device='cuda:0', grad_fn=<NllLossBackward0>), logits=tensor([[ 0.1128,  1.6406, -0.9531, -0.9883, -0.3613],
        [-0.1621,  1.3906, -0.5273, -1.0000, -0.3770],
        [-0.0664,  2.0781, -0.7773, -1.0469, -0.7930],
        [-0.1221,  1.8203, -0.7617, -1.1094, -0.4551],
        [ 0.2734,  1.6953, -1.0859, -0.8984, -0.5547],
        [ 0.0903,  1.1797, -0.8281, -0.6250, -0.7852],
        [-0.0315,  1.6250, -0.5625, -0.7773, -0.6367],
        [-0.1099,  1.4219, -0.7773, -1.0078, -0.5820],
        [-0.3262,  1.6562, -0.7383, -1.0781, -0.5586],
        [ 0.5312,  1.6719, -0.6875, -0.8320, -0.4805],
        [ 0.3438,  1.4453, -0.5508, -0.8047, -0.7305],
        [ 0.4414,  1.5938, -0.9141, -0.6992, -0.4512],
        [ 0.2617,  1.7422, -0.2578, -0.7734, -0.3320],
        [-0.0630,  1.4922, -0.7148, -0.9336, -0.6992],
        [-0.1572,  1.7109, -0.9336, -1.2422, -0.6484],
        [ 0.1055,  1.4766, -0.7617, -1.1797, -0.3516]], device='cuda:0',
       dtype=torch.bfloat16, grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None), SequenceClassifierOutput(loss=tensor(0.6896, device='cuda:0', grad_fn=<NllLossBackward0>), logits=tensor([[-0.0688,  1.5156, -0.7031, -0.8828, -0.8008],
        [ 0.4609,  1.2656, -0.6953, -0.8242, -0.3008],
        [-0.0500,  1.7969, -0.7930, -1.0391, -0.3145],
        [-0.1318,  1.4531, -0.4961, -1.0625, -0.6367],
        [ 0.0547,  1.2969, -1.0078, -0.6602, -0.2891],
        [-0.2559,  1.7344, -0.9609, -0.8047, -0.3262],
        [-0.2061,  1.6875, -0.7734, -0.9414, -0.4727],
        [ 0.1406,  1.6484, -0.7852, -0.8086, -0.6211],
        [ 0.3418,  1.1484, -0.6367, -0.9883, -0.0203],
        [ 0.1904,  1.3047, -0.5469, -0.8438, -0.5898],
        [ 0.0830,  1.4375, -0.6641, -0.9297, -0.7344],
        [ 0.1475,  1.4844, -0.4512, -0.9062, -0.6680],
        [ 0.1895,  1.4141, -1.1562, -0.9805, -0.3125],
        [-0.1108,  1.4062, -0.7500, -0.9688, -0.3691],
        [-0.3926,  1.3438, -0.3594, -1.0703, -0.7188],
        [ 0.0776,  1.5000, -0.8281, -0.9961, -0.6445]], device='cuda:0',
       dtype=torch.bfloat16, grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None), SequenceClassifierOutput(loss=tensor(0.8558, device='cuda:0', grad_fn=<NllLossBackward0>), logits=tensor([[ 0.0771,  1.3750, -0.9688, -1.1484, -0.7656],
        [-0.0413,  1.4297, -0.5898, -1.1094, -0.7734],
        [-0.0649,  1.5000, -0.7695, -0.8242, -0.5898],
        [ 0.4473,  1.3281, -0.7656, -0.8438, -0.4102],
        [ 0.0265,  1.6172, -0.6445, -0.8203, -0.6562],
        [ 0.1025,  1.6484, -0.5547, -0.7227, -0.4648],
        [ 0.0481,  1.3125, -0.9766, -0.8789, -0.6719],
        [-0.0498,  1.5078, -0.6602, -0.8828, -0.8594],
        [ 0.2197,  1.8750, -0.8047, -0.8594, -0.6250],
        [-0.0023,  1.8594, -0.9180, -0.8945, -0.7617],
        [ 0.0464,  1.3828, -0.7461, -1.2344, -0.5664],
        [ 0.0515,  0.9180, -0.5078, -1.0469, -0.5195],
        [ 0.2168,  1.2656, -0.5352, -0.6992, -0.4336],
        [-0.0559,  1.5859, -0.6094, -0.9922, -0.4785],
        [ 0.1641,  0.9766, -0.7773, -0.4668,  0.0140],
        [ 0.3730,  1.1406, -1.2734, -0.9062, -0.0815]], device='cuda:0',
       dtype=torch.bfloat16, grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None), SequenceClassifierOutput(loss=tensor(0.9324, device='cuda:0', grad_fn=<NllLossBackward0>), logits=tensor([[ 0.3320,  1.5391, -0.7812, -0.9297, -0.5781],
        [ 0.0742,  1.4766, -0.8281, -0.9961, -0.5508],
        [-0.0923,  1.3516, -0.5352, -0.7188, -0.4375],
        [ 0.0532,  1.0469, -0.9180, -0.8672, -0.5195],
        [ 0.0167,  1.3750, -0.7227, -0.7695, -0.3711],
        [ 0.1050,  1.7031, -0.6797, -0.9219, -0.8359],
        [-0.0190,  1.6875, -0.6328, -0.6992, -0.5898],
        [-0.1680,  1.4766, -0.7305, -0.9961, -0.8164],
        [-0.0198,  1.7109, -0.8633, -0.5312, -0.7500],
        [-0.2119,  1.8203, -0.1494, -0.8789, -0.6680],
        [ 0.2363,  1.0312, -0.9492, -0.8398, -0.1406],
        [-0.1504,  1.6562, -0.6055, -0.6367, -0.6562],
        [ 0.1064,  1.4766, -0.6602, -1.0000, -0.1670],
        [-0.2393,  1.5469, -0.6328, -0.7695, -0.4180],
        [-0.0845,  1.7500, -0.7383, -0.9375, -0.7461],
        [-0.2334,  1.7422, -0.6133, -0.7227, -0.7930]], device='cuda:0',
       dtype=torch.bfloat16, grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None), SequenceClassifierOutput(loss=tensor(0.6974, device='cuda:0', grad_fn=<NllLossBackward0>), logits=tensor([[-0.0615,  1.8984, -0.7852, -0.8203, -0.5898],
        [-0.1533,  1.5391, -0.5234, -0.9297, -0.6172],
        [-0.0569,  1.6250, -0.6523, -0.8633, -0.7812],
        [-0.1963,  1.4375, -0.5391, -0.5547, -0.4512],
        [ 0.0776,  1.6562, -0.7031, -0.9727, -0.5547],
        [ 0.0569,  1.0625, -0.6172, -0.6484, -0.4316],
        [ 0.1138,  1.7500, -0.6953, -0.9844, -0.4688],
        [-0.0737,  1.5234, -0.5703, -1.1484, -0.4746],
        [-0.1143,  1.1094, -0.4043, -1.2031, -0.6523],
        [-0.0869,  1.5703, -0.4648, -0.9297, -0.8398],
        [ 0.2539,  1.3594, -0.4375, -0.8906, -0.9609],
        [-0.1670,  1.4844, -0.1196, -0.7656, -0.6445],
        [ 0.0205,  1.6250, -0.7930, -0.6094, -0.5977],
        [ 0.3320,  1.8594, -0.6992, -0.9922, -0.5508],
        [-0.1387,  1.6484, -0.8438, -1.0156, -0.7031],
        [-0.0549,  1.5547, -0.8633, -1.0703, -0.8438]], device='cuda:0',
       dtype=torch.bfloat16, grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None), SequenceClassifierOutput(loss=tensor(0.6504, device='cuda:0', grad_fn=<NllLossBackward0>), logits=tensor([[-0.2871,  1.6406, -0.9531, -0.5469, -0.5820],
        [-0.0991,  1.1250, -0.6172, -0.7305, -0.7031],
        [-0.1748,  1.9062, -0.7344, -1.0469, -0.5586],
        [ 0.3516,  1.5234, -0.9414, -1.0234, -0.5742],
        [ 0.0688,  1.5703, -0.4531, -0.8945, -0.5352],
        [-0.0483,  1.5391, -0.6875, -0.8047, -0.7852],
        [ 0.4023,  1.4219, -1.0078, -1.0078, -0.5352],
        [-0.3262,  1.5156, -0.7461, -1.0000, -0.4590],
        [ 0.0413,  1.7031, -0.9805, -1.0781, -0.7188],
        [-0.3086,  1.0547, -0.6406, -0.9922, -0.9883],
        [ 0.1191,  1.3672, -0.5547, -0.7852, -0.9531],
        [ 0.2910,  1.7734, -1.0391, -0.9297, -0.3008],
        [-0.0240,  1.8047, -0.4668, -0.6094, -0.5430],
        [-0.0156,  1.3906, -0.6836, -0.6719, -0.5430],
        [ 0.1670,  1.7266, -0.9570, -0.8242, -0.5977],
        [-0.1426,  1.4922, -0.5508, -0.7578, -0.6562]], device='cuda:0',
       dtype=torch.bfloat16, grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None), SequenceClassifierOutput(loss=tensor(0.7402, device='cuda:0', grad_fn=<NllLossBackward0>), logits=tensor([[ 2.7539e-01,  1.8672e+00, -1.0000e+00, -8.4766e-01, -7.1484e-01],
        [ 1.5625e-01,  1.4453e+00, -1.0000e+00, -8.5938e-01, -6.3281e-01],
        [-5.4297e-01,  1.7266e+00, -7.6953e-01, -9.5312e-01, -6.3281e-01],
        [-1.6895e-01,  1.1797e+00, -5.2734e-01, -1.0547e+00, -6.6016e-01],
        [ 6.9824e-02,  1.5625e+00, -4.0039e-01, -8.8672e-01, -5.3125e-01],
        [-8.5068e-04,  1.3672e+00, -4.4336e-01, -5.5078e-01, -4.0430e-01],
        [ 5.8350e-02,  1.7969e+00, -7.1875e-01, -8.2812e-01, -5.5859e-01],
        [ 1.3965e-01,  1.3750e+00, -8.2812e-01, -6.0547e-01, -6.4453e-01],
        [ 2.6758e-01,  1.6250e+00, -7.1875e-01, -6.8359e-01, -8.6719e-01],
        [-1.1572e-01,  1.8125e+00, -7.6172e-01, -7.8125e-01, -6.5625e-01],
        [ 4.3701e-02,  1.6094e+00, -7.0312e-01, -6.3672e-01, -6.8750e-01],
        [-3.5352e-01,  1.7344e+00, -6.1328e-01, -1.0469e+00, -6.6406e-01],
        [-1.9531e-01,  1.2188e+00, -6.9922e-01, -1.1172e+00, -4.9805e-01],
        [ 2.3242e-01,  1.5156e+00, -8.6719e-01, -1.0312e+00, -5.0000e-01],
        [ 2.0874e-02,  1.7578e+00, -9.6094e-01, -8.5938e-01, -3.0469e-01],
        [ 2.0996e-01,  1.8594e+00, -1.2109e+00, -1.1328e+00, -5.0000e-01]],
       device='cuda:0', dtype=torch.bfloat16, grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None), SequenceClassifierOutput(loss=tensor(0.9510, device='cuda:0', grad_fn=<NllLossBackward0>), logits=tensor([[ 0.0361,  1.2891, -0.9414, -1.0078, -0.7305],
        [-0.3066,  1.4609, -0.6328, -1.1953, -0.5703],
        [ 0.4629,  1.2031, -1.4844, -0.6211,  0.1494],
        [ 0.1152,  1.6016, -0.8164, -0.6250, -0.5703],
        [ 0.2617,  1.6406, -0.9258, -1.2500, -0.6133],
        [ 0.1357,  1.5859, -0.5742, -0.8047, -0.5391],
        [ 0.2598,  1.4219, -0.8203, -0.9102, -0.4023],
        [ 0.1641,  1.4062, -0.6523, -0.9453, -0.6602],
        [-0.0125,  1.7812, -0.8828, -1.0156, -0.4160],
        [ 0.4141,  1.6875, -0.5664, -0.9570, -0.4453],
        [ 0.3008,  1.5859, -0.6133, -0.9922, -0.4492],
        [ 0.1348,  1.3359, -0.7500, -1.0156, -0.9336],
        [ 0.2012,  1.6328, -0.7930, -0.6914, -0.3574],
        [ 0.1768,  1.4609, -0.7852, -0.9844, -0.5156],
        [ 0.0962,  1.6875, -0.8398, -0.8672, -0.4258],
        [ 0.0184,  1.9453, -0.6602, -0.9844, -0.5156]], device='cuda:0',
       dtype=torch.bfloat16, grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None), SequenceClassifierOutput(loss=tensor(0.6656, device='cuda:0', grad_fn=<NllLossBackward0>), logits=tensor([[ 0.0176,  1.4844, -0.1787, -1.1875, -1.1641],
        [ 0.1387,  1.5078, -0.5078, -0.5977, -0.6328],
        [ 0.0825,  1.7266, -0.8320, -0.9258, -0.4551],
        [-0.1182,  1.7578, -0.6445, -0.9766, -0.4004],
        [-0.0322,  1.3594, -0.5898, -0.8047, -0.6172],
        [ 0.0033,  1.1719, -0.8242, -1.4531, -0.4297],
        [-0.0275,  1.6641, -0.8047, -0.7500, -0.4102],
        [ 0.0184,  1.5234, -0.5742, -1.1328, -0.8281],
        [-0.1328,  1.6719, -0.7109, -0.8477, -0.7148],
        [-0.3281,  1.4531, -0.7500, -0.6797, -1.0156],
        [ 0.0603,  1.7109, -0.5508, -1.2188, -0.8320],
        [-0.0239,  1.3047, -0.4023, -0.6016, -0.6133],
        [-0.2178,  1.2109, -0.6758, -0.8750, -0.5508],
        [ 0.0713,  1.7266, -0.6758, -0.9922, -0.5742],
        [ 0.1475,  1.3906, -0.8281, -0.7812, -0.5625],
        [ 0.2832,  1.2031, -0.8945, -0.8750, -0.3984]], device='cuda:0',
       dtype=torch.bfloat16, grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None), SequenceClassifierOutput(loss=tensor(0.8497, device='cuda:0', grad_fn=<NllLossBackward0>), logits=tensor([[ 6.0156e-01,  1.3203e+00, -7.9297e-01, -8.1641e-01, -2.7734e-01],
        [ 3.7500e-01,  1.7422e+00, -8.5156e-01, -9.7266e-01, -4.1797e-01],
        [ 3.1836e-01,  1.2422e+00, -8.7500e-01, -1.1250e+00, -2.5586e-01],
        [ 3.5742e-01,  1.5078e+00, -5.5859e-01, -1.0781e+00, -6.6406e-01],
        [-2.0312e-01,  1.5625e+00, -5.7812e-01, -6.8359e-01, -6.1328e-01],
        [ 4.1992e-01,  1.2500e+00, -6.2500e-01, -3.9258e-01, -2.7539e-01],
        [ 1.9531e-01,  1.5469e+00, -8.8672e-01, -7.6172e-01, -4.8438e-01],
        [ 1.3965e-01,  1.8359e+00, -6.6016e-01, -9.7656e-01, -5.0391e-01],
        [-5.7422e-01,  1.5547e+00, -5.3516e-01, -8.1250e-01, -4.6680e-01],
        [ 1.1719e-02,  1.6484e+00, -7.1094e-01, -1.0312e+00, -9.1016e-01],
        [ 1.8652e-01,  1.7578e+00, -1.0391e+00, -6.8750e-01, -4.9805e-01],
        [ 2.6172e-01,  9.8438e-01, -9.1797e-01, -9.5312e-01, -3.3789e-01],
        [-7.2754e-02,  1.5781e+00, -7.3828e-01, -5.8203e-01, -4.7461e-01],
        [ 2.5586e-01,  1.4141e+00, -5.5078e-01, -1.0781e+00, -2.1875e-01],
        [-2.7148e-01,  1.9219e+00, -1.0547e+00, -7.1094e-01, -5.1172e-01],
        [-8.4305e-04,  1.1641e+00, -4.4727e-01, -8.9062e-01, -5.5078e-01]],
       device='cuda:0', dtype=torch.bfloat16, grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None), SequenceClassifierOutput(loss=tensor(0.5010, device='cuda:0', grad_fn=<NllLossBackward0>), logits=tensor([[ 0.1475,  1.1328, -0.7617, -1.2578, -0.8594],
        [ 0.1016,  1.5312, -0.8320, -0.8633, -0.8398],
        [ 0.4688,  1.4531, -0.5898, -1.0156, -0.5664],
        [ 0.0231,  1.4766, -0.4961, -0.9609, -0.5469],
        [ 0.2490,  1.5938, -0.4004, -0.6484, -0.9336],
        [-0.0189,  1.5234, -0.7891, -0.9883, -0.6992],
        [-0.1177,  1.2344, -0.6914, -0.7852, -0.3887],
        [ 0.1748,  1.4453, -0.2832, -1.1875, -0.6328],
        [ 0.1748,  1.6484, -0.6445, -0.7969, -0.5859],
        [ 0.0403,  1.6094, -0.6523, -1.0625, -0.5352],
        [ 0.2500,  1.7578, -0.6953, -1.1406, -0.5469],
        [-0.2949,  1.6406, -0.4648, -0.8633, -0.7344],
        [ 0.0889,  1.6719, -0.7891, -0.7617, -0.6797],
        [ 0.0292,  1.6094, -0.8789, -1.2891, -0.5312],
        [-0.1299,  1.6328, -0.7188, -0.8945, -0.6289],
        [ 0.0447,  1.6172, -0.5859, -1.0391, -0.5234]], device='cuda:0',
       dtype=torch.bfloat16, grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None), SequenceClassifierOutput(loss=tensor(0.4691, device='cuda:0', grad_fn=<NllLossBackward0>), logits=tensor([[-0.2363,  1.5859, -0.7969, -0.7930, -0.5469],
        [ 0.0197,  1.5469, -0.6719, -0.9844, -0.6133],
        [ 0.0219,  1.6484, -0.5781, -0.8594, -0.5156],
        [ 0.0292,  1.7734, -0.7461, -1.1484, -0.6250],
        [ 0.1426,  1.9219, -0.5898, -0.8984, -0.8867],
        [-0.1104,  1.5781, -0.6836, -0.8164, -0.4746],
        [ 0.1367,  1.5469, -0.6953, -0.7852, -0.5469],
        [ 0.3125,  1.2578, -0.8477, -1.1797, -0.7695],
        [ 0.4375,  1.4844, -0.7266, -0.5508, -0.5781],
        [ 0.0869,  1.7500, -1.0156, -0.9844, -0.6211],
        [-0.1426,  1.5078, -0.6289, -0.9492, -0.8164],
        [-0.4492,  1.4375, -0.8633, -0.9609, -0.5469],
        [-0.0513,  1.6406, -0.7305, -0.8164, -0.2891],
        [-0.0437,  1.4844, -0.5625, -0.4980, -0.8477],
        [-0.0466,  1.9688, -0.4727, -0.7617, -0.4414],
        [-0.1221,  1.4844, -0.6875, -0.9883, -0.5586]], device='cuda:0',
       dtype=torch.bfloat16, grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None), SequenceClassifierOutput(loss=tensor(0.6721, device='cuda:0', grad_fn=<NllLossBackward0>), logits=tensor([[-0.0474,  1.7109, -0.7461, -0.8281, -0.5195],
        [ 0.0347,  1.6016, -0.8242, -1.0312, -0.5156],
        [ 0.1699,  1.3125, -0.7695, -0.7656, -0.6992],
        [ 0.0255,  1.3984, -0.6875, -0.7969, -0.8398],
        [-0.0579,  1.4531, -0.7461, -1.3125, -0.7500],
        [ 0.2734,  1.4297, -0.6094, -0.7656, -0.4512],
        [-0.0071,  1.5391, -0.6562, -1.1250, -0.7617],
        [-0.2061,  1.7344, -0.6914, -1.0469, -0.4043],
        [-0.0457,  1.4297, -0.3672, -0.9062, -0.4531],
        [-0.1406,  1.8281, -0.8633, -1.0469, -0.7227],
        [-0.0669,  1.6328, -0.6055, -0.9531, -0.7578],
        [ 0.2754,  1.7812, -1.0156, -1.0781, -0.4883],
        [-0.0781,  1.3125, -0.3027, -0.6289, -0.5547],
        [-0.0166,  1.5938, -0.3906, -0.7969, -0.6055],
        [ 0.3379,  1.2891, -1.0234, -0.9531, -0.6484],
        [-0.2178,  1.1797, -0.5742, -0.6367, -0.7734]], device='cuda:0',
       dtype=torch.bfloat16, grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None), SequenceClassifierOutput(loss=tensor(0.5144, device='cuda:0', grad_fn=<NllLossBackward0>), logits=tensor([[-0.0245,  1.7656, -0.8125, -1.1719, -0.5547],
        [ 0.5430,  1.5781, -0.6445, -0.7500, -0.3516],
        [-0.1699,  1.3281, -0.5078, -1.0781, -0.3691],
        [ 0.3008,  1.4688, -0.9062, -0.8906, -0.7734],
        [-0.0820,  1.2891, -0.4141, -1.0078, -0.5977],
        [ 0.0991,  1.1328, -0.5117, -0.6367, -0.4336],
        [-0.0503,  1.6172, -0.5312, -1.0312, -0.7539],
        [-0.3184,  1.5859, -0.6211, -0.5859, -0.4980],
        [-0.2314,  1.5156, -0.2812, -0.6680, -0.7344],
        [ 0.1270,  1.6484, -0.7617, -0.7266, -0.5664],
        [-0.1162,  1.6016, -0.6523, -0.8828, -0.6406],
        [-0.2188,  1.5312, -0.4141, -1.0312, -0.6172],
        [ 0.1270,  1.2578, -0.7617, -0.8516, -0.4824],
        [ 0.0518,  1.3438, -0.6758, -0.8633, -0.7500],
        [ 0.2305,  1.8516, -0.9961, -0.8867, -0.7852],
        [ 0.0140,  1.8203, -0.9336, -0.8711, -0.4336]], device='cuda:0',
       dtype=torch.bfloat16, grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None), SequenceClassifierOutput(loss=tensor(1.1083, device='cuda:0', grad_fn=<NllLossBackward0>), logits=tensor([[-0.0194,  1.4531, -0.7188, -1.1719, -0.7773],
        [ 0.1855,  1.1562, -0.7148, -0.7461, -0.4414],
        [ 0.1963,  1.4766, -1.0000, -1.3594, -0.4941],
        [-0.5742,  1.7812, -0.8047, -0.6836, -0.5117],
        [-0.2539,  1.9609, -0.5938, -0.8281, -0.4297],
        [-0.0498,  1.2734, -0.3594, -0.6836, -0.5898],
        [ 0.3281,  1.2812, -0.7422, -1.0469, -0.3477],
        [-0.2148,  1.4375, -0.6758, -0.8477, -0.5703],
        [-0.1484,  1.3203, -0.8164, -0.9648, -0.4062],
        [-0.1709,  1.2109, -0.4902, -0.6953, -0.5586],
        [ 0.2061,  1.7500, -0.8438, -0.8047, -0.7227],
        [ 0.3164,  1.6562, -0.7461, -0.7500, -0.4844],
        [-0.2393,  1.1641, -0.7266, -0.6797, -0.9297],
        [ 0.1045,  1.6328, -0.7148, -0.6055, -0.6797],
        [-0.0544,  1.4609, -0.2988, -0.9297, -0.3809],
        [-0.0732,  1.4531, -0.6328, -0.5859, -0.9180]], device='cuda:0',
       dtype=torch.bfloat16, grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None), SequenceClassifierOutput(loss=tensor(0.8195, device='cuda:0', grad_fn=<NllLossBackward0>), logits=tensor([[ 1.9531e-02,  1.2656e+00, -7.1094e-01, -8.4375e-01, -6.0156e-01],
        [ 2.3926e-01,  1.4062e+00, -6.8750e-01, -8.9453e-01, -7.5781e-01],
        [ 2.0605e-01,  1.3984e+00, -1.0312e+00, -1.1719e+00, -3.9258e-01],
        [ 2.2168e-01,  1.1250e+00, -6.7188e-01, -8.0859e-01, -6.7578e-01],
        [ 1.4355e-01,  1.6875e+00, -6.2109e-01, -9.6484e-01, -7.6562e-01],
        [-6.9336e-02,  1.2266e+00, -6.7188e-01, -7.3047e-01, -9.3359e-01],
        [-4.6143e-02,  1.6094e+00, -5.8203e-01, -9.2188e-01, -3.0078e-01],
        [ 2.1240e-02,  1.5391e+00, -4.8047e-01, -8.3984e-01, -6.0547e-01],
        [ 3.3789e-01,  1.7422e+00, -9.2969e-01, -1.3438e+00, -5.8984e-01],
        [ 1.2109e-01,  1.5391e+00, -4.4141e-01, -1.2031e+00, -4.5312e-01],
        [-2.4707e-01,  1.8438e+00, -5.7422e-01, -1.0469e+00, -7.1875e-01],
        [-1.5640e-03,  1.5234e+00, -7.6562e-01, -8.1250e-01, -6.2500e-01],
        [-3.6719e-01,  1.7500e+00, -6.3672e-01, -1.0234e+00, -6.2109e-01],
        [ 1.0010e-01,  1.8672e+00, -7.2656e-01, -1.0469e+00, -6.4453e-01],
        [-3.4766e-01,  1.6250e+00, -6.9922e-01, -7.5781e-01, -6.5625e-01],
        [ 2.2949e-01,  1.7891e+00, -7.3438e-01, -9.7656e-01, -5.6641e-01]],
       device='cuda:0', dtype=torch.bfloat16, grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None), SequenceClassifierOutput(loss=tensor(0.9774, device='cuda:0', grad_fn=<NllLossBackward0>), logits=tensor([[ 2.5781e-01,  1.3828e+00, -8.5938e-01, -1.0625e+00, -3.4766e-01],
        [-7.0801e-03,  1.3438e+00, -5.4297e-01, -9.5703e-01, -5.6250e-01],
        [ 1.6968e-02,  1.3672e+00, -3.1055e-01, -1.1406e+00, -8.4375e-01],
        [ 1.0498e-01,  1.6484e+00, -5.2734e-01, -8.2031e-01, -6.9531e-01],
        [ 2.8516e-01,  1.5703e+00, -6.9531e-01, -1.0234e+00, -6.0156e-01],
        [ 2.5391e-01,  1.5938e+00, -1.2109e+00, -1.2188e+00, -5.3906e-01],
        [-2.2168e-01,  1.3125e+00, -4.2773e-01, -8.2812e-01, -4.1602e-01],
        [ 3.5742e-01,  1.7734e+00, -5.6250e-01, -1.1797e+00, -7.6562e-01],
        [ 6.7871e-02,  1.3594e+00, -5.8984e-01, -7.1484e-01, -7.2266e-01],
        [ 1.4343e-02,  1.5078e+00, -8.1250e-01, -9.1797e-01, -4.9219e-01],
        [ 2.2754e-01,  1.0312e+00, -4.3359e-01, -9.2188e-01, -8.0078e-01],
        [-9.1791e-06,  1.6484e+00, -4.5703e-01, -8.9844e-01, -6.0156e-01],
        [-2.7148e-01,  1.2422e+00, -7.6953e-01, -7.7344e-01, -7.2656e-01],
        [ 3.4424e-02,  1.7969e+00, -7.8906e-01, -1.2109e+00, -3.9258e-01],
        [ 1.1865e-01,  1.3750e+00, -8.8281e-01, -1.2656e+00, -5.4688e-01],
        [ 3.4668e-02,  1.7109e+00, -7.5391e-01, -8.5156e-01, -5.2344e-01]],
       device='cuda:0', dtype=torch.bfloat16, grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None), SequenceClassifierOutput(loss=tensor(0.8601, device='cuda:0', grad_fn=<NllLossBackward0>), logits=tensor([[ 0.1445,  1.2422, -0.5352, -0.6250, -0.3848],
        [-0.2041,  1.8203, -0.9883, -1.1016, -0.4551],
        [ 0.2988,  1.3906, -0.4590, -0.7578, -0.6602],
        [ 0.6133,  1.0781, -1.0859, -1.1719, -0.1533],
        [ 0.5430,  0.3418, -1.2031, -0.1992,  0.6172],
        [ 0.2871,  1.7109, -0.6875, -0.7578, -0.6680],
        [ 0.4043,  1.5078, -0.8516, -1.3281, -0.7969],
        [ 0.2119,  1.4375, -0.7188, -1.1562, -0.3828],
        [ 0.1387,  1.4844, -0.4277, -1.0000, -0.5352],
        [-0.0996,  1.6406, -0.5547, -1.0312, -0.7852],
        [-0.2559,  1.7734, -0.7773, -1.0000, -0.3750],
        [ 0.1387,  1.2188, -0.6641, -0.8867, -0.4336],
        [ 0.1406,  1.6250, -0.8945, -1.2344, -0.7227],
        [ 0.0197,  1.2734, -0.8633, -1.3281, -0.5508],
        [ 0.1118,  1.7422, -0.7734, -1.0156, -0.6406],
        [-0.1260,  1.9453, -0.4414, -1.0625, -0.6719]], device='cuda:0',
       dtype=torch.bfloat16, grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None), SequenceClassifierOutput(loss=tensor(0.6313, device='cuda:0', grad_fn=<NllLossBackward0>), logits=tensor([[ 0.1807,  1.4688, -0.4043, -0.7109, -0.4062],
        [ 0.0815,  1.8281, -0.5664, -0.8438, -0.7500],
        [-0.1328,  1.5547, -1.1328, -1.2734, -0.5430],
        [-0.1748,  1.6094, -0.7344, -0.8477, -0.5977],
        [ 0.1807,  1.3750, -0.6562, -0.6875, -0.4824],
        [ 0.3203,  1.2891, -1.1172, -0.6875, -0.5156],
        [ 0.1143,  1.4688, -0.6602, -0.7656, -0.6992],
        [ 0.0121,  1.3047, -0.7969, -0.9141, -0.4004],
        [-0.0583,  1.6875, -0.6641, -0.9141, -0.7969],
        [ 0.0952,  1.5938, -0.6016, -0.8320, -0.3770],
        [-0.1016,  1.8828, -0.4570, -0.8828, -0.6367],
        [-0.0522,  1.4453, -0.4082, -0.8359, -0.4707],
        [-0.0776,  1.5625, -0.6484, -0.8633, -0.5742],
        [ 0.2383,  1.9219, -0.5508, -0.8047, -0.6836],
        [ 0.3496,  1.5078, -1.1562, -0.9766, -0.6406],
        [-0.1436,  1.5859, -0.7305, -0.6172, -0.5781]], device='cuda:0',
       dtype=torch.bfloat16, grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None), SequenceClassifierOutput(loss=tensor(0.8521, device='cuda:0', grad_fn=<NllLossBackward0>), logits=tensor([[-0.2100,  1.3672, -0.6836, -0.9609, -0.6055],
        [-0.1006,  1.1016, -0.9102, -0.7266, -0.5273],
        [ 0.0188,  1.5938, -1.0781, -1.1562, -0.8633],
        [ 0.0071,  1.7031, -0.8203, -0.9492, -0.4570],
        [-0.2480,  1.4844, -0.4375, -0.6602, -0.6367],
        [-0.0210,  1.5703, -0.5469, -1.0391, -1.1250],
        [-0.0913,  1.7031, -0.9219, -0.8398, -0.3789],
        [ 0.0659,  1.0781, -0.4805, -0.6289, -0.1162],
        [ 0.1641,  1.5234, -0.4141, -0.7969, -0.4980],
        [-0.2393,  1.4922, -0.4941, -0.7305, -0.3789],
        [ 0.0923,  1.6406, -0.6914, -0.6055, -0.5039],
        [-0.0918,  1.3906, -0.5781, -0.8047, -0.4434],
        [ 0.1045,  1.3203, -0.4141, -0.9766, -0.9336],
        [ 0.0728,  1.5312, -0.6953, -1.1719, -0.6172],
        [ 0.5391,  0.8828, -1.2812, -0.7109,  0.3145],
        [ 0.3477,  1.3594, -0.8633, -1.0234, -0.9805]], device='cuda:0',
       dtype=torch.bfloat16, grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None), SequenceClassifierOutput(loss=tensor(0.5602, device='cuda:0', grad_fn=<NllLossBackward0>), logits=tensor([[ 0.3770,  1.5312, -0.8555, -0.9883, -0.7500],
        [ 0.3125,  1.5703, -0.7617, -0.9492, -0.3516],
        [ 0.4277,  1.8438, -0.9570, -1.0859, -0.2246],
        [ 0.0199,  1.8750, -0.7812, -0.9805, -0.7656],
        [ 0.0061,  1.6250, -0.6758, -1.2500, -0.4746],
        [ 0.0996,  1.8906, -0.9414, -1.1406, -0.8320],
        [ 0.0040,  1.5547, -0.8789, -1.1250, -0.7852],
        [ 0.0261,  1.6484, -0.5234, -0.8789, -0.6797],
        [ 0.1245,  1.6797, -0.5234, -1.0938, -0.5625],
        [-0.1455,  1.5078, -0.7969, -0.8359, -0.4512],
        [ 0.5391,  1.7500, -0.6875, -1.0938, -0.6719],
        [-0.0581,  1.6172, -0.6914, -0.9219, -0.5664],
        [ 0.2363,  1.4219, -0.8984, -0.8516, -0.7070],
        [ 0.2812,  1.6172, -0.6406, -1.0234, -0.5117],
        [ 0.2754,  1.5391, -0.7148, -0.9492, -0.5898],
        [ 0.1030,  1.9922, -0.7227, -0.9453, -0.4082]], device='cuda:0',
       dtype=torch.bfloat16, grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None), SequenceClassifierOutput(loss=tensor(0.7180, device='cuda:0', grad_fn=<NllLossBackward0>), logits=tensor([[ 0.0557,  1.7812, -0.4434, -0.8867, -0.5312],
        [-0.0835,  1.5781, -0.8516, -0.7617, -0.7461],
        [ 0.0153,  1.4297, -0.7656, -0.6602, -0.6133],
        [-0.0134,  1.4688, -0.6758, -0.8281, -0.6836],
        [ 0.1592,  1.4922, -0.7500, -1.0781, -0.3418],
        [ 0.0381,  1.4922, -0.8203, -1.2500, -0.4570],
        [ 0.3711,  1.7031, -0.5586, -1.0234, -0.4512],
        [ 0.1572,  1.3281, -0.5156, -0.9883, -0.7227],
        [-0.0825,  1.6094, -0.6016, -0.8555, -0.6055],
        [ 0.1641,  1.5078, -0.2715, -0.6328, -0.4414],
        [ 0.1514,  1.6406, -0.2988, -1.4453, -0.5156],
        [-0.0894,  1.4531, -0.4336, -0.9219, -0.8633],
        [ 0.8281,  1.1016, -0.9258, -0.6367,  0.5625],
        [ 0.0332,  1.5156, -0.6445, -0.9922, -0.7070],
        [-0.1650,  1.6328, -0.7656, -1.1016, -0.4180],
        [ 0.0260,  1.8047, -0.8203, -0.8438, -0.5469]], device='cuda:0',
       dtype=torch.bfloat16, grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None), SequenceClassifierOutput(loss=tensor(0.6903, device='cuda:0', grad_fn=<NllLossBackward0>), logits=tensor([[ 0.2070,  1.1797, -0.7930, -0.9375, -0.4453],
        [ 0.0038,  1.6875, -0.7148, -0.9492, -0.7617],
        [ 0.2373,  1.7109, -0.8984, -1.0859, -0.5625],
        [ 0.0586,  1.4375, -0.7383, -0.9570, -0.4922],
        [ 0.1982,  1.4609, -0.7891, -1.0312, -0.9883],
        [ 0.1299,  1.3203, -0.6328, -1.0938, -0.5469],
        [ 0.4707,  1.3438, -0.8398, -0.8203, -0.5195],
        [ 0.0762,  1.5000, -0.9961, -0.4785, -0.2207],
        [ 0.3105,  1.9609, -0.1758, -1.0312, -0.6172],
        [ 0.2344,  1.6406, -0.4609, -0.9609, -0.6797],
        [-0.0684,  1.9141, -0.6211, -0.8047, -0.4551],
        [-0.0280,  1.5234, -0.5781, -0.7656, -0.6641],
        [-0.3633,  1.3906, -0.8203, -1.0234, -0.7109],
        [ 0.0815,  1.4453, -0.4062, -0.8086, -0.5234],
        [-0.2090,  1.6406, -0.7930, -1.1953, -0.4531],
        [ 0.1904,  1.3125, -0.7266, -0.8008, -0.6133]], device='cuda:0',
       dtype=torch.bfloat16, grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None), SequenceClassifierOutput(loss=tensor(1.1169, device='cuda:0', grad_fn=<NllLossBackward0>), logits=tensor([[-0.2061,  1.4141, -0.6719, -0.7305, -0.5508],
        [ 0.0181,  1.7969, -0.7812, -1.0234, -0.4375],
        [ 0.1924,  1.5312, -0.9414, -0.6250, -0.2832],
        [-0.1494,  1.5156, -0.9727, -0.6602, -0.4082],
        [ 0.3770,  1.3281, -0.8047, -1.0547, -0.4277],
        [ 0.1348,  1.5234, -1.1016, -0.8125, -0.8125],
        [-0.3848,  1.8047, -0.8594, -0.7539, -0.4102],
        [ 0.2256,  1.3828, -0.6914, -0.8672, -0.1387],
        [-0.1572,  1.7422, -0.4746, -0.8242, -0.7344],
        [-0.1777,  1.7812, -0.3516, -1.2109, -0.4766],
        [ 0.1143,  1.7891, -0.7695, -1.0078, -0.2324],
        [ 0.1118,  1.3828, -0.6133, -0.9766, -0.6914],
        [-0.0742,  1.4844, -0.7188, -1.0156, -0.4336],
        [ 0.4512,  1.4688, -0.9883, -0.8086, -0.6680],
        [ 0.4551,  1.1875, -0.6836, -0.7109, -0.8594],
        [ 0.0742,  1.6172, -0.8086, -0.5703, -0.4863]], device='cuda:0',
       dtype=torch.bfloat16, grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None), SequenceClassifierOutput(loss=tensor(0.6221, device='cuda:0', grad_fn=<NllLossBackward0>), logits=tensor([[ 0.3301,  1.1016, -0.1992, -0.8633, -0.8516],
        [ 0.2441,  1.7188, -0.8086, -1.0625, -0.6445],
        [-0.1201,  1.8047, -0.4766, -0.7773, -0.4375],
        [-0.0674,  1.6094, -0.8828, -0.9102, -0.5352],
        [-0.0615,  1.5234, -0.5195, -0.9492, -0.7617],
        [ 0.5000,  1.2500, -0.7266, -0.7500, -0.6484],
        [-0.1670,  1.4609, -0.6875, -0.7422, -0.5547],
        [ 0.0972,  1.5391, -0.2988, -1.1172, -0.8086],
        [ 0.0747,  1.7734, -0.7266, -0.6250, -0.4785],
        [-0.1157,  1.5781, -0.4980, -1.0625, -0.4043],
        [ 0.3379,  1.5703, -0.7578, -0.9180, -0.7188],
        [-0.0086,  1.4922, -0.9727, -1.2656, -0.3047],
        [ 0.0427,  1.2656, -0.6797, -1.0469, -0.6602],
        [ 0.3242,  1.6641, -0.7383, -0.7812, -0.7891],
        [ 0.1729,  1.5469, -0.9609, -0.8203, -0.4297],
        [ 0.0481,  0.4277, -0.8984, -0.5195, -0.0801]], device='cuda:0',
       dtype=torch.bfloat16, grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None), SequenceClassifierOutput(loss=tensor(0.8710, device='cuda:0', grad_fn=<NllLossBackward0>), logits=tensor([[ 0.1465,  1.6484, -0.8008, -0.8555, -0.6367],
        [ 0.1235,  1.8828, -0.7773, -0.7617, -0.4805],
        [-0.2676,  1.5781, -0.5273, -0.9023, -0.6250],
        [ 0.2090,  1.5391, -0.8203, -0.9961, -0.6875],
        [ 0.2236,  1.7188, -0.6328, -0.6953, -0.7695],
        [ 0.0723,  1.5781, -0.4336, -0.8594, -0.5859],
        [ 0.2812,  2.0938, -0.6836, -0.9492, -0.2852],
        [-0.1934,  1.6641, -0.5352, -0.5742, -0.4160],
        [ 0.3008,  1.5391, -0.4629, -0.9414, -0.5820],
        [ 0.2324,  0.8164, -1.0938, -0.5977, -0.4121],
        [ 0.3320,  1.4844, -0.6328, -0.8320, -0.5898],
        [-0.0425,  1.4531, -0.6328, -0.9492, -0.5859],
        [ 0.1699,  1.4766, -0.5977, -0.9453, -0.7188],
        [-0.0364,  1.7344, -1.0469, -0.8477, -0.5312],
        [-0.2988,  1.4141, -0.6055, -0.8477, -0.6719],
        [-0.1240,  1.6250, -0.6797, -0.9844, -0.6250]], device='cuda:0',
       dtype=torch.bfloat16, grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None), SequenceClassifierOutput(loss=tensor(0.6467, device='cuda:0', grad_fn=<NllLossBackward0>), logits=tensor([[ 0.2500,  1.0312, -1.2422, -1.1250,  0.2393],
        [-0.0062,  1.4375, -0.5820, -0.7969, -0.6367],
        [-0.1553,  1.5469, -0.7539, -1.0625, -0.6055],
        [ 0.1875,  1.8125, -0.5156, -0.8828, -0.9180],
        [ 0.2109,  1.3672, -1.1484, -0.9727, -0.2373],
        [ 0.1055,  1.3828, -0.6914, -0.9297, -0.3379],
        [ 0.0366,  1.5156, -1.1406, -0.8711, -0.5781],
        [ 0.0693,  1.5156, -0.8750, -1.0156, -0.7070],
        [ 0.0096,  1.5859, -0.8789, -1.1641, -0.6094],
        [-0.1118,  1.2812, -0.2578, -0.8359, -0.7500],
        [ 0.0957,  1.5469, -0.6719, -0.8125, -0.5000],
        [-0.1348,  1.7656, -0.9883, -0.7734, -0.3535],
        [-0.0898,  1.5859, -0.7617, -1.2031, -0.4336],
        [ 0.0586,  1.4922, -0.6797, -0.9258, -0.8516],
        [-0.0408,  1.6953, -0.6797, -0.7617, -0.6562],
        [ 0.1904,  1.2344, -0.9609, -0.8242, -0.4980]], device='cuda:0',
       dtype=torch.bfloat16, grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None), SequenceClassifierOutput(loss=tensor(0.6467, device='cuda:0', grad_fn=<NllLossBackward0>), logits=tensor([[ 0.0776,  1.3828, -0.8555, -1.1562, -0.6914],
        [-0.0123,  1.3594, -0.7422, -0.8281, -0.6680],
        [ 0.0273,  2.0625, -0.8633, -0.9453, -0.6016],
        [ 0.4180,  1.7266, -0.4648, -0.7422, -0.6836],
        [-0.2754,  1.8359, -0.9219, -0.9414, -0.6797],
        [ 0.1128,  1.3828, -0.8125, -0.7812, -0.4688],
        [-0.1011,  1.7031, -0.8281, -1.1328, -0.8438],
        [ 0.1147,  1.5391, -0.5703, -0.9727, -0.6836],
        [ 0.1172,  1.2812, -0.6484, -0.8359, -0.7344],
        [ 0.1865,  1.4219, -0.6680, -0.5859, -0.3223],
        [ 0.2695,  1.8516, -0.7930, -0.9922, -0.6016],
        [-0.0845,  1.9766, -1.0625, -0.8477, -0.5312],
        [-0.2832,  1.4141, -0.7852, -1.1875, -0.9727],
        [ 0.1021,  2.1094, -0.6484, -0.9883, -0.6523],
        [-0.2207,  1.4766, -0.9258, -0.8867, -0.7031],
        [ 0.0664,  1.8828, -0.9805, -0.7500, -0.7461]], device='cuda:0',
       dtype=torch.bfloat16, grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None), SequenceClassifierOutput(loss=tensor(0.6669, device='cuda:0', grad_fn=<NllLossBackward0>), logits=tensor([[ 0.1738,  1.5391, -0.7852, -0.9102, -0.3398],
        [ 0.0442,  1.4922, -1.1016, -1.3203, -0.7148],
        [-0.2041,  1.3984, -0.6602, -0.7930, -0.7344],
        [-0.0703,  1.6250, -0.7734, -0.8711, -0.7031],
        [-0.0457,  1.6484, -0.4395, -0.8438, -0.5312],
        [ 0.2148,  1.5781, -0.6836, -0.7461, -0.6953],
        [-0.0254,  1.2188, -0.7031, -0.8086, -0.6406],
        [-0.2188,  1.7500, -0.4902, -0.9766, -0.5625],
        [ 0.1709,  1.2812, -0.8555, -0.8594, -0.5195],
        [ 0.4492,  1.6797, -0.8047, -1.0781, -0.9297],
        [-0.0562,  1.7266, -0.6641, -0.8984, -0.3164],
        [ 0.3281,  1.6406, -0.5352, -0.7070, -1.1328],
        [ 0.2910,  1.3750, -0.2832, -0.6758, -1.0625],
        [ 0.1079,  1.5938, -0.3828, -0.8945, -0.6562],
        [ 0.3906,  1.6562, -0.7344, -0.7070, -0.6445],
        [-0.1787,  1.6953, -0.5664, -1.1016, -0.7539]], device='cuda:0',
       dtype=torch.bfloat16, grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None), SequenceClassifierOutput(loss=tensor(0.7360, device='cuda:0', grad_fn=<NllLossBackward0>), logits=tensor([[ 0.1006,  1.4844, -0.5039, -0.9453, -0.4180],
        [ 0.1436,  1.8203, -1.1250, -1.1250, -0.4102],
        [-0.1099,  1.8203, -0.6836, -0.5938, -0.6484],
        [-0.0894,  1.3516, -0.8711, -0.6484, -0.3613],
        [ 0.0620,  1.4609, -0.8906, -1.0859, -0.5352],
        [ 0.1494,  1.3438, -0.3887, -0.6523, -0.2734],
        [-0.0752,  1.9688, -0.8320, -1.0000, -0.6641],
        [-0.0840,  1.8594, -0.8672, -1.1016, -0.5703],
        [ 0.4004,  1.4531, -0.3457, -0.7227, -0.7539],
        [ 0.2871,  1.1328, -0.7773, -0.8359, -0.5977],
        [ 0.1328,  1.4453, -0.8203, -0.7578, -0.7266],
        [ 0.1514,  1.4375, -0.7695, -1.0781, -0.3926],
        [ 0.2930,  1.5078, -0.4668, -1.0312, -0.5898],
        [-0.2520,  1.4141, -0.8438, -0.5469, -0.6406],
        [ 0.0693,  1.8828, -1.0469, -0.8125, -0.4492],
        [ 0.0957,  1.6328, -0.7969, -1.1016, -0.3613]], device='cuda:0',
       dtype=torch.bfloat16, grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None), SequenceClassifierOutput(loss=tensor(0.7401, device='cuda:0', grad_fn=<NllLossBackward0>), logits=tensor([[ 7.9102e-02,  1.9141e+00, -8.0469e-01, -9.2969e-01, -6.1719e-01],
        [-2.2266e-01,  1.9297e+00, -6.6797e-01, -9.8047e-01, -7.3047e-01],
        [-1.8652e-01,  1.5312e+00, -6.9922e-01, -1.2344e+00, -4.0039e-01],
        [-6.5918e-02,  1.9375e+00, -1.0234e+00, -1.0469e+00, -5.2734e-01],
        [-5.6152e-02,  1.3828e+00, -5.1953e-01, -1.0469e+00, -7.5000e-01],
        [-4.0527e-02,  1.8047e+00, -8.7500e-01, -8.9062e-01, -8.4375e-01],
        [ 3.0859e-01,  1.5703e+00, -7.5391e-01, -9.3750e-01, -3.9844e-01],
        [ 9.9609e-02,  1.6094e+00, -7.6953e-01, -1.1641e+00, -4.2188e-01],
        [-1.2360e-03,  1.2344e+00, -7.1094e-01, -7.6562e-01, -5.4688e-01],
        [-1.4099e-02,  1.4375e+00, -5.4688e-01, -1.1094e+00, -5.3906e-01],
        [ 3.0078e-01,  1.2891e+00, -1.1641e+00, -9.0625e-01, -6.0547e-01],
        [ 2.7734e-01,  1.4531e+00, -7.8125e-01, -8.8281e-01, -3.9258e-01],
        [ 6.1523e-02,  1.9688e+00, -7.5000e-01, -9.5312e-01, -6.6406e-01],
        [-2.3242e-01,  1.9062e+00, -6.6016e-01, -9.9219e-01, -6.8750e-01],
        [-5.5859e-01,  1.2188e+00, -3.4961e-01, -5.7422e-01, -8.9062e-01],
        [ 3.3008e-01,  1.5000e+00, -5.7422e-01, -1.0312e+00, -5.6641e-01]],
       device='cuda:0', dtype=torch.bfloat16, grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None), SequenceClassifierOutput(loss=tensor(0.7310, device='cuda:0', grad_fn=<NllLossBackward0>), logits=tensor([[ 0.1543,  1.6172, -0.4824, -0.5430, -0.4570],
        [-0.3301,  1.7656, -0.6562, -0.9531, -0.3711],
        [-0.0051,  1.4141, -0.3301, -0.9258, -0.7500],
        [ 0.1147,  1.6719, -0.5312, -0.6328, -0.7891],
        [-0.3652,  1.6328, -0.3477, -1.1406, -0.8984],
        [-0.0598,  1.5625, -0.7578, -0.6250, -0.6680],
        [ 0.0742,  1.5391, -0.7188, -0.8203, -0.6797]], device='cuda:0',
       dtype=torch.bfloat16, grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)], [SequenceClassifierOutput(loss=tensor(2.3237, device='cuda:0', grad_fn=<NllLossBackward0>), logits=tensor([[ 0.3379,  1.6719, -0.8906, -0.9258, -0.4551],
        [ 0.3477,  0.7383, -1.2188, -1.1172, -0.1426],
        [-0.2539,  1.6562, -0.7734, -0.8047, -0.4434],
        [-0.0437,  1.3906, -0.7148, -0.8203, -0.9844],
        [ 0.0198,  1.0469, -0.8477, -0.8633, -0.5547],
        [ 0.0962,  1.1641, -1.0234, -0.7461, -0.2676],
        [ 0.0825,  1.3828, -0.8242, -0.9375, -0.4258],
        [ 0.4043,  1.2656, -0.9688, -0.8164, -0.4355],
        [ 0.0092,  1.6250, -0.7383, -1.0938, -0.5586],
        [ 0.0752,  1.0938, -0.7656, -0.6562, -0.4395],
        [-0.1660,  0.9375, -0.7031, -0.6133, -0.2617],
        [ 0.0776,  1.0781, -0.4844, -0.6328, -0.4688],
        [ 0.1387,  1.3750, -0.6211, -0.7422, -0.3145],
        [ 0.0264,  0.6602, -0.6562, -0.8125, -0.2480],
        [ 0.0347,  1.4141, -0.6211, -0.9688, -0.8047],
        [-0.1582,  1.4844, -0.6875, -0.4941, -0.3867]], device='cuda:0',
       dtype=torch.bfloat16, grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None), SequenceClassifierOutput(loss=tensor(2.3516, device='cuda:0', grad_fn=<NllLossBackward0>), logits=tensor([[ 0.0586,  1.6016, -1.0312, -0.5938, -0.7305],
        [-0.0118,  1.2344, -1.0156, -0.8711, -0.6055],
        [ 0.2041,  1.4141, -0.8086, -0.4160, -0.4199],
        [ 0.2891,  0.8828, -0.8750, -0.7500, -0.3457],
        [ 0.0884,  1.5078, -0.6680, -0.4180, -0.3086],
        [ 0.4531,  1.2891, -0.5781, -0.6289, -0.6211],
        [ 0.0087,  1.6016, -0.8711, -0.8750, -0.6719],
        [ 0.1582,  1.2188, -0.5391, -1.0312, -0.4746],
        [ 0.0977,  1.5156, -0.9141, -0.9883, -0.7930],
        [ 0.1187,  1.4453, -0.4980, -0.7891, -0.7383],
        [-0.1924,  1.5312, -0.7891, -0.9766, -0.2695],
        [-0.1147,  1.4844, -0.5039, -0.7539, -0.4844],
        [ 0.2559,  1.2031, -0.9766, -1.0781, -0.0613],9e-01, -6.2891e-01],
        [ 0.0229,  1.3750, -1.1484, -0.6367, -0.2852],7e+00, -6.5625e-01],
        [ 0.1279,  0.8984, -0.3398, -0.4141,  0.0488],5e-01, -5.9766e-01],
        [ 0.2217,  1.2031, -0.8555, -0.9219, -0.6172]], device='cuda:0',],
       dtype=torch.bfloat16, grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None), SequenceClassifierOutput(loss=tensor(2.0518, device='cuda:0', grad_fn=<NllLossBackward0>), logits=tensor([[ 0.3984,  1.2109, -0.9297, -0.4766, -0.3789],
        [-0.1069,  1.0938, -0.3496, -0.5977, -0.4844],9e-01, -5.7031e-01],
        [ 0.4082,  0.9023, -1.2891, -0.8672,  0.1543],6e-01, -6.4062e-01],
        [ 0.1699,  1.5547, -0.6445, -0.9414, -0.4727],1e-01, -3.5742e-01]],
        [ 0.4277,  1.6016, -0.7227, -0.9570, -0.1621],<AddmmBackward0>), hidden_states=None, attentions=None), SequenceClassifierOutput(loss=tensor(0.6610, device='cuda:0', grad_fn=<NllLossBackward0>), logits=tensor([[ 0.0767,  1.3906, -0.1973, -0.7969, -0.8594],
        [ 0.7656,  0.2197, -1.0312, -0.1719,  0.2227],
        [ 0.0923,  1.1094, -0.5781, -0.8086, -0.5664],
        [ 0.3633,  1.3516, -0.8711, -0.6094, -0.1118],
        [-0.0188,  0.8438, -0.6719, -0.5820, -0.3906],
        [-0.0503,  1.5391, -0.5234, -0.7734, -0.4844],
        [ 0.2334,  1.3984, -0.8242, -1.0625, -0.4473],
        [ 0.5781,  0.9766, -1.1016, -0.5781,  0.0215],
        [-0.0305,  1.2109, -0.6016, -0.3887, -0.3672],
        [ 0.1611,  1.5391, -1.2500, -0.9648, -0.4902],
        [-0.0371,  1.5938, -0.5430, -1.0234, -0.5312],
        [ 0.3945,  1.1016, -1.0625, -1.1797,  0.0087]], device='cuda:0',
       dtype=torch.bfloat16, grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None), SequenceClassifierOutput(loss=tensor(2.2612, device='cuda:0', grad_fn=<NllLossBackward0>), logits=tensor([[-0.1533,  1.7812, -0.6367, -0.7266, -0.4668],
        [-0.0045,  1.3828, -1.0078, -0.9102, -0.6797],
        [ 0.1240,  1.2344, -0.4707, -0.7930, -0.6680],
        [ 0.1123,  1.2656, -0.6797, -0.6250, -0.3848],, device='cuda:0',
        [ 0.1230,  1.2812, -0.9141, -0.9023, -0.6875],, hidden_states=None, attentions=None), SequenceClassifierOutput(loss=tensor(0.7367, device='cuda:0', grad_fn=<NllLossBackward0>), logits=tensor([[-0.0574,  1.1250, -0.4766, -0.7969, -0.7031],
        [ 0.2754,  1.4141, -0.7227, -0.8516, -0.6289],
        [ 0.1367,  1.4219, -0.7656, -1.0469, -0.1660],
        [ 0.0172,  0.8477, -0.7422, -0.7539, -0.7109],
        [ 0.5352,  0.9805, -1.2891, -1.2578,  0.0408],
        [ 0.3965,  1.3594, -0.4180, -0.5586, -0.3398],
        [-0.0024,  1.5859, -0.9922, -0.9609, -0.6445],
        [ 0.0095,  1.1953, -0.9805, -0.6719, -0.6289],
        [ 0.1475,  1.3438, -0.6719, -0.7305, -0.3594],
        [-0.0923,  1.4766, -0.8203, -1.3281, -0.2617],
        [ 0.5547,  1.0859, -0.5195, -0.5938, -0.5938],
        [ 0.3145,  1.5703, -0.9141, -0.8398, -0.7070]], device='cuda:0',
       dtype=torch.bfloat16, grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None), SequenceClassifierOutput(loss=tensor(2.6025, device='cuda:0', grad_fn=<NllLossBackward0>), logits=tensor([[-0.0718,  1.8047, -0.8281, -1.1719, -0.3457],
        [ 0.6484,  1.8594, -0.8789, -0.9297,  0.0525],
        [ 0.1514,  1.6016, -0.7539, -0.8516, -0.2246],
        [ 0.1494,  1.6016, -0.9297, -0.8711, -0.3047],, device='cuda:0',
        [-0.0928,  1.5938, -0.9688, -0.6953, -0.5938],, hidden_states=None, attentions=None), SequenceClassifierOutput(loss=tensor(0.8184, device='cuda:0', grad_fn=<NllLossBackward0>), logits=tensor([[-0.1328,  0.8203, -0.2383, -0.7344, -0.5742],
        [ 0.3008,  0.4453, -0.8438,  0.0128,  0.2295],
        [-0.0260,  1.4844, -0.9961, -0.9062, -0.4375],
        [ 0.8750,  0.8828, -1.0312, -0.7734,  0.1885],
        [ 0.3223,  0.8242, -1.3750, -0.9336,  0.2334],
        [-0.0618,  1.2188, -0.7773, -1.0938, -0.5781],
        [ 0.1816,  1.2578, -0.9453, -0.8164, -0.5195],
        [-0.1084,  1.2344, -0.6328, -0.8477, -0.6641],
        [-0.1338,  1.5625, -0.7070, -0.7188, -0.7148],
        [ 0.3887,  1.2344, -0.6914, -0.7578, -0.4004],
        [ 0.1885,  1.3828, -1.2109, -0.8633, -0.6250],
        [ 0.2988,  1.2500, -0.5703, -1.0391, -0.4238]], device='cuda:0',
       dtype=torch.bfloat16, grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None), SequenceClassifierOutput(loss=tensor(1.3418, device='cuda:0', grad_fn=<NllLossBackward0>), logits=tensor([[ 0.2402,  1.4766, -0.7344, -0.6836, -0.5547],
        [ 0.0275,  1.5078, -0.4922, -0.7891, -0.6211],
        [ 0.3711,  1.8828, -1.0312, -0.9102, -0.1250],
        [ 0.1514,  1.1641, -0.6992, -1.0078, -0.5625],, device='cuda:0',
        [ 0.1357,  1.1719, -0.8477, -0.7266, -0.2451],, hidden_states=None, attentions=None), SequenceClassifierOutput(loss=tensor(0.9958, device='cuda:0', grad_fn=<NllLossBackward0>), logits=tensor([[-3.8477e-01,  1.0547e+00, -6.1719e-01, -8.1641e-01, -1.1250e+00],
        [ 0.1895,  1.2812, -0.8398, -1.0547, -0.4453],2e-01, -9.8047e-01],
        [ 0.0378,  1.2500, -0.4492, -0.7578, -1.0156],4e-01, -1.1250e+00],
        [ 0.0225,  1.5000, -0.5430, -0.9648, -0.4922],1e-01, -8.6328e-01],
        [-0.2246,  1.4766, -0.4297, -1.0391, -0.7578],9e-01, -8.7891e-01],
        [-0.0361,  1.5938, -0.6406, -1.0234, -0.6836],0e+00, -1.0391e+00],
        [ 0.2695,  1.7500, -0.5977, -0.8359, -0.5234],1e-01, -8.1250e-01],
        [ 0.0605,  1.5391, -0.3613, -1.0312, -0.7695],9e-01, -6.4453e-01],
        [ 0.1426,  1.4766, -0.5156, -1.1641, -0.4238],2e+00, -5.6641e-01],
        [ 0.1709,  1.1562, -0.5234, -0.9375, -0.9141],9e-01, -3.1250e-01],
        [-0.2100,  1.4062, -0.7305, -0.9648, -0.7070],5e-01, -4.1406e-01],
        [ 0.2500,  1.3906, -0.8203, -0.8516, -0.8906]], device='cuda:0',],
       dtype=torch.bfloat16, grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None), SequenceClassifierOutput(loss=tensor(1.0288, device='cuda:0', grad_fn=<NllLossBackward0>), logits=tensor([[ 0.4316,  1.6172, -0.9141, -1.1172, -0.3301],
        [-0.0083,  1.3203, -0.4551, -1.1250, -0.8242],8e-01, -7.7344e-01],
        [-0.0811,  1.1172, -0.6055, -0.6133, -0.6523],5e-01, -5.7422e-01],
        [ 0.2812,  1.6484, -0.4551, -0.7422, -1.1484],1e+00, -4.8438e-01]],
        [ 0.4746,  1.4844, -0.8594, -1.2031, -0.5664],<AddmmBackward0>), hidden_states=None, attentions=None), SequenceClassifierOutput(loss=tensor(0.9296, device='cuda:0', grad_fn=<NllLossBackward0>), logits=tensor([[ 0.1123,  0.7266, -0.6875, -0.8711, -0.5977],
        [-0.2402,  1.7656, -0.1602, -0.5781, -0.9648],
        [ 0.2021,  1.5391, -0.6484, -0.9375, -0.7734],
        [-0.1279,  1.4531, -0.1914, -0.7578, -0.9141],
        [ 0.3691,  1.4844, -0.5352, -0.9219, -0.8320],
        [-0.0762,  1.4375, -0.5312, -0.9297, -0.4570],
        [ 0.1465,  1.5703, -0.3750, -1.0234, -0.7344],
        [-0.2090,  0.9688, -0.7109, -0.5781, -0.8555],
        [ 0.2451,  1.1953, -0.8281, -1.1250, -0.9766],
        [ 0.2324,  1.5312, -0.4395, -1.0000, -0.4805],
        [ 0.1406,  1.1016, -0.7539, -1.2578, -0.5898],
        [-0.1709,  1.3828, -0.5898, -0.7578, -0.3691]], device='cuda:0',
       dtype=torch.bfloat16, grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None), SequenceClassifierOutput(loss=tensor(0.5144, device='cuda:0', grad_fn=<NllLossBackward0>), logits=tensor([[ 0.0238,  1.4609, -0.5156, -0.9375, -0.6992],
        [ 0.2852,  1.5156, -0.5430, -1.0312, -0.9453],
        [ 0.1094,  1.7422, -0.3418, -0.9180, -0.6836],
        [ 0.0854,  1.4375, -0.8203, -0.7812, -1.1875],, device='cuda:0',
        [-0.1904,  1.5703, -0.5156, -0.5977, -0.7266],, hidden_states=None, attentions=None), SequenceClassifierOutput(loss=tensor(0.8668, device='cuda:0', grad_fn=<NllLossBackward0>), logits=tensor([[-0.4961,  1.0312, -0.1650, -0.5391, -0.5312],
        [ 0.2930,  1.5859, -0.9102, -0.7617, -0.8125],
        [ 0.0244,  1.5469, -0.3828, -1.0547, -0.6562],
        [-0.1738,  1.6250, -0.2656, -1.0078, -0.7969],
        [-0.3262,  1.6641, -0.2158, -1.0156, -0.4941],
        [ 0.2637,  1.7812, -0.4688, -1.0078, -0.9062],
        [-0.0054,  1.3516, -0.5703, -0.9922, -0.9961],
        [ 0.1289,  1.4766, -0.6133, -0.7109, -1.0469],
        [-0.1016,  1.5156, -0.5820, -0.8594, -0.8047],
        [ 0.2383,  1.3906, -0.6875, -0.9648, -0.8242],
        [ 0.0586,  1.7734, -0.6211, -1.3203, -0.8945],
        [-0.3145,  1.6016, -0.5820, -0.7812, -0.7930]], device='cuda:0',
       dtype=torch.bfloat16, grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None), SequenceClassifierOutput(loss=tensor(0.5616, device='cuda:0', grad_fn=<NllLossBackward0>), logits=tensor([[-0.1025,  1.7188, -0.6445, -1.1094, -0.8750],
        [ 0.4863,  1.4688, -0.6367, -0.6055, -0.4316],
        [ 0.0811,  1.6094, -0.4336, -0.9180, -0.6484],
        [-0.0374,  1.4453, -0.4922, -0.6406, -0.5820],, device='cuda:0',
        [ 0.1689,  1.2891, -0.7109, -0.9961, -0.5703],, hidden_states=None, attentions=None), SequenceClassifierOutput(loss=tensor(0.8071, device='cuda:0', grad_fn=<NllLossBackward0>), logits=tensor([[-0.2344,  1.5703, -0.5977, -0.9570, -0.5781],
        [-0.1982,  1.7969, -0.0898, -0.7617, -0.7812],
        [ 0.3203,  1.1250, -0.9023, -0.8789, -0.3770],
        [ 0.1191,  1.7344, -0.5234, -0.9453, -0.9297],
        [-0.2402,  1.5156, -0.2988, -0.8164, -0.7383],
        [-0.1289,  1.5781, -0.9648, -1.1328, -0.5859],
        [-0.2598,  1.4453, -0.6836, -0.8711, -0.6289],
        [ 0.3301,  1.1562, -0.3672, -0.9102, -0.7188],
        [-0.2695,  1.6875, -0.3730, -0.7266, -0.4316],
        [-0.1533,  1.6797, -0.4668, -1.2422, -0.7461],
        [-0.1309,  1.5078, -0.6289, -0.8789, -0.5391],
        [-0.1758,  1.4922, -0.6094, -1.2734, -0.8359]], device='cuda:0',
       dtype=torch.bfloat16, grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None), SequenceClassifierOutput(loss=tensor(0.6637, device='cuda:0', grad_fn=<NllLossBackward0>), logits=tensor([[-0.0540,  1.4531, -0.5781, -0.8047, -0.7461],
        [-0.2344,  1.3438, -0.5352, -0.8398, -0.9453],
        [ 0.4492,  1.4453, -1.0391, -0.6250, -0.5859],
        [ 0.4297,  0.9609, -1.0547, -1.2188, -0.3047],, device='cuda:0',
        [ 0.1836,  1.4844, -0.7969, -0.7461, -0.9297],, hidden_states=None, attentions=None), SequenceClassifierOutput(loss=tensor(0.7816, device='cuda:0', grad_fn=<NllLossBackward0>), logits=tensor([[ 0.0635,  1.2031, -0.2734, -0.8711, -0.7031],
        [ 0.1523,  1.3438, -0.5938, -0.6992, -0.8828],
        [ 0.0493,  1.5312, -0.8320, -1.2344, -0.3672],
        [-0.0454,  1.7969, -0.6484, -0.7617, -0.3809],
        [ 0.1328,  1.1094, -0.4395, -0.8359, -0.7266],
        [-0.1245,  1.6328, -0.4297, -0.9102, -0.8555],
        [ 0.0025,  1.3281, -0.3691, -1.3516, -0.8164],
        [ 0.0742,  1.5000, -0.4062, -0.7500, -0.8438],
        [-0.0767,  1.3750, -0.4844, -0.9609, -0.8672],
        [ 0.2598,  1.4531, -0.7656, -0.7070, -0.7578],
        [-0.0967,  1.2266, -0.6875, -0.6797, -0.4551],
        [-0.1328,  1.6953, -0.6953, -0.9766, -0.5938]], device='cuda:0',
       dtype=torch.bfloat16, grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None), SequenceClassifierOutput(loss=tensor(0.8901, device='cuda:0', grad_fn=<NllLossBackward0>), logits=tensor([[-0.0182,  1.4141, -0.6367, -0.9219, -1.0781],
        [ 0.1436,  1.1406, -0.8516, -0.5820, -0.6914],
        [-0.3496,  1.2969, -0.5273, -1.0938, -1.1484],
        [ 0.2617,  1.5234, -0.5312, -0.7656, -0.8359],, device='cuda:0',
        [ 0.2988,  1.4062, -0.5547, -0.6094, -0.5234],, hidden_states=None, attentions=None), SequenceClassifierOutput(loss=tensor(0.8848, device='cuda:0', grad_fn=<NllLossBackward0>), logits=tensor([[ 0.1631,  1.0312, -0.0549, -0.0525, -0.3887],
        [ 0.1562,  1.5625, -0.3574, -0.9414, -0.5664],
        [-0.3281,  1.2500, -0.9766, -1.0156, -0.7773],
        [ 0.2178,  1.2109, -0.7188, -0.6797, -0.6953],
        [ 0.0408,  1.4766, -0.8711, -1.3359, -0.5703],
        [-0.1484,  1.4141, -0.1689, -0.9180, -0.8242],
        [ 0.2402,  1.5469, -0.5234, -0.4629, -0.4434],
        [-0.0630,  1.5000, -0.4648, -0.8555, -0.4121],
        [ 0.2090,  1.3281, -0.4199, -0.8086, -0.7617],
        [ 0.0251,  1.3594, -0.7188, -1.0781, -0.8945],
        [-0.0776,  1.4922, -0.8750, -1.1406, -0.8086],
        [ 0.1064,  1.2500, -0.7461, -1.1797, -0.7461]], device='cuda:0',
       dtype=torch.bfloat16, grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None), SequenceClassifierOutput(loss=tensor(0.8544, device='cuda:0', grad_fn=<NllLossBackward0>), logits=tensor([[ 0.2676,  1.0703, -1.1719, -0.9961, -0.5273],
        [ 0.1748,  1.4688, -0.9102, -1.1641, -0.8047],
        [-0.0205,  1.5547, -0.9805, -0.9219, -0.5742],
        [ 0.0056,  1.4141, -0.4941, -0.9844, -0.7305],, device='cuda:0',
        [ 0.2041,  1.0391, -0.8242, -1.1094, -0.2188],, hidden_states=None, attentions=None), SequenceClassifierOutput(loss=tensor(0.9812, device='cuda:0', grad_fn=<NllLossBackward0>), logits=tensor([[ 0.0723,  1.2656, -0.2871, -0.8398, -0.7539],
        [ 0.0786,  1.3906, -0.5117, -1.3906, -0.4297],
        [ 0.1680,  1.2188, -0.6836, -1.1641, -0.5078],
        [-0.0645,  1.2031, -0.7539, -0.9023, -0.6211],
        [ 0.0386,  1.8047, -0.4277, -0.8711, -0.8672],
        [-0.2432,  1.2422, -0.5000, -1.0938, -0.9023],
        [-0.1719,  1.1953, -0.3730, -0.8750, -0.9531],
        [ 0.2852,  1.3125, -0.5977, -0.8672, -0.5234],
        [ 0.2207,  1.2656, -0.7734, -1.1797, -0.5312],
        [-0.1011,  1.6953, -0.4629, -0.6523, -0.8789],
        [-0.2451,  1.8516, -0.5820, -1.0469, -0.5977],
        [-0.1875,  1.2109, -0.6055, -1.1094, -0.7969]], device='cuda:0',
       dtype=torch.bfloat16, grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None), SequenceClassifierOutput(loss=tensor(0.7292, device='cuda:0', grad_fn=<NllLossBackward0>), logits=tensor([[-0.1787,  1.3672, -0.5469, -0.7617, -0.5039],
        [ 0.0698,  1.5859, -0.3086, -0.9570, -0.7969],
        [ 0.2617,  1.7656, -0.8203, -1.0391, -0.7266],
        [-0.0093,  1.3359, -0.5625, -0.8633, -0.6758],
        [-0.0564,  1.5781, -0.2930, -0.8438, -0.6680],
        [ 0.4473,  1.1250, -0.8984, -0.9961, -0.5195],
        [-0.0518,  1.7969, -0.3691, -0.9297, -1.0312],
        [ 0.1006,  1.6094, -0.6523, -0.9141, -0.8398],
        [ 0.3105,  1.7266, -0.5000, -0.8555, -0.4922],
        [ 0.0364,  1.3516, -0.5742, -0.9414, -0.6992],
        [-0.2021,  1.4531, -0.3340, -0.9766, -0.4805],
        [-0.2002,  1.2188, -0.5156, -0.7266, -0.3145],
        [ 0.1494,  1.3984, -0.7500, -1.2266, -0.6094],
        [-0.0398,  1.3594, -0.7031, -0.8633, -0.8398],
        [-0.2988,  1.5391, -0.2812, -1.3281, -0.7344],
        [ 0.0148,  1.7188, -0.4375, -0.7461, -0.7539]], device='cuda:0',
       dtype=torch.bfloat16, grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None), SequenceClassifierOutput(loss=tensor(0.7085, device='cuda:0', grad_fn=<NllLossBackward0>), logits=tensor([[-0.2012,  1.5625, -0.7617, -0.8281, -0.5898],
        [ 0.3027,  1.2656, -0.6836, -1.0469, -0.5977],
        [ 0.2334,  1.4375, -0.8281, -1.0078, -0.8438],
        [ 0.0105,  1.6797, -0.4688, -0.8281, -0.8086],
        [ 0.3555,  1.4062, -0.9102, -0.6602, -0.4922],
        [-0.0405,  1.2734, -0.6602, -1.0234, -0.6484],
        [-0.2793,  1.5312, -0.5312, -1.0078, -0.8047],
        [ 0.2188,  1.5234, -0.7188, -1.0625, -0.7305],
        [ 0.2695,  1.3594, -0.6211, -1.1484, -0.7969],
        [ 0.1846,  1.1719, -0.7695, -0.9609, -0.4102],
        [-0.0981,  1.8125, -0.6172, -1.0859, -0.7422],
        [-0.2832,  1.8125, -1.0859, -0.7305, -0.5195],
        [-0.4219,  1.6172, -0.4199, -0.9023, -0.4688],
        [ 0.3223,  1.0703, -1.0547, -0.7070,  0.0776],
        [-0.0076,  1.7109, -0.8398, -0.9570, -0.5938],
        [ 0.3730,  1.1875, -0.4141, -0.9297, -0.7227]], device='cuda:0',
       dtype=torch.bfloat16, grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None), SequenceClassifierOutput(loss=tensor(0.6149, device='cuda:0', grad_fn=<NllLossBackward0>), logits=tensor([[ 0.0476,  1.4453, -0.7773, -0.8477, -0.9219],
        [-0.0874,  1.5938, -0.5859, -0.8008, -0.9219],
        [-0.2871,  1.6641, -0.4941, -0.4336, -0.7461],
        [ 0.1396,  1.6250, -0.4473, -0.8086, -0.7305],
        [-0.2324,  1.6719, -0.2578, -0.8828, -0.3770],
        [-0.0669,  1.6328, -0.3438, -0.9219, -0.8242],
        [ 0.2129,  1.6797, -0.5273, -0.5859, -0.7539],
        [-0.2871,  1.4062, -0.5117, -1.2031, -0.8906],
        [ 0.0227,  1.3594, -0.6250, -1.1719, -0.6484],
        [-0.3789,  1.2891, -0.3633, -0.7773, -0.9258],
        [ 0.1934,  1.5625, -0.3555, -0.7461, -0.7383],
        [ 0.2148,  1.7812, -0.3887, -1.0938, -0.6055],
        [-0.0835,  1.6484, -0.4688, -1.2344, -0.8750],
        [-0.0850,  1.5156, -0.4336, -1.0156, -0.5117],
        [-0.1543,  1.4219, -0.6914, -0.7070, -0.9844],
        [ 0.3398,  1.2969, -0.7773, -1.0391, -0.7305]], device='cuda:0',
       dtype=torch.bfloat16, grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None), SequenceClassifierOutput(loss=tensor(0.7074, device='cuda:0', grad_fn=<NllLossBackward0>), logits=tensor([[-0.0542,  1.2344, -0.4531, -0.4180, -0.5273],
        [-0.0378,  1.8438, -0.6680, -0.9648, -0.6680],
        [-0.0119,  1.3203, -0.7656, -1.1719, -0.7500],
        [ 0.0496,  1.3672, -0.4609, -0.6289, -0.9023],
        [ 0.4629,  1.5391, -0.4961, -1.0703, -1.0391],
        [ 0.4570,  1.0625, -0.4023, -0.9883, -0.1128],
        [-0.0317,  1.1094, -0.4941, -1.0234, -1.5156],
        [ 0.1553,  1.5391, -0.5508, -0.8711, -0.7305],
        [ 0.2773,  1.1875, -0.6758, -1.1719, -0.7344],
        [ 0.2412,  1.4453, -0.5117, -0.6250, -0.6953],
        [ 0.3574,  1.4844, -0.8594, -1.0078, -0.4824],
        [ 0.4160,  1.6328, -0.4668, -0.9336, -0.7500],
        [ 0.1855,  1.1797, -0.4473, -0.9531, -0.7266],
        [ 0.2637,  1.6250, -0.2207, -1.1797, -0.6719],
        [ 0.0549,  1.3906, -0.9531, -1.0156, -0.6992],
        [-0.3594,  1.3516, -0.5898, -0.8711, -0.7656]], device='cuda:0',
       dtype=torch.bfloat16, grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None), SequenceClassifierOutput(loss=tensor(0.7747, device='cuda:0', grad_fn=<NllLossBackward0>), logits=tensor([[-0.0205,  1.2344, -0.6250, -1.2109, -0.3301],
        [ 0.0061,  1.5547, -0.4492, -1.2578, -0.7227],
        [-0.0344,  1.5234, -0.6133, -1.1016, -0.5391],
        [-0.2158,  1.7812, -0.5586, -0.9180, -0.6211],
        [ 0.5312,  1.1328, -1.1719, -1.1094,  0.0879],
        [-0.0322,  1.6250, -0.7461, -0.9688, -0.6719],
        [ 0.4414,  1.0156, -0.7773, -1.0703, -0.8086],
        [ 0.1465,  1.9688, -0.3965, -0.9609, -0.7734],
        [ 0.0229,  1.5859, -0.9844, -1.1562, -0.4863],
        [-0.1436,  1.4844, -0.7031, -0.9570, -0.8125],
        [-0.0771,  1.3594, -0.7188, -1.0234, -0.7148],
        [ 0.0554,  1.4609, -1.1172, -0.8320, -0.4531],
        [ 0.1924,  1.4453, -0.4238, -0.7539, -0.9961],
        [-0.1650,  1.5156, -0.9766, -0.8984, -0.3848],
        [ 0.0320,  1.1641, -0.6680, -0.8711, -0.6172],
        [-0.1108,  1.4922, -0.7852, -0.8555, -0.8750]], device='cuda:0',
       dtype=torch.bfloat16, grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None), SequenceClassifierOutput(loss=tensor(0.5486, device='cuda:0', grad_fn=<NllLossBackward0>), logits=tensor([[ 0.0923,  1.2188, -0.8398, -1.1406, -0.5117],
        [ 0.0684,  1.8516, -0.4141, -0.8672, -0.6992],
        [-0.1680,  1.6250, -0.6289, -0.9492, -0.6172],
        [ 0.1562,  1.3516, -0.7188, -0.8047, -0.7266],
        [ 0.4785,  1.1719, -0.7422, -1.0312, -0.5039],
        [-0.1621,  1.5234, -0.6328, -0.7969, -0.6523],
        [-0.1084,  1.5625, -0.6562, -1.0859, -0.3750],
        [-0.1885,  1.1875, -0.8633, -0.8711, -0.5859],
        [ 0.0260,  1.5859, -0.7383, -1.1016, -0.7227],
        [ 0.2617,  1.3672, -1.0469, -1.0156, -0.4238],
        [ 0.4297,  1.2891, -0.9453, -0.8008, -0.3516],
        [-0.1953,  1.1875, -0.8320, -0.8789, -0.4492],
        [ 0.2754,  1.6641, -0.4824, -0.8750, -0.4766],
        [-0.0082,  1.1562, -0.9727, -0.8398, -0.3809],
        [-0.1060,  1.2031, -0.5977, -0.7461, -0.9023],
        [-0.1084,  1.2969, -0.7930, -0.8086, -0.4434]], device='cuda:0',
       dtype=torch.bfloat16, grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None), SequenceClassifierOutput(loss=tensor(0.6093, device='cuda:0', grad_fn=<NllLossBackward0>), logits=tensor([[-0.0957,  1.5625, -0.5898, -0.6914, -0.7500],
        [ 0.3164,  1.3828, -0.7617, -1.0469, -0.5234],
        [-0.0640,  1.3750, -0.7031, -1.0391, -0.5586],
        [ 0.1826,  1.6250, -0.7578, -0.8945, -0.8984],
        [-0.1143,  1.7266, -0.5625, -0.8906, -0.8867],
        [ 0.1484,  1.7578, -0.8516, -0.7070, -0.5742],
        [ 0.2617,  1.6328, -0.4512, -0.8789, -0.9531],
        [ 0.1709,  1.6562, -0.5352, -0.6719, -0.5312],
        [ 0.0244,  1.5625, -0.9414, -0.9375, -0.5586],
        [ 0.0835,  1.3984, -0.7148, -1.0781, -0.4941],
        [ 0.2090,  1.2266, -0.2969, -0.9531, -0.5234],
        [-0.3379,  1.5859, -0.8555, -1.2578, -0.4922],
        [-0.2158,  1.5625, -0.6914, -1.1328, -0.5117],
        [ 0.0378,  1.4219, -0.7930, -0.7500, -0.5273],
        [-0.0310,  1.6797, -0.8750, -0.7812, -0.7891],
        [ 0.1055,  1.3203, -0.4902, -1.1719, -0.6250]], device='cuda:0',
       dtype=torch.bfloat16, grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None), SequenceClassifierOutput(loss=tensor(0.4014, device='cuda:0', grad_fn=<NllLossBackward0>), logits=tensor([[ 0.2100,  1.4219, -0.2852, -0.9883, -0.7031],
        [-0.3125,  1.7344, -0.3867, -0.9453, -0.5547],
        [-0.0112,  1.6172, -0.7344, -0.9336, -0.6602],
        [-0.0206,  1.5469, -0.4883, -0.9141, -0.5547],
        [-0.0325,  1.6875, -0.6680, -0.7852, -0.3301],
        [ 0.1338,  1.6875, -1.0703, -1.0234, -0.4766],
        [ 0.3496,  1.5547, -0.7578, -1.0000, -0.2207],
        [-0.2676,  1.5078, -0.6523, -0.9062, -0.6055],
        [ 0.1475,  1.3281, -1.1406, -1.1250, -0.2432],
        [ 0.2812,  2.0469, -0.6406, -0.8984, -0.8125],
        [ 0.2539,  1.6016, -0.6484, -1.1250, -0.8477],
        [ 0.1118,  1.5859, -0.4785, -0.9141, -0.6250],
        [-0.3789,  1.9922, -0.3066, -0.9531, -0.8125],
        [ 0.0811,  1.8828, -0.6289, -0.7148, -0.5898],
        [ 0.2314,  2.1406, -0.9062, -1.0625, -0.5000],
        [-0.0771,  1.4453, -0.5703, -0.5859, -0.3516]], device='cuda:0',
       dtype=torch.bfloat16, grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None), SequenceClassifierOutput(loss=tensor(0.8793, device='cuda:0', grad_fn=<NllLossBackward0>), logits=tensor([[ 7.9102e-02,  1.3125e+00, -7.6953e-01, -9.5703e-01, -6.9531e-01],
        [-2.8711e-01,  1.4141e+00, -4.5703e-01, -7.6562e-01, -9.0625e-01],
        [ 2.6758e-01,  1.8516e+00, -8.5547e-01, -8.3594e-01, -3.0469e-01],
        [ 7.2754e-02,  1.3125e+00, -2.8442e-02, -1.1719e+00, -8.9844e-01],
        [ 6.7383e-02,  1.4844e+00, -5.0781e-01, -1.0234e+00, -7.6562e-01],
        [ 1.8387e-03,  1.3750e+00, -6.9141e-01, -1.1172e+00, -5.4297e-01],
        [ 1.1444e-03,  1.2578e+00, -5.5078e-01, -6.7578e-01, -8.3203e-01],
        [ 3.1641e-01,  1.6328e+00, -8.6719e-01, -8.9062e-01, -7.9688e-01],
        [ 1.1572e-01,  1.4141e+00, -7.4219e-01, -1.2578e+00, -5.4688e-01],
        [ 2.5586e-01,  1.8750e+00, -7.1094e-01, -8.7891e-01, -4.2188e-01],
        [ 1.2109e-01,  1.3516e+00, -6.6406e-01, -1.0469e+00, -6.0547e-01],
        [-3.9062e-03,  1.3750e+00, -8.0078e-01, -7.8906e-01, -4.9805e-01],
        [ 5.7422e-01,  1.3672e+00, -6.3281e-01, -8.7109e-01, -2.6562e-01],
        [-1.4941e-01,  1.4922e+00, -5.5859e-01, -1.1172e+00, -5.7812e-01],
        [-2.0508e-01,  1.6016e+00, -7.3828e-01, -1.2734e+00, -4.6289e-01],
        [ 6.7444e-03,  1.6562e+00, -5.8594e-01, -1.0391e+00, -6.3672e-01]],
       device='cuda:0', dtype=torch.bfloat16, grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None), SequenceClassifierOutput(loss=tensor(0.8716, device='cuda:0', grad_fn=<NllLossBackward0>), logits=tensor([[ 2.4609e-01,  1.9531e+00, -6.7188e-01, -9.3750e-01, -7.3047e-01],
        [ 4.2969e-01,  1.3125e+00, -1.0703e+00, -6.4844e-01, -4.4531e-01],
        [ 2.8906e-01,  1.2188e+00, -8.4766e-01, -9.4141e-01, -5.6641e-01],
        [-2.7734e-01,  1.7266e+00, -4.8438e-01, -6.6016e-01, -4.6484e-01],
        [ 1.7285e-01,  1.2812e+00, -3.3203e-01, -1.0000e+00, -6.4844e-01],
        [-1.8652e-01,  1.3438e+00, -4.0039e-01, -9.1406e-01, -4.4922e-01],
        [-1.7090e-03,  1.8125e+00, -7.5391e-01, -8.9062e-01, -4.6094e-01],
        [ 2.1191e-01,  1.7031e+00, -6.7578e-01, -8.7109e-01, -7.5391e-01],
        [ 1.1816e-01,  1.6875e+00, -8.2031e-01, -6.3281e-01, -4.0234e-01],
        [ 2.5195e-01,  1.3203e+00, -5.2344e-01, -8.7109e-01, -5.0391e-01],
        [ 3.2227e-01,  1.7344e+00, -7.5000e-01, -1.3125e+00, -4.9219e-01],
        [ 1.5820e-01,  1.1328e+00, -9.4141e-01, -9.2578e-01, -4.2578e-01],
        [-5.5078e-01,  1.3516e+00, -6.7969e-01, -9.0234e-01, -4.2969e-01],
        [-1.1084e-01,  1.7266e+00, -7.7344e-01, -8.7500e-01, -5.0000e-01],
        [-2.5391e-02,  1.5000e+00, -7.3828e-01, -8.7891e-01, -5.5859e-01],
        [ 1.9629e-01,  1.6641e+00, -1.1953e+00, -1.0625e+00, -5.1562e-01]],
       device='cuda:0', dtype=torch.bfloat16, grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None), SequenceClassifierOutput(loss=tensor(0.6301, device='cuda:0', grad_fn=<NllLossBackward0>), logits=tensor([[ 0.3867,  1.6250, -0.9609, -0.7188, -0.6445],
        [ 0.4512,  1.5391, -0.3145, -1.0078, -0.6445],
        [ 0.2754,  1.2344, -0.5859, -1.0469, -0.3301],
        [ 0.0928,  1.4062, -0.8672, -0.8359, -0.5234],
        [ 0.0356,  1.5000, -0.8086, -0.9414, -0.7969],
        [ 0.3945,  1.6875, -0.9609, -1.1953, -0.6211],
        [-0.1455,  1.6250, -0.7812, -0.9102, -0.4316],
        [-0.3555,  1.6172, -0.8984, -0.9258, -0.8281],
        [ 0.0108,  1.7734, -0.4785, -1.0938, -0.6289],
        [ 0.1514,  1.3594, -0.9570, -0.7383, -0.3535],
        [-0.0096,  1.9141, -0.8828, -0.7031, -0.7031],
        [ 0.0623,  1.5938, -0.6172, -1.0703, -0.7148],
        [ 0.1445,  1.5938, -0.5859, -0.5508, -0.3203],
        [-0.1680,  1.8281, -0.4023, -0.6680, -0.4727],
        [-0.1611,  1.6875, -1.0391, -0.7695, -0.5586],
        [-0.0503,  1.4766, -0.4785, -0.4121, -0.7578]], device='cuda:0',
       dtype=torch.bfloat16, grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None), SequenceClassifierOutput(loss=tensor(0.4375, device='cuda:0', grad_fn=<NllLossBackward0>), logits=tensor([[ 0.3027,  1.4219, -0.8281, -1.2109, -0.0537],
        [ 0.3125,  1.6250, -0.4395, -1.2656, -0.7656],
        [ 0.0352,  1.7578, -0.8398, -0.9648, -0.5586],
        [ 0.0613,  1.7734, -0.4629, -0.9883, -0.9102],
        [ 0.1650,  1.9141, -0.8125, -0.7695, -0.2559],
        [ 0.2217,  1.5625, -0.4160, -0.9062, -0.7539],
        [ 0.1582,  1.2578, -0.7812, -1.0703, -0.5391],
        [ 0.0082,  1.4375, -0.7734, -0.8867, -0.6641],
        [-0.0447,  1.6875, -0.7773, -0.6250, -0.4062],
        [ 0.2207,  1.4375, -0.8984, -0.8125, -0.3398],
        [ 0.0938,  1.4453, -0.9258, -0.7109, -0.4219],
        [ 0.1021,  1.4531, -0.4805, -1.1094, -0.4902],
        [-0.3418,  1.5859, -0.4062, -0.6055, -0.7773],
        [ 0.0859,  1.8516, -0.8398, -0.8711, -0.6133],
        [-0.2090,  1.6719, -0.4805, -0.6758, -0.6406],
        [-0.0620,  1.2031, -0.7695, -1.0469, -0.5859]], device='cuda:0',
       dtype=torch.bfloat16, grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None), SequenceClassifierOutput(loss=tensor(0.8164, device='cuda:0', grad_fn=<NllLossBackward0>), logits=tensor([[-0.0601,  1.7734, -0.3711, -1.2656, -0.8945],
        [-0.1533,  1.3125, -0.4492, -1.0000, -0.4863],
        [ 0.0454,  1.3750, -0.4805, -1.0859, -0.8438],
        [-0.1855,  1.4375, -0.5547, -0.8438, -0.7539],
        [-0.0684,  1.4219, -0.2910, -0.7930, -0.4941],
        [-0.1016,  1.4766, -0.7500, -0.9023, -0.3711],
        [ 0.0112,  1.7422, -0.8672, -1.0391, -0.6133],
        [ 0.5547,  0.9961, -0.7266, -0.7617,  0.0991],
        [ 0.2695,  1.6797, -0.7539, -0.9844, -1.0625],
        [-0.0708,  1.0781, -0.7188, -0.6992, -0.5625],
        [ 0.1211,  1.6797, -1.0234, -0.6992, -0.2412],
        [ 0.3320,  0.9609, -0.9453, -1.0859, -0.3223],
        [ 0.0747,  1.2266, -0.9180, -0.8008, -0.5898],
        [ 0.4590,  1.3828, -1.0391, -0.7070, -0.5977],
        [ 0.2500,  1.7969, -0.8789, -0.7617, -0.5703],
        [ 0.0898,  1.6562, -0.6055, -0.7148, -0.5625]], device='cuda:0',
       dtype=torch.bfloat16, grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None), SequenceClassifierOutput(loss=tensor(0.6117, device='cuda:0', grad_fn=<NllLossBackward0>), logits=tensor([[ 0.1113,  1.2812, -0.6758, -0.9531, -0.6094],
        [-0.1426,  1.3594, -0.4727, -0.8086, -0.5586],
        [ 0.3223,  1.6172, -0.6133, -1.0547, -0.9180],
        [ 0.3789,  1.1719, -0.6992, -0.4688, -0.2930],
        [ 0.0386,  1.9141, -0.5195, -0.7656, -0.4141],
        [ 0.2656,  1.7891, -0.5078, -0.5312, -0.6992],
        [-0.2617,  1.6250, -0.8086, -0.5703, -0.6953],
        [-0.0138,  1.2266, -0.8320, -0.7930, -0.4551],
        [ 0.0625,  1.8906, -0.6992, -1.0703, -0.3555],
        [ 0.1211,  1.5625, -0.2031, -1.0391, -0.4375],
        [ 0.2500,  1.5312, -0.9219, -0.9844, -0.3027],
        [-0.1226,  1.5234, -0.7734, -1.0938, -0.4199],
        [ 0.2119,  1.4141, -0.8320, -0.8906, -0.7891],
        [ 0.0206,  1.3047, -0.6250, -0.9531, -0.9453],
        [ 0.1533,  1.5625, -0.6484, -0.6992, -0.7578],
        [ 0.1260,  1.7578, -1.0469, -0.9375, -0.6016]], device='cuda:0',
       dtype=torch.bfloat16, grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None), SequenceClassifierOutput(loss=tensor(0.5039, device='cuda:0', grad_fn=<NllLossBackward0>), logits=tensor([[-0.1582,  1.7031, -0.7148, -1.1953, -0.6328],
        [ 0.2002,  1.3750, -0.5508, -1.0938, -1.1875],
        [-0.2197,  1.5625, -0.9766, -0.8906, -0.6016],
        [ 0.5508,  1.4062, -0.4238, -0.8438, -0.4297],
        [ 0.1611,  1.5625, -0.9336, -0.7227, -0.4141],
        [-0.0083,  2.0469, -0.3848, -0.8203, -0.6172],
        [ 0.1050,  1.7344, -0.6953, -1.0078, -0.5156],
        [ 0.3555,  1.3828, -0.7109, -0.8164, -0.5078],
        [ 0.1680,  1.3359, -0.8828, -0.9766, -0.4980],
        [ 0.1963,  1.5859, -0.9922, -0.8242, -0.5625],
        [ 0.0251,  1.4609, -0.5508, -0.8867, -0.5781],
        [ 0.2129,  1.7812, -0.6680, -1.0078, -0.8828],
        [ 0.0576,  1.4062, -0.5938, -0.6875, -0.6602],
        [ 0.2109,  1.5938, -0.4688, -1.0469, -0.4902],
        [ 0.0693,  1.7344, -0.3418, -1.0547, -0.7422],
        [ 0.2656,  1.4688, -0.6875, -1.3594, -0.8398]], device='cuda:0',
       dtype=torch.bfloat16, grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None), SequenceClassifierOutput(loss=tensor(0.5470, device='cuda:0', grad_fn=<NllLossBackward0>), logits=tensor([[-0.0693,  1.4297, -0.7383, -0.6133, -0.6367],
        [-0.0537,  1.7266, -0.9492, -0.8086, -0.6758],
        [ 0.0082,  1.7422, -0.7227, -0.9258, -0.7617],
        [ 0.0197,  1.3281, -0.4902, -0.7500, -0.7227],
        [-0.0859,  1.7734, -0.8516, -1.1797, -0.6562],
        [-0.1953,  1.8906, -0.7188, -1.2266, -0.4160],
        [-0.0042,  1.9609, -0.6953, -1.1484, -0.5547],
        [ 0.1621,  1.2031, -0.5781, -1.2031, -0.4414],
        [-0.0493,  1.3516, -0.6445, -0.6289, -0.3086],
        [ 0.1924,  1.6562, -0.4824, -1.1484, -0.7773],
        [ 0.2070,  1.1641, -0.4570, -0.8477, -0.9883],
        [-0.2070,  1.6172, -0.5820, -1.0938, -0.1934],
        [ 0.5703,  0.8633, -1.0078, -1.0156,  0.3789],
        [-0.0923,  1.5781, -0.6094, -0.7617, -0.7578],
        [-0.4004,  1.7578, -0.6719, -0.8945, -0.3535],
        [-0.0742,  1.3750, -0.5508, -0.8633, -0.7109]], device='cuda:0',
       dtype=torch.bfloat16, grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None), SequenceClassifierOutput(loss=tensor(0.4867, device='cuda:0', grad_fn=<NllLossBackward0>), logits=tensor([[-0.0708,  1.8438, -1.0703, -0.8281, -0.5156],
        [-0.2070,  1.3359, -0.4082, -0.8555, -0.3867],
        [ 0.1963,  1.5938, -0.8984, -0.8711, -0.3965],
        [ 0.5352,  1.6016, -1.1641, -0.9961, -0.4180],
        [-0.1572,  2.1719, -0.4219, -0.7930, -0.5898],
        [ 0.2559,  1.4141, -0.8086, -0.9570, -0.4844],
        [ 0.2656,  1.6797, -0.6406, -0.9570, -0.7773],
        [ 0.3145,  1.6953, -0.6562, -1.0625, -0.4961],
        [-0.0767,  1.3906, -0.7305, -0.9766, -0.6992],
        [-0.1016,  1.5859, -0.9805, -0.8242, -0.7930],
        [-0.3262,  1.4531, -0.7812, -0.7109, -0.4414],
        [ 0.1367,  1.5547, -0.9023, -0.9258, -0.0620],
        [-0.1426,  1.7812, -0.6016, -1.0234, -0.4043],
        [ 0.1660,  1.5156, -0.8672, -0.9648, -0.4766],
        [ 0.1650,  1.4375, -0.7734, -1.0156, -0.4355],
        [-0.1494,  1.7188, -0.7539, -0.7109, -0.3613]], device='cuda:0',
       dtype=torch.bfloat16, grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None), SequenceClassifierOutput(loss=tensor(0.7820, device='cuda:0', grad_fn=<NllLossBackward0>), logits=tensor([[ 0.0737,  1.7422, -0.5430, -0.8867, -0.6680],
        [ 0.2617,  1.6094, -0.8086, -0.5312, -0.6602],
        [ 0.1318,  1.5859, -0.3730, -1.2891, -0.4824],
        [-0.0991,  1.5703, -0.2246, -0.9102, -1.1562],
        [ 0.4629,  0.8438, -0.8750, -0.5430,  0.1445],
        [ 0.3301,  1.3750, -0.7578, -0.8320, -0.6445],
        [ 0.1582,  1.2891, -1.1484, -0.8867, -0.4629],
        [ 0.1807,  1.5469, -0.6484, -0.7109, -0.5508],
        [ 0.4434,  1.7344, -0.8398, -0.8633, -0.4531],
        [ 0.1050,  1.4922, -0.7422, -1.0625, -0.4883],
        [-0.0723,  1.6562, -0.7461, -0.9961, -0.6523],
        [ 0.2949,  1.7734, -0.5586, -0.7383, -0.5039],
        [-0.0427,  1.3906, -0.9648, -0.8750, -0.5859],
        [ 0.4961,  1.8906, -0.7422, -1.0312, -0.7070],
        [ 0.0177,  1.1562, -0.6055, -0.3027, -0.8203],
        [ 0.0098,  1.3359, -0.6172, -0.9492, -0.4922]], device='cuda:0',
       dtype=torch.bfloat16, grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None), SequenceClassifierOutput(loss=tensor(0.6003, device='cuda:0', grad_fn=<NllLossBackward0>), logits=tensor([[ 0.2129,  1.8828, -0.9766, -1.0547, -0.5938],
        [-0.2090,  1.7656, -0.8008, -0.9648, -0.6289],
        [ 0.3184,  1.7422, -0.8477, -0.4785, -0.7070],
        [-0.0562,  1.4531, -0.9375, -0.8242, -0.7344],
        [ 0.1157,  1.5391, -0.9492, -0.6406, -0.3867],
        [ 0.0674,  1.7109, -0.8711, -0.6641, -0.5430],
        [ 0.1157,  1.6094, -0.4648, -0.9180, -0.5430],
        [ 0.1689,  1.4922, -0.9609, -0.9922, -0.5742],
        [ 0.0287,  1.1797, -0.8906, -0.7109, -0.3008],
        [ 0.3066,  1.5156, -0.7891, -1.1016, -0.2402],
        [ 0.3398,  1.4688, -0.9219, -0.9102, -0.3789],
        [ 0.1533,  1.8047, -0.6758, -0.8945, -0.5117],
        [ 0.0306,  1.2500, -0.6914, -0.7578, -0.2793],
        [-0.1348,  1.5000, -0.8008, -0.6484, -0.4121],
        [ 0.0208,  1.4297, -0.6367, -0.9961, -0.7422],
        [ 0.4277,  1.9453, -0.4062, -0.9297, -0.5117]], device='cuda:0',
       dtype=torch.bfloat16, grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None), SequenceClassifierOutput(loss=tensor(0.8821, device='cuda:0', grad_fn=<NllLossBackward0>), logits=tensor([[-0.1836,  1.5469, -0.5078, -0.9023, -0.8867],
        [ 0.1885,  1.8750, -0.5664, -0.8125, -0.6914],
        [ 0.3828,  1.1484, -0.4824, -0.4434, -0.4980],
        [ 0.1157,  1.3906, -1.0078, -1.2969, -0.2109],
        [ 0.1060,  1.5000, -0.6367, -0.8906, -0.4297],
        [-0.0157,  1.5000, -0.5938, -0.9336, -0.6445],
        [ 0.2500,  1.4141, -0.7188, -1.0938, -0.5859],
        [-0.0574,  1.4297, -0.8906, -1.0312, -0.7031],
        [-0.2051,  1.5703, -0.4375, -0.8906, -1.1016],
        [ 0.0234,  1.1484, -0.6992, -0.6172, -0.6484],
        [ 0.1494,  1.6328, -0.8789, -0.8867, -0.3105],
        [ 0.1270,  1.3594, -0.6445, -0.9922, -0.6250],
        [-0.1953,  1.5469, -0.9258, -1.0938, -0.6602],
        [ 0.2871,  0.9336, -0.6875, -0.6133, -0.3457],
        [-0.1934,  1.7109, -0.8008, -0.6914, -0.4395],
        [ 0.1982,  1.6328, -0.7734, -0.9180, -0.7500]], device='cuda:0',
       dtype=torch.bfloat16, grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None), SequenceClassifierOutput(loss=tensor(0.7783, device='cuda:0', grad_fn=<NllLossBackward0>), logits=tensor([[ 0.1758,  1.9531, -0.8672, -0.7969, -0.5586],
        [-0.0757,  1.7188, -0.9609, -1.0547, -0.5039],
        [-0.1260,  1.6484, -0.4375, -0.7148, -0.1523],
        [ 0.2129,  1.8828, -0.7070, -0.8711, -0.5664],
        [-0.2871,  1.8281, -0.5703, -1.0391, -0.2334],
        [-0.1396,  1.6094, -0.8164, -1.0703, -0.7656],
        [-0.0181,  1.6172, -0.7383, -1.1875, -0.5469],
        [-0.3496,  1.5859, -0.7070, -0.9648, -0.5625],
        [ 0.1904,  1.9219, -0.8555, -1.0234, -0.4688],
        [-0.2217,  1.7188, -0.7383, -1.0000, -0.5508],
        [ 0.0157,  1.6875, -0.7148, -1.1719, -0.8359],
        [ 0.2070,  1.3125, -0.9531, -0.7930, -0.5234],
        [ 0.2500,  1.3906, -0.2461, -0.7305, -0.5000],
        [ 0.5391,  1.6172, -0.7461, -0.8711, -0.7578],
        [-0.1924,  1.9297, -0.5625, -1.1641, -0.4746],
        [ 0.1221,  1.3828, -0.4883, -0.8047, -0.3945]], device='cuda:0',
       dtype=torch.bfloat16, grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None), SequenceClassifierOutput(loss=tensor(0.5941, device='cuda:0', grad_fn=<NllLossBackward0>), logits=tensor([[ 0.0928,  1.3906, -0.3984, -0.8242, -0.6680],
        [ 0.1094,  1.7969, -0.7617, -1.0078, -0.5078],
        [-0.2559,  1.8828, -0.8438, -1.0312, -0.5078],
        [-0.0437,  1.5312, -0.4766, -1.1328, -0.5820],
        [-0.2168,  1.5781, -0.5078, -0.8828, -0.8281],
        [ 0.2217,  0.9688, -1.0234, -0.7461, -0.1191],
        [ 0.0205,  1.7109, -0.9453, -0.8242, -0.3223],
        [-0.0654,  1.6484, -0.9492, -0.7031, -0.6445],
        [ 0.0923,  1.3203, -0.8398, -0.9609, -0.5820],
        [ 0.0625,  1.4609, -0.8906, -1.1406, -0.4473],
        [-0.0544,  1.4219, -0.5859, -0.9062, -0.6562],
        [ 0.2832,  1.2734, -0.7539, -0.7695, -0.3691],
        [-0.0376,  1.7422, -0.5000, -0.8047, -1.1016],
        [-0.0654,  2.0000, -0.7148, -0.8867, -0.9062],
        [-0.2236,  1.5078, -0.7734, -1.0547, -0.4512],
        [ 0.3906,  1.1484, -0.6758, -0.8398, -0.3535]], device='cuda:0',
       dtype=torch.bfloat16, grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None), SequenceClassifierOutput(loss=tensor(0.8186, device='cuda:0', grad_fn=<NllLossBackward0>), logits=tensor([[ 0.0261,  1.7344, -1.1250, -0.9102, -0.5586],
        [-0.1387,  1.6719, -0.6602, -1.0391, -0.5586],
        [ 0.0226,  1.5938, -0.7617, -1.0938, -0.4414],
        [-0.1494,  1.5859, -0.2451, -0.8945, -0.3164],
        [-0.0344,  1.6250, -0.6523, -1.0938, -0.7188],
        [ 0.1245,  1.7812, -0.8750, -0.7070, -0.4727],
        [ 0.0427,  1.9297, -0.9922, -0.7227, -0.7695],
        [-0.2891,  1.4688, -0.3691, -0.7383, -0.5156],
        [ 0.2383,  1.8906, -0.6758, -0.9844, -0.3535],
        [ 0.0208,  1.5391, -0.7969, -1.0156, -0.5391],
        [ 0.1963,  1.3984, -0.7852, -0.9727, -0.6758],
        [ 0.0825,  1.4141, -0.9648, -0.7891, -0.2852],
        [ 0.0049,  1.3438, -0.4824, -0.5312, -0.4609],
        [ 0.2158,  1.1406, -1.0781, -0.9336, -0.4121],
        [-0.0265,  1.7188, -0.8945, -1.0625, -0.4082],
        [ 0.0159,  1.0156, -1.0312, -0.6328, -0.8945]], device='cuda:0',
       dtype=torch.bfloat16, grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None), SequenceClassifierOutput(loss=tensor(0.7690, device='cuda:0', grad_fn=<NllLossBackward0>), logits=tensor([[-0.1572,  1.7109, -0.4785, -0.7930, -0.5938],
        [ 0.1719,  1.3359, -0.8750, -0.9219, -0.2061],
        [ 0.1816,  1.3672, -0.5078, -1.0938, -0.1924],
        [-0.0593,  1.5469, -0.5820, -1.0234, -0.4805],
        [ 0.1562,  1.4141, -0.6133, -0.6836, -0.5859],
        [ 0.1099,  1.4922, -0.9844, -0.8281, -0.7617],
        [ 0.3301,  1.6328, -0.7461, -1.1328, -0.6992],
        [ 0.2158,  1.5078, -0.9492, -0.9023, -0.2891],
        [ 0.1123,  1.5703, -0.8281, -0.6641, -0.5586],
        [ 0.2930,  1.7344, -0.7461, -0.9492, -0.6133],
        [ 0.1758,  1.5781, -1.1562, -1.0234, -0.3398],
        [ 0.2344,  1.3750, -0.7031, -0.9766, -0.6562],
        [-0.0378,  1.1172, -0.3457, -0.8477, -0.7305],
        [ 0.2637,  1.5312, -0.5469, -0.8125, -0.5898],
        [ 0.3457,  1.1406, -1.3438, -1.2656,  0.2070],
        [-0.0728,  1.7578, -0.4688, -0.6367, -0.5625]], device='cuda:0',
       dtype=torch.bfloat16, grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None), SequenceClassifierOutput(loss=tensor(0.5424, device='cuda:0', grad_fn=<NllLossBackward0>), logits=tensor([[ 2.6367e-01,  2.0312e+00, -8.6719e-01, -9.6875e-01, -4.6484e-01],
        [-3.1250e-01,  1.5234e+00, -7.8516e-01, -9.6875e-01, -5.0000e-01],
        [ 7.0312e-02,  1.6797e+00, -5.5078e-01, -1.1250e+00, -9.0625e-01],
        [ 9.1797e-02,  1.7891e+00, -6.0547e-01, -9.8828e-01, -3.0664e-01],
        [ 5.5078e-01,  6.6016e-01, -9.2578e-01, -3.3008e-01,  9.3262e-02],
        [ 2.0630e-02,  1.3594e+00, -7.4219e-01, -8.9844e-01, -4.9805e-01],
        [ 6.6895e-02,  1.6875e+00, -8.0469e-01, -7.8906e-01, -4.9609e-01],
        [-2.7100e-02,  1.6172e+00, -5.9766e-01, -9.9219e-01, -4.6875e-01],
        [ 1.0742e-01,  1.5156e+00, -6.0938e-01, -7.6562e-01, -6.7969e-01],
        [-2.2363e-01,  1.4141e+00, -8.2031e-01, -7.0312e-01, -5.6250e-01],
        [-1.2283e-03,  1.4141e+00, -6.6797e-01, -9.0234e-01, -8.9453e-01],
        [ 2.2266e-01,  1.8203e+00, -4.8242e-01, -5.4688e-01, -6.0547e-01],
        [ 8.3496e-02,  1.5781e+00, -6.4844e-01, -9.6094e-01, -7.0703e-01],
        [ 8.3984e-02,  1.7969e+00, -6.6016e-01, -8.9062e-01, -4.5312e-01],
        [ 2.1387e-01,  1.5625e+00, -1.0156e+00, -9.0234e-01, -3.8672e-01],
        [-4.3640e-03,  1.6250e+00, -7.5000e-01, -9.7656e-01, -4.0820e-01]],
       device='cuda:0', dtype=torch.bfloat16, grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None), SequenceClassifierOutput(loss=tensor(0.7577, device='cuda:0', grad_fn=<NllLossBackward0>), logits=tensor([[-0.1465,  1.7812, -0.7812, -0.8945, -0.5430],
        [ 0.1846,  1.7578, -0.7930, -1.1406, -0.7070],
        [ 0.1924,  1.6406, -0.8164, -0.9727, -0.7617],
        [-0.0576,  1.3281, -0.5742, -0.9375, -0.8906],
        [ 0.2393,  1.7734, -1.1250, -0.6602, -0.6289],
        [ 0.0452,  1.6641, -0.6133, -0.3262, -0.5508],
        [ 0.3320,  1.4609, -0.7539, -1.0000, -0.5781],
        [-0.0062,  1.5234, -0.6367, -0.7344, -1.0312],
        [ 0.3027,  1.5547, -0.9648, -0.7148, -0.5156],
        [ 0.1211,  1.3516, -0.8086, -1.1250, -0.4512],
        [ 0.3359,  1.6094, -0.8242, -0.5430, -0.4844],
        [ 0.2715,  1.6172, -0.2812, -1.0703, -0.4492],
        [ 0.3770,  1.7422, -0.5430, -0.9844, -0.6094],
        [ 0.2637,  2.0469, -0.7422, -0.8594, -0.8594],
        [ 0.2275,  1.7031, -0.7695, -0.6719, -0.5625],
        [ 0.1211,  1.7188, -1.0547, -0.8164, -0.4688]], device='cuda:0',
       dtype=torch.bfloat16, grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None), SequenceClassifierOutput(loss=tensor(1.0374, device='cuda:0', grad_fn=<NllLossBackward0>), logits=tensor([[ 0.2871,  1.2344, -0.9062, -1.0469, -0.5039],
        [-0.1187,  1.1641, -0.7266, -0.9609, -0.7852],
        [ 0.4844,  1.4609, -1.0391, -0.8984, -0.1279],
        [ 0.1025,  1.6094, -0.2256, -0.5000, -0.6445],
        [ 0.2119,  1.8828, -0.8633, -0.5273, -0.5977],
        [ 0.3223,  1.3750, -0.6875, -1.3203, -0.8203],
        [-0.2168,  1.4531, -0.4746, -0.6875, -0.6680],
        [ 0.1680,  1.4219, -0.8164, -1.1328, -0.5703],
        [ 0.3105,  1.3594, -0.7578, -0.8281, -0.4473],
        [ 0.2461,  1.4766, -0.6289, -0.5625, -0.5781],
        [ 0.0559,  1.6719, -0.8008, -1.0234, -0.5469],
        [ 0.1953,  1.4922, -0.9727, -0.9297, -0.7070],
        [ 0.0413,  1.8516, -0.7344, -0.8555, -0.3438],
        [-0.0320,  1.3984, -0.7461, -0.8945, -0.6367],
        [ 0.1582,  1.8672, -0.7461, -1.0000, -0.2539],
        [-0.2197,  1.5078, -0.8242, -0.9180, -0.6602]], device='cuda:0',
       dtype=torch.bfloat16, grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None), SequenceClassifierOutput(loss=tensor(0.9205, device='cuda:0', grad_fn=<NllLossBackward0>), logits=tensor([[ 6.5918e-02,  1.9609e+00, -8.6328e-01, -8.9453e-01, -3.9258e-01],
        [ 2.1777e-01,  1.2969e+00, -9.8828e-01, -6.0156e-01, -2.6172e-01],
        [ 1.9824e-01,  1.7109e+00, -9.3750e-01, -1.0391e+00, -8.4766e-01],
        [ 5.4932e-04,  1.7344e+00, -8.4766e-01, -9.1406e-01, -5.5078e-01],
        [-5.3223e-02,  1.5391e+00, -7.4219e-01, -8.6719e-01, -3.0859e-01],
        [-1.0693e-01,  1.7656e+00, -4.5703e-01, -1.1328e+00, -6.4062e-01],
        [ 1.7480e-01,  1.6016e+00, -8.6328e-01, -8.9062e-01, -6.7188e-01],
        [ 1.3489e-02,  1.1875e+00, -7.5391e-01, -9.6484e-01, -7.6172e-01],
        [-2.8320e-01,  1.1172e+00, -3.2422e-01, -7.5000e-01, -5.7031e-01],
        [ 2.3535e-01,  1.7266e+00, -7.4219e-01, -1.3750e+00, -5.4688e-01],
        [ 9.6680e-02,  1.9609e+00, -6.9141e-01, -1.1797e+00, -7.8516e-01],
        [ 1.5234e-01,  1.6953e+00, -6.9824e-02, -6.6797e-01, -8.7891e-01],
        [ 3.7109e-01,  1.6406e+00, -6.6797e-01, -1.0000e+00, -7.8516e-01],
        [ 6.7871e-02,  1.4141e+00, -5.8984e-01, -1.1250e+00, -7.5684e-02],
        [ 2.0508e-01,  1.5781e+00, -8.4766e-01, -9.1406e-01, -6.9531e-01],
        [-8.6426e-02,  1.6172e+00, -7.6172e-01, -6.9531e-01, -5.8203e-01]],
       device='cuda:0', dtype=torch.bfloat16, grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None), SequenceClassifierOutput(loss=tensor(0.6134, device='cuda:0', grad_fn=<NllLossBackward0>), logits=tensor([[-0.0991,  1.7188, -0.9375, -0.8008, -0.5000],
        [ 0.1245,  1.2188, -0.3906, -0.8281, -0.9062],
        [ 0.1455,  1.5078, -0.7773, -0.8555, -0.6211],
        [ 0.2109,  1.5000, -0.5078, -0.6758, -0.5117],
        [ 0.0410,  1.5078, -0.7227, -0.9648, -0.6641],
        [-0.1748,  1.5312, -0.6094, -0.9609, -0.5352],
        [ 0.3145,  1.3984, -0.8594, -1.0547, -0.1338],
        [ 0.2852,  1.3984, -1.1875, -1.0156, -0.5156],
        [-0.1338,  1.7109, -0.2793, -0.9492, -0.6367],
        [ 0.0623,  1.4375, -0.8398, -0.8242, -0.7578],
        [ 0.1094,  1.4531, -0.7344, -0.9961, -0.5039],
        [-0.1216,  1.3203, -0.6680, -0.6797, -0.7148],
        [ 0.1904,  1.5469, -0.8281, -1.2734, -0.6094],
        [-0.2578,  1.5234, -0.6523, -0.8906, -0.5859],
        [-0.2949,  1.2031, -0.4160, -0.8203, -0.7500],
        [ 0.3555,  1.3672, -1.1172, -1.0625, -0.4160]], device='cuda:0',
       dtype=torch.bfloat16, grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None), SequenceClassifierOutput(loss=tensor(0.8149, device='cuda:0', grad_fn=<NllLossBackward0>), logits=tensor([[ 0.0425,  1.4531, -0.9961, -1.0625, -0.5586],
        [ 0.1299,  1.6094, -0.8672, -0.7656, -0.6641],
        [ 0.2617,  1.3750, -0.8438, -0.6562, -0.6094],
        [ 0.1118,  1.5625, -0.3086, -0.8438, -0.4180],
        [ 0.1079,  1.4844, -0.7109, -1.0469, -0.7266],
        [ 0.2871,  1.7500, -0.8164, -0.9102, -0.7852],
        [ 0.2002,  1.5391, -0.7148, -0.8711, -0.7734],
        [ 0.2598,  1.5469, -0.3789, -0.7578, -0.5586],
        [ 0.1177,  1.5312, -0.6406, -0.5469, -0.4980],
        [ 0.0908,  1.5703, -0.7969, -1.1875, -0.5859],
        [-0.0820,  1.6562, -0.8594, -0.7891, -0.6875],
        [ 0.3066,  1.2031, -0.8320, -0.8633, -0.6211],
        [-0.1465,  1.2656, -0.4824, -0.5195, -0.8750],
        [ 0.4297,  1.7031, -0.9141, -0.9453, -0.7383],
        [ 0.0019,  1.6484, -0.4727, -0.9375, -0.3867],
        [ 0.1670,  1.8594, -1.1172, -1.0547, -0.7812]], device='cuda:0',
       dtype=torch.bfloat16, grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None), SequenceClassifierOutput(loss=tensor(0.7124, device='cuda:0', grad_fn=<NllLossBackward0>), logits=tensor([[ 0.1650,  1.7109, -0.9414, -0.7695, -0.5938],
        [-0.1436,  1.5859, -0.7188, -1.1484, -0.7617],
        [ 0.0894,  1.2578, -0.6914, -0.8789, -0.5742],
        [ 0.0139,  1.6484, -0.8008, -0.8945, -0.4922],
        [ 0.4492,  1.6328, -0.7070, -0.8164, -0.4609],
        [-0.0825,  1.5000, -0.7109, -0.8438, -0.4668],
        [ 0.0079,  1.5078, -0.8438, -0.9062, -0.5156],
        [ 0.2852,  1.2891, -1.1875, -1.0938, -0.4160],
        [ 0.0854,  1.6172, -0.3301, -0.5508, -0.6719],
        [ 0.0693,  1.7031, -0.8320, -1.1016, -0.5703],
        [ 0.1084,  1.4062, -0.9570, -1.1406, -0.5664],
        [-0.3105,  1.6172, -0.8047, -0.7461, -0.4766],
        [ 0.0713,  1.7500, -0.6953, -1.0312, -0.7188],
        [ 0.2734,  1.5391, -0.9883, -1.1094, -0.2910],
        [ 0.2715,  1.5312, -0.5781, -0.9883, -0.4395],
        [ 0.2002,  1.7656, -0.6289, -0.5625, -0.5664]], device='cuda:0',
       dtype=torch.bfloat16, grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None), SequenceClassifierOutput(loss=tensor(0.6964, device='cuda:0', grad_fn=<NllLossBackward0>), logits=tensor([[ 0.0452,  1.6406, -0.4316, -0.9219, -0.4199],
        [ 0.1206,  1.4453, -1.0234, -0.9141, -0.4297],
        [-0.2676,  1.8281, -0.6641, -0.9727, -0.5938],
        [-0.0767,  1.9141, -0.8711, -0.7266, -0.5078],
        [ 0.4688,  1.2578, -0.6328, -0.9297, -0.5977],
        [ 0.2969,  1.7969, -1.0859, -0.7734, -0.4121],
        [-0.0840,  1.3906, -0.4023, -0.6445, -0.7891],
        [ 0.0728,  1.7188, -0.6875, -0.9023, -0.6641],
        [ 0.2168,  1.8438, -0.7188, -1.0469, -0.6602],
        [ 0.0354,  1.3203, -0.5430, -1.0312, -0.6836],
        [-0.0309,  1.7812, -0.7734, -0.7773, -0.6641],
        [ 0.0286,  1.8281, -0.5625, -0.9922, -0.7461],
        [ 0.0540,  1.2812, -0.7031, -1.1016, -1.0547],
        [-0.1367,  1.1641, -0.7891, -0.4082, -0.4746],
        [-0.1729,  1.7344, -0.7969, -0.7109, -0.5625],
        [-0.0476,  1.7812, -0.9141, -0.8789, -0.5234]], device='cuda:0',
       dtype=torch.bfloat16, grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None), SequenceClassifierOutput(loss=tensor(0.8335, device='cuda:0', grad_fn=<NllLossBackward0>), logits=tensor([[ 1.4746e-01,  1.0234e+00, -1.0078e+00, -8.9453e-01, -3.7109e-01],
        [ 1.4453e-01,  1.7500e+00, -4.3164e-01, -9.2969e-01, -3.2617e-01],
        [ 3.5547e-01,  1.6484e+00, -9.6094e-01, -9.5703e-01, -6.4453e-01],
        [-2.6758e-01,  1.5781e+00, -6.6797e-01, -1.1172e+00, -5.7031e-01],
        [ 7.3242e-02,  1.4297e+00, -5.3906e-01, -1.1250e+00, -5.2344e-01],
        [ 1.7285e-01,  1.5625e+00, -9.0234e-01, -4.5312e-01, -3.3984e-01],
        [ 2.3633e-01,  1.4375e+00, -8.6719e-01, -8.9453e-01, -8.5938e-01],
        [ 3.3984e-01,  1.5312e+00, -9.5312e-01, -9.7656e-01, -3.5938e-01],
        [ 2.1582e-01,  1.5781e+00, -5.6250e-01, -1.2812e+00, -6.0547e-01],
        [ 6.3477e-02,  1.4844e+00, -7.0703e-01, -3.4766e-01, -4.5508e-01],
        [-1.2061e-01,  1.8281e+00, -9.1016e-01, -1.0703e+00, -6.1719e-01],
        [ 3.2715e-02,  1.7109e+00, -2.5977e-01, -7.0312e-01, -3.1250e-01],
        [-2.0410e-01,  1.6406e+00, -7.2656e-01, -8.1250e-01, -7.0312e-01],
        [ 4.1260e-02,  1.3125e+00, -6.3281e-01, -1.0547e+00, -3.8086e-01],
        [-1.6251e-03,  1.5391e+00, -5.8984e-01, -9.2969e-01, -6.6016e-01],
        [ 1.4844e-01,  1.5859e+00, -3.9648e-01, -9.4141e-01, -8.9844e-01]],
       device='cuda:0', dtype=torch.bfloat16, grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None), SequenceClassifierOutput(loss=tensor(0.8146, device='cuda:0', grad_fn=<NllLossBackward0>), logits=tensor([[ 0.6797,  0.7109, -0.6562, -0.5664,  0.1079],
        [ 0.1719,  1.3906, -0.4570, -0.9180, -0.4121],
        [-0.0742,  1.6797, -0.9609, -0.9062, -0.5938],
        [ 0.0065,  1.4453, -0.8477, -0.8789, -0.8438],
        [ 0.0859,  1.9844, -0.9453, -0.7969, -0.4297],
        [ 0.0547,  1.1094, -0.4551, -0.9727, -0.7500],
        [ 0.0913,  1.8047, -0.8047, -0.7188, -0.5234],
        [ 0.3008,  1.7891, -0.5312, -0.8281, -0.5781],
        [ 0.0718,  1.6406, -0.7500, -1.1328, -0.6836],
        [ 0.5039,  0.7773, -1.1484, -0.6680,  0.3379],
        [ 0.0747,  1.4609, -0.9609, -0.8750, -0.4023],
        [ 0.3203,  1.3828, -0.7969, -0.8125, -0.4375],
        [ 0.2793,  1.2422, -0.8477, -0.9648, -0.4453],
        [-0.1338,  1.0469, -0.7773, -0.7695, -0.2715],
        [-0.1699,  1.6250, -0.5977, -0.8281, -0.4922],
        [ 0.3672,  1.3047, -0.7305, -0.6367, -0.3105]], device='cuda:0',
       dtype=torch.bfloat16, grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None), SequenceClassifierOutput(loss=tensor(0.8965, device='cuda:0', grad_fn=<NllLossBackward0>), logits=tensor([[ 0.0923,  1.4844, -0.6484, -0.8867, -0.2412],
        [ 0.2441,  1.2266, -1.1719, -0.8047, -0.2002],
        [ 0.2656,  1.6719, -0.9492, -1.2109, -0.8828],
        [-0.2080,  1.3516, -0.6094, -0.6289, -0.7812],
        [ 0.2070,  1.5469, -0.6797, -0.9688, -0.4512],
        [ 0.1768,  1.5000, -0.7266, -0.7891, -0.6211],
        [ 0.2490,  1.4922, -0.5234, -0.7539, -0.7930],
        [ 0.1021,  1.6719, -0.9258, -0.8633, -0.6094],
        [-0.0981,  1.4219, -0.5625, -0.8555, -0.5117],
        [-0.4102,  1.6484, -0.6992, -0.6953, -0.8867],
        [-0.1084,  1.7891, -0.4551, -0.9570, -0.6562],
        [ 0.1196,  1.2734, -0.7148, -0.7539, -0.5195],
        [-0.2500,  1.4453, -0.6680, -0.8164, -0.4609],
        [-0.0854,  1.9141, -0.3984, -0.9023, -0.3555],
        [ 0.2451,  1.7578, -0.8477, -0.7617, -0.7461],
        [ 0.0762,  1.3672, -0.8984, -1.0547, -0.3555]], device='cuda:0',
       dtype=torch.bfloat16, grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None), SequenceClassifierOutput(loss=tensor(0.6296, device='cuda:0', grad_fn=<NllLossBackward0>), logits=tensor([[-0.1670,  1.6016, -0.7383, -1.0625, -0.5391],
        [ 0.4297,  1.1719, -1.1172, -0.7500, -0.5859],
        [ 0.0186,  1.6328, -0.8867, -0.9453, -0.4590],
        [ 0.1099,  1.6719, -0.6289, -1.1250, -0.5664],
        [-0.1318,  1.8828, -0.7227, -1.1016, -0.4941],
        [ 0.3809,  1.3438, -0.5977, -0.8203, -0.5508],
        [ 0.3945,  1.8906, -0.8672, -1.0391, -0.2500],
        [ 0.1465,  1.2109, -0.0156, -0.8477, -0.0996],
        [ 0.2910,  1.7031, -0.5625, -0.7383, -0.4531],
        [ 0.2656,  1.4844, -0.7500, -1.0938, -0.4980],
        [ 0.1357,  1.9141, -0.9141, -0.9648, -0.7266],
        [ 0.1592,  1.6875, -0.5156, -0.6797, -0.4492],
        [-0.2080,  1.2109, -0.4824, -1.0078, -0.7891],
        [ 0.0080,  1.6797, -0.5938, -0.8633, -0.7812],
        [ 0.1484,  1.5234, -0.3848, -1.1328, -0.5117],
        [-0.2412,  1.3516, -0.6680, -0.7969, -0.6836]], device='cuda:0',
       dtype=torch.bfloat16, grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None), SequenceClassifierOutput(loss=tensor(0.6913, device='cuda:0', grad_fn=<NllLossBackward0>), logits=tensor([[ 0.0898,  1.5312, -0.9453, -0.8086, -0.4883],
        [ 0.0322,  1.6328, -0.4961, -0.7578, -0.5508],
        [ 0.1245,  1.5000, -0.3398, -0.9844, -0.4902],
        [-0.1611,  1.7188, -0.6836, -0.9102, -0.8477],
        [-0.2598,  1.4531, -0.8047, -1.1250, -0.5977],
        [-0.0449,  1.7422, -0.1689, -0.9297, -0.9883],
        [ 0.2002,  1.2500, -0.7266, -1.0312, -0.2969],
        [ 0.2480,  1.3516, -0.7344, -0.7070, -0.5195],
        [ 0.1309,  1.2656, -0.4590, -1.2969, -0.6602],
        [ 0.0364,  1.4062, -0.5039, -0.7773, -0.5859],
        [ 0.1943,  1.6641, -0.4863, -0.5117, -0.4707],
        [ 0.2051,  1.5859, -0.8359, -0.5625, -0.4531],
        [-0.2295,  1.6562, -0.6484, -1.1641, -0.5078],
        [-0.0679,  1.9453, -0.8438, -1.0156, -0.6406],
        [ 0.1729,  1.6172, -0.6875, -1.0000, -0.7930],
        [ 0.0084,  1.1406, -1.1641, -0.5430, -0.5117]], device='cuda:0',
       dtype=torch.bfloat16, grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None), SequenceClassifierOutput(loss=tensor(0.5732, device='cuda:0', grad_fn=<NllLossBackward0>), logits=tensor([[ 0.4375,  1.3438, -0.6172, -0.9688, -0.8594],
        [-0.3027,  1.5938, -0.5273, -0.6953, -0.6133],
        [ 0.1357,  1.4688, -0.5664, -0.8203, -0.4219],
        [ 0.1338,  1.5391, -0.8203, -0.7070, -0.5586],
        [-0.1147,  1.5781, -0.7148, -0.9492, -0.8594],
        [-0.0042,  1.7344, -0.7617, -1.0312, -0.5977],
        [ 0.0762,  1.5156, -0.8164, -0.7930, -0.8398],
        [ 0.0405,  1.6406, -0.6211, -0.8008, -0.6133],
        [-0.2734,  1.5078, -0.8320, -0.4082, -0.7188],
        [ 0.1738,  1.7500, -0.8672, -0.8438, -0.3926],
        [ 0.0669,  1.4453, -0.8281, -0.7969, -0.5234],
        [-0.1338,  1.4844, -0.6523, -0.4219, -0.4688],
        [-0.0045,  1.3203, -0.7266, -0.7578, -0.4102],
        [-0.2188,  1.4609, -0.6914, -1.1250, -0.4922],
        [ 0.2480,  1.1875, -0.7305, -1.1562, -0.5117],
        [ 0.3457,  1.1953, -0.7500, -0.5430, -0.7500]], device='cuda:0',
       dtype=torch.bfloat16, grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None), SequenceClassifierOutput(loss=tensor(0.7096, device='cuda:0', grad_fn=<NllLossBackward0>), logits=tensor([[ 0.2207,  1.4688, -0.7227, -0.9805, -0.4355],
        [-0.0299,  1.4609, -0.9336, -0.9766, -0.6641],
        [-0.2051,  1.9219, -0.5977, -1.0625, -0.6562],
        [-0.1084,  1.4062, -0.6562, -0.5430, -0.5586],
        [ 0.3633,  1.6250, -1.0469, -1.0312, -0.7070],
        [ 0.0420,  1.7344, -0.7539, -0.9922, -0.3145],
        [-0.0986,  1.5391, -0.2227, -1.3203, -1.2500],
        [-0.3301,  1.7188, -0.7812, -0.9141, -0.4961],
        [ 0.4961,  1.5781, -1.0469, -0.6641, -0.6133],
        [ 0.2012,  1.7500, -0.9219, -0.7656, -0.4004],
        [ 0.2910,  0.9258, -0.8828, -0.9219, -0.4746],
        [ 0.2490,  1.4219, -0.8047, -0.8516, -0.7852],
        [ 0.1309,  1.7734, -0.6445, -0.7930, -0.5391],
        [ 0.0967,  1.5156, -0.6328, -0.8164, -0.4609],
        [-0.1875,  1.8906, -0.7969, -1.3281, -0.5898],
        [-0.3633,  1.3750, -0.7539, -0.6953, -0.6172]], device='cuda:0',
       dtype=torch.bfloat16, grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None), SequenceClassifierOutput(loss=tensor(0.5533, device='cuda:0', grad_fn=<NllLossBackward0>), logits=tensor([[-0.1504,  1.4766, -0.6914, -1.1172, -0.3613],
        [ 0.0869,  1.6406, -0.7930, -0.8828, -0.2793],
        [-0.0515,  1.3438, -0.7773, -0.8320, -0.4473],
        [ 0.3398,  1.2656, -1.0469, -1.1250, -0.7539],
        [-0.2178,  1.6094, -0.4277, -0.9609, -0.4473],
        [-0.2227,  1.4219, -0.4512, -1.0312, -1.1406],
        [ 0.1719,  1.3828, -0.8086, -0.8594, -0.6758],
        [ 0.2295,  1.3672, -0.6758, -1.1250, -0.6250],
        [ 0.1016,  1.8203, -0.3633, -0.7656, -0.3320],
        [ 0.1689,  1.6484, -0.9375, -0.8672, -0.7109],
        [-0.2305,  1.4453, -0.6719, -0.8672, -0.4473],
        [-0.0767,  1.7969, -0.2422, -0.9102, -0.8750],
        [ 0.3594,  1.6641, -0.7930, -0.9102, -0.4551],
        [ 0.3027,  1.3984, -0.7422, -0.7227, -0.3574],
        [ 0.0649,  1.7812, -0.6484, -1.1172, -0.6641],
        [ 0.8672,  0.6055, -1.0234, -0.2832,  0.2695]], device='cuda:0',
       dtype=torch.bfloat16, grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None), SequenceClassifierOutput(loss=tensor(0.6034, device='cuda:0', grad_fn=<NllLossBackward0>), logits=tensor([[ 3.8672e-01,  1.4766e+00, -1.1953e+00, -1.0547e+00, -2.0898e-01],
        [-3.6914e-01,  1.4922e+00, -5.1562e-01, -7.5000e-01, -4.1406e-01],
        [-5.2344e-01,  1.7500e+00, -6.0156e-01, -9.8438e-01, -8.5938e-01],
        [ 1.7969e-01,  1.9453e+00, -6.2109e-01, -1.1328e+00, -6.0938e-01],
        [ 5.2734e-01,  1.7266e+00, -8.7109e-01, -6.4844e-01, -5.7031e-01],
        [-6.7139e-04,  1.3594e+00, -1.0625e+00, -9.5312e-01, -7.4219e-01],
        [ 1.2012e-01,  1.6328e+00, -5.9375e-01, -8.0078e-01, -2.9883e-01],
        [ 8.0566e-02,  1.5000e+00, -6.4453e-01, -6.9922e-01, -5.3516e-01],
        [ 1.4941e-01,  1.2969e+00, -8.1641e-01, -9.3359e-01, -4.1992e-01],
        [ 4.0430e-01,  1.3984e+00, -8.5547e-01, -6.7188e-01, -3.4570e-01],
        [ 2.4805e-01,  1.5625e+00, -5.2734e-01, -1.0391e+00, -5.8203e-01],
        [-1.4062e-01,  1.4375e+00, -7.1484e-01, -5.5078e-01, -6.4453e-01],
        [ 7.8613e-02,  1.6875e+00, -8.0859e-01, -8.2031e-01, -6.5625e-01],
        [ 1.0840e-01,  1.5781e+00, -5.6641e-01, -1.0703e+00, -9.0625e-01],
        [ 1.1133e-01,  1.8359e+00, -8.5547e-01, -7.9297e-01, -6.0156e-01],
        [-4.1260e-02,  1.4141e+00, -8.3594e-01, -9.5312e-01, -5.0781e-01]],
       device='cuda:0', dtype=torch.bfloat16, grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None), SequenceClassifierOutput(loss=tensor(0.7029, device='cuda:0', grad_fn=<NllLossBackward0>), logits=tensor([[ 0.1670,  1.7031, -0.9219, -0.9648, -0.5430],
        [ 0.3379,  1.4922, -0.7031, -1.0000, -0.5156],
        [-0.1523,  1.8984, -0.7500, -0.8281, -0.4062],
        [ 0.0466,  1.4375, -0.4707, -1.1641, -0.7617],
        [ 0.0131,  1.2500, -1.0703, -1.0625, -0.3906],
        [-0.1045,  1.6484, -0.8008, -0.8555, -0.0574],
        [ 0.0452,  1.8203, -0.6289, -0.9414, -0.6094],
        [ 0.3164,  1.7266, -0.8047, -0.8008, -0.4492],
        [ 0.0737,  0.9688, -0.8555, -0.5156, -0.0776],
        [ 0.3789,  1.4844, -0.8086, -0.9648, -0.3594],
        [ 0.3809,  1.4922, -0.6680, -1.2812, -0.4082],
        [ 0.0069,  1.2969, -0.8047, -0.7852, -0.6914],
        [ 0.3301,  1.7812, -0.8750, -1.3594, -0.8203],
        [-0.1055,  1.6484, -0.5977, -1.1562, -0.8203],
        [-0.1475,  1.5156, -0.5820, -0.8438, -0.6562],
        [-0.0898,  1.6172, -0.8086, -0.8320, -0.4824]], device='cuda:0',
       dtype=torch.bfloat16, grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None), SequenceClassifierOutput(loss=tensor(0.8391, device='cuda:0', grad_fn=<NllLossBackward0>), logits=tensor([[ 0.1299,  1.3281, -0.6680, -0.9766, -0.6094],
        [ 0.3301,  1.5391, -0.8008, -1.0156, -0.6055],
        [ 0.0527,  1.5234, -0.9453, -1.0391, -0.6523],
        [ 0.2432,  1.7812, -0.6250, -0.8359, -0.5859],
        [ 0.1514,  1.7266, -0.9336, -0.7734, -0.4473],
        [-0.1699,  1.9297, -0.6680, -1.0312, -0.5859],
        [ 0.1074,  1.3906, -0.9805, -0.9023, -0.6484],
        [ 0.1309,  1.6406, -0.6484, -0.6211, -0.8594],
        [-0.2070,  1.8203, -0.6523, -1.0156, -0.6016],
        [-0.1650,  1.9141, -0.8125, -1.0234, -0.4941],
        [ 0.2988,  1.6328, -1.0234, -1.2891, -0.6836],
        [ 0.1934,  1.4297, -0.2988, -0.8203, -0.6055],
        [-0.0498,  1.4219, -0.6211, -1.0156, -0.5234],
        [ 0.2021,  1.4141, -0.8438, -0.7656, -0.6719],
        [ 0.0146,  0.8906, -0.4902, -0.7500, -0.1196],
        [ 0.0544,  1.8203, -0.8398, -0.8789, -0.6992]], device='cuda:0',
       dtype=torch.bfloat16, grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None), SequenceClassifierOutput(loss=tensor(0.9419, device='cuda:0', grad_fn=<NllLossBackward0>), logits=tensor([[-0.1177,  1.6328, -0.5859, -0.9180, -0.8477],
        [-0.1050,  1.5234, -0.5586, -0.9297, -0.7773],
        [-0.0283,  1.5000, -0.5273, -0.7773, -0.4941],
        [-0.2080,  1.5000, -0.8633, -0.9766, -0.2910],
        [-0.0703,  1.5469, -0.6641, -0.8945, -0.6328],
        [ 0.2207,  1.4453, -0.7461, -0.5938, -0.6484],
        [ 0.0096,  1.5625, -0.8242, -0.8789, -0.7031],
        [-0.2090,  1.4141, -0.4238, -1.2656, -0.5977],
        [-0.0025,  1.3516, -0.7500, -0.3867, -0.7891],
        [ 0.0153,  1.3828, -0.6250, -1.1719, -0.5312],
        [ 0.1187,  1.3984, -1.0781, -1.0156, -0.2266],
        [ 0.0172,  1.1797, -0.8750, -0.8555, -0.5430],
        [ 0.0039,  1.9062, -0.6250, -1.2266, -0.5820],
        [-0.0520,  1.6094, -0.6758, -0.6406, -0.5703],
        [ 0.2695,  1.5391, -0.9023, -0.9648, -0.3750],
        [-0.3262,  1.3359, -0.6680, -0.9609, -0.4023]], device='cuda:0',
       dtype=torch.bfloat16, grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None), SequenceClassifierOutput(loss=tensor(0.6703, device='cuda:0', grad_fn=<NllLossBackward0>), logits=tensor([[ 0.0732,  1.7656, -0.8672, -1.0156, -0.8750],
        [-0.4102,  1.7344, -0.5391, -0.5820, -0.6758],
        [-0.1611,  1.8281, -0.4805, -0.7070, -0.8594],
        [-0.0078,  1.6797, -0.8945, -0.9531, -0.5664],
        [ 0.0491,  1.7812, -0.5625, -0.9336, -0.3184],
        [ 0.2129,  1.2969, -0.4121, -0.8516, -0.1895],
        [-0.1396,  1.6406, -0.7344, -0.8203, -0.5859],
        [ 0.1338,  1.5078, -0.7344, -1.0469, -0.7148],
        [-0.1357,  1.3359, -0.3887, -1.2422, -0.7656],
        [ 0.2393,  1.6484, -0.5977, -0.9336, -0.8320],
        [ 0.1904,  1.2656, -0.5078, -0.8984, -0.8242],
        [ 0.3203,  1.6250, -0.5469, -0.9141, -0.8164],
        [ 0.1680,  1.6719, -1.1094, -0.5117, -0.3379],
        [ 0.1250,  1.8047, -1.2656, -0.7305, -0.5664],
        [-0.1982,  1.7031, -0.9023, -0.9961, -0.8516],
        [ 0.4043,  1.6562, -0.5430, -1.0156, -0.6445]], device='cuda:0',
       dtype=torch.bfloat16, grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None), SequenceClassifierOutput(loss=tensor(0.6350, device='cuda:0', grad_fn=<NllLossBackward0>), logits=tensor([[-0.2754,  1.7734, -0.7930, -0.7539, -0.4609],
        [ 0.1484,  1.4375, -0.9180, -1.2656, -0.4883],
        [ 0.0309,  2.0000, -1.2734, -1.0000, -0.7500],
        [-0.0388,  1.3750, -0.8047, -1.1016, -0.6484],
        [-0.0608,  1.4844, -0.3320, -1.0547, -0.6836],
        [-0.2793,  1.3828, -0.6445, -0.4609, -0.6445],
        [ 0.2559,  1.2266, -1.4453, -1.1719, -0.2871],
        [ 0.2715,  1.6875, -0.9609, -0.8711, -0.2598],
        [-0.0586,  1.4453, -0.7461, -1.0703, -0.4473],
        [ 0.1367,  1.0234, -0.9766, -0.7266, -0.9805],
        [ 0.2715,  1.2266, -0.6680, -0.8164, -0.8008],
        [ 0.0278,  1.5938, -1.0078, -0.9922, -0.5430],
        [-0.1836,  1.6797, -0.7969, -0.9375, -0.4688],
        [-0.2754,  1.6797, -0.4434, -0.8945, -0.3828],
        [ 0.1289,  1.7891, -0.9883, -0.8984, -0.4590],
        [ 0.0143,  1.7500, -0.3691, -0.7188, -0.6016]], device='cuda:0',
       dtype=torch.bfloat16, grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None), SequenceClassifierOutput(loss=tensor(0.7330, device='cuda:0', grad_fn=<NllLossBackward0>), logits=tensor([[ 0.2139,  1.8438, -0.9727, -1.1172, -0.6250],
        [ 0.0552,  1.6406, -0.6797, -0.7148, -0.4922],
        [-0.0688,  1.8516, -0.8438, -0.8086, -0.6133],
        [-0.1484,  1.5078, -0.8008, -1.4219, -0.8594],
        [ 0.1050,  1.3594, -0.7266, -0.5586, -0.4766],
        [-0.0081,  1.4766, -0.6719, -0.8711, -0.3613],
        [-0.2217,  1.6875, -0.6523, -0.8555, -0.6914],
        [ 0.0413,  1.5000, -0.5273, -0.8516, -0.4824],
        [-0.1060,  1.6562, -0.6367, -1.0781, -0.6523],
        [ 0.0771,  1.5391, -0.8594, -1.2109, -0.8555],
        [ 0.0083,  1.3984, -0.5508, -0.7539, -0.6289],
        [ 0.0898,  1.6797, -0.9805, -0.7148, -0.5508],
        [-0.3086,  1.5859, -0.6562, -0.9883, -0.4316],
        [-0.0718,  1.4766, -0.9336, -1.2344, -0.6914],
        [ 0.3965,  1.5391, -0.8086, -0.9609, -0.6211],
        [ 0.0977,  1.8828, -0.8047, -0.9922, -0.5820]], device='cuda:0',
       dtype=torch.bfloat16, grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None), SequenceClassifierOutput(loss=tensor(0.9451, device='cuda:0', grad_fn=<NllLossBackward0>), logits=tensor([[ 0.1689,  0.8828, -1.0547, -0.6914, -0.6602],
        [-0.0986,  1.2969, -0.5039, -1.2422, -0.3887],
        [ 0.3184,  1.2969, -1.1406, -0.8320,  0.2080],
        [-0.0151,  1.7578, -0.8750, -0.8594, -0.8555],
        [ 0.2793,  1.6484, -1.0391, -1.2109, -0.4668],
        [ 0.0454,  1.6328, -0.3711, -1.0859, -0.5312],
        [-0.0718,  1.3906, -0.9570, -0.8242, -0.6641],
        [ 0.0352,  1.4766, -0.7539, -1.3047, -0.5078],
        [-0.0864,  1.3359, -0.8594, -0.7422, -0.4336],
        [ 0.0045,  1.8516, -0.4629, -0.8125, -0.5820],
        [ 0.2021,  1.5156, -0.5938, -0.8750, -0.2275],
        [ 0.1309,  1.7188, -0.8047, -1.0391, -0.9258],
        [ 0.0203,  1.1328, -1.2188, -0.9609, -0.3770],
        [ 0.1377,  1.6562, -0.9375, -1.0859, -0.5469],
        [ 0.3340,  1.6328, -1.0469, -1.0469, -0.4414],
        [ 0.0101,  1.8359, -0.8164, -0.7812, -0.5977]], device='cuda:0',
       dtype=torch.bfloat16, grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None), SequenceClassifierOutput(loss=tensor(0.6458, device='cuda:0', grad_fn=<NllLossBackward0>), logits=tensor([[-0.2354,  1.4922, -0.3145, -1.0625, -0.9570],
        [ 0.1748,  1.4141, -0.7891, -0.7734, -0.5586],
        [ 0.2168,  1.7266, -0.7617, -0.6953, -0.4961],
        [-0.1719,  1.4531, -0.7266, -0.5938, -0.4355],
        [ 0.2148,  1.2031, -0.8047, -0.7695, -0.2734],
        [-0.1279,  1.5234, -0.7383, -0.9883, -0.9453],
        [-0.2520,  1.7344, -1.0000, -0.7773, -0.5781],
        [ 0.2422,  1.6953, -0.6875, -1.0547, -0.4922],
        [ 0.1533,  1.2266, -0.8477, -0.8984, -0.5703],
        [-0.1289,  1.4453, -0.6445, -0.7656, -0.6172],
        [ 0.1689,  1.5859, -0.4668, -1.2031, -1.0625],
        [-0.0928,  1.4141, -0.1816, -1.0078, -0.5625],
        [-0.1797,  1.4219, -0.5273, -1.1016, -0.6641],
        [ 0.1113,  1.5859, -0.8008, -1.0703, -0.3008],
        [ 0.1855,  1.3359, -1.0156, -0.9688, -0.6797],
        [ 0.0623,  1.1719, -0.6953, -0.8125, -0.5391]], device='cuda:0',
       dtype=torch.bfloat16, grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None), SequenceClassifierOutput(loss=tensor(0.8789, device='cuda:0', grad_fn=<NllLossBackward0>), logits=tensor([[ 0.3828,  1.5234, -0.8867, -0.8672, -0.4023],
        [ 0.1030,  1.3047, -0.6953, -1.0938, -0.7812],
        [ 0.2002,  1.3281, -0.9023, -1.0469, -0.2158],
        [ 0.3223,  1.1172, -0.9492, -0.9062, -0.7852],
        [-0.1387,  1.5859, -0.7852, -0.7266, -0.3691],
        [ 0.1396,  1.3203, -0.7578, -0.6250, -0.7500],
        [-0.0894,  1.8438, -0.9180, -1.0469, -0.8672],
        [ 0.2793,  1.7266, -1.0859, -1.2031, -0.7344],
        [-0.5273,  1.4531, -0.7891, -0.9258, -0.5273],
        [-0.0859,  1.3984, -0.8086, -0.9258, -0.6445],
        [-0.0527,  1.6875, -0.6016, -0.9844, -0.7617],
        [ 0.1807,  0.9727, -0.9180, -0.8125, -0.6328],
        [-0.0082,  1.7500, -0.8438, -0.9531, -0.5547],
        [ 0.4531,  1.1406, -0.7188, -1.2344, -0.1699],
        [ 0.0366,  1.5391, -0.6836, -0.6523, -0.6680],
        [ 0.0474,  1.5859, -0.4727, -0.7227, -1.0078]], device='cuda:0',
       dtype=torch.bfloat16, grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None), SequenceClassifierOutput(loss=tensor(0.4795, device='cuda:0', grad_fn=<NllLossBackward0>), logits=tensor([[ 0.2891,  0.9375, -0.8477, -0.8320, -0.4336],
        [-0.2021,  1.5156, -0.7695, -0.9805, -0.6992],
        [ 0.2197,  1.1641, -0.6172, -0.9648, -0.7695],
        [ 0.2910,  1.2500, -0.4590, -0.8242, -0.5117],
        [ 0.1235,  1.6328, -0.6172, -0.6641, -0.7969],
        [ 0.0640,  1.7344, -0.6562, -0.7891, -0.5352],
        [-0.1133,  1.3828, -0.8281, -0.6406, -0.6406],
        [ 0.2324,  1.3750, -0.6055, -0.8359, -0.7188],
        [-0.0398,  1.6641, -0.8828, -1.0469, -0.4531],
        [-0.1357,  1.8906, -0.7852, -0.9180, -0.6133],
        [-0.0393,  1.5391, -0.8672, -1.0859, -0.6250],
        [-0.0967,  1.6797, -0.6523, -1.0859, -0.6562],
        [ 0.2402,  1.5391, -0.9844, -0.8750, -0.7383],
        [-0.2812,  1.7266, -0.7227, -1.1016, -0.4082],
        [-0.4062,  1.7812, -0.6914, -1.1953, -0.5391],
        [-0.1934,  1.5156, -0.8281, -1.0625, -0.5039]], device='cuda:0',
       dtype=torch.bfloat16, grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None), SequenceClassifierOutput(loss=tensor(0.5016, device='cuda:0', grad_fn=<NllLossBackward0>), logits=tensor([[-0.0437,  1.1562, -0.9453, -0.7109, -0.6406],
        [-0.0742,  1.6875, -0.8047, -1.1172, -0.5938],
        [ 0.0640,  1.4844, -0.8594, -0.8750, -0.7500],
        [ 0.0175,  1.5859, -0.8203, -0.9141, -0.5312],
        [ 0.1943,  1.8438, -0.6562, -0.8633, -0.5859],
        [-0.0532,  1.6250, -0.7148, -1.1172, -0.7422],
        [ 0.2109,  1.7656, -0.6406, -0.7109, -0.5820],
        [ 0.1758,  1.5312, -0.5977, -1.1172, -0.4004],
        [ 0.2461,  1.3594, -0.6562, -0.7695, -0.6094],
        [ 0.0742,  1.5078, -0.9570, -1.0625, -0.6797],
        [-0.1611,  1.4141, -0.5898, -0.7188, -0.4980],
        [-0.3477,  1.6016, -0.7617, -0.6797, -0.4668],
        [-0.1865,  1.5938, -0.8438, -1.0391, -0.3145],
        [ 0.2578,  1.6328, -0.7461, -0.8828, -0.6992],
        [ 0.1069,  1.9844, -0.6484, -0.9102, -0.9141],
        [-0.2480,  1.4219, -0.5000, -1.0469, -0.6641]], device='cuda:0',
       dtype=torch.bfloat16, grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None), SequenceClassifierOutput(loss=tensor(0.6318, device='cuda:0', grad_fn=<NllLossBackward0>), logits=tensor([[-0.2676,  1.7422, -0.7109, -1.2188, -0.6094],
        [ 0.1069,  1.5000, -0.7695, -0.8828, -0.5469],
        [ 0.2383,  1.1562, -0.7109, -0.5508, -0.3281],
        [ 0.1533,  1.6094, -0.8750, -0.8789, -0.6367],
        [-0.0200,  1.4922, -0.8828, -0.5117, -0.4941],
        [ 0.3516,  1.2578, -0.6484, -0.7812, -0.5234],
        [-0.1895,  1.5781, -0.7930, -1.0312, -0.2578],
        [-0.1064,  1.6797, -0.6953, -0.7539, -0.5547],
        [-0.3066,  1.7266, -0.4805, -0.8984, -0.7617],
        [ 0.0806,  2.0938, -0.8750, -0.9609, -0.3164],
        [ 0.0134,  1.8438, -0.6836, -0.9727, -0.3105],
        [-0.0427,  1.7500, -0.8438, -1.1562, -0.4707],
        [-0.1865,  1.3828, -0.3457, -1.0547, -0.6875],
        [-0.0605,  1.7031, -0.3594, -0.8320, -0.7656],
        [ 0.2041,  1.5859, -1.0078, -1.0078, -0.5938],
        [ 0.1895,  1.3594, -0.6289, -0.5391, -0.5586]], device='cuda:0',
       dtype=torch.bfloat16, grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None), SequenceClassifierOutput(loss=tensor(0.4711, device='cuda:0', grad_fn=<NllLossBackward0>), logits=tensor([[-0.0728,  1.5859, -0.7539, -0.9375, -0.6172],
        [ 0.2832,  1.0312, -1.1562, -0.9570, -0.1973],
        [ 0.0325,  1.6719, -0.6836, -0.8555, -0.7109],
        [ 0.1475,  1.6250, -0.4961, -0.9336, -0.6328],
        [-0.2832,  1.2109, -0.6641, -0.8086, -0.8398],
        [-0.0101,  1.3203, -0.5742, -0.7539, -0.5117],
        [-0.0962,  1.5859, -0.6328, -0.8477, -0.8633],
        [-0.1777,  1.6953, -1.0469, -0.7344, -0.4062],
        [-0.0064,  1.3906, -0.4707, -0.6445, -0.6055],
        [-0.0845,  1.6016, -0.8008, -1.0156, -0.9062],
        [ 0.2373,  1.9062, -0.8789, -0.9688, -0.4648],
        [-0.1865,  1.4062, -0.7773, -0.7891, -0.5352],
        [ 0.0145,  1.7656, -0.5273, -0.7930, -0.4609],
        [-0.0884,  1.6250, -0.7266, -0.7500, -0.4707],
        [-0.1445,  1.9297, -0.5312, -0.9883, -0.5195],
        [ 0.0444,  1.6719, -0.7812, -0.9023, -0.4395]], device='cuda:0',
       dtype=torch.bfloat16, grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None), SequenceClassifierOutput(loss=tensor(1.0397, device='cuda:0', grad_fn=<NllLossBackward0>), logits=tensor([[ 0.1348,  1.3594, -0.9453, -1.2188, -1.0703],
        [ 0.3535,  1.3672, -0.8047, -0.6133, -0.6484],
        [ 0.1299,  1.7500, -0.7344, -0.8320, -0.6797],
        [-0.1621,  1.9609, -0.6484, -0.6875, -0.5625],
        [ 0.1758,  2.0312, -0.5117, -0.7891, -0.2617],
        [-0.2910,  1.2422, -0.2715, -0.9023, -0.4102],
        [ 0.5508,  0.7734, -0.9180, -0.5039,  0.2656],
        [ 0.1709,  1.3984, -0.5117, -1.0156, -0.2988],
        [ 0.2637,  1.5312, -0.7539, -1.0000, -0.6094],
        [-0.0151,  1.4375, -0.8086, -1.0234, -0.2676],
        [ 0.0491,  2.0156, -0.6523, -0.9453, -0.9727],
        [ 0.2520,  1.4844, -0.8008, -0.7305, -0.6016],
        [-0.1157,  1.5234, -0.4883, -0.6289, -0.6211],
        [ 0.1523,  1.7969, -0.7070, -0.8125, -0.7617],
        [ 0.0435,  1.5234, -0.5625, -0.8789, -0.4590],
        [ 0.3555,  1.4141, -0.6289, -0.6562, -0.6094]], device='cuda:0',
       dtype=torch.bfloat16, grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None), SequenceClassifierOutput(loss=tensor(0.8712, device='cuda:0', grad_fn=<NllLossBackward0>), logits=tensor([[-0.0398,  1.4609, -0.7852, -1.1797, -0.4512],
        [ 0.3340,  1.6328, -0.6406, -1.1406, -0.5938],
        [ 0.2207,  1.3438, -0.8164, -0.7812, -0.5625],
        [-0.2676,  1.4297, -0.5547, -0.5859, -0.7969],
        [ 0.2090,  1.5078, -0.8398, -0.9375, -0.5195],
        [ 0.2012,  0.8828, -1.1094, -0.7422, -0.6562],
        [-0.1387,  1.1484, -0.5938, -0.6797, -0.4707],
        [ 0.3301,  1.5859, -0.5742, -1.0000, -0.7305],
        [ 0.0347,  1.9375, -0.9570, -0.8750, -0.5508],
        [ 0.4219,  1.2500, -0.8555, -0.8320, -0.4023],
        [ 0.0850,  1.7656, -0.8477, -0.9844, -0.5859],
        [ 0.0332,  1.4375, -0.8711, -1.0859, -0.7344],
        [-0.1465,  1.7188, -0.8828, -0.7422, -0.1953],
        [-0.0400,  1.8281, -0.7266, -1.0469, -0.7578],
        [ 0.3301,  1.7422, -0.8008, -0.7383, -0.6094],
        [ 0.2432,  1.9219, -0.7227, -1.1250, -0.5078]], device='cuda:0',
       dtype=torch.bfloat16, grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None), SequenceClassifierOutput(loss=tensor(0.9489, device='cuda:0', grad_fn=<NllLossBackward0>), logits=tensor([[ 0.0413,  1.4453, -0.7539, -0.7383, -0.1465],
        [-0.2129,  1.3984, -0.4785, -0.9609, -0.4336],
        [ 0.2080,  1.2109, -0.4824, -1.1172, -0.7031],
        [-0.1611,  1.4766, -0.8867, -1.0625, -0.4355],
        [ 0.3008,  1.3594, -0.6758, -1.0391, -0.5039],
        [ 0.0112,  1.4766, -0.7188, -1.0781, -0.4082],
        [ 0.2168,  0.7305, -0.9141, -0.6992, -0.0215],
        [ 0.3047,  1.7344, -0.8203, -1.1328, -0.6562],
        [-0.1074,  1.5547, -0.4941, -0.7422, -0.7695],
        [ 0.3340,  1.4141, -0.8867, -1.0078, -0.5195],
        [ 0.1768,  1.1719, -0.3691, -0.8945, -0.6758],
        [ 0.0815,  1.8984, -0.6992, -1.0078, -0.5547],
        [-0.1475,  1.4922, -0.8242, -0.6406, -0.1406],
        [ 0.3242,  1.8047, -0.6680, -1.2188, -0.6289],
        [ 0.0854,  1.5078, -0.9219, -1.0781, -0.3848],
        [ 0.2715,  1.9062, -0.7344, -0.9180, -0.6406]], device='cuda:0',
       dtype=torch.bfloat16, grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None), SequenceClassifierOutput(loss=tensor(0.7985, device='cuda:0', grad_fn=<NllLossBackward0>), logits=tensor([[ 0.5469,  1.3281, -0.4766, -0.8867, -0.1406],
        [-0.2344,  1.7578, -0.8516, -0.9258, -0.8516],
        [ 0.4121,  1.7578, -0.4434, -1.1484, -0.6953],
        [ 0.3770,  1.2969, -0.8984, -0.9023, -0.6328],
        [ 1.0547,  0.3691, -1.0781, -0.2002,  0.2432],
        [ 0.2910,  1.2422, -1.1875, -0.6797, -0.4336],
        [ 0.2471,  1.7812, -0.6836, -0.9141, -0.5352],
        [-0.0923,  1.3125, -0.3887, -0.9531, -0.5859],
        [ 0.1494,  1.6328, -0.2656, -1.0781, -0.9219],
        [-0.1592,  1.7344, -0.7539, -0.6250, -0.5547],
        [ 0.2695,  1.2969, -0.9180, -0.9219, -0.4062],
        [-0.1318,  1.6953, -0.8477, -0.9414, -0.3906],
        [ 0.3945,  1.6562, -0.9453, -0.9844, -0.5391],
        [-0.2295,  1.7891, -0.5039, -1.0000, -0.8555],
        [ 0.0767,  1.8906, -0.8906, -1.0547, -0.6133],
        [ 0.1484,  1.4062, -0.8906, -0.9766, -0.7109]], device='cuda:0',
       dtype=torch.bfloat16, grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None), SequenceClassifierOutput(loss=tensor(0.6768, device='cuda:0', grad_fn=<NllLossBackward0>), logits=tensor([[-0.2715,  1.2812, -0.3789, -0.8320, -0.5742],
        [ 0.0571,  1.7656, -0.7969, -0.9609, -0.4844],
        [ 0.0131,  1.4531, -0.5938, -0.9102, -0.5234],
        [-0.0747,  1.1172, -0.6797, -0.8711, -0.5625],
        [ 0.1631,  1.4609, -0.6602, -0.8438, -0.6133],
        [ 0.0116,  1.4219, -0.8672, -1.0000, -0.6289],
        [-0.0757,  1.4531, -0.7695, -0.7266, -0.5312],
        [-0.0532,  1.5000, -0.8789, -0.6523, -0.4570],
        [-0.3672,  1.8594, -0.5664, -1.1172, -0.7070],
        [ 0.3691,  1.2734, -0.6875, -0.8516, -0.4531],
        [-0.0559,  1.7969, -0.9688, -0.8672, -0.4180],
        [-0.0947,  1.6719, -0.8945, -0.7383, -0.6680],
        [ 0.1621,  1.3828, -0.7617, -0.6562, -0.7266],
        [ 0.4375,  1.7344, -0.8945, -0.7422, -0.4883],
        [ 0.2158,  1.3906, -1.2734, -1.1484, -0.7812],
        [ 0.1011,  1.3906, -0.8320, -0.7148, -0.4863]], device='cuda:0',
       dtype=torch.bfloat16, grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None), SequenceClassifierOutput(loss=tensor(0.8621, device='cuda:0', grad_fn=<NllLossBackward0>), logits=tensor([[-0.0503,  1.6250, -0.5391, -0.6523, -0.3770],
        [-0.0356,  1.0625, -0.8828, -0.8047, -0.9336],
        [ 0.0327,  1.4922, -1.0078, -1.1328, -0.9570],
        [ 0.1729,  1.6172, -0.7539, -0.7812, -0.4844],
        [-0.1426,  1.2969, -0.6797, -0.9688, -0.5039],
        [ 0.0693,  1.3906, -0.4395, -0.9414, -0.9492],
        [ 0.3438,  1.4688, -0.9219, -0.7148, -0.3242],
        [ 0.0947,  1.1719, -0.4668, -0.6602, -0.2812],
        [ 0.2695,  1.6172, -0.6016, -0.6953, -0.4570],
        [ 0.1689,  1.7344, -0.6328, -0.7773, -0.5625],
        [ 0.2109,  1.6406, -0.9336, -0.8906, -0.4922],
        [ 0.0623,  1.6172, -0.5820, -0.9492, -0.5273],
        [ 0.6250,  0.7539, -1.0000, -0.8398, -0.5156],
        [ 0.3984,  1.3750, -0.9531, -0.9609, -0.7930],
        [ 0.0869,  1.4844, -0.9336, -1.0781, -0.4102],
        [-0.0923,  1.4062, -0.7461, -0.8906, -0.9297]], device='cuda:0',
       dtype=torch.bfloat16, grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None), SequenceClassifierOutput(loss=tensor(0.5593, device='cuda:0', grad_fn=<NllLossBackward0>), logits=tensor([[ 0.2139,  1.4453, -0.9336, -1.1016, -0.3926],
        [ 0.4805,  1.4609, -0.8672, -0.8906, -0.5352],
        [-0.1064,  1.6172, -0.8711, -1.1328, -0.6523],
        [ 0.1377,  1.7422, -1.0547, -1.0234, -0.3340],
        [ 0.0825,  2.2500, -0.9023, -1.3281, -0.5430],
        [ 0.1465,  1.6719, -0.9414, -1.0156, -0.5234],
        [ 0.2969,  1.6641, -0.6055, -0.8477, -0.5195],
        [ 0.0825,  1.5625, -0.4336, -1.0312, -0.5859],
        [ 0.1133,  1.3672, -0.7148, -0.8789, -0.4531],
        [-0.2598,  1.7422, -1.0391, -0.9727, -0.5195],
        [ 0.0620,  1.8594, -0.7891, -0.8867, -0.7617],
        [ 0.0518,  1.4766, -0.4551, -0.7109, -0.7617],
        [ 0.1963,  1.2188, -1.0938, -0.8906, -0.8242],
        [ 0.4609,  2.0469, -0.7812, -1.0859, -0.7461],
        [ 0.0049,  1.9297, -0.8555, -1.0781, -0.7617],
        [ 0.1807,  1.7891, -0.8281, -1.0625, -0.4863]], device='cuda:0',
       dtype=torch.bfloat16, grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None), SequenceClassifierOutput(loss=tensor(0.6777, device='cuda:0', grad_fn=<NllLossBackward0>), logits=tensor([[ 0.1064,  1.6250, -0.9297, -0.8320, -0.7500],
        [-0.0100,  1.8828, -0.7578, -0.8594, -0.8945],
        [ 0.1133,  1.7344, -0.7734, -0.9922, -0.4473],
        [-0.0054,  1.5469, -0.6211, -0.7188, -0.6914],
        [-0.1504,  1.3984, -0.6914, -0.9375, -0.6484],
        [ 0.3809,  1.4688, -0.7031, -1.3906, -0.2637],
        [ 0.0796,  1.6953, -0.7461, -1.2344, -0.4941],
        [ 0.0752,  1.6016, -0.5039, -1.1875, -0.5508],
        [-0.2246,  1.7266, -0.7148, -1.0312, -0.4961],
        [-0.0295,  1.4844, -0.7500, -0.5352, -0.2432],
        [ 0.2422,  1.9062, -0.7500, -0.8203, -0.7070],
        [-0.2246,  1.5078, -0.6797, -0.8516, -0.7617],
        [ 0.8789,  0.8906, -1.2734, -0.9336,  0.6914],
        [ 0.2188,  1.3047, -0.2910, -0.8633, -0.6719],
        [ 0.0688,  1.6172, -0.6562, -1.1250, -0.6602],
        [ 0.3594,  1.6250, -1.0078, -0.7656, -0.5156]], device='cuda:0',
       dtype=torch.bfloat16, grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None), SequenceClassifierOutput(loss=tensor(0.7219, device='cuda:0', grad_fn=<NllLossBackward0>), logits=tensor([[ 0.2002,  1.4844, -0.9062, -1.1875, -0.2891],
        [-0.0588,  1.5156, -0.5273, -1.0469, -0.4434],
        [-0.2559,  1.8438, -0.7422, -0.8008, -0.5508],
        [ 0.2949,  1.5000, -0.4512, -0.9883, -0.4062],
        [ 0.3438,  1.1641, -1.0156, -1.0469, -0.4492],
        [-0.1318,  1.7500, -0.7383, -1.0312, -0.5820],
        [ 0.3574,  1.0547, -0.9297, -0.8906, -0.4707],
        [ 0.1147,  1.5625, -1.0781, -0.9297, -0.3809],
        [ 0.1631,  1.5078, -0.6211, -1.0234, -0.5898],
        [ 0.0291,  1.6172, -0.9492, -0.9414, -0.5586],
        [-0.1128,  1.8594, -0.7227, -1.0703, -0.7422],
        [-0.1406,  1.7188, -1.0312, -1.0859, -0.6797],
        [-0.0413,  1.0938, -0.6562, -1.0625, -0.5781],
        [ 0.1484,  1.3906, -0.4863, -1.0391, -0.4336],
        [-0.0938,  1.5000, -0.5703, -1.0469, -0.6094],
        [ 0.3184,  1.2109, -0.9062, -1.0547, -0.2676]], device='cuda:0',
       dtype=torch.bfloat16, grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None), SequenceClassifierOutput(loss=tensor(1.1044, device='cuda:0', grad_fn=<NllLossBackward0>), logits=tensor([[-0.3047,  1.4609, -0.6445, -0.9141, -1.1172],
        [ 0.2559,  1.7109, -0.9375, -0.6328, -0.5469],
        [ 0.4297,  1.8750, -0.7422, -0.8789, -0.5312],
        [-0.0549,  1.6484, -0.8242, -0.6562, -0.5430],
        [ 0.2773,  1.4766, -0.7539, -1.1562, -0.4062],
        [ 0.3145,  1.6797, -0.9727, -0.7305, -0.4102],
        [ 0.2168,  1.8672, -0.9141, -1.1094, -0.4844],
        [ 0.2275,  1.5781, -0.8633, -0.7969, -0.6445],
        [ 0.0767,  1.6172, -0.6094, -0.9062, -0.7656],
        [ 0.1787,  1.4141, -0.7617, -1.2812, -0.5781],
        [ 0.4453,  1.7344, -0.6953, -0.8867, -0.3848],
        [ 0.1475,  1.6172, -0.4941, -0.7305, -0.8008],
        [ 0.0723,  1.5000, -0.7227, -1.0312, -0.4980],
        [ 0.4316,  1.2969, -0.9219, -0.8125, -0.3496],
        [ 0.0918,  1.5703, -0.5781, -0.8516, -0.7930],
        [ 0.0913,  1.4688, -0.7891, -0.8047, -0.4570]], device='cuda:0',
       dtype=torch.bfloat16, grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None), SequenceClassifierOutput(loss=tensor(0.6356, device='cuda:0', grad_fn=<NllLossBackward0>), logits=tensor([[ 0.2207,  1.2109, -0.8320, -0.7852, -0.5859],
        [ 0.1299,  1.5078, -0.8555, -1.1797, -0.5742],
        [-0.1387,  1.7500, -0.7148, -1.0234, -0.4297],
        [ 0.4199,  1.6641, -0.8867, -1.2188, -0.3613],
        [ 0.5273,  1.3672, -0.5664, -0.6133, -0.5977],
        [ 0.3145,  1.3438, -0.8633, -0.7383, -0.3066],
        [-0.6055,  1.7109, -0.3281, -0.4473, -0.6992],
        [-0.1064,  1.6250, -0.3906, -0.8164, -0.8086],
        [ 0.0226,  1.9688, -0.8945, -0.4746, -0.6875],
        [ 0.1729,  1.3906, -0.9570, -0.8672, -0.5859],
        [-0.0815,  1.5156, -0.7070, -0.8984, -0.4824],
        [ 0.0786,  1.6250, -0.8516, -0.9180, -0.5234],
        [ 0.2139,  1.5938, -0.8867, -1.0938, -0.6641],
        [ 0.4238,  1.9453, -0.7148, -1.0703, -0.7109],
        [-0.1367,  1.6250, -0.7266, -1.0391, -0.5508],
        [ 0.2520,  0.6914, -0.7422, -0.6992,  0.0201]], device='cuda:0',
       dtype=torch.bfloat16, grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None), SequenceClassifierOutput(loss=tensor(0.8433, device='cuda:0', grad_fn=<NllLossBackward0>), logits=tensor([[ 0.1768,  1.3906, -0.9258, -0.7656, -0.6836],
        [ 0.1943,  1.7500, -0.6406, -1.1953, -0.3867],
        [ 0.1250,  1.5156, -0.7695, -1.2188, -0.4238],
        [ 0.1348,  1.6406, -1.0703, -0.9102, -0.4785],
        [ 0.0552,  1.5781, -0.6484, -0.8359, -0.5781],
        [ 0.2354,  1.5078, -0.7812, -0.9609, -0.7109],
        [ 0.2354,  1.9531, -0.8086, -0.9492, -0.4941],
        [ 0.2910,  1.3984, -0.7305, -0.8516, -0.6602],
        [ 0.1367,  1.6875, -0.5781, -0.8945, -0.5508],
        [ 0.0859,  0.5742, -0.8359, -0.6211, -0.3633],
        [ 0.0182,  1.7969, -0.4980, -0.8047, -0.6055],
        [ 0.1206,  1.4922, -0.8984, -1.0703, -0.5430],
        [ 0.2119,  1.5547, -0.8711, -0.9219, -0.5820],
        [ 0.0217,  2.1719, -0.9336, -0.7227, -0.5547],
        [-0.0151,  1.3359, -0.8281, -1.0781, -0.1924],
        [ 0.1699,  1.7656, -1.0469, -0.7383, -0.4863]], device='cuda:0',
       dtype=torch.bfloat16, grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None), SequenceClassifierOutput(loss=tensor(0.6090, device='cuda:0', grad_fn=<NllLossBackward0>), logits=tensor([[ 0.1650,  1.2109, -1.1094, -1.1875, -0.0366],
        [ 0.0093,  1.4219, -0.6289, -0.7461, -0.6250],
        [-0.2490,  1.6094, -0.5273, -0.8789, -0.8164],
        [-0.0898,  1.6406, -0.5430, -1.0469, -0.5391],
        [ 0.0452,  1.4922, -0.9375, -1.2656, -0.3418],
        [-0.2256,  1.9922, -0.7969, -0.7461, -0.3086],
        [-0.3418,  1.3359, -0.8242, -0.8203, -0.5195],
        [-0.1670,  1.3203, -0.6133, -0.7578, -0.6250],
        [ 0.0623,  1.6172, -0.7969, -0.9648, -0.6992],
        [-0.1543,  1.3125, -0.7969, -0.9453, -0.6328],
        [ 0.0036,  1.6328, -1.0547, -0.9961, -0.5352],
        [ 0.0417,  1.8672, -0.8750, -0.9297, -0.3574],
        [-0.0767,  1.8203, -0.7188, -1.1094, -0.5820],
        [-0.0728,  1.2969, -0.9023, -1.0859, -0.5586],
        [-0.0703,  1.8750, -0.7031, -1.0156, -0.6289],
        [ 0.2910,  1.4141, -0.8359, -0.6719, -0.5508]], device='cuda:0',
       dtype=torch.bfloat16, grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None), SequenceClassifierOutput(loss=tensor(0.6758, device='cuda:0', grad_fn=<NllLossBackward0>), logits=tensor([[ 0.1426,  1.5703, -0.7969, -0.8594, -0.5430],
        [ 0.1885,  1.5000, -0.3730, -0.7930, -0.6562],
        [-0.1206,  1.8906, -0.6523, -0.8359, -0.4551],
        [ 0.1455,  1.8281, -0.6133, -0.9453, -0.4238],
        [-0.1152,  1.6406, -0.8203, -0.8906, -0.5547],
        [ 0.2891,  1.4844, -0.7617, -0.8281, -0.5820],
        [-0.3477,  1.4453, -0.4902, -0.8594, -0.6094],
        [-0.2070,  1.7578, -0.5469, -0.8281, -0.6641],
        [ 0.1113,  1.5547, -0.6758, -0.8398, -0.9492],
        [-0.1089,  1.5781, -0.8086, -0.9102, -0.3574],
        [ 0.1138,  1.8125, -1.0234, -0.8633, -0.7930],
        [ 0.0659,  2.0469, -1.0938, -0.8594, -0.7070],
        [ 0.1768,  1.3359, -0.7227, -1.1328, -0.6094],
        [ 0.1475,  1.8359, -0.7070, -0.8398, -0.6094],
        [-0.0239,  1.6875, -0.5508, -1.2656, -0.4727],
        [ 0.1040,  1.8281, -0.8672, -0.7188, -0.8359]], device='cuda:0',
       dtype=torch.bfloat16, grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None), SequenceClassifierOutput(loss=tensor(0.6787, device='cuda:0', grad_fn=<NllLossBackward0>), logits=tensor([[ 0.3672,  1.6797, -0.6719, -0.6914, -0.6055],
        [ 0.2773,  1.3047, -1.2500, -1.2188, -0.3555],
        [-0.2910,  1.3750, -0.7891, -0.5859, -0.4863],
        [ 0.0947,  1.7734, -0.7539, -1.0625, -0.6875],
        [-0.2432,  1.3906, -0.3984, -0.4941, -0.5586],
        [-0.0376,  1.7266, -0.9219, -1.1562, -0.9297],
        [ 0.2100,  1.1953, -0.6484, -0.8672, -0.4785],
        [ 0.0532,  1.3359, -1.0938, -1.1094, -0.5312],
        [-0.0698,  1.4297, -1.1328, -0.9805, -0.2852],
        [ 0.3926,  1.1797, -0.5703, -0.8281, -0.6836],
        [ 0.0552,  1.7578, -0.6055, -1.0469, -0.5469],
        [ 0.1396,  1.6094, -0.6133, -0.7930, -0.9688],
        [ 0.2578,  1.4062, -0.5898, -0.9062, -0.8828],
        [ 0.2520,  1.5547, -0.5156, -0.7852, -0.3965],
        [ 0.0201,  1.8750, -0.5391, -1.0625, -0.4473],
        [ 0.0854,  1.6484, -0.8516, -1.3203, -0.8438]], device='cuda:0',
       dtype=torch.bfloat16, grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None), SequenceClassifierOutput(loss=tensor(0.7633, device='cuda:0', grad_fn=<NllLossBackward0>), logits=tensor([[ 0.2295,  1.4219, -0.6367, -1.1016, -0.6602],
        [ 0.2051,  1.6562, -1.1484, -1.1016, -0.3418],
        [-0.2539,  1.8281, -0.5430, -0.8594, -0.5547],
        [ 0.1436,  1.7031, -0.5273, -0.8359, -0.4180],
        [ 0.2578,  2.0312, -0.8633, -1.4766, -0.6367],
        [ 0.1465,  1.4375, -0.6328, -0.4961, -0.3633],
        [ 0.0967,  1.7188, -1.0625, -1.0547, -0.5508],
        [ 0.2539,  1.5156, -0.9062, -0.9492, -0.3965],
        [ 0.3164,  1.3672, -0.6562, -1.0547, -0.6836],
        [ 0.2148,  1.4766, -0.5391, -0.8750, -0.4473],
        [-0.2461,  1.8047, -0.5352, -0.9453, -0.4785],
        [ 0.1807,  1.8750, -0.7383, -1.0156, -0.5820],
        [ 0.3809,  1.3438, -0.5234, -0.8906, -0.4785],
        [ 0.2148,  1.3750, -0.8398, -0.7930, -0.4980],
        [-0.0159,  1.7344, -0.9844, -0.9023, -0.4941],
        [ 0.1846,  1.3750, -0.7422, -1.0547, -0.5352]], device='cuda:0',
       dtype=torch.bfloat16, grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None), SequenceClassifierOutput(loss=tensor(0.7924, device='cuda:0', grad_fn=<NllLossBackward0>), logits=tensor([[ 0.2285,  1.8750, -0.8516, -1.0078, -0.5273],
        [-0.0166,  1.7656, -0.6758, -1.1953, -0.5273],
        [ 0.1514,  1.5234, -0.6562, -1.1016, -0.5859],
        [-0.1040,  1.9766, -0.7383, -0.9453, -0.5820],
        [ 0.2617,  1.0234, -1.1250, -0.7852,  0.5742],
        [ 0.1055,  1.6953, -0.9922, -0.7344, -0.5547],
        [ 0.3281,  1.3047, -0.8750, -0.9375, -0.5391],
        [ 0.3203,  1.7344, -0.7969, -0.6250, -0.4727],
        [ 0.0376,  1.7891, -0.6406, -0.9258, -0.6133],
        [ 0.1260,  1.3672, -0.7305, -1.1172, -0.6328],
        [ 0.2041,  1.5078, -0.6953, -1.0781, -0.5117],
        [ 0.1426,  1.7891, -0.7305, -1.0000, -0.5117],
        [-0.2539,  1.9141, -0.8438, -0.8125, -0.5742],
        [ 0.0635,  1.7031, -0.9688, -1.2266, -0.5898],
        [-0.3594,  1.4453, -0.5195, -0.8516, -0.7500],
        [-0.1338,  1.3906, -0.7734, -0.9102, -0.6836]], device='cuda:0',
       dtype=torch.bfloat16, grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None), SequenceClassifierOutput(loss=tensor(0.7137, device='cuda:0', grad_fn=<NllLossBackward0>), logits=tensor([[ 0.0143,  1.6719, -0.5586, -0.8438, -0.4727],
        [ 0.1992,  1.6250, -0.8750, -0.9844, -0.4277],
        [ 0.3125,  1.4609, -0.6562, -0.9531, -0.6680],
        [ 0.1182,  1.5469, -0.5664, -0.8750, -0.6328],
        [-0.1245,  1.7266, -0.2656, -0.9375, -0.9727],
        [ 0.3906,  1.1875, -0.8555, -0.7773, -0.6406],
        [ 0.0282,  1.5469, -0.8750, -0.8203, -0.5898]], device='cuda:0',
       dtype=torch.bfloat16, grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)], [SequenceClassifierOutput(loss=tensor(2.3232, device='cuda:0', grad_fn=<NllLossBackward0>), logits=tensor([[ 0.0864,  1.7656, -0.5586, -0.7461, -0.4883],
        [ 0.6875,  1.0938, -1.3750, -1.0156, -0.2109],
        [ 0.0645,  1.5938, -0.8516, -0.8828, -0.4766],
        [ 0.0146,  1.5312, -0.5117, -0.6914, -0.9219],
        [-0.1895,  1.0625, -0.7305, -0.6797, -0.3164],
        [ 0.2832,  1.2188, -0.9102, -0.7617, -0.6641],
        [ 0.2617,  1.3906, -1.2812, -1.0312, -0.3926],
        [ 0.1973,  1.4766, -0.7422, -1.2422, -0.4316],
        [ 0.1797,  1.6328, -0.5156, -0.8320, -0.6953],
        [ 0.0253,  1.0391, -1.0234, -0.8516, -0.6172],
        [ 0.0053,  1.0625, -0.5508, -0.4609, -0.4199],
        [-0.1016,  1.1641, -0.5234, -0.3359, -0.6406],
        [ 0.3906,  1.5156, -0.8711, -0.8516, -0.2148],
        [ 0.2578,  0.7031, -0.6484, -0.8359, -0.4297],
        [ 0.2910,  0.9883, -1.0078, -0.6055, -0.6367],
        [ 0.1719,  1.5156, -0.4824, -0.7070, -0.1328]], device='cuda:0',
       dtype=torch.bfloat16, grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None), SequenceClassifierOutput(loss=tensor(2.5107, device='cuda:0', grad_fn=<NllLossBackward0>), logits=tensor([[ 0.3652,  1.7031, -0.8359, -0.6328, -0.6914],
        [-0.1357,  1.6016, -0.6406, -1.0859, -0.8203],
        [ 0.3477,  1.1016, -0.6484, -0.6602, -0.5977],
        [ 0.2168,  1.1172, -0.8359, -1.0234, -0.1387],
        [-0.0258,  1.4688, -0.9570, -0.7344, -0.4160],
        [ 0.2891,  1.2344, -0.6562, -0.8086, -0.4824],
        [-0.0261,  1.7578, -0.8828, -1.1016, -0.6250],
        [ 0.1660,  1.2656, -0.5078, -0.9375, -0.4922],
        [ 0.1719,  1.2891, -0.9023, -0.7812, -0.9805],
        [ 0.1113,  1.5078, -0.7500, -0.9492, -0.5039],
        [ 0.0781,  1.5234, -0.8594, -1.1953, -0.2412],
        [-0.2715,  1.8516, -0.4629, -0.9258, -0.3262],
        [ 0.4258,  0.8359, -1.0625, -0.7656,  0.1387],
        [-0.0132,  1.4844, -1.0234, -0.9297, -0.5977],
        [ 0.2539,  1.2188, -0.8203, -0.5664, -0.2480],
        [-0.0425,  1.4766, -0.8867, -0.8711, -0.5977]], device='cuda:0',
       dtype=torch.bfloat16, grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None), SequenceClassifierOutput(loss=tensor(2.0674, device='cuda:0', grad_fn=<NllLossBackward0>), logits=tensor([[ 0.3242,  0.9141, -0.7656, -0.4316, -0.4531],
        [-0.0996,  0.9375, -0.3145, -0.5000, -0.5703],
        [ 0.2578,  1.0312, -1.0156, -0.9453, -0.3477],
        [ 0.3379,  1.5703, -0.8242, -0.8047, -0.8008],
        [ 0.0864,  1.1016, -0.8633, -0.4180, -0.4805],
        [ 0.4180,  0.2793, -1.2188, -0.6797, -0.1416],
        [-0.0369,  1.2734, -0.5820, -0.5547, -0.5898],
        [ 0.1484,  1.2812, -0.7930, -0.4395, -0.2217],
        [-0.0540,  1.1875, -0.7461, -0.5195, -0.4727],
        [-0.0469,  1.5547, -0.6445, -0.8320, -0.3770],
        [ 0.2500,  1.4453, -1.0859, -0.8203, -0.4824],
        [ 0.3242,  1.1875, -1.2500, -0.8984, -0.2197],
        [ 0.2734,  1.2188, -0.7773, -0.2754, -0.3633],
        [ 0.3340,  1.6172, -1.2109, -1.2266, -0.3828],
        [-0.0674,  1.6719, -0.5547, -0.8867, -0.5859],
        [ 0.0898,  0.8398, -1.3047, -1.0156,  0.2197]], device='cuda:0',
       dtype=torch.bfloat16, grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None), SequenceClassifierOutput(loss=tensor(2.2407, device='cuda:0', grad_fn=<NllLossBackward0>), logits=tensor([[-0.1367,  1.8203, -0.8516, -0.8086, -0.3477],
        [ 0.0552,  1.4766, -0.6641, -0.8945, -0.6641],
        [-0.0684,  1.5000, -0.5273, -0.8867, -0.4824],
        [ 0.2812,  1.0938, -1.1094, -0.9414, -0.3613],
        [ 0.1211,  1.1484, -1.1641, -1.0938, -0.2480],
        [ 0.2051,  1.4531, -0.9453, -1.1406, -0.3711],
        [ 0.2139,  1.3359, -0.8125, -1.0391, -0.3535],
        [-0.0275,  1.0000, -0.5195, -0.4375, -0.3418],
        [ 0.6367,  1.1484, -1.1484, -0.9648,  0.2412],
        [ 0.3223,  0.9922, -0.8906, -0.7852, -0.3887],
        [ 0.1152,  1.6016, -0.6797, -0.8594, -0.4766],
        [-0.3691,  1.2578, -0.8516, -0.4766, -0.7930],
        [ 0.0364,  1.3125, -0.8750, -0.6211, -0.1484],
        [-0.1006,  1.5312, -0.8086, -0.9609, -0.4062],
        [ 0.4355,  1.1484, -0.6250, -0.6680, -0.7852],
        [ 0.4277,  1.6875, -0.7812, -0.8320, -0.4590]], device='cuda:0',
       dtype=torch.bfloat16, grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None), SequenceClassifierOutput(loss=tensor(2.4492, device='cuda:0', grad_fn=<NllLossBackward0>), logits=tensor([[ 1.1719e-01,  1.9141e+00, -7.3828e-01, -9.2969e-01, -5.9375e-01],
        [ 2.5000e-01,  1.5078e+00, -8.1641e-01, -6.3672e-01, -4.0625e-01],
        [ 1.8164e-01,  1.3828e+00, -7.5781e-01, -5.5469e-01, -2.2656e-01],
        [ 1.4746e-01,  1.6250e+00, -9.8438e-01, -7.3047e-01, -4.9414e-01],
        [-8.2520e-02,  1.4609e+00, -7.6953e-01, -8.3984e-01, -6.4844e-01],
        [ 2.4121e-01,  6.4062e-01, -8.0469e-01,  6.7871e-02,  3.7891e-01],
        [ 5.8594e-03,  1.8672e+00, -9.6484e-01, -1.0547e+00, -2.8711e-01],
        [ 8.4375e-01,  7.7734e-01, -1.0703e+00, -3.1641e-01,  2.2168e-01],
        [ 2.6953e-01,  1.2578e+00, -8.2422e-01, -7.3438e-01, -4.5117e-01],
        [-3.6523e-01,  1.6172e+00, -7.1875e-01, -1.0000e+00, -5.1172e-01],
        [ 2.4902e-02,  1.4531e+00, -6.9531e-01, -9.6875e-01, -8.1250e-01],
        [-5.1514e-02,  9.3359e-01, -6.7188e-01, -4.9609e-01, -2.5195e-01],
        [-2.1094e-01,  1.6328e+00, -5.2344e-01, -8.2031e-01, -7.3438e-01],
        [ 2.5195e-01,  1.2500e+00, -6.9922e-01, -8.1641e-01, -2.6953e-01],
        [ 8.3984e-02,  1.2656e+00, -1.2188e+00, -7.5000e-01, -1.9824e-01],
        [-1.2512e-03,  1.4219e+00, -8.5547e-01, -7.4219e-01, -6.2500e-01]],
       device='cuda:0', dtype=torch.bfloat16, grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None), SequenceClassifierOutput(loss=tensor(1.2910, device='cuda:0', grad_fn=<NllLossBackward0>), logits=tensor([[ 0.2090,  1.1016, -0.7539, -0.8867, -0.5273],
        [ 0.0615,  1.6328, -0.3867, -0.8125, -0.4551],
        [ 0.0508,  1.6875, -0.6758, -0.7539, -0.2695],
        [-0.0359,  1.3438, -0.8789, -0.8086, -0.4648],
        [-0.0674,  1.3125, -1.1953, -1.0938, -0.4473],
        [ 0.3359,  1.4375, -0.9023, -0.7656, -0.6602],
        [-0.0752,  1.5078, -0.5117, -0.8320, -0.9141],
        [-0.0388,  1.5000, -0.4531, -0.9375, -0.3457],
        [ 0.1250,  1.6250, -0.6758, -1.1016, -0.9375],
        [ 0.2500,  1.9141, -0.4238, -0.9727, -0.7734],
        [ 0.4023,  1.5859, -0.8203, -0.8125, -0.7305],
        [-0.0188,  1.3828, -0.6328, -1.0703, -0.5352],
        [ 0.0035,  1.6016, -0.6133, -0.8398, -0.4668],
        [ 0.3418,  1.0469, -0.7812, -1.0234, -0.8516],
        [-0.0408,  1.4062, -0.6914, -0.9219, -0.8516],
        [-0.0188,  1.4922, -0.6406, -0.7891, -0.7422]], device='cuda:0',
       dtype=torch.bfloat16, grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None), SequenceClassifierOutput(loss=tensor(1.0703, device='cuda:0', grad_fn=<NllLossBackward0>), logits=tensor([[ 0.1279,  1.4531, -0.9219, -1.1406, -0.4277],
        [-0.0522,  1.4531, -0.3184, -1.0547, -0.9492],
        [ 0.0140,  1.1484, -0.6914, -1.0703, -0.3730],
        [ 0.3438,  1.4766, -0.5234, -1.1875, -0.8594],
        [ 0.3184,  1.2422, -0.8203, -1.1641, -0.6562],
        [-0.2969,  1.5781, -0.5000, -0.6641, -0.9102],
        [-0.0659,  1.3828, -0.5195, -0.9336, -0.6016],
        [-0.4082,  1.5000, -0.4180, -0.7383, -0.7734],
        [ 0.3086,  1.4062, -0.6055, -0.7461, -0.6211],
        [-0.1553,  1.4844, -0.5859, -1.2422, -0.7031],
        [-0.0469,  1.2344, -0.3613, -1.0156, -0.6992],
        [-0.0908,  1.1953, -0.9023, -0.7461, -0.5039],
        [-0.0840,  1.7109, -0.5234, -0.7656, -0.9414],
        [ 0.1748,  1.6172, -0.7500, -0.8086, -0.7109],
        [-0.1494,  1.2656, -0.9375, -1.2578, -0.5234],
        [ 0.0889,  1.5156, -0.6328, -0.7969, -0.5742]], device='cuda:0',
       dtype=torch.bfloat16, grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None), SequenceClassifierOutput(loss=tensor(0.5292, device='cuda:0', grad_fn=<NllLossBackward0>), logits=tensor([[ 0.1221,  1.7969, -0.1143, -0.9492, -0.6094],
        [ 0.1050,  1.8047, -0.6211, -1.1484, -0.8164],
        [-0.1562,  1.7734, -0.5820, -1.0859, -0.8516],
        [ 0.2891,  1.3594, -0.8242, -0.8750, -0.9609],
        [-0.2754,  1.2656, -0.3418, -0.6875, -0.7812],
        [ 0.0057,  1.3828, -0.6875, -0.9180, -0.6797],
        [ 0.1592,  1.6172, -0.6094, -0.8516, -0.4023],
        [ 0.3867,  1.3438, -0.5820, -0.7695, -0.7266],
        [ 0.0474,  1.5859, -0.3652, -1.0391, -1.0000],
        [-0.0400,  1.8125, -0.6367, -0.9023, -1.0469],
        [ 0.0728,  1.1406, -0.6055, -0.7812, -0.5547],
        [ 0.0148,  1.2266, -0.7891, -1.1797, -0.7383],
        [ 0.1074,  1.6719, -0.7031, -1.1250, -0.8203],
        [-0.0427,  1.2891, -0.6680, -1.3984, -0.7812],
        [-0.0532,  1.5938, -0.6289, -1.1094, -0.9219],
        [ 0.1021,  1.6172, -0.6836, -0.9453, -0.8750]], device='cuda:0',
       dtype=torch.bfloat16, grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None), SequenceClassifierOutput(loss=tensor(0.6367, device='cuda:0', grad_fn=<NllLossBackward0>), logits=tensor([[ 0.1328,  1.4062, -0.4258, -1.1328, -0.8867],
        [ 0.0981,  1.3672, -0.3516, -0.9023, -0.4648],
        [ 0.0520,  1.7500, -0.3301, -0.8086, -0.4590],
        [ 0.0021,  1.5078, -0.4414, -0.9883, -0.9883],
        [ 0.0400,  1.2969, -0.4648, -0.8320, -0.8438],
        [-0.0066,  1.7031, -0.4395, -0.9531, -0.7695],
        [ 0.0579,  1.1719, -0.5820, -0.8203, -0.6875],
        [ 0.2090,  1.5312, -0.5703, -1.2031, -0.8516],
        [ 0.2061,  1.6562, -0.5703, -1.1797, -0.4961],
        [ 0.1670,  1.5703, -0.9570, -0.9453, -0.7969],
        [-0.1543,  1.2500, -0.7617, -0.8828, -0.6797],
        [ 0.1973,  1.2031, -0.4180, -0.8125, -0.8125],
        [-0.0454,  1.5469, -0.2197, -0.9062, -0.4648],
        [ 0.0039,  1.3125, -0.5586, -1.0938, -0.6094],
        [-0.3828,  0.7891, -0.6562, -1.1094, -0.3848],
        [-0.1406,  1.2344, -0.9102, -0.9297, -0.1699]], device='cuda:0',
       dtype=torch.bfloat16, grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None), SequenceClassifierOutput(loss=tensor(0.6433, device='cuda:0', grad_fn=<NllLossBackward0>), logits=tensor([[-0.0562,  1.6406, -0.6289, -0.8594, -0.9766],
        [-0.0205,  1.3516, -0.6602, -1.1953, -0.7695],
        [ 0.4746,  1.6797, -0.7812, -0.6289, -0.4375],
        [ 0.3145,  1.1797, -0.9258, -1.2578, -0.2891],
        [ 0.0425,  1.4219, -0.9023, -1.0625, -0.3750],
        [ 0.0933,  1.4609, -0.2832, -0.9922, -0.7695],
        [ 0.1465,  1.7188, -0.7148, -0.9297, -0.7227],
        [-0.1719,  1.8125, -0.8750, -0.8711, -0.4316],
        [ 0.0508,  1.4688, -0.3203, -0.8555, -0.7969],
        [ 0.1494,  1.5703, -0.5039, -1.2500, -1.1250],
        [-0.1592,  1.3984, -0.5547, -1.2969, -0.8047],
        [-0.2734,  1.4922, -0.6016, -0.6133, -0.6133],
        [-0.1777,  1.5625, -0.7188, -1.0469, -1.1406],
        [ 0.1943,  1.9922, -0.7500, -0.9570, -0.7422],
        [ 0.1074,  1.4453, -0.9883, -0.7344, -0.7656],
        [ 0.0041,  1.5703, -0.4707, -0.7500, -0.6562]], device='cuda:0',
       dtype=torch.bfloat16, grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None), SequenceClassifierOutput(loss=tensor(0.8905, device='cuda:0', grad_fn=<NllLossBackward0>), logits=tensor([[ 0.0371,  1.4297, -0.6172, -0.8945, -0.6094],
        [ 0.0260,  1.1797, -0.5977, -0.8789, -0.6836],
        [ 0.0244,  1.1172, -0.6953, -0.8242, -1.0547],
        [-0.0304,  1.5703, -0.5391, -1.0312, -0.7773],
        [ 0.3633,  1.5312, -0.6914, -0.9375, -0.7773],
        [ 0.1445,  1.2422, -0.5703, -1.0859, -0.8008],
        [-0.0815,  1.4141, -0.9062, -0.9805, -0.7539],
        [ 0.1562,  1.3125, -0.8594, -1.0391, -0.4023],
        [ 0.0957,  1.2812, -0.7422, -1.3281, -0.5703],
        [ 0.1279,  1.1953, -0.5938, -0.8906, -0.4238],
        [ 0.1904,  1.6094, -0.7578, -0.9141, -0.4258],
        [-0.1455,  1.6250, -0.5508, -1.0078, -0.7891],
        [ 0.1768,  1.3203, -0.5469, -0.7656, -0.9219],
        [ 0.1943,  1.6797, -0.7305, -0.9492, -0.7422],
        [-0.4336,  1.6641, -0.6641, -0.8359, -0.6211],
        [ 0.2256,  0.9141, -0.9141, -1.0469, -0.6719]], device='cuda:0',
       dtype=torch.bfloat16, grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None), SequenceClassifierOutput(loss=tensor(0.8427, device='cuda:0', grad_fn=<NllLossBackward0>), logits=tensor([[ 0.2852,  1.0156, -1.1953, -1.1719, -0.4141],
        [ 0.3867,  1.4922, -0.8359, -0.8672, -0.6172],
        [-0.0732,  1.4141, -0.9375, -1.0078, -0.3457],
        [ 0.0294,  1.6406, -0.6445, -1.1484, -0.8203],
        [-0.0869,  1.5156, -0.6992, -1.0469, -0.4727],
        [ 0.2119,  1.4297, -0.8867, -1.1797, -0.6523],
        [-0.1523,  1.2109, -0.8242, -0.9141, -0.5859],
        [-0.0791,  1.2422, -0.2637, -0.6758, -0.3398],
        [ 0.3770,  1.5078, -1.0547, -1.2734, -0.6211],
        [ 0.0046,  1.4766, -0.4648, -1.2812, -1.0938],
        [-0.0664,  1.3594, -0.6523, -1.0000, -0.6602],
        [ 0.4434,  1.2422, -0.4844, -0.9062, -0.6758],
        [ 0.1240,  1.7969, -0.5273, -1.1797, -0.4570],
        [ 0.0398,  1.8750, -0.2754, -1.0547, -0.6758],
        [-0.1484,  1.8281, -0.5703, -1.2812, -0.4277],
        [-0.2139,  1.2812, -0.6445, -0.7930, -0.6484]], device='cuda:0',
       dtype=torch.bfloat16, grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None), SequenceClassifierOutput(loss=tensor(0.7076, device='cuda:0', grad_fn=<NllLossBackward0>), logits=tensor([[-0.2100,  1.2891, -0.1826, -1.0000, -0.4668],
        [ 0.0613,  1.4766, -0.4121, -0.8281, -0.9219],
        [ 0.1660,  1.6484, -0.5859, -1.1719, -0.9258],
        [-0.1406,  1.3047, -0.5312, -0.8477, -0.5898],
        [-0.1299,  1.8672, -0.3770, -0.7617, -0.4785],
        [ 0.1494,  1.4453, -0.4902, -0.9844, -0.8633],
        [ 0.0732,  1.7500, -0.6680, -0.6914, -0.8906],
        [ 0.1768,  1.5078, -0.6289, -1.0312, -1.0156],
        [ 0.1206,  1.7891, -0.5195, -0.8711, -0.7773],
        [ 0.1138,  1.5703, -0.8125, -1.1250, -0.5195],
        [ 0.1738,  1.2812, -0.2656, -0.8125, -0.4395],
        [ 0.1084,  1.1562, -0.9102, -0.6758, -0.3242],
        [-0.0052,  1.3906, -0.8711, -1.0938, -0.5625],
        [ 0.0043,  1.5000, -0.3203, -1.1172, -0.5859],
        [-0.1021,  1.5859, -0.3848, -1.0547, -0.7617],
        [-0.2158,  1.4922, -0.3418, -0.8438, -0.6875]], device='cuda:0',
       dtype=torch.bfloat16, grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None), SequenceClassifierOutput(loss=tensor(0.6676, device='cuda:0', grad_fn=<NllLossBackward0>), logits=tensor([[-0.0476,  1.8672, -0.6484, -0.8945, -0.4004],
        [ 0.1436,  1.3828, -0.7500, -0.9258, -0.5664],
        [ 0.0374,  1.3438, -0.6055, -0.9336, -0.8828],
        [-0.1631,  1.6016, -0.5508, -1.1094, -0.7852],
        [ 0.4199,  1.3672, -0.7578, -0.7891, -0.5586],
        [ 0.0112,  1.5625, -0.5508, -0.8867, -0.6992],
        [-0.1406,  1.4609, -0.5078, -0.9297, -0.6758],
        [-0.0879,  1.3438, -0.5938, -0.8242, -0.5117],
        [-0.0162,  1.5078, -0.3750, -1.2891, -0.8125],
        [ 0.2949,  1.1953, -0.8203, -1.0078, -0.5977],
        [-0.2930,  1.7500, -0.5195, -1.0859, -0.7188],
        [-0.3027,  1.6094, -1.1484, -0.9844, -0.5742],
        [-0.1011,  1.7266, -0.4668, -0.9570, -0.4336],
        [ 0.9805,  1.0234, -1.1016, -0.4629,  0.0845],
        [-0.0903,  1.3828, -0.4434, -0.9375, -0.7812],
        [-0.0060,  1.5156, -0.4121, -0.9961, -0.8125]], device='cuda:0',
       dtype=torch.bfloat16, grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None), SequenceClassifierOutput(loss=tensor(0.6514, device='cuda:0', grad_fn=<NllLossBackward0>), logits=tensor([[-0.1523,  1.7422, -0.5195, -1.1484, -0.6562],
        [-0.2012,  1.5000, -0.5547, -0.6836, -0.6719],
        [-0.2559,  1.4922, -0.2090, -0.7461, -0.6250],
        [-0.0199,  1.4688, -0.5977, -0.9219, -0.6914],
        [-0.0457,  1.2812, -0.2852, -1.0000, -0.6797],
        [ 0.3066,  1.6797, -0.4023, -0.8633, -0.7500],
        [-0.0481,  1.7500, -0.4004, -0.7773, -0.8867],
        [ 0.1338,  1.3594, -0.5742, -1.2188, -0.6602],
        [ 0.3320,  1.6641, -0.3105, -1.0234, -1.0234],
        [ 0.2480,  1.3984, -0.5625, -1.0703, -0.6211],
        [-0.2207,  1.7656, -0.3809, -1.2109, -0.7109],
        [-0.0405,  1.5781, -0.7930, -0.9102, -0.5898],
        [ 0.0723,  1.8359, -0.4102, -1.2578, -0.7852],
        [-0.0674,  1.3984, -0.4160, -0.9805, -0.3945],
        [-0.1504,  1.3594, -0.8984, -0.9961, -0.7031],
        [ 0.2793,  1.5000, -0.7461, -0.9688, -0.8594]], device='cuda:0',
       dtype=torch.bfloat16, grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None), SequenceClassifierOutput(loss=tensor(0.7490, device='cuda:0', grad_fn=<NllLossBackward0>), logits=tensor([[ 0.2852,  1.2969, -0.6758, -0.7539, -0.5391],
        [-0.2246,  1.8672, -0.3125, -0.8203, -0.7383],
        [ 0.1709,  1.4219, -0.6875, -0.8242, -0.8281],
        [ 0.1318,  1.5312, -0.5430, -0.7461, -0.9062],
        [-0.0381,  1.5000, -0.7773, -0.8555, -0.8789],
        [ 0.2158,  1.1719, -0.4492, -0.8125, -0.3887],
        [-0.1260,  1.5391, -0.4707, -1.0156, -0.8555],
        [ 0.0854,  1.6250, -0.6797, -0.9766, -0.4004],
        [ 0.2266,  1.1719, -0.4141, -0.9492, -0.6758],
        [ 0.3223,  1.5000, -0.4102, -0.9180, -0.7383],
        [ 0.2656,  1.4297, -0.6602, -0.9375, -0.7539],
        [ 0.4375,  1.4688, -0.5273, -1.0234, -0.6719],
        [ 0.1357,  1.4219, -0.7188, -0.9258, -0.7070],
        [ 0.1377,  1.6953, -0.2832, -1.0547, -1.1719],
        [ 0.0879,  1.4219, -0.4902, -0.8047, -0.4961],
        [ 0.0640,  1.2500, -0.3867, -0.5391, -0.7930]], device='cuda:0',
       dtype=torch.bfloat16, grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None), SequenceClassifierOutput(loss=tensor(0.8209, device='cuda:0', grad_fn=<NllLossBackward0>), logits=tensor([[ 6.2866e-03,  1.4297e+00, -6.9141e-01, -1.1641e+00, -8.8281e-01],
        [ 1.9434e-01,  1.3906e+00, -3.1641e-01, -6.3672e-01, -8.0469e-01],
        [-1.3885e-03,  1.5078e+00, -4.7070e-01, -1.0234e+00, -5.0000e-01],
        [ 1.6895e-01,  1.6797e+00, -4.7070e-01, -1.2188e+00, -6.0547e-01],
        [ 2.7734e-01,  1.1641e+00, -1.0234e+00, -1.1484e+00, -1.1133e-01],
        [ 3.0664e-01,  1.6875e+00, -5.7812e-01, -7.5781e-01, -9.7656e-01],
        [ 1.9434e-01,  1.5859e+00, -4.0820e-01, -7.4609e-01, -9.5312e-01],
        [-5.2002e-02,  1.6328e+00, -7.7344e-01, -1.2656e+00, -8.4375e-01],
        [-7.1777e-02,  1.6641e+00, -6.8359e-01, -1.0938e+00, -5.3516e-01],
        [ 6.1035e-02,  1.6875e+00, -4.5898e-01, -8.5938e-01, -7.4219e-01],
        [ 4.3945e-03,  1.3516e+00, -7.4609e-01, -9.0625e-01, -9.4922e-01],
        [-8.2031e-02,  1.6094e+00, -5.1562e-01, -6.4453e-01, -4.1797e-01],
        [ 4.5471e-03,  1.5156e+00, -4.3359e-01, -1.1172e+00, -1.0234e+00],
        [ 1.2012e-01,  1.4219e+00, -1.0391e+00, -9.8438e-01, -6.7871e-02],
        [ 3.5645e-02,  1.5703e+00, -5.6641e-01, -8.9453e-01, -6.4844e-01],
        [ 7.5684e-02,  1.3594e+00, -9.3359e-01, -1.1562e+00, -4.9805e-01]],
       device='cuda:0', dtype=torch.bfloat16, grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None), SequenceClassifierOutput(loss=tensor(0.4869, device='cuda:0', grad_fn=<NllLossBackward0>), logits=tensor([[-0.0210,  1.5234, -0.6328, -1.1016, -0.6367],
        [ 0.1250,  1.7031, -0.6016, -0.9727, -0.7734],
        [-0.2246,  1.6406, -0.6367, -1.0547, -0.8164],
        [ 0.1680,  1.5078, -0.7734, -1.0000, -0.9805],
        [ 0.1060,  1.6562, -1.0078, -1.1016, -0.6719],
        [-0.0505,  1.5156, -0.6914, -0.7656, -0.5977],
        [-0.0396,  1.5391, -0.5312, -0.9453, -0.6641],
        [-0.0588,  1.4297, -0.7305, -0.7695, -0.6016],
        [-0.0544,  1.6641, -0.7148, -0.8594, -0.5781],
        [-0.0364,  1.7734, -0.8008, -1.0391, -0.5352],
        [ 0.2119,  1.4219, -0.6484, -1.1719, -0.5898],
        [-0.3281,  1.3828, -0.6914, -0.8203, -0.5430],
        [ 0.1709,  1.6484, -0.8086, -1.4297, -0.5898],
        [-0.2891,  1.9062, -0.5703, -0.7461, -0.6484],
        [-0.0271,  0.8555, -0.7070, -0.8242, -0.7969],
        [-0.1074,  1.8750, -0.8008, -0.7695, -0.5547]], device='cuda:0',
       dtype=torch.bfloat16, grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None), SequenceClassifierOutput(loss=tensor(0.6146, device='cuda:0', grad_fn=<NllLossBackward0>), logits=tensor([[ 0.1030,  1.7891, -0.8203, -0.9180, -0.7617],
        [ 0.1211,  1.4141, -0.9414, -1.1484, -0.5898],
        [-0.1904,  1.4453, -0.7891, -1.0156, -0.5625],
        [-0.1426,  1.6484, -0.8164, -0.8750, -0.5312],
        [-0.1260,  1.6094, -0.4648, -0.7891, -0.8750],
        [ 0.1855,  1.5781, -1.0000, -0.7227, -0.4570],
        [ 0.3516,  1.3594, -0.4570, -0.6992, -0.6797],
        [ 0.0967,  1.8203, -0.7656, -0.8047, -0.6758],
        [-0.0981,  1.7656, -0.7500, -1.0469, -0.4414],
        [ 0.0332,  1.5391, -0.8320, -0.8984, -0.5898],
        [-0.0376,  1.5469, -0.6094, -0.8906, -0.6055],
        [-0.3086,  2.0000, -0.6172, -1.1328, -0.8086],
        [-0.0449,  1.5781, -0.7734, -1.2266, -0.6641],
        [-0.0410,  1.6484, -0.6406, -0.6406, -0.5117],
        [-0.0923,  1.9688, -1.0703, -1.0391, -0.8477],
        [-0.0825,  1.7422, -0.7227, -0.7930, -0.5664]], device='cuda:0',
       dtype=torch.bfloat16, grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None), SequenceClassifierOutput(loss=tensor(0.4404, device='cuda:0', grad_fn=<NllLossBackward0>), logits=tensor([[ 0.0349,  1.7109, -0.2217, -1.0547, -0.9844],
        [-0.0020,  1.5156, -0.4922, -0.6641, -0.4961],
        [ 0.1895,  1.2891, -1.0391, -0.8359, -0.6992],
        [-0.1328,  1.7891, -0.6680, -0.9492, -0.5977],
        [ 0.0444,  1.5859, -0.7148, -0.9102, -0.6953],
        [-0.2520,  1.7109, -1.1094, -1.0156, -0.5078],
        [ 0.1367,  1.7656, -0.8438, -0.9961, -0.5234],
        [ 0.0957,  1.5859, -0.5664, -0.8906, -0.7188],
        [ 0.5508,  0.8945, -1.2344, -1.0781, -0.2197],
        [ 0.1943,  1.4375, -0.7070, -0.9062, -0.8672],
        [ 0.1201,  1.4453, -0.6133, -0.9609, -0.7500],
        [ 0.2021,  1.7266, -0.4297, -1.0625, -0.4922],
        [-0.0645,  1.8047, -0.5469, -1.0000, -0.6328],
        [ 0.1777,  1.5234, -0.9180, -1.1094, -0.5039],
        [ 0.1846,  1.7422, -0.4473, -0.7578, -0.4746],
        [-0.5078,  1.2656, -0.6328, -1.0078, -0.4336]], device='cuda:0',
       dtype=torch.bfloat16, grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None), SequenceClassifierOutput(loss=tensor(0.8777, device='cuda:0', grad_fn=<NllLossBackward0>), logits=tensor([[ 0.0457,  1.2891, -0.6445, -1.0781, -0.8906],
        [-0.0554,  1.5703, -0.2734, -0.6133, -0.9531],
        [-0.0903,  1.6875, -0.9336, -0.8477, -0.5977],
        [ 0.2344,  1.2578, -0.3887, -1.0391, -0.7734],
        [-0.0640,  1.2109, -0.6016, -1.1406, -0.6523],
        [ 0.2812,  1.5391, -0.5508, -0.8750, -0.2852],
        [ 0.0569,  1.0625, -0.4883, -1.0781, -0.6641],
        [ 0.0031,  1.5547, -0.8242, -1.3750, -0.5820],
        [ 0.2314,  1.6562, -0.7578, -1.0469, -0.3438],
        [ 0.5820,  1.5391, -0.6133, -0.6445, -0.6914],
        [-0.0042,  1.2422, -0.5547, -0.7578, -0.9805],
        [-0.0128,  1.3906, -0.8633, -0.7656, -0.5195],
        [ 0.3730,  1.4297, -0.8477, -1.0859, -0.3613],
        [ 0.2852,  1.6016, -0.7812, -0.9023, -0.5312],
        [ 0.1426,  1.5781, -0.8711, -0.7148, -0.6211],
        [-0.0820,  1.7891, -0.7891, -1.2969, -0.6250]], device='cuda:0',
       dtype=torch.bfloat16, grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None), SequenceClassifierOutput(loss=tensor(0.7711, device='cuda:0', grad_fn=<NllLossBackward0>), logits=tensor([[ 0.0104,  1.6328, -0.8945, -0.7891, -0.8594],
        [ 0.2041,  1.6797, -0.6406, -1.0312, -0.4883],
        [ 0.3418,  1.3047, -0.5938, -0.9062, -0.7734],
        [-0.0796,  1.5625, -0.4688, -0.8359, -0.5977],
        [-0.1338,  1.6562, -0.5234, -0.6836, -0.5312],
        [ 0.1650,  1.1562, -0.7148, -0.7539, -0.5664],
        [ 0.2246,  1.5469, -0.9805, -1.0234, -0.6406],
        [ 0.3652,  1.8828, -0.4004, -1.0000, -0.8047],
        [-0.0894,  1.9844, -0.5039, -0.7656, -0.5820],
        [ 0.1348,  1.6016, -0.2812, -1.1641, -0.5273],
        [ 0.2256,  1.9062, -0.7305, -0.9219, -0.7656],
        [-0.0033,  1.1406, -0.8281, -0.8359, -0.3340],
        [ 0.5664,  0.8633, -1.1562, -0.5781,  0.1182],
        [ 0.1846,  1.8203, -0.7734, -0.5156, -0.3086],
        [-0.0505,  1.7734, -0.8359, -0.9609, -0.4121],
        [-0.0593,  1.8203, -0.9180, -1.0859, -0.3730]], device='cuda:0',
       dtype=torch.bfloat16, grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None), SequenceClassifierOutput(loss=tensor(0.5889, device='cuda:0', grad_fn=<NllLossBackward0>), logits=tensor([[-0.0574,  1.8125, -0.7891, -0.5977, -0.4023],
        [ 0.2539,  1.5000, -0.4062, -0.7852, -0.5312],
        [ 0.1367,  1.5391, -0.8555, -1.1875, -0.4219],
        [ 0.1846,  1.1172, -1.3828, -1.1172, -0.0664],
        [ 0.2676,  1.6484, -0.7188, -1.0469, -0.6523],
        [-0.1045,  1.8750, -0.9062, -1.0469, -0.5820],
        [-0.0317,  1.3047, -0.9648, -1.0469, -0.4238],
        [-0.3340,  1.7109, -0.9688, -0.7656, -0.7422],
        [ 0.1699,  1.7188, -0.2393, -0.8008, -0.8125],
        [ 0.0300,  1.7812, -0.9688, -1.1484, -0.7969],
        [-0.1787,  1.8594, -0.8906, -0.8281, -0.4922],
        [-0.0258,  1.6328, -0.5195, -1.0859, -0.6484],
        [ 0.0344,  1.1719, -0.4375, -0.3848, -0.5156],
        [ 0.0540,  1.4922, -0.7031, -0.6836, -0.4414],
        [ 0.3008,  1.6875, -0.6680, -0.6992, -0.4297],
        [-0.0496,  1.7812, -0.2637, -0.8359, -0.9453]], device='cuda:0',
       dtype=torch.bfloat16, grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None), SequenceClassifierOutput(loss=tensor(0.3882, device='cuda:0', grad_fn=<NllLossBackward0>), logits=tensor([[ 3.8867e-01,  1.6406e+00, -9.9219e-01, -1.0625e+00, -3.9844e-01],
        [ 5.6396e-02,  1.5938e+00, -4.4531e-01, -1.0938e+00, -9.1406e-01],
        [-1.2158e-01,  1.7734e+00, -6.6797e-01, -9.6094e-01, -7.9297e-01],
        [-4.3213e-02,  1.7109e+00, -6.9141e-01, -1.1875e+00, -1.0234e+00],
        [ 2.3438e-01,  1.8516e+00, -5.4297e-01, -1.1719e+00, -7.3438e-01],
        [-6.0059e-02,  1.8906e+00, -5.4297e-01, -1.0547e+00, -7.3828e-01],
        [ 2.7930e-01,  1.5703e+00, -8.0078e-01, -8.9062e-01, -3.0664e-01],
        [-3.0859e-01,  1.5625e+00, -8.7109e-01, -1.0000e+00, -5.7031e-01],
        [ 7.0312e-02,  1.7812e+00, -7.8516e-01, -8.6719e-01, -3.6719e-01],
        [ 6.5613e-04,  1.3672e+00, -8.5156e-01, -8.9844e-01, -5.5078e-01],
        [-4.4678e-02,  1.3984e+00, -8.4766e-01, -7.7734e-01, -8.4375e-01],
        [ 3.3594e-01,  1.6172e+00, -6.7188e-01, -9.5703e-01, -4.3555e-01],
        [-5.1172e-01,  1.5234e+00, -7.3438e-01, -1.0547e+00, -6.6016e-01],
        [-1.4453e-01,  1.7734e+00, -7.1875e-01, -9.2578e-01, -4.7461e-01],
        [-7.3730e-02,  1.6328e+00, -4.8242e-01, -6.5234e-01, -4.8828e-01],
        [-1.3867e-01,  1.6641e+00, -5.1562e-01, -1.1797e+00, -5.1172e-01]],
       device='cuda:0', dtype=torch.bfloat16, grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None), SequenceClassifierOutput(loss=tensor(0.8373, device='cuda:0', grad_fn=<NllLossBackward0>), logits=tensor([[-0.2188,  1.4453, -0.3398, -0.8555, -0.8984],
        [ 0.0281,  1.6094, -0.6367, -0.8477, -0.7617],
        [ 0.1455,  1.5703, -0.6055, -1.0703, -0.8086],
        [ 0.1631,  1.3359, -0.4219, -0.9922, -0.6953],
        [ 0.1152,  1.2344, -0.5039, -0.9609, -0.6250],
        [ 0.2969,  1.5781, -0.7969, -0.9219, -0.4746],
        [ 0.0299,  1.6094, -0.7656, -1.2031, -0.9414],
        [ 0.8438,  0.7852, -0.8008, -0.7344,  0.3262],
        [ 0.1069,  1.7500, -0.8711, -0.9805, -0.9180],
        [-0.0339,  1.0078, -0.8477, -0.5312, -0.4199],
        [ 0.0327,  1.7109, -0.7891, -0.6914, -0.7422],
        [ 0.1895,  1.4922, -0.9375, -0.6172, -0.7852],
        [ 0.1611,  1.4609, -0.7891, -0.8750, -0.8477],
        [ 0.2178,  1.6562, -0.7930, -0.5195, -0.3652],
        [ 0.1738,  1.8125, -0.8047, -0.7109, -0.4766],
        [-0.0815,  1.6016, -0.8906, -1.1094, -0.7969]], device='cuda:0',
       dtype=torch.bfloat16, grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None), SequenceClassifierOutput(loss=tensor(0.6051, device='cuda:0', grad_fn=<NllLossBackward0>), logits=tensor([[-9.5215e-02,  1.3672e+00, -5.7031e-01, -4.7461e-01, -7.6953e-01],
        [-2.8516e-01,  1.6250e+00, -4.8047e-01, -9.2578e-01, -7.6172e-01],
        [ 5.3516e-01,  1.5391e+00, -7.2266e-01, -7.7734e-01, -6.4453e-01],
        [ 3.7305e-01,  1.3281e+00, -7.3438e-01, -8.2422e-01, -5.0781e-01],
        [-6.4453e-02,  1.5469e+00, -9.0234e-01, -8.3984e-01, -6.2891e-01],
        [ 1.4551e-01,  1.7656e+00, -5.8594e-01, -7.7344e-01, -4.2188e-01],
        [ 9.0820e-02,  1.6250e+00, -6.9922e-01, -7.3438e-01, -5.6641e-01],
        [ 1.1572e-01,  1.5312e+00, -9.6094e-01, -9.6094e-01, -6.4844e-01],
        [ 1.5527e-01,  1.6250e+00, -7.5781e-01, -7.8516e-01, -4.6484e-01],
        [ 6.3477e-02,  1.8438e+00, -4.5898e-01, -9.0234e-01, -4.2188e-01],
        [-1.1621e-01,  1.6641e+00, -7.8906e-01, -1.1406e+00, -4.6875e-01],
        [-1.7700e-03,  1.3750e+00, -7.1484e-01, -9.2969e-01, -5.3125e-01],
        [ 1.0742e-01,  1.5859e+00, -5.9766e-01, -8.1641e-01, -5.1953e-01],
        [-5.0537e-02,  1.2656e+00, -6.9922e-01, -9.6484e-01, -6.5625e-01],
        [ 2.5977e-01,  1.5156e+00, -7.8516e-01, -8.4766e-01, -6.4844e-01],
        [ 2.9102e-01,  1.3281e+00, -9.1797e-01, -8.7500e-01, -5.7422e-01]],
       device='cuda:0', dtype=torch.bfloat16, grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None), SequenceClassifierOutput(loss=tensor(0.4753, device='cuda:0', grad_fn=<NllLossBackward0>), logits=tensor([[-0.0139,  1.4453, -0.6562, -0.8047, -0.6445],
        [ 0.3750,  1.3359, -0.7812, -1.1328, -1.2812],
        [ 0.2402,  1.7812, -0.7930, -1.0391, -0.5664],
        [ 0.3379,  1.4453, -0.4961, -1.0156, -0.4434],
        [ 0.2236,  1.7422, -1.1797, -0.9766, -0.6680],
        [-0.1279,  1.9062, -0.3301, -0.8086, -0.6797],
        [-0.2500,  2.0312, -0.7383, -1.0703, -0.7773],
        [ 0.2793,  1.2734, -0.6562, -0.9375, -0.9062],
        [ 0.0659,  1.7266, -1.0703, -0.7578, -0.1689],
        [ 0.2305,  1.6562, -0.8125, -0.8945, -0.6484],
        [ 0.2334,  1.5000, -0.7148, -0.7305, -0.7188],
        [ 0.0216,  1.8281, -0.5977, -0.8086, -0.5117],
        [ 0.0206,  1.5859, -0.6953, -0.7734, -0.6133],
        [ 0.1543,  1.6250, -0.7031, -1.0156, -0.6992],
        [-0.1748,  1.6797, -0.4219, -1.1094, -0.7344],
        [-0.0884,  1.3359, -0.6328, -0.9609, -0.8789]], device='cuda:0',
       dtype=torch.bfloat16, grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None), SequenceClassifierOutput(loss=tensor(0.5719, device='cuda:0', grad_fn=<NllLossBackward0>), logits=tensor([[-0.1455,  1.6172, -0.5117, -0.5547, -0.5508],
        [-0.1436,  1.6953, -1.0078, -1.3594, -0.5859],
        [-0.2031,  1.7344, -0.6953, -0.7578, -0.5664],
        [-0.0583,  1.5938, -0.5859, -0.9883, -0.7852],
        [-0.0732,  1.8281, -0.9023, -0.8945, -1.1719],
        [-0.0209,  1.6328, -0.8242, -1.2188, -0.6055],
        [ 0.3027,  1.5234, -0.7422, -0.9141, -0.6367],
        [-0.2178,  1.3750, -0.4980, -0.8086, -0.7266],
        [ 0.1216,  1.4609, -0.8477, -0.7852, -0.4199],
        [-0.2754,  1.7109, -0.6445, -1.1250, -0.5625],
        [ 0.0698,  1.5391, -0.4551, -0.5781, -0.9062],
        [-0.3691,  1.3750, -0.4297, -0.8789, -0.5117],
        [ 0.4863,  0.8008, -1.0234, -0.5312,  0.3086],
        [-0.0747,  1.1953, -0.3555, -0.7695, -0.7617],
        [-0.3730,  1.5938, -0.8672, -0.6875, -0.6445],
        [-0.2305,  1.3281, -0.5781, -1.1250, -0.8047]], device='cuda:0',
       dtype=torch.bfloat16, grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None), SequenceClassifierOutput(loss=tensor(0.5027, device='cuda:0', grad_fn=<NllLossBackward0>), logits=tensor([[ 0.0306,  1.6562, -0.8242, -1.1172, -0.3223],
        [-0.1196,  1.4766, -0.5586, -1.0781, -0.4688],
        [ 0.0095,  1.5859, -0.8359, -1.0781, -0.4629],
        [ 0.3613,  1.5859, -0.8047, -1.2031, -0.5703],
        [ 0.1680,  1.6797, -0.8828, -1.0078, -0.4023],
        [-0.0869,  1.6719, -0.6211, -0.9102, -0.5469],
        [-0.0203,  1.3984, -0.6328, -0.8672, -0.9453],
        [-0.1143,  1.6562, -0.8828, -0.8633, -0.5430],
        [-0.2480,  1.3438, -0.5938, -1.0859, -0.8594],
        [ 0.4492,  1.6016, -0.7500, -0.6250, -0.4922],
        [ 0.1494,  1.6172, -0.7891, -0.9883, -0.4355],
        [ 0.1670,  1.7656, -0.7969, -0.9141, -0.3711],
        [ 0.2158,  1.6875, -0.7266, -1.1406, -0.3574],
        [ 0.0693,  1.6641, -1.0000, -1.2734, -0.6406],
        [ 0.5273,  1.6328, -0.9102, -0.6758, -0.6055],
        [ 0.1318,  1.2109, -1.0781, -1.1797, -0.3008]], device='cuda:0',
       dtype=torch.bfloat16, grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None), SequenceClassifierOutput(loss=tensor(0.7574, device='cuda:0', grad_fn=<NllLossBackward0>), logits=tensor([[ 0.2734,  1.5234, -0.6289, -0.8516, -0.9570],
        [-0.0552,  1.6328, -0.8242, -0.9180, -0.4980],
        [-0.0070,  1.5000, -0.7734, -0.9961, -0.6523],
        [ 0.0243,  1.6562, -0.2256, -0.9727, -0.7031],
        [ 0.4141,  1.0156, -1.0781, -1.0391,  0.3867],
        [ 0.4082,  1.2109, -0.4648, -0.7578, -0.3750],
        [-0.0159,  1.5156, -1.0547, -1.0156, -0.4805],
        [-0.2080,  1.5547, -0.8242, -0.6758, -0.5312],
        [ 0.2812,  1.3906, -0.9766, -0.9531, -0.7734],
        [-0.2871,  1.5156, -0.6758, -0.6133, -0.7344],
        [ 0.1445,  1.6172, -1.1094, -0.8438, -0.3496],
        [ 0.0066,  1.7500, -1.0234, -1.0469, -0.6172],
        [-0.1318,  1.7500, -0.7266, -0.9688, -0.8281],
        [ 0.1865,  1.4922, -0.8047, -1.0156, -0.6211],
        [-0.1865,  1.4766, -0.2754, -0.4492, -0.7344],
        [ 0.1768,  1.4688, -0.8281, -0.9375, -0.6406]], device='cuda:0',
       dtype=torch.bfloat16, grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None), SequenceClassifierOutput(loss=tensor(0.6106, device='cuda:0', grad_fn=<NllLossBackward0>), logits=tensor([[-0.0879,  1.6875, -0.8828, -0.4570, -0.5977],
        [-0.0762,  1.8984, -0.9258, -1.0625, -0.2891],
        [-0.0898,  1.6562, -0.7695, -0.9102, -0.5898],
        [-0.1016,  1.9297, -0.7500, -0.9180, -0.6602],
        [-0.2656,  1.9766, -0.6484, -0.7656, -0.6836],
        [ 0.0889,  1.6875, -0.8984, -0.9375, -0.4082],
        [-0.1582,  1.7344, -0.3711, -0.8984, -0.3633],
        [ 0.1660,  1.4766, -0.8320, -0.8555, -0.5898],
        [ 0.0811,  1.2188, -0.8086, -0.4375, -0.3242],
        [ 0.3496,  1.2891, -0.6836, -1.1328, -0.6445],
        [ 0.2930,  1.3438, -0.8164, -0.9492, -0.6758],
        [-0.0554,  1.9141, -0.6523, -0.9180, -0.6484],
        [ 0.2715,  1.1094, -0.6797, -0.7852,  0.0781],
        [-0.0679,  1.4688, -0.6406, -0.9297, -0.5312],
        [-0.0157,  1.5312, -0.5312, -0.8438, -0.7188],
        [ 0.2637,  1.7578, -0.6641, -1.0078, -0.4590]], device='cuda:0',
       dtype=torch.bfloat16, grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None), SequenceClassifierOutput(loss=tensor(0.8368, device='cuda:0', grad_fn=<NllLossBackward0>), logits=tensor([[ 0.2754,  1.6562, -0.5195, -1.1250, -1.0703],
        [ 0.3730,  1.8359, -1.1250, -1.0781, -0.6719],
        [ 0.4453,  1.5312, -0.7031, -0.8438, -0.4902],
        [ 0.2695,  1.5859, -0.7852, -0.9688, -0.5508],
        [ 0.1445,  1.5859, -1.0469, -0.8281, -0.5430],
        [-0.0623,  1.4453, -0.5898, -0.8242, -0.6836],
        [ 0.1660,  1.6172, -0.8750, -1.2891, -0.5547],
        [ 0.0776,  1.4062, -0.8594, -1.0547, -0.8047],
        [-0.2441,  1.4375, -0.6641, -1.2031, -1.0312],
        [-0.1758,  1.5000, -0.4199, -0.7109, -0.6992],
        [ 0.2852,  1.3359, -1.0547, -0.9531, -0.0742],
        [ 0.2637,  1.7031, -0.4785, -0.6797, -0.5039],
        [ 0.1807,  1.4141, -0.9570, -0.9102, -0.4453],
        [ 0.0540,  0.9805, -0.5625, -0.6992, -0.4375],
        [ 0.2871,  1.7031, -0.8047, -0.9102, -0.5547],
        [-0.1201,  1.9062, -0.7031, -0.6992, -0.5781]], device='cuda:0',
       dtype=torch.bfloat16, grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None), SequenceClassifierOutput(loss=tensor(0.7937, device='cuda:0', grad_fn=<NllLossBackward0>), logits=tensor([[ 2.1777e-01,  1.9844e+00, -1.0781e+00, -1.1641e+00, -4.8438e-01],
        [-8.9844e-02,  1.5547e+00, -8.0859e-01, -1.1094e+00, -6.9141e-01],
        [ 2.0703e-01,  1.6094e+00, -7.8125e-01, -7.3047e-01, -4.9609e-01],
        [ 3.3936e-02,  1.5859e+00, -6.5234e-01, -1.1953e+00, -6.8750e-01],
        [-2.0020e-01,  1.6641e+00, -6.0938e-01, -9.5312e-01, -3.8672e-01],
        [ 1.5945e-03,  1.6797e+00, -7.1875e-01, -1.0469e+00, -6.3281e-01],
        [ 3.5742e-01,  1.5547e+00, -9.5703e-01, -1.1328e+00, -6.6406e-01],
        [-1.3672e-01,  1.7422e+00, -8.6328e-01, -8.2031e-01, -5.5078e-01],
        [ 6.2256e-02,  1.8906e+00, -8.7891e-01, -7.8906e-01, -4.2773e-01],
        [-4.6484e-01,  1.6250e+00, -9.3750e-01, -7.4219e-01, -6.4453e-01],
        [ 5.7129e-02,  1.7344e+00, -7.3828e-01, -1.0859e+00, -5.8594e-01],
        [ 1.4062e-01,  1.9766e+00, -1.0469e+00, -7.8516e-01, -4.8438e-01],
        [ 2.3242e-01,  1.3359e+00, -7.5000e-01, -7.4609e-01, -5.6250e-01],
        [ 5.3906e-01,  1.5625e+00, -7.2266e-01, -8.5547e-01, -6.6797e-01],
        [ 1.9043e-01,  1.7109e+00, -5.1562e-01, -9.8828e-01, -4.6289e-01],
        [ 9.7656e-02,  1.1250e+00, -4.1992e-01, -5.7812e-01, -6.7578e-01]],
       device='cuda:0', dtype=torch.bfloat16, grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None), SequenceClassifierOutput(loss=tensor(0.5574, device='cuda:0', grad_fn=<NllLossBackward0>), logits=tensor([[ 0.3652,  1.9219, -0.5273, -0.8711, -0.6406],
        [ 0.1885,  1.6562, -0.9141, -1.0312, -0.4004],
        [ 0.0212,  1.9844, -0.9844, -0.8398, -0.6367],
        [ 0.0334,  1.2422, -0.7773, -1.0859, -0.7188],
        [-0.4473,  1.4297, -0.3867, -0.9453, -0.7031],
        [ 0.2305,  1.2109, -1.1953, -0.8945,  0.1650],
        [ 0.3340,  1.8047, -0.9766, -0.9805, -0.5898],
        [ 0.1270,  1.7812, -0.8086, -0.8516, -0.5508],
        [ 0.1680,  0.8203, -0.8672, -0.5117, -0.5195],
        [-0.0767,  1.5234, -0.6289, -1.0547, -0.6875],
        [-0.3730,  1.5078, -0.7344, -0.8047, -0.5547],
        [ 0.3594,  1.3828, -1.2031, -0.8477, -0.8477],
        [ 0.0214,  1.5469, -0.5781, -0.7734, -0.8320],
        [-0.0292,  1.9453, -1.0234, -0.9414, -0.5625],
        [ 0.3379,  1.6250, -0.7695, -1.1562, -0.3320],
        [ 0.4531,  1.3750, -0.9023, -0.9062, -0.3496]], device='cuda:0',
       dtype=torch.bfloat16, grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None), SequenceClassifierOutput(loss=tensor(0.7612, device='cuda:0', grad_fn=<NllLossBackward0>), logits=tensor([[ 4.5703e-01,  1.2812e+00, -1.0391e+00, -1.0703e+00, -7.4219e-01],
        [ 1.1368e-03,  1.8203e+00, -3.8477e-01, -7.3438e-01, -6.3672e-01],
        [ 1.7969e-01,  1.5703e+00, -1.0547e+00, -8.5547e-01, -5.7031e-01],
        [-2.4121e-01,  1.5391e+00, -5.2344e-01, -1.1406e+00, -9.0625e-01],
        [-1.4551e-01,  1.7734e+00, -7.0312e-01, -5.8594e-01, -7.7344e-01],
        [ 2.0312e-01,  1.7031e+00, -5.7812e-01, -5.6250e-01, -6.6797e-01],
        [ 1.5039e-01,  1.8203e+00, -8.0078e-01, -8.8281e-01, -8.3594e-01],
        [-6.2012e-02,  1.4141e+00, -4.7070e-01, -5.3125e-01, -8.0859e-01],
        [ 2.3730e-01,  1.8672e+00, -8.1250e-01, -1.0469e+00, -6.5625e-01],
        [ 1.8945e-01,  1.6094e+00, -1.0234e+00, -1.0156e+00, -5.6250e-01],
        [ 4.1797e-01,  1.7812e+00, -9.1406e-01, -8.1641e-01, -4.1602e-01],
        [-4.2480e-02,  1.3906e+00, -5.5859e-01, -7.0312e-01, -1.9531e-01],
        [ 5.8838e-02,  1.2734e+00, -9.6484e-01, -7.0703e-01, -3.4375e-01],
        [ 1.6992e-01,  1.3438e+00, -8.8672e-01, -9.8828e-01, -4.2578e-01],
        [-4.0283e-02,  1.5625e+00, -1.0391e+00, -8.5156e-01, -3.9258e-01],
        [-8.3008e-03,  1.3281e+00, -7.7344e-01, -5.8203e-01, -1.0234e+00]],
       device='cuda:0', dtype=torch.bfloat16, grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None), SequenceClassifierOutput(loss=tensor(0.8414, device='cuda:0', grad_fn=<NllLossBackward0>), logits=tensor([[ 0.2422,  1.6484, -0.6250, -0.8477, -0.7344],
        [ 0.2617,  1.6875, -0.8086, -0.7227, -0.4219],
        [ 0.2246,  1.7344, -0.7266, -0.8164, -0.4121],
        [ 0.0601,  1.5000, -0.8164, -0.7383, -0.4590],
        [ 0.1377,  1.7422, -0.7188, -0.9297, -0.4609],
        [ 0.2109,  1.2891, -0.6758, -0.6172, -0.6953],
        [-0.1426,  1.6250, -0.5859, -0.7227, -0.5508],
        [ 0.1504,  1.5391, -0.5430, -0.7539, -0.5352],
        [-0.3320,  1.7266, -0.9141, -0.9297, -0.5703],
        [ 0.0850,  1.7188, -0.8281, -1.0469, -0.6250],
        [ 0.1011,  1.3438, -0.9609, -0.9609, -0.4785],
        [ 0.0134,  1.4922, -0.8086, -0.9258, -0.4824],
        [ 0.4199,  0.9414, -0.3008, -0.8164, -0.4473],
        [-0.1494,  1.3203, -0.6484, -0.6680, -0.5312],
        [ 0.1875,  1.1719, -1.2578, -1.2578, -0.2520],
        [-0.1260,  1.5547, -0.5977, -0.8125, -0.5430]], device='cuda:0',
       dtype=torch.bfloat16, grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None), SequenceClassifierOutput(loss=tensor(0.6097, device='cuda:0', grad_fn=<NllLossBackward0>), logits=tensor([[ 0.1699,  1.9922, -0.6992, -0.7344, -0.7070],
        [ 0.0454,  1.7031, -0.4766, -1.0469, -0.7109],
        [-0.0659,  1.6797, -0.3594, -1.0938, -0.6289],
        [ 0.0693,  1.6719, -0.9102, -0.7852, -0.2969],
        [ 0.4375,  1.0547, -1.2812, -0.6328,  0.5234],
        [-0.4727,  1.4766, -0.4961, -0.9727, -0.5117],
        [ 0.0898,  1.5703, -0.6133, -1.2188, -0.5117],
        [-0.1167,  1.4297, -0.4609, -0.7734, -0.3789],
        [ 0.0684,  1.9141, -0.8711, -1.0156, -0.4863],
        [-0.1953,  1.6797, -0.6797, -0.7930, -0.5898],
        [ 0.0796,  0.8477, -0.6289, -0.9531, -0.4785],
        [ 0.1602,  1.5078, -0.6445, -1.0078, -0.6992],
        [ 0.4082,  1.4609, -0.8789, -1.0469, -0.6836],
        [ 0.1162,  1.8359, -0.7227, -1.0547, -0.5977],
        [-0.0845,  1.9141, -1.1250, -1.0078, -0.6641],
        [ 0.1309,  1.5234, -0.8789, -0.8086, -0.1748]], device='cuda:0',
       dtype=torch.bfloat16, grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None), SequenceClassifierOutput(loss=tensor(0.7714, device='cuda:0', grad_fn=<NllLossBackward0>), logits=tensor([[ 0.0698,  1.7656, -0.3574, -0.9531, -0.7969],
        [ 0.0400,  1.4219, -0.5000, -0.8672, -0.4746],
        [ 0.2773,  1.5234, -0.7734, -0.8203, -0.4453],
        [-0.1865,  1.5234, -0.7500, -1.0156, -0.9531],
        [-0.0039,  1.5625, -1.0703, -0.9062, -0.7656],
        [-0.0298,  1.4531, -0.8633, -0.7305, -0.7109],
        [ 0.1689,  1.6562, -0.8203, -1.0703, -0.4902],
        [ 0.1943,  1.3828, -1.0312, -0.8906, -0.7500],
        [ 0.2344,  1.4453, -0.8555, -0.9531, -0.6719],
        [ 0.0618,  1.0469, -0.9336, -0.9258, -0.5625],
        [ 0.1406,  1.2734, -1.2109, -0.6250, -0.2852],
        [ 0.1475,  1.4453, -0.4453, -0.7734, -0.3242],
        [ 0.1816,  1.6016, -0.5547, -0.8906, -0.4609],
        [ 0.3359,  1.3281, -0.6367, -0.8594, -0.4570],
        [ 0.1328,  1.6641, -0.5312, -0.8438, -0.5898],
        [ 0.0374,  1.6719, -0.9922, -0.8906, -0.4668]], device='cuda:0',
       dtype=torch.bfloat16, grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None), SequenceClassifierOutput(loss=tensor(1.0531, device='cuda:0', grad_fn=<NllLossBackward0>), logits=tensor([[ 0.2334,  1.4844, -0.5156, -1.0859, -0.8281],
        [-0.3203,  1.4062, -0.7031, -1.1641, -0.6602],
        [ 0.3633,  1.9219, -0.7891, -0.6719, -0.2285],
        [ 0.1777,  1.2734, -0.2314, -0.8594, -0.6172],
        [ 0.2080,  1.9297, -0.9336, -0.9102, -0.6445],
        [-0.0054,  1.4922, -1.0859, -0.9648, -0.8008],
        [ 0.1045,  1.3438, -0.6680, -0.6445, -0.6758],
        [-0.4160,  1.7891, -0.6914, -1.0156, -0.5195],
        [ 0.4922,  1.6641, -0.6875, -0.5742, -0.2617],
        [ 0.4668,  1.4219, -0.7305, -0.6133, -0.4395],
        [-0.1553,  2.0156, -0.7617, -1.0625, -0.5195],
        [ 0.0972,  1.5391, -0.9336, -1.2031, -0.6211],
        [ 0.1924,  1.5078, -0.5430, -0.7500, -0.7109],
        [ 0.3066,  1.5234, -0.5820, -0.8203, -0.5898],
        [-0.0303,  1.7031, -0.4805, -0.9023, -0.4980],
        [-0.0310,  1.6797, -0.7695, -1.0781, -0.6641]], device='cuda:0',
       dtype=torch.bfloat16, grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None), SequenceClassifierOutput(loss=tensor(0.8700, device='cuda:0', grad_fn=<NllLossBackward0>), logits=tensor([[ 0.0957,  1.5391, -1.0781, -1.3750,  0.1143],
        [ 0.2236,  1.2656, -0.9961, -0.6367, -0.4883],
        [ 0.1055,  1.8828, -0.6094, -0.8711, -0.5938],
        [ 0.1128,  1.8516, -0.7930, -1.0781, -0.6992],
        [ 0.0947,  1.3984, -0.7734, -1.0469, -0.4199],
        [ 0.2656,  1.5391, -0.9375, -0.8984, -0.5273],
        [ 0.4805,  1.5000, -1.1328, -0.9492, -0.5078],
        [ 0.0640,  1.5469, -0.4648, -0.8711, -0.9492],
        [ 0.0115,  1.2969, -0.6094, -0.9023, -0.7188],
        [-0.0767,  1.6094, -0.7500, -1.0000, -0.2832],
        [ 0.2812,  1.9922, -0.6797, -0.8516, -0.4199],
        [ 0.2158,  1.4297, -0.5547, -0.7734, -0.9492],
        [ 0.1377,  1.6406, -0.7617, -0.8828, -0.6289],
        [ 0.0718,  1.3750, -0.5352, -0.5234, -0.4395],
        [ 0.0452,  1.7500, -0.5625, -0.6758, -0.4414],
        [-0.2324,  1.6016, -1.0234, -0.7188, -0.4668]], device='cuda:0',
       dtype=torch.bfloat16, grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None), SequenceClassifierOutput(loss=tensor(0.5889, device='cuda:0', grad_fn=<NllLossBackward0>), logits=tensor([[-9.2285e-02,  1.7578e+00, -5.8594e-01, -1.1094e+00, -6.4844e-01],
        [-2.7847e-04,  1.4531e+00, -6.2891e-01, -9.0625e-01, -9.0234e-01],
        [-2.7148e-01,  1.5547e+00, -4.9219e-01, -5.6641e-01, -5.3516e-01],
        [ 2.0898e-01,  1.3906e+00, -7.9297e-01, -1.1016e+00, -4.9219e-01],
        [-3.6523e-01,  1.4375e+00, -4.3945e-01, -8.8281e-01, -4.5508e-01],
        [ 4.5312e-01,  1.1875e+00, -6.6797e-01, -9.6875e-01, -6.6406e-01],
        [ 4.1602e-01,  1.5703e+00, -7.3828e-01, -1.2422e+00, -4.1211e-01],
        [ 1.3867e-01,  1.6094e+00, -9.6875e-01, -1.0234e+00, -3.9258e-01],
        [ 9.6680e-02,  1.7734e+00, -4.5117e-01, -1.0156e+00, -5.7031e-01],
        [ 1.9043e-01,  1.7656e+00, -8.0469e-01, -1.1016e+00, -7.8125e-01],
        [ 1.9922e-01,  1.4453e+00, -1.2422e+00, -1.1094e+00, -2.7148e-01],
        [-2.1387e-01,  1.7734e+00, -5.7812e-01, -8.3594e-01, -2.0605e-01],
        [ 3.8086e-01,  1.5938e+00, -7.3438e-01, -1.1953e+00, -3.3398e-01],
        [-1.3574e-01,  1.4766e+00, -5.8594e-01, -6.6406e-01, -6.9922e-01],
        [-5.3711e-02,  1.2031e+00, -6.7578e-01, -9.0234e-01, -5.4297e-01],
        [ 3.3203e-01,  1.1250e+00, -1.0547e+00, -8.5938e-01, -4.2969e-01]],
       device='cuda:0', dtype=torch.bfloat16, grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None), SequenceClassifierOutput(loss=tensor(0.7902, device='cuda:0', grad_fn=<NllLossBackward0>), logits=tensor([[-0.1455,  1.9844, -0.7812, -0.6953, -0.5625],
        [ 0.0129,  2.0625, -1.1172, -0.8398, -0.4707],
        [ 0.2617,  1.5625, -0.5195, -1.0391, -0.5195],
        [ 0.0070,  1.4453, -0.5117, -0.9219, -0.3652],
        [ 0.4023,  1.5312, -0.9609, -1.1016, -0.6914],
        [ 0.2012,  1.8047, -1.0234, -0.9766, -0.4609],
        [ 0.1328,  1.5000, -0.3359, -0.6562, -0.7617],
        [ 0.1216,  1.1719, -0.6445, -0.6445, -0.5039],
        [-0.1182,  1.3906, -0.7578, -1.0625, -0.8789],
        [ 0.2520,  1.6797, -0.7656, -1.0469, -0.7422],
        [ 0.0703,  1.6328, -0.8750, -0.8594, -0.6094],
        [ 0.2402,  1.0312, -0.7617, -0.9453, -0.6797],
        [-0.0332,  1.7031, -0.3672, -0.7852, -0.6523],
        [-0.0178,  1.7812, -0.9727, -0.6992, -0.4629],
        [ 0.2695,  1.5312, -0.8438, -0.9336, -0.3516],
        [ 0.2637,  1.8125, -0.6719, -1.0078, -0.5078]], device='cuda:0',
       dtype=torch.bfloat16, grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None), SequenceClassifierOutput(loss=tensor(0.6670, device='cuda:0', grad_fn=<NllLossBackward0>), logits=tensor([[ 0.0322,  1.7891, -0.6484, -0.8867, -0.5664],
        [-0.2109,  1.6406, -0.8672, -0.7227, -0.5742],
        [ 0.1807,  1.3672, -0.7539, -0.7852, -0.3594],
        [-0.0049,  1.6016, -0.7148, -0.6367, -0.3203],
        [ 0.5352,  1.5312, -0.5430, -0.6250, -0.5469],
        [-0.2988,  1.2422, -0.3730, -0.7812, -0.3633],
        [ 0.0242,  1.4531, -0.5938, -1.1250, -0.4629],
        [ 0.0610,  1.9062, -1.1328, -0.8398, -0.7227],
        [ 0.2080,  1.3594, -0.7422, -0.6133, -0.4902],
        [ 0.0060,  1.8516, -0.5977, -0.8828, -0.7461],
        [-0.0520,  1.5312, -0.6289, -0.8203, -0.6836],
        [-0.1836,  1.7891, -0.8789, -1.0781, -0.4082],
        [-0.0286,  1.6797, -0.8008, -0.7305, -0.3203],
        [ 0.4492,  1.0547, -1.1641, -0.9258, -0.2773],
        [ 0.0291,  1.8438, -0.5508, -1.0859, -0.7422],
        [ 0.2910,  1.9219, -0.8633, -0.8242, -0.5820]], device='cuda:0',
       dtype=torch.bfloat16, grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None), SequenceClassifierOutput(loss=tensor(0.6337, device='cuda:0', grad_fn=<NllLossBackward0>), logits=tensor([[-0.0767,  1.7578, -0.6875, -1.0469, -0.2969],
        [ 0.0708,  1.1328, -0.8711, -0.9648, -0.4453],
        [ 0.0272,  1.9844, -0.9258, -1.0391, -0.4727],
        [-0.0576,  1.9375, -0.8984, -1.1016, -0.3750],
        [ 0.3008,  1.5703, -0.4961, -1.0391, -0.7812],
        [ 0.3145,  1.4141, -1.2422, -0.9102, -0.1387],
        [-0.3555,  1.7344, -0.4961, -0.9102, -0.7656],
        [ 0.2363,  1.5234, -0.8477, -0.8984, -0.5547],
        [ 0.1299,  1.8516, -0.9492, -0.7227, -0.5117],
        [ 0.1406,  1.3750, -0.8477, -0.7734, -0.6094],
        [-0.0322,  1.4844, -0.8711, -1.0078, -0.6094],
        [ 0.1069,  1.5078, -0.9336, -0.9609, -0.7227],
        [ 0.1089,  1.3047, -0.4121, -1.3125, -1.1094],
        [-0.2598,  1.1797, -0.6523, -0.3555, -0.7578],
        [-0.0669,  1.5469, -0.6719, -1.0781, -0.6484],
        [-0.1162,  2.2031, -0.9219, -1.0547, -0.4512]], device='cuda:0',
       dtype=torch.bfloat16, grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None), SequenceClassifierOutput(loss=tensor(0.7935, device='cuda:0', grad_fn=<NllLossBackward0>), logits=tensor([[ 0.2969,  1.2422, -1.0625, -0.6289, -0.5547],
        [-0.3105,  1.5000, -0.4805, -0.8867, -0.3477],
        [-0.0226,  1.4609, -0.9258, -0.9375, -0.6289],
        [ 0.1152,  1.8906, -0.8984, -0.7891, -0.3340],
        [-0.1562,  1.3672, -0.6406, -1.1016, -0.7031],
        [ 0.1904,  1.4609, -0.6953, -0.6445, -0.1777],
        [ 0.4160,  1.1641, -1.3203, -0.9297, -0.7969],
        [-0.0147,  1.5938, -0.7227, -0.8125, -0.7305],
        [ 0.0095,  1.9453, -0.4395, -0.8008, -0.7422],
        [ 0.0762,  1.2812, -0.9727, -0.6992, -0.4844],
        [ 0.0030,  1.8281, -1.0938, -1.2266, -0.6797],
        [ 0.1250,  1.5547, -0.7266, -0.8203, -0.2871],
        [ 0.0129,  1.4297, -0.7617, -0.8398, -0.3340],
        [-0.1064,  1.3594, -0.6523, -1.1328, -0.6914],
        [ 0.2363,  1.6719, -0.5352, -0.9453, -0.6445],
        [ 0.3535,  1.4375, -0.6641, -0.8594, -0.7617]], device='cuda:0',
       dtype=torch.bfloat16, grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None), SequenceClassifierOutput(loss=tensor(0.7361, device='cuda:0', grad_fn=<NllLossBackward0>), logits=tensor([[ 0.6445,  0.5000, -0.6992, -0.2070,  0.3926],
        [ 0.2080,  1.4531, -0.7930, -0.6992, -0.2031],
        [ 0.1543,  1.7734, -1.1484, -0.9961, -0.4668],
        [ 0.1162,  1.1172, -1.0234, -0.8242, -0.6758],
        [ 0.1167,  1.8203, -0.8164, -0.9258, -0.7695],
        [-0.3477,  1.3125, -0.4707, -0.7305, -0.5898],
        [ 0.2812,  1.6484, -1.0312, -1.1016, -0.3027],
        [ 0.0067,  1.8672, -0.5078, -0.8320, -0.6484],
        [ 0.1011,  1.4922, -0.7070, -1.0156, -0.7461],
        [ 0.0815,  1.4297, -0.8555, -1.0391, -0.5234],
        [-0.0046,  1.5078, -0.9648, -0.7109, -0.2734],
        [ 0.0498,  1.4766, -0.8711, -0.9414, -0.3477],
        [ 0.3223,  1.5312, -0.7305, -0.8555, -0.6992],
        [ 0.0047,  0.7734, -1.2656, -1.0312, -0.0684],
        [-0.1279,  1.6328, -0.6836, -0.7422, -0.6289],
        [ 0.3320,  1.5547, -0.9258, -0.9141, -0.3496]], device='cuda:0',
       dtype=torch.bfloat16, grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None), SequenceClassifierOutput(loss=tensor(0.9663, device='cuda:0', grad_fn=<NllLossBackward0>), logits=tensor([[-2.9297e-02,  1.7031e+00, -5.7422e-01, -6.8359e-01, -4.3164e-01],
        [-1.5039e-01,  1.4453e+00, -8.2422e-01, -8.2812e-01, -8.5938e-01],
        [ 1.1523e-01,  1.7109e+00, -8.9062e-01, -1.1641e+00, -6.2500e-01],
        [ 1.1475e-01,  1.3984e+00, -5.9766e-01, -7.3047e-01, -8.0469e-01],
        [ 1.0147e-03,  1.8438e+00, -5.3906e-01, -9.7266e-01, -5.9766e-01],
        [ 1.3379e-01,  1.4297e+00, -5.5469e-01, -8.9844e-01, -7.5391e-01],
        [ 2.4316e-01,  1.8438e+00, -3.1836e-01, -7.1094e-01, -1.0156e+00],
        [ 1.7285e-01,  1.8750e+00, -7.5391e-01, -1.1094e+00, -7.4609e-01],
        [ 3.1641e-01,  1.4141e+00, -6.4453e-01, -1.1875e+00, -7.9297e-01],
        [-7.8613e-02,  1.7891e+00, -8.9453e-01, -9.8828e-01, -5.6641e-01],
        [ 4.6680e-01,  8.8281e-01, -7.8125e-01, -6.8750e-01,  2.6758e-01],
        [ 2.7539e-01,  1.5078e+00, -9.1016e-01, -6.3672e-01, -7.4609e-01],
        [-3.4961e-01,  1.7734e+00, -7.5391e-01, -1.1641e+00, -6.7578e-01],
        [-7.5684e-02,  1.6641e+00, -3.6133e-01, -9.0625e-01, -4.2578e-01],
        [-8.3496e-02,  1.6406e+00, -9.6484e-01, -7.4609e-01, -6.2500e-01],
        [-1.1084e-01,  1.7578e+00, -6.2109e-01, -1.0312e+00, -2.7734e-01]],
       device='cuda:0', dtype=torch.bfloat16, grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None), SequenceClassifierOutput(loss=tensor(0.6219, device='cuda:0', grad_fn=<NllLossBackward0>), logits=tensor([[-0.1079,  1.5938, -1.0234, -0.9336, -0.2676],
        [ 0.1250,  1.4844, -0.7148, -0.8867, -0.5703],
        [ 0.0698,  1.5234, -0.8633, -0.9648, -0.2832],
        [-0.1699,  1.4688, -0.7852, -0.9570, -0.5586],
        [ 0.2285,  1.7188, -0.9961, -1.0156, -0.4355],
        [ 0.1060,  1.5391, -0.7539, -0.7617, -0.5859],
        [ 0.2715,  1.5391, -0.8242, -0.8750, -0.3730],
        [-0.1113,  1.2969, -0.3086, -0.7852, -0.3340],
        [ 0.1270,  1.4766, -0.6641, -0.7812, -0.6484],
        [ 0.1738,  1.1953, -0.8555, -0.8594, -0.2051],
        [ 0.3984,  1.7500, -0.8867, -0.8008, -0.6055],
        [-0.1729,  1.7344, -0.5234, -0.7734, -0.5312],
        [-0.0261,  1.4062, -0.3945, -1.1172, -0.8945],
        [ 0.1104,  1.5391, -0.7578, -0.9336, -0.3965],
        [-0.2373,  1.7891, -0.4043, -1.2422, -0.9375],
        [-0.1611,  1.3359, -0.3457, -0.9648, -0.6094]], device='cuda:0',
       dtype=torch.bfloat16, grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None), SequenceClassifierOutput(loss=tensor(0.6688, device='cuda:0', grad_fn=<NllLossBackward0>), logits=tensor([[ 0.0732,  1.2578, -0.8398, -1.1875, -0.5820],
        [-0.0294,  1.4062, -0.4160, -0.9023, -0.4805],
        [ 0.0928,  1.3125, -0.4805, -0.9258, -0.7188],
        [ 0.2031,  1.6562, -0.6602, -0.8984, -0.5781],
        [-0.0062,  1.6016, -0.8125, -0.9180, -0.5664],
        [-0.1631,  1.7734, -0.3047, -1.5000, -0.9688],
        [ 0.0583,  1.1797, -0.8398, -0.9336, -0.6289],
        [-0.1650,  1.5859, -0.8594, -0.5547, -0.5703],
        [ 0.1484,  1.3750, -0.6289, -1.4453, -0.5352],
        [ 0.1318,  1.4375, -0.5430, -0.9297, -0.6328],
        [ 0.1060,  1.7422, -0.7617, -0.7031, -0.6250],
        [ 0.1934,  1.7891, -0.9414, -0.6992, -0.4844],
        [ 0.1699,  1.4141, -0.6562, -1.1875, -0.6445],
        [-0.1865,  1.6328, -1.0469, -1.1484, -1.0312],
        [ 0.1387,  1.4531, -0.3398, -1.0000, -0.6055],
        [ 0.3457,  1.2500, -0.8516, -0.9258, -0.3008]], device='cuda:0',
       dtype=torch.bfloat16, grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None), SequenceClassifierOutput(loss=tensor(0.5764, device='cuda:0', grad_fn=<NllLossBackward0>), logits=tensor([[ 1.6602e-01,  1.4141e+00, -3.9648e-01, -9.3359e-01, -4.7852e-01],
        [-3.6621e-02,  1.7188e+00, -5.2734e-01, -1.0234e+00, -7.5391e-01],
        [-1.6699e-01,  1.6797e+00, -4.6289e-01, -5.5469e-01, -6.7969e-01],
        [ 1.2402e-01,  1.5625e+00, -7.4609e-01, -7.8516e-01, -4.7266e-01],
        [ 1.6992e-01,  1.6406e+00, -6.7969e-01, -1.0938e+00, -6.5625e-01],
        [-1.5411e-03,  1.5312e+00, -9.6094e-01, -1.1094e+00, -2.3438e-01],
        [ 2.5977e-01,  1.5391e+00, -7.5781e-01, -1.0625e+00, -5.4688e-01],
        [ 3.5400e-02,  1.6641e+00, -7.0312e-01, -7.5000e-01, -5.2344e-01],
        [-5.0781e-02,  1.7109e+00, -7.1484e-01, -5.3906e-01, -7.0312e-01],
        [ 2.0215e-01,  1.7812e+00, -9.5703e-01, -6.0547e-01, -5.2734e-01],
        [-3.4766e-01,  1.5234e+00, -6.9531e-01, -9.2188e-01, -6.0156e-01],
        [ 5.0781e-01,  1.3203e+00, -5.7812e-01, -4.9219e-01, -5.9766e-01],
        [-1.2109e-01,  1.4219e+00, -7.5391e-01, -9.2969e-01, -4.7656e-01],
        [-2.5195e-01,  1.7031e+00, -5.2344e-01, -8.5938e-01, -3.7109e-01],
        [ 2.2461e-01,  1.2734e+00, -7.6172e-01, -1.1094e+00, -7.5391e-01],
        [ 2.5586e-01,  1.2578e+00, -3.7109e-01, -5.5859e-01, -8.8672e-01]],
       device='cuda:0', dtype=torch.bfloat16, grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None), SequenceClassifierOutput(loss=tensor(0.7067, device='cuda:0', grad_fn=<NllLossBackward0>), logits=tensor([[ 0.3848,  1.5000, -0.9297, -0.9805, -0.4707],
        [ 0.0576,  1.4609, -1.0703, -1.0547, -0.4336],
        [-0.0027,  1.8359, -0.9414, -0.9141, -0.4473],
        [-0.1641,  1.2578, -0.5664, -0.4531, -0.4570],
        [ 0.2041,  1.8672, -0.6016, -1.2734, -0.6562],
        [ 0.2490,  1.4922, -0.7266, -0.8945, -0.8008],
        [ 0.1406,  1.4844, -0.6953, -0.7539, -0.9023],
        [ 0.0238,  1.5156, -0.9570, -0.9453, -0.4668],
        [ 0.2734,  1.5391, -0.7500, -0.9961, -0.6758],
        [ 0.1172,  1.7344, -0.8398, -0.9883, -0.4316],
        [ 0.2637,  1.0703, -0.9648, -1.0859, -0.5117],
        [ 0.0957,  1.4062, -0.8242, -1.0859, -0.5703],
        [ 0.1001,  1.8594, -0.6680, -0.6406, -0.7031],
        [-0.3555,  1.7422, -0.5039, -0.6055, -0.5703],
        [ 0.1631,  2.0156, -0.8867, -0.8281, -0.4102],
        [-0.4746,  1.6484, -0.7695, -0.7070, -0.4082]], device='cuda:0',
       dtype=torch.bfloat16, grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None), SequenceClassifierOutput(loss=tensor(0.5605, device='cuda:0', grad_fn=<NllLossBackward0>), logits=tensor([[-0.0562,  1.4297, -0.6758, -1.3281, -0.5547],
        [ 0.2676,  1.8438, -0.6328, -0.7539, -0.5820],
        [ 0.0889,  1.3906, -0.7695, -0.7344, -0.4609],
        [-0.0047,  1.3047, -0.7578, -0.8555, -0.6797],
        [-0.0233,  1.3203, -0.3711, -1.0312, -0.4375],
        [ 0.0420,  1.6016, -0.1895, -0.7305, -0.8594],
        [ 0.3203,  1.5625, -0.9414, -1.0938, -0.3320],
        [ 0.2461,  1.4688, -0.6562, -0.9609, -0.6836],
        [-0.2871,  1.4453, -0.6602, -0.8594, -0.5039],
        [ 0.2100,  1.3672, -0.8242, -1.0078, -0.7070],
        [-0.1216,  1.6250, -0.6133, -1.0938, -0.6172],
        [ 0.0253,  1.2266, -0.5117, -1.0234, -0.4199],
        [ 0.4043,  1.7422, -0.8672, -1.1250, -0.5547],
        [ 0.0820,  1.5938, -0.8555, -0.6836, -0.5703],
        [ 0.0087,  1.8594, -0.5430, -1.0000, -0.7812],
        [ 0.3711,  0.9766, -1.1797, -0.7578,  0.4512]], device='cuda:0',
       dtype=torch.bfloat16, grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None), SequenceClassifierOutput(loss=tensor(0.5861, device='cuda:0', grad_fn=<NllLossBackward0>), logits=tensor([[ 3.2031e-01,  1.5391e+00, -1.0234e+00, -1.1094e+00, -7.3828e-01],
        [-1.7578e-01,  1.5859e+00, -5.8594e-01, -7.7344e-01, -4.9805e-01],
        [-1.1182e-01,  1.8125e+00, -7.7734e-01, -1.0000e+00, -6.2891e-01],
        [ 5.2246e-02,  1.8047e+00, -9.2578e-01, -9.9609e-01, -7.1484e-01],
        [ 2.5000e-01,  1.5234e+00, -9.1406e-01, -9.2188e-01, -7.1484e-01],
        [ 3.0664e-01,  1.2109e+00, -1.3594e+00, -8.2812e-01, -8.2422e-01],
        [-3.8086e-02,  1.4375e+00, -3.6719e-01, -6.7969e-01, -2.3828e-01],
        [ 2.1289e-01,  1.8750e+00, -7.5391e-01, -6.6016e-01, -4.8828e-01],
        [ 2.8711e-01,  1.4141e+00, -6.5625e-01, -5.9375e-01, -4.5117e-01],
        [ 1.2695e-01,  1.6641e+00, -7.3828e-01, -8.3203e-01, -5.6250e-01],
        [ 4.1602e-01,  1.2656e+00, -6.5234e-01, -9.7266e-01, -6.6406e-01],
        [ 3.7891e-01,  1.3594e+00, -8.9453e-01, -8.2422e-01, -2.8711e-01],
        [-7.8964e-04,  1.9453e+00, -5.5469e-01, -7.9297e-01, -6.0156e-01],
        [ 1.9336e-01,  1.5078e+00, -5.5859e-01, -1.0859e+00, -8.9844e-01],
        [ 8.8379e-02,  1.7188e+00, -8.1250e-01, -7.6953e-01, -6.3281e-01],
        [ 2.2949e-01,  1.3516e+00, -7.1484e-01, -1.0391e+00, -4.8633e-01]],
       device='cuda:0', dtype=torch.bfloat16, grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None), SequenceClassifierOutput(loss=tensor(0.6665, device='cuda:0', grad_fn=<NllLossBackward0>), logits=tensor([[-0.0461,  1.6797, -0.6836, -1.0234, -0.5938],
        [ 0.3750,  1.6562, -0.5273, -0.8242, -0.6367],
        [ 0.2002,  2.1094, -0.7461, -1.1094, -0.6172],
        [-0.0209,  1.6250, -0.5352, -1.1328, -0.7070],
        [-0.1348,  1.5938, -0.8555, -0.7891, -0.5898],
        [-0.0505,  1.6875, -0.8438, -0.9453, -0.6406],
        [-0.0588,  1.8281, -0.6641, -1.1016, -0.6758],
        [ 0.0310,  1.7578, -0.8867, -0.9883, -0.6016],
        [-0.0518,  1.1328, -0.7812, -1.0391,  0.1885],
        [ 0.0679,  1.6172, -0.8125, -0.9688, -0.7070],
        [ 0.0918,  1.9375, -0.5508, -1.2578, -0.6406],
        [-0.0527,  1.4453, -0.6719, -0.6836, -0.9805],
        [ 0.1787,  1.6172, -0.8672, -1.0781, -0.7617],
        [-0.0806,  1.4922, -0.3496, -1.0000, -0.2988],
        [ 0.0289,  1.3359, -0.7070, -0.9844, -0.8594],
        [ 0.4902,  1.6641, -0.6562, -0.9453, -0.5977]], device='cuda:0',
       dtype=torch.bfloat16, grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None), SequenceClassifierOutput(loss=tensor(0.8667, device='cuda:0', grad_fn=<NllLossBackward0>), logits=tensor([[ 0.1465,  1.4219, -0.7344, -1.2734, -0.4922],
        [ 0.0223,  1.5859, -0.8086, -1.0547, -0.6250],
        [ 0.1270,  1.5781, -0.7461, -1.0156, -0.6094],
        [ 0.0166,  1.5781, -0.6172, -0.5781, -0.5156],
        [ 0.1206,  1.7500, -1.1172, -0.8164, -0.6172],
        [ 0.0157,  1.5625, -0.5938, -0.8867, -0.5469],
        [-0.0150,  1.4688, -0.9102, -0.6406, -0.6523],
        [ 0.0820,  1.2500, -0.5195, -0.9492, -0.6602],
        [-0.0255,  1.9766, -0.7656, -0.7891, -0.5586],
        [-0.0801,  1.8047, -0.9883, -0.9336, -0.4707],
        [ 0.0449,  1.5156, -0.7930, -1.3047, -0.5625],
        [ 0.2617,  1.4062, -0.5781, -0.8281, -0.4609],
        [ 0.0693,  1.1250, -0.6367, -0.9219, -0.8789],
        [-0.0486,  1.6406, -0.7812, -1.1328, -0.7734],
        [-0.0052,  1.4375, -0.7812, -0.7070, -0.2969],
        [-0.0066,  1.4062, -0.6953, -1.0078, -0.4219]], device='cuda:0',
       dtype=torch.bfloat16, grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None), SequenceClassifierOutput(loss=tensor(0.9382, device='cuda:0', grad_fn=<NllLossBackward0>), logits=tensor([[ 0.3301,  1.6094, -0.5391, -0.8672, -0.7578],
        [-0.1582,  1.7812, -0.4199, -0.9961, -0.5352],
        [ 0.1387,  1.6016, -0.8945, -0.9414, -0.5781],
        [-0.1328,  1.1875, -0.6562, -0.6602, -0.6680],
        [ 0.3223,  1.6641, -0.6953, -0.6875, -0.5391],
        [-0.0942,  1.7188, -0.6289, -0.9414, -0.6172],
        [ 0.0825,  1.2812, -0.8164, -0.6758, -0.6328],
        [-0.0366,  1.5234, -0.5039, -0.9023, -0.6445],
        [ 0.1250,  1.5156, -0.7305, -0.7617, -0.6680],
        [ 0.2891,  1.2656, -0.6992, -0.9922, -0.7773],
        [ 0.3770,  1.2656, -0.6367, -0.9336, -0.2471],
        [-0.0064,  1.5078, -0.6250, -1.0938, -0.6758],
        [ 0.3145,  1.5625, -0.8789, -1.0547, -0.4473],
        [-0.2031,  1.3750, -0.6445, -0.6211, -0.4219],
        [ 0.1187,  1.5234, -0.8438, -1.0469, -0.3691],
        [ 0.0469,  1.6484, -0.4199, -0.8125, -0.3008]], device='cuda:0',
       dtype=torch.bfloat16, grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None), SequenceClassifierOutput(loss=tensor(0.6599, device='cuda:0', grad_fn=<NllLossBackward0>), logits=tensor([[-0.2910,  1.9531, -0.8984, -1.1250, -0.4961],
        [-0.0452,  1.5156, -0.6719, -0.6992, -0.7539],
        [ 0.0366,  1.5859, -0.6719, -1.0625, -0.5234],
        [ 0.0737,  1.5312, -1.0625, -0.9922, -0.4512],
        [-0.1436,  2.1250, -0.3496, -0.7578, -0.4141],
        [ 0.3359,  1.2891, -0.7539, -0.9609, -0.3965],
        [-0.0820,  1.8047, -0.5742, -1.2500, -0.3379],
        [-0.0417,  1.6328, -0.6055, -0.8555, -0.4180],
        [ 0.0199,  1.2578, -0.3594, -1.3750, -0.6602],
        [ 0.1050,  1.6172, -0.6484, -0.8242, -0.8047],
        [ 0.4102,  1.3281, -0.9375, -0.9883, -0.7109],
        [-0.0089,  1.6953, -0.3262, -1.0078, -0.7695],
        [-0.1787,  1.9219, -0.6094, -0.6641, -0.5898],
        [ 0.1533,  1.5703, -0.8945, -0.7617, -0.6484],
        [ 0.0835,  1.7031, -0.5625, -1.3359, -0.4414],
        [-0.0258,  1.8125, -0.5859, -1.1016, -0.5156]], device='cuda:0',
       dtype=torch.bfloat16, grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None), SequenceClassifierOutput(loss=tensor(0.6365, device='cuda:0', grad_fn=<NllLossBackward0>), logits=tensor([[-0.3242,  1.7188, -0.7344, -0.6641, -0.9219],
        [ 0.0674,  1.4141, -0.8594, -1.1250, -0.3535],
        [ 0.1338,  1.4375, -1.2969, -1.0938, -0.5781],
        [ 0.3145,  1.7422, -0.7891, -1.2344, -0.5781],
        [-0.1064,  1.5469, -0.6602, -1.1484, -0.7344],
        [ 0.1680,  1.6172, -0.5547, -0.5039, -0.1235],
        [ 0.4316,  1.1328, -1.5156, -0.8359, -0.2988],
        [-0.3301,  1.5469, -0.4238, -0.8477, -0.5273],
        [ 0.0137,  1.6172, -0.7812, -1.2266, -0.5781],
        [ 0.0210,  1.5312, -0.7891, -0.9570, -0.6758],
        [ 0.2383,  1.5547, -0.7539, -0.6914, -0.8711],
        [ 0.3574,  1.7734, -0.6719, -0.8477, -0.6211],
        [-0.0510,  1.6797, -0.5430, -0.6680, -0.7070],
        [ 0.0825,  1.3438, -0.8750, -0.5547, -0.6641],
        [ 0.1021,  1.6562, -1.2891, -0.9492, -0.8594],
        [-0.1357,  1.8281, -0.5312, -0.8867, -0.4980]], device='cuda:0',
       dtype=torch.bfloat16, grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None), SequenceClassifierOutput(loss=tensor(0.7113, device='cuda:0', grad_fn=<NllLossBackward0>), logits=tensor([[ 0.0894,  1.8438, -1.0312, -0.9414, -0.3281],
        [-0.1021,  1.6484, -0.5508, -1.1875, -0.5352],
        [-0.0253,  1.6797, -0.7812, -0.6641, -0.5547],
        [-0.0598,  1.5859, -0.5430, -1.1562, -0.7734],
        [-0.1279,  1.4375, -0.5938, -0.9922, -0.4766],
        [ 0.1689,  1.7969, -0.8516, -0.9453, -0.6992],
        [-0.1318,  1.9297, -0.6094, -0.9961, -0.5625],
        [ 0.1572,  1.2578, -0.7227, -0.9648, -0.5156],
        [ 0.1060,  1.8906, -0.5703, -0.9609, -0.6562],
        [ 0.2207,  1.5469, -0.8477, -0.8125, -0.5352],
        [-0.1079,  1.4844, -0.5977, -0.9844, -0.3848],
        [-0.0645,  1.5781, -0.6289, -0.8750, -0.5820],
        [-0.0371,  1.5469, -0.8164, -0.6172, -0.6328],
        [ 0.5078,  1.4922, -0.6602, -1.1484, -0.8008],
        [ 0.2227,  1.7188, -0.7305, -0.7266, -0.2832],
        [ 0.0155,  2.0312, -1.0234, -1.0391, -0.5703]], device='cuda:0',
       dtype=torch.bfloat16, grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None), SequenceClassifierOutput(loss=tensor(0.9260, device='cuda:0', grad_fn=<NllLossBackward0>), logits=tensor([[ 0.1875,  0.8984, -0.7148, -0.6250, -0.3242],
        [-0.0435,  1.3438, -0.7188, -0.9922, -0.4805],
        [ 0.2061,  1.2266, -1.1719, -0.7344,  0.2500],
        [ 0.0933,  1.8125, -0.7344, -1.0234, -0.7500],
        [ 0.0742,  1.9141, -0.9375, -1.0938, -0.6094],
        [ 0.2988,  1.8516, -0.7461, -0.8086, -0.6406],
        [ 0.0554,  1.6328, -0.7578, -1.1328, -0.7969],
        [-0.1348,  1.2188, -1.0156, -1.0781, -0.8516],
        [ 0.1348,  1.6953, -0.5117, -1.1016, -0.7148],
        [ 0.0491,  1.4297, -1.1016, -0.8047, -0.3809],
        [ 0.0786,  1.6406, -0.6094, -0.9297, -0.6445],
        [ 0.0361,  1.5625, -0.9609, -1.0391, -0.7266],
        [-0.0369,  1.4219, -0.9258, -0.8945, -0.3574],
        [ 0.1943,  1.5703, -0.9375, -0.7422, -0.2695],
        [ 0.1689,  1.9297, -0.8906, -1.1250, -0.6367],
        [ 0.1885,  1.7891, -0.9141, -1.0938, -0.5391]], device='cuda:0',
       dtype=torch.bfloat16, grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None), SequenceClassifierOutput(loss=tensor(0.6731, device='cuda:0', grad_fn=<NllLossBackward0>), logits=tensor([[ 0.3066,  1.8516, -0.6836, -0.9688, -0.9180],
        [ 0.3965,  1.4609, -0.7773, -0.7188, -0.4688],
        [ 0.0574,  1.4297, -1.1016, -0.8516, -0.4512],
        [-0.1187,  1.8516, -0.6055, -1.0938, -0.4297],
        [ 0.2051,  1.2734, -0.8789, -0.8594, -0.4805],
        [-0.0121,  1.5781, -0.6211, -1.3047, -0.7578],
        [ 0.0293,  1.5391, -0.8086, -0.9766, -0.3047],
        [-0.0214,  1.7656, -0.7188, -0.9805, -0.7969],
        [ 0.0500,  1.6406, -0.9023, -0.6953, -0.5430],
        [-0.2373,  1.7109, -0.4609, -0.7383, -0.6328],
        [-0.1748,  1.6328, -0.9883, -1.2188, -0.5586],
        [ 0.1079,  1.1641, -0.2793, -1.0234, -0.8125],
        [-0.1826,  0.9297, -0.2539, -0.5586, -0.6289],
        [-0.2207,  1.8281, -0.8125, -0.7812, -0.1387],
        [ 0.2910,  1.1562, -0.9023, -1.0391, -0.8086],
        [ 0.1523,  1.3984, -0.8477, -1.0156, -0.7344]], device='cuda:0',
       dtype=torch.bfloat16, grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None), SequenceClassifierOutput(loss=tensor(0.8723, device='cuda:0', grad_fn=<NllLossBackward0>), logits=tensor([[ 0.5391,  1.5391, -0.7461, -0.6953, -0.2949],
        [ 0.1641,  1.7422, -0.6914, -1.0625, -0.7070],
        [-0.0325,  1.5781, -0.7305, -1.0781, -0.3789],
        [ 0.2207,  1.5391, -0.4648, -0.6914, -0.7812],
        [-0.0047,  1.6016, -0.8164, -0.7969, -0.3750],
        [ 0.1562,  1.3125, -1.0547, -0.6016, -0.7695],
        [-0.0613,  1.6016, -0.7188, -0.7969, -0.6484],
        [-0.0718,  1.6094, -0.7227, -1.1719, -0.5586],
        [-0.1895,  1.5156, -0.6133, -0.7539, -0.4883],
        [-0.2314,  1.5859, -0.5742, -0.9922, -0.8008],
        [ 0.0284,  1.8359, -0.7188, -0.8711, -0.7812],
        [ 0.2891,  0.6914, -0.9922, -0.6953, -0.3320],
        [-0.1211,  1.8359, -0.5469, -0.8672, -0.7031],
        [ 0.1963,  1.5000, -0.8242, -0.9922, -0.2949],
        [ 0.2041,  1.8125, -0.8789, -0.8320, -0.5820],
        [-0.4199,  1.7500, -0.3867, -0.7617, -0.6094]], device='cuda:0',
       dtype=torch.bfloat16, grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None), SequenceClassifierOutput(loss=tensor(0.4965, device='cuda:0', grad_fn=<NllLossBackward0>), logits=tensor([[ 0.1670,  1.0547, -0.7930, -0.7031, -0.5703],
        [ 0.1196,  1.5781, -0.8281, -0.9336, -0.7578],
        [ 0.5703,  1.2500, -0.9688, -0.9922, -0.6992],
        [-0.1709,  1.4531, -0.7734, -0.8984, -0.5234],
        [-0.2520,  1.8047, -0.5156, -0.9336, -1.0469],
        [-0.3418,  1.5703, -0.7500, -0.6836, -0.6133],
        [ 0.0322,  1.3984, -0.6484, -0.8906, -0.4414],
        [ 0.2314,  1.5859, -0.5352, -0.7070, -0.4883],
        [ 0.1157,  1.3828, -0.8711, -1.0938, -0.4023],
        [-0.0200,  1.5234, -0.8203, -0.6953, -0.4844],
        [ 0.0352,  1.7344, -1.2344, -1.0391, -0.6523],
        [-0.1836,  1.4453, -0.6406, -0.5703, -0.5195],
        [ 0.0593,  1.5938, -0.6289, -0.9023, -0.8320],
        [ 0.0310,  1.8594, -0.8281, -0.8984, -0.5508],
        [-0.1309,  1.5469, -0.8281, -0.8789, -0.4492],
        [ 0.0903,  1.5625, -0.6172, -0.8164, -0.8125]], device='cuda:0',
       dtype=torch.bfloat16, grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None), SequenceClassifierOutput(loss=tensor(0.4752, device='cuda:0', grad_fn=<NllLossBackward0>), logits=tensor([[-0.1279,  1.8594, -0.5703, -0.8398, -0.4902],
        [-0.0164,  1.8047, -0.6055, -0.9453, -0.8828],
        [-0.0400,  1.5312, -0.8320, -0.5664, -0.3379],
        [ 0.3867,  1.9453, -0.8281, -1.1250, -0.6992],
        [ 0.3223,  1.8672, -0.7578, -0.6289, -0.3105],
        [ 0.0165,  1.4062, -0.8398, -0.7305, -0.5234],
        [ 0.1602,  1.6562, -0.9492, -0.8555, -0.4219],
        [ 0.1089,  1.4609, -1.1016, -0.6836, -0.3379],
        [ 0.1396,  1.6094, -0.8945, -0.7344, -0.6562],
        [ 0.1377,  1.7812, -0.8164, -1.0781, -0.5156],
        [-0.0388,  1.5078, -0.6289, -0.8047, -0.5156],
        [-0.3457,  1.6094, -0.6953, -1.0234, -0.7109],
        [-0.0396,  1.7734, -0.5391, -1.0547, -0.6758],
        [ 0.2422,  1.5469, -0.7305, -0.8477, -0.7852],
        [-0.1196,  1.9453, -0.4785, -0.7148, -0.4668],
        [-0.0503,  1.5703, -0.7227, -1.1484, -0.7188]], device='cuda:0',
       dtype=torch.bfloat16, grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None), SequenceClassifierOutput(loss=tensor(0.5958, device='cuda:0', grad_fn=<NllLossBackward0>), logits=tensor([[-0.1108,  1.6406, -0.8477, -0.9414, -0.4473],
        [ 0.0069,  1.8828, -0.8867, -0.9844, -0.4668],
        [ 0.5117,  1.1484, -0.8828, -0.6992, -0.6523],
        [ 0.3887,  1.2422, -0.5234, -0.4258, -0.2949],
        [ 0.1206,  1.7891, -0.6680, -0.7383, -0.6836],
        [ 0.2891,  1.2188, -0.4492, -0.8047, -0.5977],
        [-0.0148,  1.8906, -0.6953, -0.9414, -0.6406],
        [-0.0879,  1.6562, -1.0234, -0.7383, -0.6172],
        [ 0.1104,  1.9297, -0.5547, -1.2812, -0.5469],
        [-0.1001,  1.9688, -0.8281, -0.9805, -0.6836],
        [ 0.1055,  1.5312, -0.9414, -0.8789, -0.4727],
        [ 0.0796,  1.4922, -0.7344, -1.1250, -0.5156],
        [-0.1904,  1.1328, -0.3457, -0.7266, -0.6953],
        [ 0.0566,  1.7422, -0.4043, -0.8242, -0.8203],
        [-0.0510,  1.6641, -0.8945, -0.8008, -0.4512],
        [-0.1758,  1.3125, -0.4453, -0.4023, -0.3848]], device='cuda:0',
       dtype=torch.bfloat16, grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None), SequenceClassifierOutput(loss=tensor(0.5052, device='cuda:0', grad_fn=<NllLossBackward0>), logits=tensor([[-0.1367,  1.7812, -1.1250, -0.9609, -0.5195],
        [ 0.3672,  1.2344, -0.8242, -0.8711, -0.2734],
        [ 0.2793,  1.5625, -0.4766, -1.2109, -0.4395],
        [ 0.1973,  1.6250, -0.7969, -0.9531, -0.5234],
        [-0.4219,  1.4453, -0.6680, -0.6680, -0.6992],
        [-0.1416,  1.2812, -0.4746, -0.8633, -0.3945],
        [ 0.2295,  1.6406, -0.6406, -0.7422, -0.8828],
        [-0.1963,  1.5547, -0.6758, -0.7344, -0.6289],
        [ 0.1211,  1.3438, -0.2373, -0.8203, -0.6836],
        [-0.0498,  1.6094, -0.6758, -1.0625, -0.4668],
        [ 0.1133,  1.6641, -0.6094, -0.8984, -0.6445],
        [-0.0947,  1.6172, -0.8750, -1.0156, -0.5430],
        [ 0.3770,  1.4375, -0.5430, -0.6016, -0.5430],
        [ 0.2002,  1.2812, -0.9023, -0.7266, -0.4883],
        [-0.1973,  1.7578, -0.7656, -1.0312, -0.3105],
        [ 0.1338,  1.7344, -0.7344, -0.7539, -0.3828]], device='cuda:0',
       dtype=torch.bfloat16, grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None), SequenceClassifierOutput(loss=tensor(1.0227, device='cuda:0', grad_fn=<NllLossBackward0>), logits=tensor([[ 0.1069,  1.7578, -0.3711, -1.1562, -0.9102],
        [ 0.3477,  1.4297, -0.7734, -0.6523, -0.5234],
        [ 0.0344,  1.7578, -1.2266, -0.9688, -0.1670],
        [-0.3477,  1.5469, -0.8828, -0.8516, -0.4805],
        [-0.0195,  1.8125, -0.4688, -0.9219, -0.4980],
        [ 0.0287,  1.5859, -0.4316, -0.6211, -0.5938],
        [ 0.4141,  0.7969, -0.9570, -0.5195,  0.2168],
        [-0.3613,  1.6484, -0.3516, -0.6406, -0.6836],
        [ 0.2812,  1.5938, -0.8359, -0.7344, -0.4707],
        [-0.1064,  1.3984, -0.5195, -0.9258, -0.4316],
        [ 0.0708,  1.5078, -0.7305, -1.1641, -0.9180],
        [-0.0977,  1.8984, -0.6758, -0.7461, -0.4980],
        [ 0.1572,  1.5625, -0.9375, -1.0078, -0.4238],
        [ 0.3770,  1.5625, -1.3047, -1.0078, -0.5469],
        [-0.1826,  1.3281, -0.4219, -1.0859, -0.7812],
        [ 0.1914,  1.7031, -0.8203, -0.5977, -0.8594]], device='cuda:0',
       dtype=torch.bfloat16, grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None), SequenceClassifierOutput(loss=tensor(0.8691, device='cuda:0', grad_fn=<NllLossBackward0>), logits=tensor([[-0.1855,  1.6172, -0.6758, -1.0312, -0.3164],
        [ 0.0420,  1.3750, -0.4336, -0.9219, -0.7070],
        [ 0.1147,  1.5234, -1.0469, -1.3281, -0.4824],
        [ 0.2520,  1.2891, -0.4980, -0.7617, -0.8789],
        [ 0.3008,  1.3516, -0.7617, -1.1016, -0.9531],
        [ 0.3184,  0.9492, -1.0469, -0.6562, -0.6289],
        [ 0.0698,  1.2656, -0.8008, -0.9375, -0.4531],
        [ 0.1992,  1.2500, -0.5664, -0.9531, -0.6562],
        [ 0.0160,  1.6875, -0.7383, -1.1094, -0.9219],
        [ 0.2539,  1.4531, -0.8555, -1.1484, -0.4512],
        [-0.1523,  1.6172, -0.8945, -0.7773, -0.3066],
        [-0.0154,  0.8359, -1.2344, -0.8789, -0.4219],
        [-0.0542,  1.8125, -0.6641, -0.8281, -0.5430],
        [ 0.0099,  2.0000, -0.7305, -0.8477, -0.5391],
        [-0.1377,  1.4375, -0.5195, -0.8086, -0.4375],
        [ 0.1348,  1.6406, -0.4668, -0.8242, -0.8125]], device='cuda:0',
       dtype=torch.bfloat16, grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None), SequenceClassifierOutput(loss=tensor(0.9325, device='cuda:0', grad_fn=<NllLossBackward0>), logits=tensor([[-0.1348,  1.4453, -0.7812, -0.6992, -0.1514],
        [ 0.1250,  1.6250, -0.4062, -0.8750, -0.8320],
        [ 0.0371,  1.3281, -0.2266, -1.0703, -1.0234],
        [-0.0537,  1.8438, -0.5742, -0.6836, -0.5586],
        [ 0.2227,  1.6094, -0.3945, -0.9961, -0.5859],
        [ 0.3105,  1.3828, -0.6953, -0.8984, -0.2871],
        [ 0.2461,  0.8242, -0.5938, -0.7852, -0.1562],
        [ 0.2070,  1.7500, -0.7578, -1.2500, -0.8867],
        [-0.0214,  1.3516, -0.8359, -0.7578, -0.6758],
        [ 0.0835,  1.5703, -0.5352, -0.9023, -0.5273],
        [ 0.0576,  1.1484, -0.4863, -0.7305, -0.5977],
        [ 0.2334,  1.4922, -0.6484, -0.9375, -0.4648],
        [-0.2490,  1.5625, -0.6680, -0.8906, -0.5078],
        [-0.0796,  1.7344, -0.8789, -1.1797, -0.7227],
        [ 0.2100,  1.5859, -0.7734, -1.0156, -0.2490],
        [ 0.2139,  1.7109, -0.5898, -0.7656, -0.5195]], device='cuda:0',
       dtype=torch.bfloat16, grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None), SequenceClassifierOutput(loss=tensor(0.8445, device='cuda:0', grad_fn=<NllLossBackward0>), logits=tensor([[-0.2832,  1.5859, -0.6562, -0.8672, -0.4297],
        [ 0.1855,  1.8125, -1.0625, -1.0625, -0.6562],
        [ 0.1270,  1.2188, -0.8086, -0.9062, -0.8828],
        [-0.0315,  1.2969, -1.0000, -1.0234, -0.6680],
        [ 0.5352,  0.4258, -0.8828, -0.3027,  0.3828],
        [ 0.2617,  1.3594, -0.9609, -0.8164, -0.6875],
        [ 0.0518,  1.6094, -0.9062, -1.0703, -0.6211],
        [-0.0491,  1.7422, -0.4766, -1.0000, -0.6367],
        [ 0.1943,  1.5312, -0.5508, -0.7500, -0.7109],
        [ 0.0674,  1.7109, -0.5117, -1.2500, -0.7188],
        [ 0.1279,  1.6875, -0.6523, -0.6797, -0.4688],
        [ 0.1279,  1.3906, -0.7188, -1.0938, -0.3418],
        [ 0.1826,  1.5547, -1.0703, -1.2109, -0.6094],
        [-0.2236,  1.3516, -0.4062, -0.8945, -0.6836],
        [ 0.0728,  1.6484, -0.9180, -0.6523, -0.6641],
        [ 0.2461,  1.3594, -0.6914, -0.9102, -0.9531]], device='cuda:0',
       dtype=torch.bfloat16, grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None), SequenceClassifierOutput(loss=tensor(0.6417, device='cuda:0', grad_fn=<NllLossBackward0>), logits=tensor([[ 0.2578,  1.4453, -0.7461, -0.8125, -0.6250],
        [ 0.1406,  1.9219, -0.6992, -0.8047, -0.5938],
        [ 0.2256,  1.5938, -0.9336, -0.9531, -0.4062],
        [ 0.0762,  1.3750, -0.5586, -0.9844, -0.5430],
        [ 0.0085,  1.5859, -0.6094, -0.7031, -0.6914],
        [ 0.1079,  1.6484, -1.0938, -0.9375, -0.4277],
        [-0.0198,  1.4141, -0.5469, -0.9414, -0.4766],
        [-0.0157,  1.4688, -0.5312, -0.6367, -0.3555],
        [-0.0825,  1.5078, -0.4336, -0.9062, -0.4590],
        [-0.2695,  1.3438, -0.8320, -0.7773, -0.7227],
        [-0.2285,  2.0469, -0.6797, -1.1328, -0.5625],
        [ 0.0322,  1.6719, -0.6992, -0.9023, -0.7500],
        [ 0.1221,  1.4609, -0.8086, -0.8008, -0.5625],
        [ 0.3516,  1.8906, -0.6914, -0.8906, -0.8594],
        [ 0.1406,  1.5938, -0.7969, -0.8203, -0.5391],
        [ 0.0698,  1.6250, -0.8867, -0.9258, -0.2871]], device='cuda:0',
       dtype=torch.bfloat16, grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None), SequenceClassifierOutput(loss=tensor(0.8298, device='cuda:0', grad_fn=<NllLossBackward0>), logits=tensor([[-0.1465,  1.7891, -0.6094, -0.7266, -0.5820],
        [ 0.2578,  1.1484, -0.9570, -0.6680, -0.9414],
        [ 0.1245,  1.5547, -1.0625, -1.2188, -0.7188],
        [ 0.3477,  1.3984, -0.9609, -0.8398, -0.5117],
        [ 0.0474,  1.6797, -0.7852, -0.4941, -0.4922],
        [ 0.1084,  1.6328, -0.0620, -0.9805, -0.9062],
        [ 0.1641,  1.5234, -0.8086, -0.7891, -0.1089],
        [ 0.1011,  1.1406, -0.4258, -0.6914, -0.4941],
        [ 0.4180,  1.5312, -0.6445, -0.6211, -0.5156],
        [-0.1406,  1.8906, -0.6875, -0.8008, -0.6953],
        [-0.0121,  1.7266, -0.8906, -0.8945, -0.4688],
        [-0.3262,  1.7031, -0.8281, -0.9258, -0.7969],
        [ 0.2363,  1.1328, -0.7891, -0.7422, -0.7344],
        [ 0.0271,  1.6641, -0.8750, -0.9883, -0.5508],
        [ 0.4590,  1.0312, -1.4141, -1.1719, -0.2266],
        [ 0.0806,  1.1250, -1.0469, -0.8203, -0.7188]], device='cuda:0',
       dtype=torch.bfloat16, grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None), SequenceClassifierOutput(loss=tensor(0.5447, device='cuda:0', grad_fn=<NllLossBackward0>), logits=tensor([[-0.0884,  1.7578, -0.7266, -1.0938, -0.7422],
        [ 0.2578,  1.7109, -0.9062, -0.9180, -0.6992],
        [ 0.1514,  1.4141, -1.0469, -1.1797, -0.4160],
        [-0.2041,  1.8984, -0.7031, -1.0625, -0.5391],
        [ 0.2598,  1.5469, -0.7695, -1.2344, -0.4434],
        [ 0.0141,  1.4609, -0.8906, -1.3906, -0.4473],
        [ 0.0525,  1.9219, -0.7031, -0.8672, -0.7266],
        [ 0.0361,  1.4453, -0.6953, -1.1094, -0.6602],
        [ 0.0072,  1.7578, -0.4492, -0.9609, -0.8047],
        [-0.2334,  1.5703, -1.0391, -1.1328, -0.4629],
        [ 0.2910,  1.9688, -0.8398, -1.1484, -0.6992],
        [ 0.2266,  1.5703, -0.6016, -0.7617, -0.5625],
        [ 0.3711,  1.3203, -0.8828, -0.8242, -0.8164],
        [-0.0640,  1.6406, -0.7539, -1.1797, -0.6641],
        [-0.0045,  1.7812, -0.9766, -1.2031, -0.7773],
        [ 0.0542,  2.0469, -0.9297, -1.2109, -0.7383]], device='cuda:0',
       dtype=torch.bfloat16, grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None), SequenceClassifierOutput(loss=tensor(0.6721, device='cuda:0', grad_fn=<NllLossBackward0>), logits=tensor([[ 4.1797e-01,  1.6797e+00, -7.3828e-01, -1.0469e+00, -8.3984e-01],
        [-2.3926e-01,  1.6016e+00, -6.9531e-01, -1.0781e+00, -7.2266e-01],
        [ 2.5977e-01,  1.5156e+00, -8.0469e-01, -9.9219e-01, -3.5547e-01],
        [-1.9897e-02,  1.5547e+00, -6.6406e-01, -9.9609e-01, -5.0781e-01],
        [-1.4648e-01,  1.6328e+00, -8.3594e-01, -9.0234e-01, -6.5625e-01],
        [ 4.1602e-01,  1.4844e+00, -7.1094e-01, -1.1875e+00,  1.2360e-03],
        [ 3.5352e-01,  1.6016e+00, -7.5781e-01, -8.9062e-01, -6.2109e-01],
        [ 3.0273e-01,  1.6719e+00, -6.6406e-01, -9.8438e-01, -5.8984e-01],
        [-1.4551e-01,  1.5703e+00, -7.9297e-01, -1.0781e+00, -6.8359e-01],
        [ 2.6953e-01,  1.4062e+00, -6.3672e-01, -6.3281e-01, -6.1719e-01],
        [-4.7607e-02,  1.7109e+00, -6.6016e-01, -8.0469e-01, -6.2500e-01],
        [ 6.8848e-02,  1.4531e+00, -8.3203e-01, -8.1250e-01, -4.8047e-01],
        [ 8.7109e-01,  4.4922e-01, -5.1562e-01, -2.5391e-01,  7.1875e-01],
        [ 2.1851e-02,  1.1641e+00, -2.5586e-01, -7.1875e-01, -8.0859e-01],
        [ 7.2754e-02,  1.7500e+00, -6.1719e-01, -1.2188e+00, -6.9141e-01],
        [-7.2266e-02,  1.7891e+00, -5.6250e-01, -8.3203e-01, -6.6797e-01]],
       device='cuda:0', dtype=torch.bfloat16, grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None), SequenceClassifierOutput(loss=tensor(0.7177, device='cuda:0', grad_fn=<NllLossBackward0>), logits=tensor([[ 0.3340,  1.6094, -0.8984, -1.0703, -0.3730],
        [-0.1494,  1.9453, -0.5898, -1.0312, -0.6680],
        [-0.0261,  1.5781, -0.8633, -1.2500, -0.7695],
        [ 0.0052,  1.6250, -0.7617, -1.0469, -0.6445],
        [-0.0491,  1.2656, -0.9102, -0.8516, -0.6289],
        [-0.1416,  1.4922, -0.7031, -1.1094, -0.4902],
        [ 0.2754,  1.2500, -0.7305, -1.0156, -0.6055],
        [-0.0019,  1.2344, -1.1562, -1.1719, -0.5078],
        [ 0.2852,  1.6250, -0.5039, -1.0156, -0.4902],
        [ 0.2969,  1.5547, -0.6484, -0.8867, -0.7109],
        [ 0.0342,  1.6953, -0.8594, -0.7344, -0.4727],
        [ 0.2734,  1.4141, -0.6719, -1.0156, -0.5703],
        [-0.0188,  1.4219, -0.7617, -1.1484, -0.5000],
        [ 0.1279,  1.1875, -0.5430, -0.7656, -0.4961],
        [-0.3984,  1.5703, -0.6406, -0.9961, -0.6133],
        [ 0.0410,  1.2734, -1.0625, -0.9102, -0.4316]], device='cuda:0',
       dtype=torch.bfloat16, grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None), SequenceClassifierOutput(loss=tensor(1.0872, device='cuda:0', grad_fn=<NllLossBackward0>), logits=tensor([[-0.1553,  1.4688, -0.4082, -0.9531, -0.9609],
        [-0.0105,  1.9375, -1.0469, -0.6055, -0.6055],
        [-0.0500,  1.5547, -0.6211, -1.0781, -0.5234],
        [-0.2041,  1.4844, -0.7422, -0.7617, -0.4570],
        [ 0.1885,  1.6094, -0.7109, -0.9258, -0.6523],
        [-0.2061,  1.5391, -1.0859, -0.7031, -0.6758],
        [-0.0845,  1.5391, -0.7578, -0.7070, -0.5352],
        [ 0.1445,  1.4609, -0.6641, -0.8750, -0.4570],
        [ 0.1104,  1.4922, -0.6289, -0.6914, -0.8359],
        [-0.1021,  1.7188, -0.4316, -0.9453, -0.5000],
        [ 0.1934,  1.5000, -0.7539, -1.0938, -0.4551],
        [ 0.0618,  1.2422, -0.8203, -0.6836, -0.3027],
        [-0.0532,  1.6562, -0.7227, -0.8164, -0.7695],
        [ 0.4023,  1.2109, -0.9922, -0.6836, -0.1797],
        [ 0.0052,  1.5234, -0.6523, -0.5820, -0.5820],
        [-0.1494,  1.5703, -0.8906, -0.4805, -0.3301]], device='cuda:0',
       dtype=torch.bfloat16, grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None), SequenceClassifierOutput(loss=tensor(0.6354, device='cuda:0', grad_fn=<NllLossBackward0>), logits=tensor([[ 0.3711,  1.1797, -0.4902, -0.8789, -0.4941],
        [-0.2402,  1.5156, -0.4668, -0.9570, -0.8164],
        [-0.0425,  1.6953, -0.7148, -1.0859, -0.4961],
        [ 0.3594,  1.4219, -0.7383, -0.7734, -0.5625],
        [-0.0845,  1.4922, -0.3379, -0.5156, -0.5586],
        [ 0.0630,  1.4922, -0.5117, -0.8867, -0.5039],
        [ 0.2070,  1.7109, -0.7227, -0.6914, -0.5625],
        [ 0.2168,  1.3594, -0.1260, -0.7773, -0.8320],
        [ 0.2275,  1.7109, -0.8789, -0.8789, -0.4062],
        [ 0.0042,  1.3906, -0.7500, -1.0391, -0.4844],
        [ 0.2324,  1.8750, -0.8516, -0.8945, -0.5117],
        [ 0.0698,  1.6797, -1.1562, -1.1016, -0.5547],
        [ 0.0576,  1.4297, -0.8086, -0.9727, -0.7969],
        [-0.0037,  1.8047, -0.8203, -0.7109, -0.8438],
        [-0.0322,  1.6172, -0.6484, -0.9961, -0.5625],
        [ 0.3398,  0.3047, -0.9727, -0.5352,  0.3711]], device='cuda:0',
       dtype=torch.bfloat16, grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None), SequenceClassifierOutput(loss=tensor(0.8552, device='cuda:0', grad_fn=<NllLossBackward0>), logits=tensor([[ 0.1299,  1.8047, -0.8555, -0.6914, -0.7227],
        [ 0.0498,  1.9688, -0.8633, -1.0156, -0.2852],
        [ 0.2910,  1.4453, -0.8906, -0.9648, -0.1689],
        [ 0.2520,  1.5703, -1.0312, -0.9297, -0.2363],
        [ 0.3691,  1.3281, -0.5859, -0.7305, -0.7109],
        [ 0.0532,  1.8359, -0.6094, -1.0391, -0.7656],
        [ 0.3770,  1.6562, -0.5312, -0.8633, -0.3672],
        [ 0.4707,  1.3281, -0.6484, -0.8789, -0.9531],
        [-0.1494,  1.6172, -0.4785, -1.0234, -0.7891],
        [-0.1113,  0.9531, -0.9609, -0.5078, -0.4414],
        [ 0.0214,  1.9219, -0.7773, -0.6406, -0.4375],
        [ 0.2295,  1.5781, -0.5391, -0.9688, -0.5117],
        [ 0.1436,  1.6641, -0.5977, -1.0547, -0.8125],
        [-0.2578,  1.7734, -0.9922, -0.8906, -0.5547],
        [ 0.1426,  1.4375, -0.9258, -0.6406, -0.4727],
        [-0.0156,  1.9297, -0.6797, -1.1406, -0.3887]], device='cuda:0',
       dtype=torch.bfloat16, grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None), SequenceClassifierOutput(loss=tensor(0.5797, device='cuda:0', grad_fn=<NllLossBackward0>), logits=tensor([[ 0.1953,  1.3047, -1.1641, -0.9297, -0.1865],
        [-0.0102,  1.6406, -0.6289, -0.9766, -0.7812],
        [ 0.0566,  1.6016, -0.7812, -0.9805, -0.5820],
        [ 0.0483,  1.6719, -0.6992, -0.9805, -0.6680],
        [-0.0452,  1.8516, -0.8633, -1.3516, -0.3203],
        [-0.0618,  1.6562, -0.6562, -0.8359, -0.6055],
        [ 0.0349,  1.5312, -1.1172, -0.7383, -0.4082],
        [ 0.3359,  1.0859, -0.4922, -0.8516, -0.7031],
        [ 0.4844,  1.9766, -0.7812, -0.9297, -0.4258],
        [ 0.0520,  1.4062, -0.7539, -0.9297, -0.6094],
        [ 0.0630,  1.4688, -0.7969, -0.9102, -0.4629],
        [ 0.2021,  1.4766, -1.0312, -1.0469, -0.2090],
        [-0.0718,  1.7734, -0.5078, -1.0234, -0.6484],
        [ 0.1826,  1.2734, -0.6641, -1.0625, -0.8711],
        [-0.0996,  1.7266, -0.7344, -0.9492, -0.5586],
        [ 0.2422,  1.4922, -0.9570, -0.5234, -0.4590]], device='cuda:0',
       dtype=torch.bfloat16, grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None), SequenceClassifierOutput(loss=tensor(0.7152, device='cuda:0', grad_fn=<NllLossBackward0>), logits=tensor([[ 2.4512e-01,  1.5469e+00, -8.1641e-01, -1.0625e+00, -3.9844e-01],
        [-2.6489e-02,  1.2578e+00, -6.4062e-01, -9.3750e-01, -8.3203e-01],
        [ 6.7871e-02,  1.4688e+00, -9.0625e-01, -7.5000e-01, -2.1191e-01],
        [-5.7068e-03,  1.8359e+00, -6.6406e-01, -9.3359e-01, -3.9648e-01],
        [ 6.4941e-02,  1.5391e+00, -8.8672e-01, -1.0312e+00, -6.6797e-01],
        [ 1.8750e-01,  1.5234e+00, -1.0625e+00, -5.0000e-01, -5.1953e-01],
        [ 7.5195e-02,  1.6562e+00, -9.1406e-01, -1.0469e+00, -7.2656e-01],
        [ 1.5039e-01,  1.6406e+00, -6.7969e-01, -1.2188e+00, -5.6250e-01],
        [ 1.9434e-01,  1.3125e+00, -6.2891e-01, -7.7734e-01, -5.7812e-01],
        [ 7.6660e-02,  1.7656e+00, -6.7188e-01, -9.4141e-01, -4.0820e-01],
        [-8.1055e-02,  1.7422e+00, -8.2422e-01, -1.1172e+00, -6.1719e-01],
        [-1.2256e-01,  1.8047e+00, -6.9141e-01, -6.1719e-01, -4.8242e-01],
        [-2.1777e-01,  1.3047e+00, -7.8906e-01, -1.1953e+00, -7.2266e-01],
        [ 3.4961e-01,  1.9219e+00, -8.7109e-01, -7.8125e-01, -3.6133e-01],
        [-1.4062e-01,  1.6172e+00, -4.9219e-01, -6.6016e-01, -7.5000e-01],
        [-1.2512e-03,  1.7969e+00, -8.2422e-01, -1.0234e+00, -5.3516e-01]],
       device='cuda:0', dtype=torch.bfloat16, grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None), SequenceClassifierOutput(loss=tensor(0.6545, device='cuda:0', grad_fn=<NllLossBackward0>), logits=tensor([[ 0.3945,  1.5312, -1.0391, -0.9844, -0.6211],
        [-0.0630,  1.5859, -1.0234, -1.1797, -0.6914],
        [-0.1826,  1.4062, -0.6719, -0.8164, -0.8711],
        [-0.0908,  1.3281, -0.9414, -0.7539, -0.6875],
        [-0.1216,  1.3203, -0.8867, -0.8516, -0.6836],
        [ 0.1553,  1.7109, -0.6367, -0.5820, -0.7070],
        [ 0.1768,  1.6016, -0.7305, -0.9766, -0.5898],
        [-0.0991,  1.4766, -0.8711, -0.9492, -0.7695],
        [ 0.5703,  1.3828, -0.6797, -0.8867, -0.6719],
        [ 0.2871,  1.8047, -0.6445, -1.0781, -1.1641],
        [ 0.1895,  2.0781, -0.8594, -0.9023, -0.5273],
        [ 0.0684,  1.2969, -0.5039, -0.7852, -0.8164],
        [ 0.3242,  1.4922, -0.2969, -1.0312, -1.0391],
        [ 0.1357,  1.6484, -0.5664, -0.7891, -0.6016],
        [ 0.2949,  1.4062, -0.7578, -0.9336, -0.3398],
        [-0.1133,  1.6641, -0.4707, -0.7461, -0.6992]], device='cuda:0',
       dtype=torch.bfloat16, grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None), SequenceClassifierOutput(loss=tensor(0.7588, device='cuda:0', grad_fn=<NllLossBackward0>), logits=tensor([[ 0.1514,  1.6016, -0.5352, -1.1328, -0.7539],
        [ 0.0201,  1.8047, -0.8320, -0.9180, -0.7383],
        [ 0.3789,  1.8125, -0.9570, -1.0469, -0.4688],
        [-0.0354,  1.6641, -0.8672, -0.8945, -0.3574],
        [ 0.0479,  1.4609, -1.0156, -1.3047, -0.6445],
        [ 0.3516,  1.3359, -0.6367, -0.8203, -0.5977],
        [ 0.0967,  1.7578, -0.9805, -0.5898, -0.2812],
        [ 0.3145,  1.3672, -1.2578, -1.1641, -0.1494],
        [ 0.0913,  1.9375, -0.4961, -0.8867, -0.6758],
        [ 0.4648,  1.0625, -1.2891, -0.8555, -0.3477],
        [ 0.1309,  1.9297, -0.6133, -0.9844, -0.4258],
        [-0.0654,  1.7266, -0.7734, -1.0391, -0.2676],
        [ 0.2178,  1.3203, -0.5586, -0.8477, -0.3926],
        [-0.2695,  1.9219, -0.7930, -0.6914, -0.4746],
        [ 0.1934,  1.8125, -1.1797, -0.8242, -0.6016],
        [ 0.3223,  1.6250, -0.8633, -1.0703, -0.3789]], device='cuda:0',
       dtype=torch.bfloat16, grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None), SequenceClassifierOutput(loss=tensor(0.7173, device='cuda:0', grad_fn=<NllLossBackward0>), logits=tensor([[ 0.1709,  1.8594, -0.9727, -0.9688, -0.4824],
        [-0.0366,  1.6562, -0.7344, -1.0078, -0.4180],
        [ 0.0981,  1.6562, -0.5781, -0.7812, -0.6289],
        [ 0.0579,  1.8516, -0.6484, -0.6914, -0.4824],
        [ 0.2969,  1.2969, -0.8711, -0.7695, -0.1797],
        [ 0.0461,  1.7109, -0.6641, -0.8516, -0.6055],
        [ 0.1299,  1.4688, -0.7148, -1.0469, -0.3496],
        [ 0.3164,  1.7188, -0.9219, -0.7109, -0.5312],
        [-0.0476,  1.6016, -0.5742, -0.8398, -0.4805],
        [ 0.4609,  1.5625, -0.7969, -0.9531, -0.7148],
        [ 0.2080,  1.5703, -0.9688, -0.9336, -0.8906],
        [ 0.2871,  1.4688, -1.1484, -0.8281, -0.3320],
        [ 0.1348,  1.7734, -0.9062, -1.1719, -0.4062],
        [-0.2949,  1.7734, -0.6133, -0.9023, -0.7227],
        [-0.0938,  1.4922, -0.8594, -0.5195, -0.3652],
        [-0.0269,  1.8984, -0.5742, -0.8672, -0.2500]], device='cuda:0',
       dtype=torch.bfloat16, grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None), SequenceClassifierOutput(loss=tensor(0.7363, device='cuda:0', grad_fn=<NllLossBackward0>), logits=tensor([[ 0.0566,  1.6953, -0.7148, -0.6562, -0.5273],
        [ 0.0620,  1.7422, -0.9727, -1.2109, -0.4766],
        [ 0.1592,  1.2031, -0.4297, -1.2422, -0.8906],
        [ 0.2539,  1.6328, -0.5781, -0.7891, -0.6328],
        [ 0.1562,  1.6641, -0.4258, -1.0938, -0.7344],
        [ 0.4062,  1.3203, -1.0938, -0.9180, -0.6836],
        [ 0.0508,  1.4531, -0.9688, -0.9297, -0.4355]], device='cuda:0',
       dtype=torch.bfloat16, grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)]]
{'best-epoch': 0, 'loss': 0.06803385416666667, 'accuracy': 0.5416666666666666, 'precision': 0.18828976034858388, 'recall': 0.2652384091408482, 'f1': 0.1668115469930352}
OptimizedModule(
  (_orig_mod): BertForSequenceClassification(
    (bert): BertModel(
      (embeddings): BertEmbeddings(
        (word_embeddings): Embedding(32784, 768)
        (position_embeddings): Embedding(512, 768)
        (token_type_embeddings): Embedding(2, 768)
        (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
        (dropout): Dropout(p=0.1, inplace=False)
      )
      (encoder): BertEncoder(
        (layer): ModuleList(
          (0-11): 12 x BertLayer(
            (attention): BertAttention(
              (self): BertSdpaSelfAttention(
                (query): Linear(in_features=768, out_features=768, bias=True)
                (key): Linear(in_features=768, out_features=768, bias=True)
                (value): Linear(in_features=768, out_features=768, bias=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
              (output): BertSelfOutput(
                (dense): Linear(in_features=768, out_features=768, bias=True)
                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
            (intermediate): BertIntermediate(
              (dense): Linear(in_features=768, out_features=3072, bias=True)
              (intermediate_act_fn): GELUActivation()
            )
            (output): BertOutput(
              (dense): Linear(in_features=3072, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
        )
      )
      (pooler): BertPooler(
        (dense): Linear(in_features=768, out_features=768, bias=True)
        (activation): Tanh()
      )
    )
    (dropout): Dropout(p=0.1, inplace=False)
    (classifier): Linear(in_features=768, out_features=5, bias=True)
  )
)
<class 'torch._dynamo.eval_frame.OptimizedModule'>
<class 'transformers.models.bert.modeling_bert.BertForSequenceClassification'>