Some weights of BertForSequenceClassification were not initialized from the model checkpoint at tohoku-nlp/bert-base-japanese-v3 and are newly initialized: ['classifier.bias', 'classifier.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
Error executing job with overrides: ['model_name=tohoku-nlp/bert-base-japanese-v3', 'experiment_type=exclude_mo']
Traceback (most recent call last):
  File "/home/takuma-s/Desktop/discourse_relation_explainability/src/fine_tuning.py", line 366, in main
    input_ids = train_data[i]["input_ids"]
                ~~~~~~~~~~^^^
  File "/home/takuma-s/Desktop/discourse_relation_explainability/src/process_data.py", line 69, in __getitem__
    inputs = self.tokenizer.encode_plus(
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/takuma-s/Desktop/discourse_relation_explainability/.venv/lib/python3.12/site-packages/transformers/tokenization_utils_base.py", line 3127, in encode_plus
    return self._encode_plus(
           ^^^^^^^^^^^^^^^^^^
  File "/home/takuma-s/Desktop/discourse_relation_explainability/.venv/lib/python3.12/site-packages/transformers/tokenization_utils.py", line 788, in _encode_plus
    first_ids = get_input_ids(text)
                ^^^^^^^^^^^^^^^^^^^
  File "/home/takuma-s/Desktop/discourse_relation_explainability/.venv/lib/python3.12/site-packages/transformers/tokenization_utils.py", line 774, in get_input_ids
    raise ValueError(
ValueError: Input None is not valid. Should be a string, a list/tuple of strings or a list/tuple of integers.
Set the environment variable HYDRA_FULL_ERROR=1 for a complete stack trace.
num_added_tokens: 16, added_special_tokens: <NOUN>,<PRONOUN>,<ADJECTIVAL-NOUN>,<PRENOUN-ADJECTIVAL>,<ADVERB>,<CONJUNCTION>,<INTERJECTION>,<VERB>,<ADJECTIVE>,<AUXILIARY-VERB>,<PARTICLE>,<PREFIX>,<SUFFIX>,<SYMBOL>,<AUXILIARY-SYMBOL>,<BLANK>