Some weights of BertForSequenceClassification were not initialized from the model checkpoint at tohoku-nlp/bert-base-japanese-v3 and are newly initialized: ['classifier.bias', 'classifier.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
Error executing job with overrides: ['model_name=tohoku-nlp/bert-base-japanese-v3', 'experiment_type=exclude_koso', 'training.epochs=15']
Traceback (most recent call last):
  File "/home/takuma-s/Desktop/discourse_relation_explainability/src/fine_tuning.py", line 419, in main
    input_ids = train_data[i]["input_ids"]
                ~~~~~~~~~~^^^
  File "/home/takuma-s/Desktop/discourse_relation_explainability/src/process_data.py", line 67, in __getitem__
    text = exclude_koso(item)
           ^^^^^^^^^^^^^^^^^^
  File "/home/takuma-s/Desktop/discourse_relation_explainability/src/process_data.py", line 168, in exclude_koso
    koso_count += item["ordinal_forms"].count("こそ")
    ^^^^^^^^^^
NameError: name 'koso_count' is not defined
Set the environment variable HYDRA_FULL_ERROR=1 for a complete stack trace.
num_added_tokens: 16, added_special_tokens: <NOUN>,<PRONOUN>,<ADJECTIVAL-NOUN>,<PRENOUN-ADJECTIVAL>,<ADVERB>,<CONJUNCTION>,<INTERJECTION>,<VERB>,<ADJECTIVE>,<AUXILIARY-VERB>,<PARTICLE>,<PREFIX>,<SUFFIX>,<SYMBOL>,<AUXILIARY-SYMBOL>,<BLANK>