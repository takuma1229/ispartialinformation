Some weights of BertForSequenceClassification were not initialized from the model checkpoint at tohoku-nlp/bert-base-japanese-v3 and are newly initialized: ['classifier.bias', 'classifier.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
Traceback (most recent call last):
  File "/home/takuma-s/Desktop/discourse_relation_explainability/src/run_multiple_experiments.py", line 145, in <module>
    main()
  File "/home/takuma-s/Desktop/discourse_relation_explainability/src/run_multiple_experiments.py", line 122, in main
    all_label_metrics, by_label_metrics = fine_tuning_main(cfg)
                                          ^^^^^^^^^^^^^^^^^^^^^
  File "/home/takuma-s/Desktop/discourse_relation_explainability/src/fine_tuning.py", line 433, in main
    wandb.log({"negation_position_dict": test_data["negation_position_dict"]})
                                         ~~~~~~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/takuma-s/Desktop/discourse_relation_explainability/src/process_data.py", line 68, in __getitem__
    item: dict = self.data[index]
                 ~~~~~~~~~^^^^^^^
TypeError: list indices must be integers or slices, not str
num_added_tokens: 16, added_special_tokens: <NOUN>,<PRONOUN>,<ADJECTIVAL-NOUN>,<PRENOUN-ADJECTIVAL>,<ADVERB>,<CONJUNCTION>,<INTERJECTION>,<VERB>,<ADJECTIVE>,<AUXILIARY-VERB>,<PARTICLE>,<PREFIX>,<SUFFIX>,<SYMBOL>,<AUXILIARY-SYMBOL>,<BLANK>