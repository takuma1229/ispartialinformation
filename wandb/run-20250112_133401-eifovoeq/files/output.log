Traceback (most recent call last):
  File "/home/takuma-s/Desktop/discourse_relation_explainability/.venv/lib/python3.12/site-packages/transformers/utils/hub.py", line 402, in cached_file
    resolved_file = hf_hub_download(
                    ^^^^^^^^^^^^^^^^
  File "/home/takuma-s/Desktop/discourse_relation_explainability/.venv/lib/python3.12/site-packages/huggingface_hub/utils/_validators.py", line 106, in _inner_fn
    validate_repo_id(arg_value)
  File "/home/takuma-s/Desktop/discourse_relation_explainability/.venv/lib/python3.12/site-packages/huggingface_hub/utils/_validators.py", line 160, in validate_repo_id
    raise HFValidationError(
huggingface_hub.errors.HFValidationError: Repo id must use alphanumeric chars or '-', '_', '.', '--' and '..' are forbidden, '-' and '.' cannot start or end the name, max length is 96: 'model_name=tohoku-nlp/bert-base-japanese-v3'.
The above exception was the direct cause of the following exception:
Traceback (most recent call last):
  File "/home/takuma-s/Desktop/discourse_relation_explainability/src/run_multiple_experiments.py", line 81, in <module>
    main()
  File "/home/takuma-s/Desktop/discourse_relation_explainability/src/run_multiple_experiments.py", line 70, in main
    all_label_metrics, by_label_metrics = fine_tuning_main(cfg)
                                          ^^^^^^^^^^^^^^^^^^^^^
  File "/home/takuma-s/Desktop/discourse_relation_explainability/src/fine_tuning.py", line 409, in main
    tokenizer, model = load_model_and_tokenizer(cfg.model_name, cfg.num_labels)
                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/takuma-s/Desktop/discourse_relation_explainability/src/fine_tuning.py", line 47, in load_model_and_tokenizer
    tokenizer = AutoTokenizer.from_pretrained(model_name, word_tokenizer_type="mecab")
                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/takuma-s/Desktop/discourse_relation_explainability/.venv/lib/python3.12/site-packages/transformers/models/auto/tokenization_auto.py", line 826, in from_pretrained
    tokenizer_config = get_tokenizer_config(pretrained_model_name_or_path, **kwargs)
                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/takuma-s/Desktop/discourse_relation_explainability/.venv/lib/python3.12/site-packages/transformers/models/auto/tokenization_auto.py", line 658, in get_tokenizer_config
    resolved_config_file = cached_file(
                           ^^^^^^^^^^^^
  File "/home/takuma-s/Desktop/discourse_relation_explainability/.venv/lib/python3.12/site-packages/transformers/utils/hub.py", line 466, in cached_file
    raise EnvironmentError(
OSError: Incorrect path_or_model_id: 'model_name=tohoku-nlp/bert-base-japanese-v3'. Please provide either the path to a local folder or the repo_id of a model on the Hub.