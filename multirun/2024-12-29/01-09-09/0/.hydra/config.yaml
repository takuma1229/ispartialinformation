model_name: tohoku-nlp/bert-base-japanese-v3
experiment_type: normal
is_debug: false
num_labels: 5
training:
  batch_size: 8
  epochs: 30
  weight_decay: 0.01
evaluation:
  metric_average: macro
  zero_division: np.nan
