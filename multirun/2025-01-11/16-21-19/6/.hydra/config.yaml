model_name: tohoku-nlp/bert-base-japanese-v3
experiment_type: exclude_content_words
is_debug: true
num_labels: 5
training:
  batch_size: 8
  epochs: 30
  weight_decay: 0.01
  early_stopping_patience: 3
  early_stopping_threshold: 0.001
evaluation:
  metric_average: macro
  zero_division: np.nan
