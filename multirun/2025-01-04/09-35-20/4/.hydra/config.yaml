model_name: tohoku-nlp/bert-base-japanese-v3
experiment_type: exclude_content_words
is_debug: false
num_labels: 5
training:
  batch_size: 8
  epochs: 15
  weight_decay: 0.01
evaluation:
  metric_average: macro
  zero_division: np.nan
